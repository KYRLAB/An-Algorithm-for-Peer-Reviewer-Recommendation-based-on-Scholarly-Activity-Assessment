date,submitter_orcid,submitter_name,submitter_institution,submitter_email,submitter_title_doi,submitter_title,submitter_intro,submitter_author1_name,submitter_author1_institution,submitter_author1_email,submitter_author2_name,submitter_author2_institution,submitter_author2_email,submitter_author3_name,submitter_author3_institution,submitter_author3_email,submitter_author4_name,submitter_author4_institution,submitter_author4_email,submitter_author5_name,submitter_author5_institution,submitter_author5_email,submitter_author6_name,submitter_author6_institution,submitter_author6_email,submitter_author7_name,submitter_author7_institution,submitter_author7_email,submitter_author8_name,submitter_author8_institution,submitter_author8_email,submitter_author9_name,submitter_author9_institution,submitter_author9_email,submitter_author10_name,submitter_author10_institution,submitter_author10_email
20200101,326,Lu Qin,"The Chinese University of Hong Kong, Hong Kong, China",lqin@se.cuhk.edu.hk,,Diversifying Top-K Results,"ABSTRACT This paper proposes a general framework for matching similar subsequences in both time series and string databases. The matching results are pairs of query subsequences and database subsequences. The framework finds all possible pairs of similar subsequences if the distance measure satis- fies the ""consistency"" property, which is a property intro- duced in this paper. We show that most popular distance functions, such as the Euclidean distance, DTW, ERP, the Freche?t distance for time series, and the Hamming distance and Levenshtein distance for strings, are all ""consistent"". We also propose a generic index structure for metric spaces named ""reference net"". The reference net occupies O(n) space, where n is the size of the dataset and is optimized to work well with our framework. The experiments demon- strate the ability of our method to improve retrieval perfor- mance when combined with diverse distance measures. The experiments also illustrate that the reference net scales well in terms of space overhead and query time. 1. INTRODUCTION Sequence databases are used in many real-world applica- tions to store diverse types of information, such as DNA and protein data, wireless sensor observations, music and video streams, and financial data. Similarity-based search in such databases is an important functionality, that allows identi- fying, within large amounts of data, the few sequences that contain useful information for a specific task at hand. For example, identifying the most similar database matches for a query sequence can be useful for classification, forecasting, or retrieval of similar past events. The most straightforward way to compare the similarity between two sequences is to use a global similarity mea- sure, that computes an alignment matching the entire first sequence to the entire second sequence. However, in many scenarios it is desirable to perform subsequence matching, where, given two sequences Q and X, we want to identify pairs of subsequences SQ of Q and SX of X, such that the similarity between SQ and SX is high. When a large database of sequences is available, it is important to be able to identify, given a query Q, an optimally matching pair SQ and SX, where SX can be a subsequence of any database sequence. A well-known example of the need for subsequence match- ing is in comparisons of biological sequences. It is quite possible that two DNA sequences Q and X have a large Levenshtein distance [22] (also known as edit distance) be- tween them (e.g., a distance equal to 90% of the length of the sequences), while nonetheless containing subsequences SQ and SX that match at a very high level of statistical significance. Identifying these optimally matching subse- quences [34] helps biologists reason about the evolutionary relationship between Q and X, and possible similarities of functionality between those two pieces of genetic code. Similarly, subsequence matching can be useful in searching music databases, video databases, or databases of events and activities represented as time series. In all the above cases, while the entire query sequence may not have a good match in the database, there can be highly informative and statistically significant matches between subsequences of the query and subsequences of database sequences. Several methods have been proposed for efficient subse- quence matching in large sequence databases. However, all the proposed techniques are targeted to specific distance or similarity functions, and it is not clear how and when these techniques can be generalized and applied to other dis- tances. Especially, subsequence retrieval methods for string databases are difficult to be used for time-series databases. Furthermore, when a new distance function is proposed, we need to develop new techniques for efficient subsequence matching. In this paper we present a general framework, which can be applied to any arbitrary distance metric, as long as the metric satisfies a specific property that we call ""consistency"". Furthermore, we show that many well-known existing distance functions satisfy consistency. Thus, our framework can deal with both sequence types, i.e., strings and time series, including cases where each element of the sequence is a complex object. The framework in this paper consists of a number of steps: dataset segmentation, query segmentation, range query, can- didate generation, and subsequence retrieval. Brute-force search would require evaluating a total of O (|Q|2 |X|2) pairs of subsequences of Q and X. However our filtering method produces a shortlist of candidates after considering O (|Q| |X|) pairs of segments only. For the case where the distance is a metric, we also present a hierarchical reference 1579 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Articles from this volume were invited to present their results at The 38th International Conference on Very Large Data Bases, August 27th - 31st 2012, Istanbul, Turkey. Proceedings of the VLDB Endowment, Vol. 5, No. 11 Copyright 2012 VLDB Endowment 2150-8097/12/07... $ 10.00. net, a novel generic index structure that can be used within our framework to provide efficient query processing. Overall, this paper makes the following contributions: ? We propose a framework that, compared to alterna- tive methods, makes minimal assumptions about the underlying distance, and thus can be applied to a large variety of distance functions. ? We introduce the notion of âconsistencyâ as an im- portant property for distance measures applied to se- quences. ? We propose an efficient filtering method, which pro- duces a shortlist of candidates by matching only O(|Q| |X|) pairs of subsequences, whereas brute force would match O (|Q|2 |X|2) pairs of subsequences. ? We make this filtering method even faster, by using a generic indexing structure with linear space based on reference nets, that efficiently supports range similar- ity queries. ? Experiments demonstrate the ability of our method to provide good performance when combined with diverse metrics such as the Levenshtein distance for strings, and ERP [8] and the discrete Freche?t distance [11]) for time series. 2. RELATEDWORK Typically, the term ""sequences"" can refer to two different data types: strings and time-series. There has been a lot of work in subsequence retrieval for both time series and string databases. However, in almost all cases, existing methods concentrate on a specific distance function or specific type of queries. Here we review some of the recent works on subse- quence matching. Notice that this review is not exhaustive since the topic has received a lot of attention and a complete survey is beyond the scope of this paper. Time-series databases and efficient similarity retrieval have received a lot of attention in the last two decades. The first method for subsequence similarity retrieval under the Eu- clidean (L2?norm) distance appeared in the seminal paper of Faloutsos et al. [12]. The main idea is to use a sliding win- dow to create smaller sequences of fixed length and then use a dimensionality reduction technique to map each window to a small number of features that are indexed using a spa- tial index (e.g., R?-tree). Improvements of this technique have appeared in [28, 27] that improve both the window- based index construction and the query time using sliding windows on the query and not on the database. However, all these techniques are ap",Lu Qin,"The Chinese University of Hong Kong, Hong Kong, China",lqin@se.cuhk.edu.hk,Jeffrey Xu Yu,"The Chinese University of Hong Kong, Hong Kong, China",yu@se.cuhk.edu.hk,Lijun Chang,"The Chinese University of Hong Kong, Hong Kong, China",ljchang@se.cuhk.edu.hk,,,,,,,,,,,,,,,,,,,,,
20200102,897,Haohan Zhu,Department of Computer Science Boston University,zhu@cs.bu.edu,,A Generic Framework for Efficient and Effective Subsequence Retrieval,"ABSTRACT Top-k query processing finds a list of k results that have largest scores w.r.t the user given query, with the assumption that all the k results are independent to each other. In practice, some of the top-k results returned can be very similar to each other. As a re- sult some of the top-k results returned are redundant. In the lit- erature, diversified top-k search has been studied to return k re- sults that take both score and diversity into consideration. Most existing solutions on diversified top-k search assume that scores of all the search results are given, and some works solve the diver- sity problem on a specific problem and can hardly be extended to general cases. In this paper, we study the diversified top-k search problem. We define a general diversified top-k search problem that only considers the similarity of the search results themselves. We propose a framework, such that most existing solutions for top- k query processing can be extended easily to handle diversified top-k search, by simply applying three new functions, a sufficient stop condition sufficient(), a necessary stop condition necessary(), and an algorithm for diversified top-k search on the current set of generated results, div-search-current(). We propose three new algorithms, namely, div-astar, div-dp, and div-cut to solve the div-search-current() problem. div-astar is an A? based algorithm, div-dp is an algorithm that decomposes the results into components which are searched using div-astar independently and combined using dynamic programming. div-cut further decomposes the cur- rent set of generated results using cut points and combines the re- sults using sophisticated operations. We conducted extensive per- formance studies using two real datasets, enwiki and reuters. Our div-cut algorithm finds the optimal solution for diversified top-k search problem in seconds even for k as large as 2, 000. 1. INTRODUCTION Top-k queries are one of the most fundamental queries used in the IR and database areas. Given a user query, the top-k results of the query are a list of k results that have largest scores/relevances with respect to the user query, under the assumption that all of the k results are independent to each other. In some situations, for a cer- tain top-k query, some of the results returned can be very similar to each other. For example, if we search  íì§¸apple íì§¹ in Google image1, 7 out of the top-10 results returned are the logo of the Apple com- pany. In order to remove the redundancy in the results, and at the same time keep the quality of the top-k results, diversity should be considered in the top-k search problems. For top-k search algorithms. In the literature, most of them aim at finding an early stop condition, such that they can find the top- k results without exploring all the possible search results. Based on this, two frameworks are generally used, namely, the incremen- tal top-k framework and the bounding top-k framework. The in- cremental top-k framework outputs the results one by one in non- increasing order of their scores, and stops as soon as k results are generated. It aims to find a polynomial delay algorithm such that given the existing generated results, the next result with largest score can be generated in polynomial time w.r.t. the size of the input only [16, 15, 20, 14]. In the bounding top-k framework, re- sults are not necessarily generated in non-increasing order of their scores. It maintains a score upper bound for the unseen results ev- ery time when a new result is generated. The algorithm stops when the current k-th largest score is no smaller than the upper bound for the unseen results. The threshold algorithm based approaches [7, 9] fall in this framework and other approaches include [12, 17]. Diversity aware search has been studied in recent years. Most of the existing solutions that support diversity on top-k search results assume the ranking of all the search results are given in advance. Based on which, a diversity search algorithm is given to output k results based on a scoring function that takes both query relevance and diversity into consideration [6, 1, 11, 5, 2]. Other works give algorithms that solve the diversity problem for a special area, i.e., graph search [18], document search [22], etc. and can hardly be extended to support general top-k diversity search. In this paper, we propose a general framework to handle the di- versified top-k search problem. We keep the advantages for the existing top-k search algorithms, that can stop early without ex- ploring all search results, and at the same time, we take diversity into consideration. We show that any top-k search algorithm that can be used in the incremental top-k framework or the bounding top-k framework can be easily extended to handle diversified top- k search, by adding three new functions studied in this paper: a sufficient stop condition sufficient(), a necessary stop condition necessary(), and a diversity search function div-search-current(). All of them are application independent. The only assumption in our framework is that, given any two search results vi and vj , whether vi and vj are similar to each other can be decided, e.g., us- ing a similarity function sim(vi, vj) > íì§íì¨ for a user given threshold íì§íì¨ . We output a list of k results with maximum total scores such that 1 http://www.google.com/imghp 1124 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Articles from this volume were invited to present their results at The 38th International Conference on Very Large Data Bases, August 27th - 31st 2012, Istanbul, Turkey. Proceedings of the VLDB Endowment, Vol. 5, No. 11 Copyright 2012 VLDB Endowment 2150-8097/12/07... $ 10.00. no two of them are similar to each other. We make the following contributions in this paper. (1) We formalize the diversified top-k search problem. Based on our definition, the optimal solution only depend on the similarity of search results themselves, and no other information is needed. (2) We study two categories of algorithms generally used in finding top-k results with early stop in the literature, namely, the incre- mental top-k framework and the bounding top-k framework. We show both frameworks can be extended to diversified top-k search by simply adding three application independent functions studied in this paper, namely, a sufficient stop condition sufficient(), a nec- essary stop condition necessary(), and a diversity search function div-search-current(). The sufficient stop condition helps to early stop and the necessary stop condition helps to reduce the number of div-search-current() processes, since div-search-current() is usu- ally a costly operation. (3) We show that div-search-current() is an NP-Hard problem and is hard to be approximated. We propose three new algorithms, namely, div-astar, div-dp, and div-cut, to find the optimal solution for div-search-current(). div-astar is an A? based algorithm and is slow to handle a large number of results. div-dp decomposes the results into disconnected components in order to reduce the graph size to be searched using div-astar. Results in div-dp are com- bined using dynamic programming. div-cut further decomposes each component into several subgraphs to form a cptree, based on the cut points of each component. A tree based search is applied on cptree to find the optimal solution. (4) We conducted extensive performance studies using two real datasets, to test the performance of the three algorithms. Our div-cut approach can find the diversified top-k results within seconds when k is as large as 2, 000. The rest of this paper is organized as follows. Section 2 formally defines the diversified top-k search problem. Section 3 shows the two existing frameworks on general top-k search problems. Sec- tion 4 shows how to extend the two categories of top-k search ap- proaches to solve diversified top-k search, by defining a sufficient stop condition sufficient(), a necessary stop condition necessary(), and a diversified top-k search algorithm div-search-current() to search on the current result set. Section 5, 6, and 7 give three al- gorithms to solve the div-search-current() problem. We show our experimental results in Section 8, and introduce the related work in Section 9. Finally, we conclude our paper in Section 10. 2. PROBLEM DEFINITION We consider a list of results S = {v1, v2,  íì§  íì§  íì§ }. For each vi  íì¨ S, the score of vi is denoted as score(vi). For any two results vi  íì¨ S and vj  íì¨ S, there is a user defined similarity function sim(vi, vj) denoting the similarity between the two results vi and vj . Without loss of generality, we assume 0  íí sim(vi, vj)  íí 1 for any two results vi  íì¨ S and vj  íì¨ S, and sim(v, v) = 1 for any v  íì¨ S. Given an integer k where 1  íí k  íí |S|, the top-k results of S is a list of k results Sk that satisfy the following two conditions. 1) Sk  íì¨ S and |Sk| = k. 2) For any vi  íì¨ Sk and vj  íì¨ S ? Sk, score(vi)  íí score(vj). Here, S ? Sk is the set of results that are in S but not in Sk, i.e., S ? Sk = {v|v  íì¨ S, v / íì¨ Sk}. Given two results vi  íì¨ S and vj  íì¨ S, vi is similar to vj iff sim(vi, vj) > íì§íì¨ where íì§íì¨ is a user defined threshold, and 0 < íì§íì¨  íí 1. We use vi ? vj to define that vi is similar to vj . Definition 1 (Diversified Top-k Results) Given a list of search results S = {v1, v2,  íì§  íì§  íì§ }, and an integer k where 1  íí k  íí |S|, the v1 v3 v5 v2 v4 v6 10 6 8 7 7 1 v1 v3 v5 v2 v4 v6 10 6 8 7 7 1 K=3K=2 Figure 1: A sample diversity graph diversified top-k results of S, denoted as D(S), is a list of results that satisfy the following three conditions. 1) D(S)  íì¨ R and |D(S)|  íí k. 2) For any two results vi  íì¨ R and vj  íì¨ R and vi 6= vj , if vi ? vj , then {vi, vj} * D(S). 3) íì§2 v íì¨D(S) score(v) is maximized. Intuitively, D(S) is the set of at most k results, such that no two results are similar with each other, and the total score of the results is maximized. We use score(D(S)) to denote the total score of results in D(S), i.e., score(D(S)) = íì§2 v íì¨D(S) score(v). In this paper, we are to find the diversified top-k results. Our aim is to find a general approach, such that for any existing algo- rithm that returns the top-k results of a certain problem, it can be easily changed to return the diversified top-k results by applying our framework, in which the result set S is not necessarily to be computed in advanced but grows incrementally with an early stop condition. We first give the definition of the diversity graph. Definition 2 (Diversity Graph) Given a list of results S = {v1, v2,  íì§  íì§  íì§ }, the diversity graph of S, denoted as G(S) = (V,E), is an undirected graph such that for any result v  íì¨ R, there is a corresponding node v  íì¨ V , and for any two results vi  íì¨ S and vj  íì¨ R, there is an edge (vi, vj)  íì¨ E iff vi ? vj . We use V (G(S)) and E(G(S)) to denote the set of nodes and the set of edges in the diversity graph G(S) respectively, and use v.adj(G(S)) to denote the set of nodes that are adjacent to v in G(S). If the context is obvious, we use vi to denote both the result vi in S and the node vi in G(S), we use G to denote G(S), and we use D to denote D(S). Without loss of generality, we assume nodes in G(S) are arranged in non-increasing order of their scores, i.e., for any 1  íí i < j  íí |V (G(S))|, score(vi)  íí score(vj). The diversified top-k results D(S) can be equivalently defined as a subset of nodes in G(S), that satisfy the three conditions. 1) |D(S)|  íí k. 2) D(S) is an independent set of G(S). 3) score(D(S)) is maximized. Here, an independent set of a graph is a set of nodes in a graph, where no two nodes are adjacent. Example 1 Fig. 1 shows the diversity graph for 6 results S = {v1, v2,  íì§  íì§  íì§ , v6}. Suppose k = 2, the optimal solution D(S) includes two points v1 and v2 with score 18, as shown on the left part of Fig. 1. Suppose k = 3, the optimal solution D(S) includes three points v3, v4 and v5 with score 20, as shown on the right part of Fig. 1. In the following, we first show the two existing frameworks to solve top-k search problems, namely, the incremental top-k frame- work and the bounding top-k framework, which are most generally used in top-k search algorithms. Then we show the framework of 1125 Algorithm 1 incremental(k) 1: S  íì§  ?; 2: for i=1 to k do 3: v  íì§  incremental-next(); 4: if v = ? then 5: break; 6: S  íì§  S ? {v}; 7: return S; Algorithm 2 bounding(k) 1: S  íì§  ?; 2: unseen íì§  + íí; 3: while the k-th largest score of S < unseen do 4: v  íì§  bounding-next(); 5: if v = ? then 6: break; 7: S  íì§  S ? {v}; 8: update unseen; 9: return top-k results in S; our approach to extend the two frameworks to handle diversified top-k search. 3. TOP-K SEARCH FRAMEWORKS In the literature, the framework of most algorithms that find top- k results falls into two categories, namely, the incremental top-k framework and the bounding top-k framework. Incremental Top-k: In the incremental top-k framework, results are generated one by one by calling a procedure incremental-next(), with non-increasing order of their scores. The algorithm stops after k results are generated, and the k results are the final top-k results for the problem. The framework named incremental is shown in Algorithm 1. A lot of existing work fall into this category, e.g., finding top-k shortest paths in graphs, finding top-k steiner trees, communities and r-cliques in graphs, etc [16, 15, 20, 14]. A lot of works have been done to assume that the time complexity of each incremental-next() procedure to generate the next result with largest score is polynomial w.r.t. the size of the input only. Bounding Top-k: In the bounding top-k framework, results are generated one by one by calling a procedure bounding-next(), but not necessarily with non-increasing order of their scores. A bound unseen is defined to be the upper bound of the scores for the un- seen results. After each result is generated by bounding-next(), unseen is also updated to be a possibly smaller value. The algo- rithm stops when the k-th largest score of all generated results is no smaller than the upper bound for the unseen results unseen. The framework named bounding is shown in Algorithm 2. The thresh- old algorithm that is generally used to return top-k results falls into this category [7, 9]. Other works that fall into this category include [12, 17]. 4. DIVERSIFIED TOP-K SEARCH In this section, we show how to extend the incremental top-k framework incremental and bounding top-k framework bounding to handle diversified top-k search. We mainly focus on two tasks. First, a new early stop conditions is needed. Second, an algorithm that finds the diversified top-k results for the current generated re- sult set is needed. For the early stop condition, in the original al- gorithm, the stop condition for incremental is simply |S| = k and the stop condition for bounding is the current k-th largest score  íí unseen. Obviously, both of them cannot be applied to handle Algorithm 3 div-search(k) 1: S  íì§  ?; D(S) íì§  ?; 2: while sufficient() do 3: the code to update S (and unseen); 4: if necessary() then 5: D(S) íì§  div-search-current(G(S), k); 6: return D(S); diversified top-k search. Consider an extreme case, when the al- gorithm stops using the original stop condition, it is possible that all the results generated are similar to each other. Thus the current diversified top-k results only contain 1 result with the largest score. It is not the optimal solution because it is possible that an unseen result is not similar to the current one. Here, D(S) computed for the current generated result set S can be used to check the new stop condition, and if the new stop condition is satisfied, D(S) is the optimal solution for the diversified top-k search. We extend both incremental and bounding using the same frame- work, which is shown in Algorithm 3, by adding three new func- tions, a new sufficient stop condition sufficient(), a new necessary stop condition necessary() and an algorithm div-search-current() to search the diversified top-k results on the current generated re- sult set. The algorithm executes the code of the original top-k al- gorithm to update S and stops when sufficient() is satisfied. For incremental, the code is line 3-6 in algorithm 1, and for bounding, the code is line 4-8 in algorithm 2. After updating S, we construct the diversity graph G(S) on S based on the similarity function sim() for any given two results. If the necessary stop condition is satisfied, we find the diversified top-k results for the current result set S using div-search-current(). The necessary stop condition is used to reduce the number of calling div-search-current(), because div-search-current() is a costly work. In the following, we will in- troduce the sufficient stop condition, the necessary stop condition, and the search algorithm for current set. Sufficient Stop Condition: Given the current result set S, we need to calculate an upper bound best(S) for the possible optimal solu- tions considering both the current result set S and the unseen re- sults. Let Di(S) be the best diversified results of S with exactly i elements for 1  íí i  íí k, i.e., Di(S) is a subset of nodes in V (G(S)), that satisfies the following three conditions. 1) |Di(S)| = k. 2) Di(S) is an independent set of G(S). 3) score(Di(S)) is maximized. Lemma 1 Given Di(S) for 1  íí i  íí k and the score upper bound of all the unseen results u. The upper bound best(S) can be calcu- lated as follows. best(S) = max 1 ííi íík {score(Di(S)) + (k ? i) íì© u} (1) where u is the score of the last generated result v, score(v), for incremental and is the upper bound of the unseen results, unseen, for bounding. Proof Sketch: Suppose the final optimal solution is O, then we can divide O into two parts, O = O1 ? O2, where O1 is the set of generated results, and O2 is the set of unseen results. Suppose O1 has n1 elements and O2 has n2 elements. We have n1 + n2  íí k. Since O1 is the set of generated results, we have (1) score(O1)  íí score(Dn1(S)), since Dn1(S) is the optimal solution with n1 el- ements. We also have (2) score(O2)  íí n2  íì© u  íí (k ? n1)  íì© u, 1126 since (u) is the score upper bound for all unseen results. Com- bine (1) and (2), we have score(O) = score(O1) + score(O2)  íí score(Dn1(S)) + (k ? n1)  íì© u  íí max1 ííi íík{score(Di(S)) + (k? i) íì©u} = best(S). best(S) is an upper bound for the optimal solution. ? Having the score upper bound best(S) for the optimal solution, the sufficient stop condition for div-search can be defined as fol- lows. score(D(S))  íí best(S) (2) The following lemma shows that, after every iteration, div-search moves towards the sufficient stop condition. Lemma 2 For any S íí  íì¨ S, best(S íí)  íí best(S) and best(S íí)  íí best(S). Proof Sketch: Since S íí  íì¨ S, the best solution on S íí is a feasible solution on S, thus best(S íí)  íí best(S). Comparing to best(S íí), best(S) is calculated by changing some upper bounds u íí when cal- culating best(S íí) into the real scores no larger than u íí and chang- ing the other unseen upper bounds from u íí to u, where u  íí u íí is assumed by the original algorithm. Thus best(S íí)  íí best(S). ? Necessary Stop Condition: We discuss the necessary stop con- dition for div-search. The necessary stop condition is used as fol- lows. In each iteration, before invoking div-search-current(), if the necessary stop condition is not satisfied, then div-search-current() is not necessarily to be invoked in this iteration. Lemma 3 For div-search, if it can stop in a certain iteration, one of the following conditions should be satisfied before invoking the procedure div-search-current(): 1) The last generated result v = ?. 2) |S|  íí |S íí|+ k ?max{i|1  íí i  íí k,Di(S  íí) 6= ?} and the k-th largest score in S  íí u. Here S íí is the set of results when the last div-search-current() is invoked or ? if div-search-current() is never invoked. Proof Sketch: The first condition is trivial. Now suppose v 6= ?. For the second condition, when the k-th largest score in S < u, it is possible that a new result can be added that updates the k-th largest score, and thus improves the current best solution. Now we discuss |S|  íí |S íí| + k ? max{i|1  íí i  íí k,Di(S  íí) 6= ?}. max{i|1  íí i  íí k,Di(S  íí) 6= ?} is the size of the maximum independent set for G(S íí) if it is smaller than k, and k?max{i|1  íí i  íí k,Di(S  íí) 6= ?} is the minimum number of nodes needed to be added in order to generate a result of size k. If such a result does not exist, we cannot stop because we can always add some unseen nodes to any existing solution with a size smaller than k to make the score larger. As a result, we should add at least k ?max{i|1  íí i  íí k,Di(S  íí) 6= ?} nodes into S íí. ? Searching Current Set: The most important operation in our frame- work is the the algorithm div-search-current() to search the diver- sified top-k results for the current result set S. We first show the difficulties of the problems in this section and give three algorithms, namely div-astar, div-dp, and div-cut on div-search-current() in the next three sections respectively. The following lemma shows that finding the diversified top-k results is an NP-Hard problem. Lemma 4 Finding D(S) on G(S) is an NP-Hard problem. Proof Sketch: We consider a special case of the problem, where score(v) = 1 for all v  íì¨ V (G(S)), and k = |V (G(S))|. In such a case, finding Dk(R) on G(S) is equivalent to finding the v1 v2 v3 v0  íì§  íì§  íì§ u1 u2 u3  íì§  íì§  íì§ u100 v100 99 99 99 1 1 100 99 0.5 1 (a) The Greedy Solution v1 v2 v3 v0  íì§  íì§  íì§ u1 u2 u3  íì§  íì§  íì§ u100 v100 99 99 99 1 1 100 99 0.5 1 (b) The Optimal Solution Figure 2: The greedy algorithm div?astar div?dp div?cut NP NP NP NP NP NP NP NP NP NP NP NP NP NP NP NP NP NP NP NP NP NP NP NPNP Figure 3: Overview of three algorithms maximum independent set on graph G(S), which is an NP-Hard problem. Thus, the original problem is an NP-Hard problem. Greedy is Not Good: Given G(S) and k, a simple greedy algo- rithm to find D(S) works as follows. It processes in iterations. In each iteration, the node v with the maximum score is selected and put into D(S). After that, all the nodes that are adjacent to v in G(S) is removed from G(S). The process stops when G(S) is empty or D(S) contains k results. The quality of the greedy algorithm can be arbitrarily bad. The approximation ratio for the greedy algorithm is not bounded by a constant factor. Even for its special case, the maximum indepen- dent set problem is known to be hard to approximate in the litera- ture. We give an example. Fig. 2 shows a diversity graph with 201 nodes and 200 edges. Suppose k = 100. Using the greedy algo- rithm, the solution is shown in Fig. 2(a), where the selected results are marked gray. The score of the greedy solution is 199. The op- timal solution for the problem is shown in Fig. 2(b). The score of the optimal solution is 9, 900, which is nearly 50 times of the score of the greedy solution. In the following, we propose to find the optimal solution of D(S). We propose three algorithms, namely, div-astar, div-dp, and div-cut. div-astar searches the whole space S using the A? based heuris- tics by designing an upper bound function astar-bound(). Based on the NP-Hardness of the problem, div-astar can hardly handle problems with large diversity graph G. In our second div-dp al- gorithm, we decompose G into connected components. The size of each component can be much smaller than the original graph G, and is searched independently using div-astar. We combine the components using an efficient operation ? based on dynamic pro- gramming. In our third div-cut algorithm, we further decompose each connected component into subgraphs, where subgraphs are connected through a set of cut points. Each subgraph is searched independently for at most 4 times under different conditions. We combine the components using two efficient operations ? and ?. The general ideas of the three algorithms are illustrated in Fig. 3. 5. AN A? BASED APPROACH As discussed in Section 4, div-search-current(G(S), k) should return the optimal solution Di(S) for 1  íí i  íí k in order to find the early stop condition. For simplicity, we use D to denote the set of solutions, and we use D.solutioni to denote the optimal solution 1127 Algorithm 4 div-astar(G, k) Input: The diversity graph G, the top-k value. Output: Search result D. 1: H  íì§  ?; D  íì§  ?; 2: H.push((?, 0, 0, 0)); 3: for k íí = k down to 1 do 4: astar-search(G,H, D, k íí); 5: for all e  íì¨ H do 6: e.bound íì§  astar-bound(G, e, k íí); 7: update e inH; 8: return D; 9: procedure astar-search(G,H, D, k íí) 10: whileH 6= ? andH.top.bound > maxi íík íí{D.scorei} do 11: e íì§  H.pop(); 12: for i = e.pos+ 1 to |V (G)| do 13: if vi.adj(G) ? e.solution = ? then 14: e íí  íì§  (e.solution ? {vi}, i, e.score+ score(vi), 0); 15: e íí.bound íì§  astar-bound(G, e íí, k íí); 16: H.push(e íí); 17: update D using e íí.solution; 18: procedure astar-bound(G, e, k íí) 19: p íì§  |e.solution|; i íì§  e.pos+ 1; 20: bound íì§  e.score; 21: while p < k íí and i < |V (G)| do 22: if vi.adj(G) ? e.solution = ? then 23: bound íì§  bound+ score(vi); 24: p íì§  p+ 1; 25: i íì§  i+ 1; 26: return bound; with i results Di(S), and use D.scorei to denote the score for the optimal solution score(Di(S)). Our first algorithm is an A? based algorithm. The algorithm is shown in Algorithm 4. We define a max heap H to store the entries in the A? search. Each entry e  íì¨ H is with the form e = (solution, pos, score, bound). Each entry e is ranked in H according to e.bound, which is the estimated upper bound of the solution if we further expand it in the A? search. e.solution is the partial solution searched and e.pos is the position of the last searched node in e.solution. e.score is the score of the partial solu- tion, i.e., e.score = score(e.solution). The algorithm should return D.solutioni for all 1  íí i  íí k. Suppose we have an A ? algorithm that finds the optimal solution for a certain D.solutioni, the algo- rithm should be invoked k times to find the k solutions, which is costly. We show that after searching D.solutioni for a certain i, the partial solutions in H can be reused when searching D.solutionj for j < i. In the following, we first discuss the estimated upper bound for partial solutions. Then we discuss the A? algorithm to find the optimal solution D.solutioni for a certain i. At last, we discuss how the partial solutions in H can be reused to find the optimal solutions D.solutioni for all 1  íí i  íí k. Upper Bound Estimation: Given a partial solution e, for a cer- tain k íí, we show how to estimate the score upper bound if we ex- pand the partial solution to be a solution of at most k íí elements. The algorithm astar-bound is shown in Algorithm 4, line 18-26. The newly added nodes should at least satisfy the following two conditions: 1) they can not be one of e.solution, and 2) they are not adjacent to any node in e.solution. Under such conditions, we can just add the set of nodes with largest scores, and after adding the nodes, the total number of nodes is no larger than k íí. In or- der to satisfy condition 1), we visit nodes in G from the posi- tion e.pos + 1 (line 19). Since nodes in G are sorted in the non- increasing order of their scores, we add nodes one by one until the size p reaches k íí. For each node added, condition 2) can be checked using vi.adj(G) ? e.solution = ? (line 22). Lemma 5 astar-bound(G, e, k íí) finds the score upper bound for the partial solution e.solution to be expanded to a solution of at most k íí elements. Proof Sketch: Suppose we have removed all the nodes from G that are adjacent to at least one node in e.solution, then the func- tion astar-bound(G, e, k íí) calculates the upper bound by expand- ing e.solution using the set of nodes after position e.pos in G with largest scores. The optimal solution that e.solution can be ex- panded also selects the expanded nodes from the set of nodes after position e.pos but it may not select all with the largest scores since some of them may be adjacent to each other. Thus the optimal so- lution can not be larger than astar-bound(G, e, k íí). As a result, astar-bound(G, e, k íí) is a score upper bound for all expansions of e.solution. ? A? Search for a Certain k: To find the optimal solution for a certain k = k íí, the A? search algorithm astar-search is shown in Algorithm 4, line 9-17. It runs in iterations. In each iteration, the partial solution e with the largest estimated upper bound is popped out from H (line 11). e can then be expanded to new partial solu- tions by adding a new node into e.solution. The nodes are added from position e.pos + 1 in G since all nodes before the position has been processed (line 12). The newly added node vi should not be adjacent to one of e.solution(line 13), and after adding the new node, the upper bound of the new partial solution should be updated using astar-bound(), and the new partial solution should be pushed into H for further expansion (line 14-16). In line 17, suppose the",Haohan Zhu,Department of Computer Science Boston University,zhu@cs.bu.edu,George Kollios,Department of Computer Science Boston University,gkollios@cs.bu.edu,Vassilis Athitsos,Computer Science and Engineering Department University of Texas at Arlington,athitsos@uta.edu,,,,,,,,,,,,,,,,,,,,,
20200103,1387,Stefan Aulbach,"Technische Universit?t M?nchen, Germany",stefan.aulbach@in.tum.de,,A Comparison of Flexible Schemas for Software as a Service,"ABSTRACT A multi-tenant database system for Software as a Service (SaaS) should offer schemas that are flexible in that they can be extended for different versions of the application and dynamically modified while the system is on-line. This pa- per presents an experimental comparison of five techniques for implementing flexible schemas for SaaS. In three of these techniques, the databaseì°½íµownsì°½í¶the schema in that its struc- ture is explicitly defined in DDL. Included here is the com- monly-used mapping where each tenant is given their own private tables, which we take as the baseline, and a map- ping that employs Sparse Columns in Microsoft SQL Server. These techniques perform well, however they offer only lim- ited support for schema evolution in the presence of existing data. Moreover they do not scale beyond a certain level. In the other two techniques, the application ì°½íµownsì°½í¶ the schema in that it is mapped into generic structures in the database. Included here are XML in DB2 and Pivot Tables in HBase. These techniques give the application complete control over schema evolution, however they can produce a significant decrease in performance. We conclude that the ideal data- base for SaaS has not yet been developed and offer some suggestions as to how it should be designed. Categories and Subject Descriptors H.4 [Information Systems Applications]: Miscellaneous; H.2.1 [Information Systems]: Database Managementì°½í¬ Logical Design General Terms Design, Performance Keywords Multi-Tenancy, Software as a Service, Flexible Schemas, Ex- tensibility, Evolution Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGMODì°½í²09, June 29?July 2, 2009, Providence, Rhode Island, USA. Copyright 2009 ACM 978-1-60558-551-2/09/06 ...$5.00. 1. INTRODUCTION In the Software as a Service (SaaS) model, a service pro- vider owns and operates an application that is accessed by many businesses over the Internet. A key benefit of this model is that, by careful engineering, it is possible to lever- age economy of scale to reduce total cost of ownership rel- ative to on-premises solutions. Common practice in this regard is to consolidate multiple businesses into the same database to reduce operational expenditures, since there are fewer processes to manage, as well as capital expenditures, since resource utilization is increased. A multi-tenant database system for SaaS should offer sche- mas that are flexible in two respects. First, it should be possible to extend the base schema to support multiple spe- cialized versions of the application, e.g., for particular ver- tical industries or geographic regions. An extension may be private to an individual tenant or shared by multiple ten- ants. Second, it should be possible to dynamically evolve the base schema and its extensions while the database is on-line. Evolution of a tenant-owned extension should be totally ì°½íµself-serviceì°½í¶: the service provider should not be in- volved; otherwise operational costs will be too high. This paper presents an experimental comparison of five techniques for implementing flexible schemas for SaaS. In three of these techniques, the database ì°½íµownsì°½í¶ the schema in that its structure is explicitly defined in DDL: Private Tables: Each tenant is given their own private in- stance of the base tables that are extended as required. In contrast, in all of the other mappings, tenants share tables. We take Private Tables as the experimental baseline. Extension Tables: The extensions are vertically partitio- ned into separate tables that are joined to the base tables along a row ID column. Sparse Columns: Every extension field of every tenant is added to its associated base table as a Sparse Column. Our experiments here use Microsoft SQL Server 2008 [1]. To implement Sparse Columns efficiently, SQL Server uses a variant of the Interpreted Storage Format [4, 7], where a value is stored in the row together with an identifier for its column. Our experimental results show that these techniques per- form well, however they offer only limited support for schema evolution. DDL commands over existing data, if they are supported at all, consume considerable resources and neg- atively impact performance. In the on-line setting, the ap- 881 plication must be given control over when and how bulk data transformations occur. An additional issue is that these techniques do not scale beyond a certain level. In the other two techniques, the application ì°½íµownsì°½í¶ the schema in that it is mapped into generic structures in the database: XML: Each base table is augmented by a column that stores all extension fields for a tenant in a flat XML document. Since these documents necessarily vary by tenant, they are untyped. Our experiments here use pureXML in IBM DB2 [20]. Pivot Tables: Each value is stored along with an identifier for its column in a tall narrow table [2]. Our exper- iments here use HBase [11], which is an open source version of Google BigTable [6]. BigTable and HBase were originally designed to support the exploration of massive web data sets, but they are increasingly be- ing used to support enterprise applications [14]. The Pivot Table mapping into HBase that we employ is consistent with best practices. These two techniques give the application complete con- trol over schema evolution, however our experimental results show that they can produce a significant decrease in perfor- mance from the baseline. For XML, the decrease is greatest for reads, which require parsing the untyped documents and reassembling typed rows. The decrease is proportional to the number of extension fields. For Pivot Tables, the de- crease is more than an order of magnitude in some cases. Note that these results should not be taken as a negative statement about the quality of these systems, since they have not been optimized for our use case. Moreover, HBase is an early-stage open source project, not a mature com- mercial product. Our results are intended to give a general indication of the trade-offs in implementing flexible schemas. Several major SaaS vendors have developed mapping tech- niques in which the application owns the schema. This ap- proach has been elevated to a design principle whereby the application derives essential capabilities by managing the metadata itself [19, 23]. To achieve acceptable performance, these applications re-implement significant portions of the database, including indexing and query optimization, from the outside. We believe that databases should be enhanced to directly support the required capabilities. Our experiments are based on a multi-tenant database testbed that simulates a simple but realistic Customer Re- lationship Management (CRM) service. The workload con- tains single- and multi-row create, read, and update oper- ations as well as basic reporting tasks. The schema can be extended for individual tenants and it can evolve over time. Our previous work with a more limited version of this testbed (no extensions) showed that the performance of Pri- vate Tables degrades if there are too many tables [3]. This effect is due to the large amount of memory needed to hold the metadata as well as an inability to keep index pages in the buffer pool. In this paper, we create only a moderate number of tables, take Private Tables as the baseline, and use it to compare the other mappings. This paper is organized as follows. Section 2 describes our multi-tenant database testbed and the CRM application that it simulates. Section 3 describes the schema mapping techniques. Section 4 presents the results of our experi- ments. Section 5 concludes that the ideal database for SaaS LineItem Product Case Contract Lead Opportunity Asset Contact Campaign Account Figure 1: CRM Application Schema has not yet been developed and offers some suggestions as to how it should be designed. 2. MULTI-TENANT DATABASE TESTBED The experiments in this paper are based on a multi-tenant database testbed we have developed that can be adapted for different database configurations. Each configuration re- quires a plug-in to the testbed that transforms abstract ac- tions into operations that are specific to and optimized for the target database. The testbed simulates a simple but realistic CRM ser- vice. Figure 1 shows the entities and relationships in the base schema. The base entities are extended with additional fields of various types for each tenant. Tenants have different sizes and tenants with more data have more extension fields, ranging from 0 to 100. The characteristics of the dataset are modeled on salesforce.comì°½í²s published statistics [13]. The testbed has nine request classes. The distribution of these requests is controlled using a mechanism similar to TPCì°½í²s card decks. Select 1: Select all attributes of a single entity as if it was being displayed in a detail page in the browser. Select 50: Select all attributes of 50 entities as if they were being displayed in a list in the browser. Select 1000: Select all attributes of the first 1000 entities as if they were being exported through a Web Services interface. Reporting: Run one of five reporting queries that perform aggregation and/or parent-child-roll-ups. Insert 1: Insert one new entity instance as if it was being manually entered into the browser. Insert 50: Insert 50 new entity instances as if data were being synchronized through a Web Services interface. Insert 1750: Insert 1750 new entity instances as if data were being imported through a Web Services interface. Update 1: Update a single entity as if it was being modi- fied in an edit page in the browser. Update 100: Update 100 entity instances as if data were being synchronized through a Web Services interface. The testbed mimics a typical application serverì°½í²s behav- ior by creating a configurable number of connections to the database backend. To avoid blockings, the connections are distributed among a set of worker hosts, each of them han- dling a few connections only. Distributing these connec- tions among multiple hosts allows for modeling various sized, multi-threaded application servers. 882 3. SCHEMA MAPPING TECHNIQUES Within a SaaS application, each tenant has a logical sche- ma consisting of the base schema and a set of extensions. To implement multi-tenancy, the logical schemas from mul- tiple tenants are mapped into one physical schema in the database. The mapping layer transforms queries against the logical schemas into queries against the physical schema so multi-tenancy is transparent to application programmers. The physical schemas for the five mapping techniques stud- ied in this paper are illustrated in Figure 2. The example data set used in this Figure is most clearly shown in the Pri- vate Tables mapping (Figure 2(a)). There are three tenants ? 17, 35, and 42 ? each of which has an Account table with Account ID (Aid) and Name fields. Tenant 17 has extended the Account table with two fields for the health care indus- try: Hospital and Beds. Tenant 42 has extended the Account table with one field for the automotive industry: Dealers. In the Extension Tables mapping (Figure 2(b)), the industry extensions are split off into separate tables that are joined to the base Account table using a new Row number column (Row). Tenants share the tables using a tenant ID column (Tenant). This section describes the other three mappings in more detail. 3.1 Sparse Columns in Microsoft SQL Server Sparse Columns were originally developed to manage data such as parts catalogs where each item has only a few out of thousands of possible attributes. Storing such data in con- ventional tables with NULL values can decrease performance even with advanced optimizations for NULL handling. To implement Sparse Columns, SQL Server 2008 uses a variant of the Interpreted Storage Format [4, 7], where a value is stored in the row together with an identifier for its column. In our mapping for SaaS, the base tables are shared by all tenants and every extension field of every tenant is added to the corresponding base table as a Sparse Column, as il- lustrated in Figure 2(c). Sparse columns must be explicitly defined by a CREATE/ALTER TABLE statement in the DDL and, in this sense, are owned by the database. Nev- ertheless, the application must maintain its own description of the extensions, since the column names cannot be stati- cally embedded in the code. For writes, the application must ensure that each tenant uses only those columns that they have declared, since the namespace is global to all tenants. For reads, the application must do an explicit projection on the columns of interest, rather than doing a SELECT ?, to ensure that NULL values are treated correctly. Sparse Columns requires only a small, fixed number of tables, which gives it a performance advantage over Pri- vate Tables; [3] shows that having many tables negatively impacts performance. On the other hand, there is some overhead for managing Sparse Columns. As an example, the SQL Server documentation recommends using a Sparse Column for an INT field only if at least 64% of the values are NULL [15]. Both of these factors are reflected in the performance results presented in Section 4. 3.2 XML in IBM DB2 IBM pureXML was designed to allow processing of semi- structured data alongside of structured relational data [20]. The mapping for SaaS that we use follows the recommenda- tions in the pureXML documentation for supporting multi- tenancy [21]. The base tables are shared by all tenants and Account17 Aid Name Hospital Beds 1 Acme St. Mary 135 2 Gump State 1042 Account35 Aid Name 1 Ball Account42 Aid Name Dealers 1 Big 65 (a) Private Tables AccountExt Tenant Row Aid Name 17 0 1 Acme 17 1 2 Gump 35 0 1 Ball 42 0 1 Big HealthcareAccount Tenant Row Hospital Beds 17 0 St. Mary 135 17 1 State 1042 AutomotiveAccount Tenant Row Dealers 42 0 65 (b) Extension Tables Account Tenant Aid Name SPARSE 17 1 Acme Hospital St. Mary Bed 135 17 2 Gump Hospital State Bed 1042 35 1 Ball 42 1 Big Dealer 65 (c) Sparse Columns Account Tenant Aid Name Ext XML 17 1 Acme <ext><hospital>St. Mary</hospital> <beds>135</beds></ext> 17 2 Gump <ext><hospital>State</hospital> <beds>1042</beds></ext> 35 1 Ball 42 1 Big <ext><dealers>65</dealers></ext> (d) XML RowKey Account Contact 17Act1 [name:Acme, hospital:St. Mary, beds:135 ] 17Act2 [name:Gump, hospital:State, beds:1042 ] 17Ctc1 [íì¨ íì¨ íì¨ ] 17Ctc2 [íì¨ íì¨ íì¨ ] 35Act1 [name:Ball] 35Ctc1 [íì¨ íì¨ íì¨ ] 42Act1 [name:Big, dealers:65 ] (e) Pivot Tables Figure 2: Schema Mapping Techniques each base table is augmented by a column (Ext XML) that stores all extension fields for a tenant in a flat XML docu- ment, as illustrated in Figure 2(d). Since these documents necessarily vary by tenant, they are untyped. This repre- sentation keeps the documents as small as possible, which is an important consideration for performance [16]. pureXML offers a hybrid query language that provides native access to both the structured and semi-structured representations. Our testbed manipulates data in the struc- tured format, thus accessing extension data requires a corre- lated subquery to manage the XML. This subquery extracts the relevant extension fields using the XMLTABLE function which converts an XML document into a tabular format us- ing XPath. The query with the XMLTABLE function has 883 SELECT b.Tenant, b.Aid, b.Name, e.Dealers FROM Accounts b, XMLTABLE(ì°½í²i/extì°½í² PASSING b.Ext_XML AS ""i"" COLUMNS Dealers INTEGER PATH ì°½í²dealersì°½í² ) AS e WHERE Tenant = 42 AND Aid = 1; (a) Physical SELECT Query accounts tid,aid IXSCAN XSCANFETCH NLJOIN accounts RETURN (b) Query Execution Plan Figure 3: Correlated Subquery for XML in DB2 to be generated client- and query-specific to access clientsì°½í² extension fields relevant in the particular query. Figure 3(a) shows an example query against the physical schema that selects three base fields and one extension field; Figure 3(b) shows the associated query plan. In our testbed, rows are always accessed through base fields, hence there is no need to use the special XML indexes offered by pureXML [20]. To insert a new tuple with extension data, the application has to generate the appropriate XML document; our per- formance results generally include the time to perform this operation. Updates to extension fields are implemented us- ing XQuery 2.0 features to modify documents in place. 3.3 Pivot Tables in HBase HBase [11], which is an open source version of Google BigTable [6], was originally designed to support the explo- ration of massive web data sets. These systems are increas- ingly being used to support enterprise applications in a SaaS setting [14]. In an HBase table, columns are grouped into column fam- ilies. Column families must be explicitly defined in advance in the HBase ì°½íµDDLì°½í¶, for this reason they are owned by the database. There should not be more than tens of column families in a table and they should rarely be changed while the system is in operation. Columns within a column family may be created on-the-fly, hence they are owned by the ap- plication. Different rows in a table may use the same column family in different ways. All values in a column are stored as Strings. There may be an unbounded number of columns within a column family. Data in a column family is stored together on disk and in memory. Thus, a column family is essentially a Pivot Table; each value is stored along with an identifier for its column in a tall narrow table [2]. HBase was designed to scale out across a large farm of servers. Rows are range-partitioned across the servers by key. Applications define the key structure, therefore implic- itly control the distribution of data. Rows with the same key SELECT p.Name, COUNT(c.Case_id) AS cases FROM Products p, Assets a, Cases c WHERE c.Asset = a.Asset_id AND a.Product = p.Product_id GROUP BY p.Name ORDER BY cases DESC Figure 4: Logical Reporting Query prefix will be adjacent but, in general, may end up on differ- ent servers. The rows on each server are physically broken up into their column families. The mapping for SaaS that we use is illustrated in Fig- ure 2(e). In keeping with best practices for HBase, this map- ping ensures that data that is likely to be accessed within one query is clustered together. A single HBase table is used to store all tables for all tenants. The physical row key in HBase consists of the concatenation of the tenant ID, the name of the logical table, and the key of the row in the log- ical table. Each logical table is packed into its own column family, thus each row has values in only one column family. Within a column family, each column in the logical table is mapped into its own physical HBase column. Thus, since columns are dynamic, tenants may individually extend the base tables. The reporting queries in our testbed require join, sort and group operations, which are not currently provided by HBase. We therefore implemented these operators outside the database in an adaptation layer that runs in the client. The adaptation layer utilizes operations in the HBase client API such as update single-row, get single-row and multi-row scan with row-filter. As an example, consider the reporting query shown in Figure 4, which produces a list of all Prod- ucts with Cases by joining through Assets. To implement this query, our adaptation layer scans through all Cases for the given tenant and, for each one, retrieves the associated Asset and Product. It then groups and sorts the data for all Cases to produce the final result. In our experiments, HBase was configured to run on a sin- gle node and the Hadoop distributed map-reduce framework was not employed. In our experience, hundreds of tenants for an application like CRM can be managed by a database on a single commodity processor. In this setting, spreading the data for a tenant across multiple nodes and doing dis- tributed query processing would not be advantageous; the overhead for managing the distribution would nullify any benefits of parallelization. Of course, in addition to scal- ing up to handle many small tenants, the ideal SaaS data- base should also scale out to handle large tenants. But even in this case, map-reduce is problematic for queries such as the one in Figure 4, since it requires that data be clustered around Products. Other queries, such as pipeline reports on Opportunities, might require that the data be clustered in other ways. We conclude this section with several comments about the usage of HBase in our experiments. First, HBase offers only row-at-a-time transactions and we did not add a layer to extend the scope to the levels provided by the commer- cial databases. Second, compression of column families was turned off. Third, neither major nor minor compactions oc- curred during any of the experiments. Fourth, replication of data in the Hadoop file system was turned off. Fifth, column families were not pinned in memory. Sixth, the system was configured so that old attribute values were not maintained. 884  1  10  100  1000  10000  100000 Sel 1 Sel 50 Sel 1000 R eport Ins 1 Ins 50 Ins 1750 U pd 1 U pd 100 R e s p o n s e  T im e  [ m s e c ] Query Classes Private Tables Extension Tables Sparse Column (a) Overall  1  10  100  1000  10000  100000 Sel 1 Sel 50 Sel 1000 R eport Ins 1 Ins 50 Ins 1750 U pd 1 U pd 100 R e s p o n s e  T im e  [ m s e c ] Query Classes Priv. Table (Small Tenant) Priv. Table (Medium Tenant) Priv. Table (Large Tenant) Sparse (Small Tenant) Sparse (Medium Tenant) Sparse (Large Tenant) (b) By Tenant Size Figure 5: SQL Server Performance 4. EXPERIMENTAL RESULTS This section presents the results of our experiments on schema extensibility and evolution. To study schema evolu- tion, we issued a series of schema alteration statements dur- ing a run of the testbed and measured the drop in through- put. The experiments were run on Microsoft SQL Server 2008, IBM DB2 V.9.5 on Windows 2008, and HBase 0.19 on Linux 2.6.18 (CentOS 5.2). The database host was a VM on VMWare ESXi with 4 3.16 GHz vCPUs and 8 GB of RAM. 4.1 Microsoft SQL Server Figure 5(a) shows the results of running our testbed on Microsoft SQL Server using three different mappings: Pri- vate Tables, Extension Tables, and Sparse Columns. The horizontal axis shows the different request classes, as de- scribed in Section 2, and the vertical axis shows the response time in milliseconds on a log scale. In comparison to Private Tables, Extension Tables clearly exhibits the effects of vertical partitioning: wide reads (Sel 1, Sel 50, Sel 1000) are slower because an additional join is re- quired, while narrow reads (Report) are faster because some unnecessary loading of data is avoided. Updates (Upd 1, Upd 100) perform similarly to wide reads because our tests modify both base and extension fields. Extension Tables is faster for inserts because tables are shared among tenants so there is a greater likelihood of finding a page in the buffer pool with free space. Sparse Columns performs as well or better than Private Tables in most cases. The additional overhead for managing the Interpreted Storage Format appears to be offset by the fact that there are fewer tables. Sparse Columns performs worse for large inserts (Ins 1750), presumably because the implementation of the Interpreted Storage Format is tuned to favor reads over writes. Figure 5(b) shows a break down of the Private Table and Sparse Column results by tenant size. Recall from Section 2 that larger tenants have more extension fields, ranging from 0 to 100. The results show that the performance of both mappings decreases to some degree as the number of exten- sion fields goes up. SQL Server permits up to 30,000 Sparse Columns per ta- ble. Our standard configuration of the testbed has 195 ten- ants, which requires about 12,000 columns per table. We also tried a configuration with 390 tenants and about 24,000 columns per table and there was little performance degra- dation. The number of extension fields per tenant in our testbed is drawn from actual usage, so SQL Server is unlikely to be able to scale much beyond 400 tenants. As a point of comparison, salesforce.com maintains about 17,000 tenants in one (very large) database [13]. Figures 6(a) and 6(b) show the impact of schema evolu- tion on throughput in SQL Server. In these graphs, the horizontal axis is time in minutes and the vertical axis is transactions per minute. The overall trend of the lines is downward because data is inserted but not deleted during a run. Part way through each run, ALTER TABLE state- ments on 5 base tables were submitted. The first two lines in each graph show schema-only DDL statements: add a new column and increase the size of a VARCHAR column. The third line in each graph shows a DDL statement that affects existing data: decrease the size of a VARCHAR column. To implement this statement, SQL Server scans through the table and ensures that all values fit in the reduced size. A more realistic alteration would perform more work than this, so the results indicate a lower bound on the impact of evo- lution. The gray bar on each graph indicates the period during which this third operation took place. In the Private Tables case (Figure 6(a)), 975 ALTER TA- BLE statements were submitted, 5 for each of the 195 ten- ants. Individual schema-only alterations completed very rapidly, but nevertheless had an impact on throughput be- cause there were so many of them. Adding a new column took about 1 minute to complete while increasing the size of a VARCHAR column took about 3 minutes. Decreasing the size of a VARCHAR column took about 9 minutes and produced a significant decrease in throughput. The overall loss of throughput in each case is indicated by the amount of time it took to complete the run. In the Sparse Columns case (Figure 6(b)), the tables are shared and 5 ALTER TABLE statements were submitted. The schema-only changes completed almost immediately and had no impact on throughput. Decreasing the size of a VAR- CHAR column took about 2 minutes, during which through- 885  0  2000  4000  6000  8000  10000  12000  14000  16000  0 5 10 15 20 25 30 T ra n s a c ti o n s  p e r  M in u te Testbed runtime (min) Add new column Increase VARCHAR size Decrease VARCHAR size (a) Private Tables  0  2000  4000  6000  8000  10000  12000  14000  16000  0 5 10 15 20 25 30 T ra n s a c ti o n s  p e r  M in u te Testbed runtime (min) Add new column Increase VARCHAR size Decrease VARCHAR size (b) Sparse Columns Figure 6: SQL Server Throughput put dropped almost to zero. The overall loss of throughput was greater for Private Tables, as indicated by the amount of time it took to complete the runs. However the behavior of Private Tables is probably preferable in the SaaS setting be- cause the throughput drop is never as deep, thus the servers donì°½í²t need to be overprovisioned as much. In any case, nei- ther of these mappings is ideal in that the application should have more control over when such resource-intensive opera- tions occur. 4.2 IBM DB2 Figure 7(a) shows the results of running our testbed on IBM DB2 using three different mappings: Private Tables, Extension Tables, and XML using pureXML. The axes are the same as in Figure 5. In comparison to Private Tables, Extension Tables ex- hibits the same performance variations as in SQL Server. However XML produces a decrease in performance in most cases. The decrease is particularly severe for reads, which re- quire executing a correlated subquery containing an XQuery statement embedded in a call to the XMLTABLE function, as described in Section 3.2. Figure 7(b) shows a break down of the Private Table and XML results by tenant size. Re-  1  10  100  1000  10000  100000 Sel 1 Sel 50 Sel 1000 R eport Ins 1 Ins 50 Ins 1750 U pd 1 U pd 100 R e s p o n s e  T im e  [ m s e c ] Query Classes Private Tables Extension Tables XML (a) Overall  1  10  100  1000  10000  100000 Sel 1 Sel 50 Sel 1000 R eport Ins 1 Ins 50 Ins 1750 U pd 1 U pd 100 R e s p o n s e  T im e  [ m s e c ] Query Classes Private Table (Small Tenant) Private Table (Medium Tenant) Private Table (Large Tenant) XML (Small Tenant) XML (Medium Tenant) XML (Large Tenant) (b) By Tenant Size Figure 7: DB2 Performance call from Section 2 that larger tenants have more extension fields, ranging from 0 to 100. The results show that for reads, the performance decrease of XML is proportional to the number of extension fields. Note that in the Insert 1750 case, the results do not include the time to construct the XML document (for no particularly good reason) and there is no variation based on tenant size. XML gives the application complete control over schema evolution. In this setting, the application is responsible for performing any bulk transformations associated with schema alterations that impact existing data. To st",Stefan Aulbach,"Technische Universit?t M?nchen, Germany",stefan.aulbach@in.tum.de,Dean Jacobs,"SAP AG, Walldorf, Germany",dean.jacobs@sap.com,Alfons Kemper,"Technische Universit?t M?nchen, Germany",alfons.kemper@in.tum.de,Michael Seibold,"Technische Universit?t M?nchen, Germany",michael.seibold@in.tum.de,,,,,,,,,,,,,,,,,,
20200104,1126,Orestis Polychroniou,Columbia University,orestis@cs.columbia.edu,,A Comprehensive Study of Main-Memory Partitioning and its Application to Large-Scale Comparison- and Radix-Sort,"ABSTRACT Analytical database systems can achieve high throughput main-memory query execution by being aware of the dynam- ics of highly-parallel modern hardware. Such systems rely on partitioning to cluster or divide data into smaller pieces and thus achieve better parallelism and memory locality. This paper considers a comprehensive collection of variants of main-memory partitioning tuned for various layers of the memory hierarchy. We revisit the pitfalls of in-cache parti- tioning, and utilizing the crucial performance factors, we in- troduce new variants for partitioning out-of-cache. Besides non-in-place variants where linear extra space is used, we introduce large-scale in-place variants, and propose NUMA- aware partitioning that guarantees locality on multiple pro- cessors. Also, we make range partitioning comparably fast with hash or radix, by designing a novel cache-resident index to compute ranges. All variants are combined to build three NUMA-aware sorting algorithms: a stable LSB radix-sort; an in-place MSB radix-sort using different variants across memory layers; and a comparison-sort utilizing wide-fanout range partitioning and SIMD-optimal in-cache sorting. To the best of our knowledge, all three are the fastest to date on billion-scale inputs for both dense and sparse key domains. As shown for sorting, our work can serve as a tool for build- ing other operations (e.g., join, aggregation) by combining the most suitable variants that best meet the design goals. 1. INTRODUCTION The increasing main-memory capacity of contemporary hardware allows query execution to occur entirely in mem- ory. If the entire database also fits in RAM, analytical query workloads that are typically read-only need no disk access after the initial load, setting the memory bandwidth as the only performance bound. Since analytics are at the core of business intelligence tasks today, the need for high- throughput main-memory query execution is apparent. ?This work was supported by National Science Foundation grant IIS-0915956 and a gift from Oracle Corporation. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full cita- tion on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGMOD  íí14, June 22?27, 2014, Snowbird, UT, USA. Copyright 2014 ACM 978-1-4503-2376-5/14/06 ...$15.00. http://dx.doi.org/10.1145/2588555.2610522. To maximize memory bandwidth and capacity, a few CPUs can be combined in a shared-memory system using a fast interconnection. Such hardware combines the parallelism of multiple multi-core CPUs with a higher aggregate memory bandwidth. The shared-memory functionality is provided by the non-uniform-memory-access (NUMA) interconnection, adding an additional layer in the memory hierarchy. In a modern multi-core CPU, the best performance is achieved when all cores work in a shared-nothing fashion and the working set is small enough to fit in the fast (and private per core) caches. The same approach was more effi- cient even before the advent of the multi-core era [11], since random RAM accesses are too expensive out-of-cache. Query execution is decomposed into a series of operations, the most time consuming of which are typically joins and aggregations. To speed up these operations using hardware parallelism, we partition into small pieces using the keys, then process each piece independently. For instance, an effi- cient algorithm for joins is to hash partition in parallel until the input is split into cache resident pieces before we execute the join using a hash table [11]. In fact, even in-cache join can further partition into trivial parts with very few distinct items, before executing a nested loop to join them [7]. This paper considers a comprehensive menu of partition- ing options across several dimensions. The three types of partitioning are hash, radix and range partitioning, depend- ing on the function that takes the key as an input and out- puts the destination partition. Partitioning also depends on the layer of the memory hierarchy that it targets, namely in-cache, out-of-cache and across NUMA regions. Finally, we distinguish partitioning variants based on whether they use auxiliary space that is linear to the input size or not. Until recently, prior work used the in-cache versions of partitioning and, if parallel, the non-in-place variant, which can be trivially distributed across threads. In all cases, when the input is larger than the cache, the performance is throt- tled by TLB misses [11] and cache conflicts [14]. Satish et al. [14] suggested in-cache buffering to mitigate TLB misses and Wassenberg et al. [15] used non-temporal writes on cache line sized buffers to facilitate hardware write-combining. Efficient out-of-cache partitioning [14, 15] assumes free access to linear auxiliary space, to write the output. We introduce several in-place out-of-cache partitioning variants utilizing the same crucial performance factors: an in-place method analogous to the shared-nothing non-in-place method; a modified non-in-place method that generates the output as a list of blocks that overwrites the input; and a parallel non-in-place method that combines the previous two. 755 To support scaling to multiple CPUs, we consider how the NUMA layer affects partitioning performance and modify both in-place and non-in-place out-of-cache partitioning to guarantee minimal transfers across NUMA boundaries, also ensuring sequential accesses so that hardware pre-fetching can hide the latency of the NUMA interconnection [1]. All of the above algorithms target the data shuffling part of partitioning and implicitly assume that the partition func- tion is cheap and can be computed at virtually no cost. While this assumption holds for radix and hash partitioning given a suitable hash function choice, the cost of computing a range partition function is higher than the cost to transfer the tuple, especially when the number of partitions increases beyond the TLB capacity. The standard (and slow) imple- mentation is a binary search in a sorted array of delimiters that define the partition ranges. The slowdown is caused by the logarithmic number of cache loads. We introduce a specialized SIMD-based cache-resident index that speeds up range function computation up to 6 times and makes range partitioning a practical choice for many applications. All partitioning variants discussed in this paper are shown in Figure 1, where we also mark our contributions. We ap- ply all variants to design and implement three large-scale NUMA-aware sorting algorithms. We use sorting, rather than joins or aggregations for two reasons. First, because it is a wider problem that can be a sub-problem for both join and aggregation. Second, we can apply all partitioning variants to build unique sorting algorithms, such that each is more scalable in distinct cases depending on input size, key domain size, space requirements, and skew efficiency. The first sorting algorithm we propose is stable least- significant-bit (LSB) radix-sort based on non-in-place out- of-cache radix partitioning [14] where we add two innova- tions. We use hybrid range-radix partitioning to provide perfect load balancing, and guarantee that each tuple will cross NUMA boundaries at most once, even if the algorithm would by default re-organize the entire array in each pass. The second sorting algorithm we propose is an in-place most-significant-bit (MSB) radix-sort that uses all variants of in-place partitioning that we introduce, one for each dis- tinctive level in the memory hierarchy: shared out-of-cache in-place partitioning, shared-nothing out-of-cache partition- ing, and in-cache partitioning. We reuse the range-radix idea of LSB radix-sort for NUMA optimality and load balancing. The third sorting algorithm we propose is a comparison- sort that uses the newly optimized range partitioning. We perform very few out-of-cache range partitioning passes with a very wide fanout until we reach the cache, providing NUMA optimality. In the cache, we employ sorting [6] that scales to the SIMD length, modified to use the cache more effectively. radix hash range partition in-cache non-in-place out-of-cache out-of-cache in-place in-cache shared in block lists in segments sharedshared-nothing NUMA oblivious NUMA aware previously known our contributions Figure 1: Partitioning variants and contributions We use fixed length integer keys and payloads, typical of analytical database applications. We evaluate on both dense and sparse key domains. If order-preserving compression is used [12, 16], any sparse or dense domain with fixed or vari- able length data is compacted into a dense integer domain. Skewed workload distribution can reduce parallelism in some na?íì§¤íì§ve approaches. In our context, when we statically distribute partitions to threads, we ensure that the parti- tions are as balanced as possible. Specifically, we never use radix partitioning to any range of bits to divide the work- load. Instead, we combine range with radix partitioning to guarantee that, if specific bit ranges have very few distinct values, we can find delimiters that split the workload equally among threads, independently of the key value range. We summarize our contributions: ? We introduce several new variants for main-memory partitioning, most notably large-scale in-place parti- tioning and efficient range partitioning, and also guar- antee minimal NUMA transfers across multiple CPUs. ? We combine partitioning variants to design three sort- ing algorithms, all the fastest of their class for billion- scale inputs, and evaluate the best options depending on input size, key domain, available space, and skew. The rest of the paper is organized as follows. Section 2 outlines related work. In Section 3 we describe partitioning variants. In Section 4 we discuss sorting. Section 5 presents our experimental results, and we conclude in Section 6. 2. RELATED WORK We first outline related work on partitioning. Manegold et al. [11] identified the TLB thrashing problem when na?íì§¤íì§vely partitioning to a large number of outputs. Satish et al. [14] introduced efficient out-of-cache partitioning and Wassen- berg et al. [15] identified the significance of write-combining. Manegold et al. [11] proposed partitioning to cache-resident hash tables to join and Kim et al. [7] reused the same design on a multi-core CPU. Wu et al. [17] proposed hardware ac- celerated partitioning for performance and power efficiency. We briefly outline recent work on sorting for modern hard- ware, due to space constraints. Inoue et al. [6] proposed in- cache SIMD-vector comb-sort followed 2-way SIMD merg- ing. Chhugani et al. [5] proposed in-cache sorting networks followed by cyclic merging in buffers to avoid being memory bound. Kim et al. [7] compared sort-merge-join against hash join, projecting that sort-merge-join will eventually outper- form hash with wider SIMD. Satish et al. [14] compared radix-sort and merge-sort in CPUs and GPUs on multiple key domain sizes and concluded in favor of merge-sort. How- ever, the result is based on small arrays and only LSB radix- sort is considered. Wassenberg et al. [15] improved over Satish et al. [14] and claimed that radix-sort is better. Kim et al. [9] studied network-scale sorting maximizing network transfer with CPU computation overlap. Albutiu et al. [1] studied the NUMA effects using sort-merge-join on multiple CPUs with billion-scale arrays. Balkesen et al. [4] claimed that non-partitioning hash joins are competitive, but Balke- sen et al. [3] improved over Blanas et al. [4] and concluded that partitioning joins are generally faster, even without us- ing fast partitioning [14, 15]. Balkesen et al. [2] further im- proved joins on multiple CPUs using fast partitioning. Thus, on the fastest CPUs [3], partitioning appears to be the best choice for both hash joins and radix-sort-merge-joins. 756 3. PARTITIONING 3.1 In-Cache We start by considering the versions that best operate when the table fits in the cache. The non-in-place version (Algorithm 1) uses a separate array from the input to store the output, while the in-place version (Algorithm 2) uses one array for both input and output. Each partition is generated in a single segment. In-cache partitioning can be run in parallel, if the threads operate in a shared-nothing fashion. Algorithm 1 Non-in-place in-cache partitioning i íì§  0 // P : the number of partitions for p íì§  0 to P -1 do offset[p] íì§  i // point at the start of each partition i íì§  i + histogram[p] end for for iin  íì§  0 to |Tin|-1 do t íì§  Tin[iin] // Tin: the input table iout  íì§  offset[f(t.key)] + + // f : the partition function Tout[iout] íì§  t // Tout: the output table end for The simplest non-in-place version does only two random accesses per item. When operating in the cache, we need the output and the offset array to be cache-resident. A slightly more complicated version of the algorithm allows the parti- tioning to happen in-place, by swapping items across loca- tions. In short, we start by reading an item, find the correct partition and the output destination through the offset ar- ray, swap it with the item stored there, and continue for the new item until the cycle is closed. Each item is moved exactly once and we stop when the whole array is covered. Item swaps are performed in cycles of transfers, defined as swap cycles. When the items are processed low-to-high [1], the cycle starts by doing a read and then swaps until it reaches the same location it initially read from, to write back. This case occurs 1/P of time on average but requires branching. In Algorithm 2 below, the partitions are written high-to-low and swap cycles close when all items of a parti- tion have been placed, avoiding branching for every tuple. Algorithm 2 In-place in-cache partitioning i íì§  0 // P : the number of partitions for p íì§  0 to P -1 do i íì§  i + histogram[p] offset[p] íì§  i // point at the end of each partition end for p íì§  iend  íì§  0 while histogram[p] = 0 do p+ + // skip initial empty partitions end while repeat t íì§  T [iend] // T : the input & output table repeat p íì§  f(t.key) // f : the partition function i íì§  ??offset[p] T [i] íì§§ t // swap until i = iend repeat iend  íì§  iend + histogram[p+ +] // skip if empty until p = P or iend 6= offset[p] until p = P 3.2 Out-of-Cache Out-of-cache performance is throttled by increased cache conflicts [14] and cache pollution with output tuples [15]. TLB thrashing occurs when the number of partitions ex- ceeds the TLB capacity [11], unless the entire dataset can be placed in equally few large OS pages to be TLB resident. 3.2.1 Non-in-place To mitigate these problems, prior work [14] proposed using the cache as an intermediate buffer before writing back to memory. Also, when write backs occur, they bypass the higher cache levels entirely and avoid polluting the cache [15]. Recent work [2] uses the same basic technique for out- of-cache radix partitioning during hash join execution. Buffering data for each partition reduces the working set size and eliminates the TLB problem when operating in the buffer. TLB misses still occur, but 1/L of the time, if L is the number of tuples buffered for each partition before writing to output. If the buffer for each partition is exactly as big as a cache line, writing the full cache line to memory is accel- erated by write-combining and avoids polluting the higher cache levels with output data. The partitioning fanout is now bounded by the number of cache lines in the fast core- private cache, rather than the TLB entries. Buffer flushing is optimally done using wider registers [15]. To maximize the cache use, we use the last buffer slot to save the output offset and access one cache line per iteration (Algorithm 3). To extend the above method to multiple columns stored in separate arrays, the standard case in RAM-resident database data, we use one cache line per column in the buffer of each partition. A generic implementation can use one cache line per column and flush it separately depending on the column width. We can also interleave the columns in a single tuple and de-interleave the columns when the buffer is flushed. For example, when partitioning arrays of 32-bit keys and 32-bit payloads, we store 64-bit tuples in the cached buffer. Tuple (de-)interleaving can be accelerated using SIMD. Parallel execution of the non-in-place out-of-cache parti- tioning is trivial. The input can be split to equal pieces, one for each thread. By executing a prefix sum of all individ- ual histograms, one can ensure that each partition output is written in a distinct location. Threads are only synchronized after individual histograms are built. This is the only known technique for parallel partitioning on shared segments. Algorithm 3 Non-in-place out-of-cache partitioning iout  íì§  0 // P : the number of partitions for p íì§  0 to P -1 do buffer[p][L-1] íì§  iout // L: # of tuples per cache line iout  íì§  iout + histogram[p] end for for iin  íì§  0 to |Tin|-1 do t íì§  Tin[iin] // Tin/Tout: the input/output table p íì§  f(t.key) // f : the partition function iout  íì§  buffer[p][L-1] + + buffer[p][iout mod L] íì§  t if iout mod L = L-1 then for ibuf  íì§  0 to L-1 do Tout[iout + ibuf ? L] íì§  buffer[p][ibuf ] // no cache end for buffer[p][L-1] íì§  iout + 1 end if end for 757 3.2.2 In-place, Shared-Nothing Segments Adapting the out-of-cache buffering technique to in-place partitioning requires a more complicated approach. The ba- sic idea is to perform the swaps inside the buffer, so that the sparse RAM locations are accessed only 1/L of the time, re- ducing the overhead from TLB misses. Compared with non- in-place out-of-cache partitioning, which uses the buffer as an intermediate layer to group tuples before writing them, in-place out-of-cache partitioning performs all tuples swaps in the buffer, and accesses RAM one cache line at-a-time. Before the main partitioning loop starts, we load cache lines from all starting partition locations. Item swaps be- tween partitions occur using the last L tuples that are stored in the buffer. When a buffer has swapped all L items, the cache line is streamed to the RAM location it was loaded from and the buffer is re-filled with the next L items of the same partition. Thus, we operate in the buffer (L? 1)/L of the time and do not miss in the TLB. The offsets are stored inside the buffer and the last L items of each partition are handled differently to eliminate branching in the inner loop. If having T contiguous segments per partition (T is the number of threads) is acceptable, then we can run in-place partitioning in parallel. However, unlike the non-in-place variant, generating one segment per partition across threads is impossible with coarse-grained synchronization. Algorithm 4 In-place out-of-cache partitioning i íì§  0 // P : the number of partitions for p íì§  0 to P -1 do end[p] íì§  i i íì§  i + histogram[p] for ibuf  íì§  0 to L-1 do buffer[p][ibuf ] íì§  T [i? (i mod L) + ibuf ] end for item0[p] íì§  buffer[p][0] // save 1st item out of buffer buffer[p][0] íì§  i [...] // special handling for partitions smaller than L end for p íì§  0 while histogram[p] = 0 do p+ + // skip initial empty partitions end while t íì§  T [0] // T : the input & output table loop repeat p íì§  f(t.key) // f : the partition function i íì§  ??buffer[p][0] buffer[p][i mod L]  íì§§ t // swap until i mod L = 0 // L: # of tuples per cache line [...] // (rare) branch for end of partition (exits here) for ibuf  íì§  0 to L-1 do T [i] íì§  buffer[p][ibuf ] // no cache end for for ibuf  íì§  0 to L-1 do buffer[p][ibuf ] íì§  T [i? L] // cache end for t íì§  item0[p] item0[p] íì§  buffer[p][0] buffer[p][0] íì§  i if i ? end[p] < L then [...] // (rare) branch for last L items of partition end if end loop 3.2.3 In-place, List of Blocks For large-scale out-of-cache partitioning, the requirement of producing all partitions in P non-splitting segments can be relaxed. Instead of writing each partition output sequen- tially, we can write large blocks that only contain items from a single partition. When the block is full, we get a new block at some new available location. The block size must be large enough to amortize sequential writes, but not too large, in order to avoid external fragmentation from non-full blocks. To access data from a single partition only, we create a small linked list that connects all blocks that contain data of the same partition. While the access is not entirely se- quential as in the single segment case, the list hops after scanning each block are amortized by a sufficient block size. This method can be done in place, if we remove P  íì§B items from the start of the input and save it in private space (B is the block capacity in tuples). We start range partitioning the input from the (P  íì§B)-th tuple. By the time any partition is filled, the input pointer will have advanced enough for the output to safely use the space of input we read before, without overwriting tuples not yet read. At the end, we also add the data initially copied out back to the correct block lists. For each partition, only the last block of the block list can be non-full. Thus, unused space has an upper bound of P  íì§B and is negligible compared to size of the input. Block-based partitioning has a number of nice properties. First, it uses the fast non-in-place out-of-cache partition- ing. Second, it does not require the pre-computation of a histogram. Third, it can be done in place by ensuring no overlap between input and output data. Finally thread par- allelism is trivial; the only requirement is to connect the linked lists of blocks from all threads for each partition. 3.2.4 In-place, Shared Segments In order to partition and shuffle data in parallel inside the same segment, we need fine-grain synchronization. Since us- ing OS latches are overly expensive, we use atomic instruc- tions. Atomic fetch-and-add reads a memory location, in- crements it by some value and returns its previous value. Imagine an array of items and multiple threads where each item must be processed by exactly one thread. Each thread can safely use item at index i which is returned by invok- ing fetch-and-add(c,1) on a shared counter c. When done, the thread asks for the next item to process or terminates if i exceeds the number of items. We apply the same idea to in-place partitioning using one shared counter for each partition to represent the number of tuples swapped so far. We use fetch-and-add on the shared counter of partition p, to  íì§¸lock íì§¹ the cell of the next yet unread item of partition p. We store the first index that initiates the cycle. After swapping an arbitrary number of keys, when we return to the original partition p, we store the last tuple in the initial location. Only the P counters are shared across threads. As mentioned in Section 3.1, we define a swap cycle as a sequence of swaps that starts by reading a key from a specific partition and after a number of swaps, returns to the same partition to write a key in the initially read location. We cannot know in advance how large a swap cycle will be, thus we cannot lock all locations the cycle will go through before actually moving tuples. Imagine a scenario where the first partition has only one item found in the last cell of the array. Then, one thread would perform a single swap cycle covering all items before the last cell is reached and the cycle is closed. 758 To solve this first problem, threads lock only one location at a time for one swap. However, when close to comple- tion, multiple threads may compete for swap cycles, creating deadlocks. For example, assuming one item per partition, if thread t1 reads item kx (must go to lx) from location ly (must bring ky here) and thread t2 reads kz (must go to lz) from lx (must bring kx here), t1 will find no space for kx, be- cause the offset of partition X was incremented by t2 when it read kz. If t1 waits, t2 will reach ky. Then, a deadlock will occur, since t1 holds (kx, ly) and t2 holds (ky, lx). To solve this second problem and avoid waiting for others, when a thread finds a partition to be full, it records both the current key and the locked location that the swap cycle started from. In the above example, t1 records (kx, ly) and t2 records (ky, lx). A final fix step occurs  íì§¸offline íì§¹ and takes trivial time, as the number of such pairs is upper bounded by the number of partitions P , times the number of threads. So far, we presented a way for multiple threads to partition items in-place concurrently, but this solution is impractical if used as is. First, we make no use of buffering to improve out-of-cache performance and second, for each key we move to its destination, we update a shared variable triggering cache invalidations on every tuple move. To make this ap- proach practical, we change the unit of transfer from tuples to blocks. Each block must have a fixed size and all tuples must belong to the same partition. We generate such blocks using the technique described previously (see Section 3.2.3). Out-of-cache accesses are amortized by the block size, as is the synchronization cost of accessing shared variables. Algorithm 5 Synchronized in-place partitioning Pactive  íì§  {} // Pactive: set of yet unfinished partitions Tdeadlock  íì§  {} // Tdeadlock: set of tuple & location pairs i íì§  0 // P : the number of partitions for p íì§  0 to P -1 do Pactive  íì§  Pactive + {p} offset[p] íì§  i i íì§  i + histogram[p] end for while |Pactive| > 0 do p íì§  any  íì¨ Pactive i íì§  used[p] + + // atomic fetch-and-add if i  íí histogram[p] then Pactive  íì§  Pactive? {p} goto loop-end end if ibeg  íì§  i + offset[p] t íì§  T [ibeg] // T : the input & output table pnext  íì§  f(t.key) // f : the partition function while p 6= pnext do i íì§  used[p] + + // atomic fetch-and-add if i  íí histogram[pnext] then Tdeadlock  íì§  Tdeadlock + {t, iinit} goto loop-end end if i íì§  i + offset[pnext] T [i] íì§§ t // swap pnext  íì§  f(t.key) end while T [ibeg] íì§  t loop-end: end while [...] // handle tuples that could cause deadlock (Tdeadlock) 3.3 Across NUMA Moving RAM-resident data across multiple CPUs raises questions about the effectiveness of NUMA RAM transfers. Accessing remote memory locations goes through an inter- connection channel that issues operations to remote RAM modules, increasing the latency. Normally, random accesses are much slower than sequential access and the gap increases when the accesses reference remote RAM regions and go through the CPU interconnection. Prior work [1] proposed doing sequential accesses to remote memory, since hardware pre-fetching hides the latency. To avoid imbalanced use of the NUMA layer when all transfers are directed to a subset of CPUs, we can pre-schedule the transfers and supervise them via synchronization to ensure load balancing [10]. One way to make NUMA-oblivious code scale on multiple CPUs is to allocate both arrays to be physically interleaved across all RAM regions. The OS can support interleaved al- location, where the physical locations of a single array are in- terleaved across all NUMA regions. Randomization of page placement balances accesses across the NUMA interconnec- tion, but precludes NUMA locality. Thus, if we do random accesses, we pay the extra NUMA latency. Cache-line buffer- ing, used by out-of-cache partitioning to avoid TLB misses and facilitate write-combining, also mitigates the NUMA overhead. Still, we measured out-of-cache partitioning to be up to 55% slower on four NUMA regions on interleaved space. The overhead for single tuple random access is higher. A more NUMA-friendly allocation is to split space into large segments bound to a specific region. We can have one segment per thread or one segment per NUMA region. We use the second approach for sorting (see Section 4.1). 3.3.1 Non-in-place Using NUMA-bound segmented allocation for threads or CPUs and if extra space is allowed, we can ensure that all tuples will cross the NUMA boundaries at most once. We use shared-nothing partitioning locally and then use a sep- arate step to shuffle across CPUs. We can use the NUMA interconnection in a balanced way without manual sched- ules [10]. We distribute each segment across all threads of the destination CPU, and do the transfers in a per thread random order. Since some tuples are already on destination, the expected number of transfers is (x? 1)/x for x regions. The NUMA-oblivious partitioning might perform faster than the two step method of shared-nothing partitioning followed by NUMA shuffling, since out-of-cache partition- ing mitigates latencies. The decision to guarantee minimal transfers by incurring shuffling, depends on the hardware. 3.3.2 In-place Assuming NUMA-bound segmented allocation, the only in-place variant where threads do not work in a shared- nothing fashion is during block shuffling (see Section 3.2.4). During the phase of block shuffling on multiple NUMA re- gions, threads can read and write blocks from all regions, but all",Orestis Polychroniou,Columbia University,orestis@cs.columbia.edu,Kenneth A. Ross,Columbia University,kar@cs.columbia.edu,,,,,,,,,,,,,,,,,,,,,,,,
20200105,1388,Yang Cao,"RCBD and SKLSDE Lab, Beihang University",yang.cao@ed.ac.uk,,Making Pattern Queries Bounded in Big Graphs,"Abstract  It is cost-prohibitive to find matches Q(G) of a pattern query Q in a big graph G. We approach this by fetching a small subgraph GQ of G such that Q(GQ) = Q(G). We show that many practical patterns are effectively bounded under access constraints A commonly found in real life, such that GQ can be identified in time determined by Q and A only, independent of the size |G| of G. This holds no matter whether pattern queries are localized (e.g., via subgraph isomorphism) or non-localized (graph simulation). We provide algorithms to decide whether a pattern Q is effectively bounded, and if so, to generate a query plan that computes Q(G) by accessing GQ, in time independent of |G|. When Q is not effectively bounded, we give an algorithm to extend access constraints and make Q bounded in G. Using real-life data, we experimentally verify the effectiveness of the approach, e.g., about 60% of queries are effectively bounded for subgraph isomorphism, and for such queries our approach outperforms the conventional methods by 4 orders of magnitude. I. INTRODUCTION Given a pattern query Q and a graph G, graph pattern matching is to find the set Q(G) of matches of Q in G. It is used in, e.g., social marketing, knowledge discovery, mobile network analysis, intelligence analysis for identifying terrorist organizations [25], and the study of adolescent drug use [17]. When G is big, graph pattern matching is cost-prohibitive. Facebook has 1.26 billion nodes and 140 billion links in its social graph, about 300PB of user data [28]. When the size |G| of G is 1PB, a linear scan of G takes 1.9 days using SSD with scanning speed of 6GB/s. Worse still, graph pattern matching is intractable if it is defined with subgraph isomorphism [31], and it takes O((|V |+ |VQ|)(|E|+ |EQ|))-time if we use graph simulation [20], where |G| = |V |+|E| and |Q| = |VQ|+|EQ|. Can we still efficiently compute exact answers Q(G) when G is big while we have constrained resources, such as a single processor? We approach this by making big graphs small, capitalizing on a set A of access constraints, which are a combination of indices and simple cardinality constraints defined on the labels of neighboring nodes of G. We determine whether Q is effectively bounded under A, i.e., for all graphs G that satisfy A, there exists a subgraph GQ  íì¨ G such that (a) Q(GQ) = Q(G), and (b) the size |GQ| of GQ and the time for identifying GQ are both determined by A and Q only, independent of |G|. If Q is effectively bounded, we can generate a query plan that for all G satisfying A, computes Q(G) by accessing (visiting and fetching) a small GQ in time independent of |G|, no matter how big G is. Otherwise, we will identify extra access constraints on an input G and make Q bounded in G. A large number of real-life queries are effectively bounded under simple access constraints, as illustrated below. award year movie actressactor country 2011-2013u 1 u 2 u 3 u 4 u 5 u 6 Fig. 1. Pattern query Q0 on IMDb Example 1: Consider IMDb [22], a graph G0 in which nodes represent movies, casts, and awards from 1880 to 2014, and edges denote various relationships between the nodes. An example search on IMDb is to find pairs of first-billed actor and actress (main characters) from the same country who co- stared in a award-winning film released in 2011-2013. The search can be represented as a pattern query Q0 shown in Fig. 1. Graph pattern matching here is to find the set Q0(G0) of matches, i.e., subgraphs G íí of G0 that are isomorphic to Q0; we then extract and return actor-actress pairs from each match G íí. The challenge is that G0 is large: the IMDb graph has 5.1 million nodes and 19.5 million edges. Add to this that subgraph isomorphism is NP-complete. Not all is lost. Using simple aggregate queries one can readily find the following real-life cardinality constraints on the movie dataset from 1880?2014: (1) in each year, every award is presented to no more than 4 movies (C1); (2) each movie has at most 30 first-billed actors and actresses (C2), and each person has only one country of origin (C3); and (3) there are no more than 135 years (C4, i.e., 1880-2014), 24 major movie awards (C5) and 196 countries (C6) in total [22]. An index can be built on the labels and nodes of G0 for each of the constraints, yielding a set A0 of 8 access constraints. Under A0, pattern Q0 is effectively bounded. We can find Q0(G0) by accessing at most 17923 nodes and 35136 edges in G0, regardless of the size of G0, by the following query plan: (a) identify a set V1 of 135 year nodes, 24 award nodes and 196 country nodes, by using the indices for constraints C4-C6; (b) fetch a set V2 of at most 24 íì© 3 íì© 4 = 288 award-winning movies released in 2011?2013, with no more than 288 íì© 2 = 576 edges connecting movies to awards and years, by using those award and year nodes in V1 and the index for C1; (c) fetch a set V3 of at most (30+30)?288 = 17280 actors and actresses with 17280 edges, using V2 and the index for C2; (d) connect the actors and actresses in V3 to country nodes in V1, with at most 17280 edges by using the index for C3. Output (actor, actress) pairs connected to the same country in V1. The query plan visits at most 135 + 24 + 196 + 288 + 17280 = 17923 nodes, and 576 + 17280 + 17280 = 35136 978-1-4799-7964-6/15/$31.00 ? 2015 IEEE ICDE Conference 2015161 edges, using the cardinality constraints and indices in A0, as opposed to tens of millions of nodes and edges in IMDb. 2 This example tells us that graph pattern matching is feasible in big graphs within constrained resources, by making use of effectively bounded pattern queries. To develop a practical ap- proach out of the idea, several questions have to be answered. (1) Given a pattern query Q and a set A of access constraints, can we determine whether Q is effectively bounded under A? (2) If Q is effectively bounded, how can we generate a query plan to compute Q(G) in big G by accessing a bounded GQ? (3) If Q is not bounded, can we make it  íì§¸bounded íì§¹ in G by adding simple extra constraints? (4) Does the approach work on both localized queries (e.g., via subgraph isomorphism) and non-localized queries (via graph simulation)? Contributions. This paper aims to answer these questions for graph pattern matching. The main results are as follows. (1) We introduce effective boundedness for graph pattern queries (Section II). We formulate access constraints on graphs, and define effectively bounded pattern queries. We also show how to find simple access constraints from real-life data. (2) We characterize effectively bounded subgraph queries Q, i.e., patterns defined by subgraph isomorphism (Section III). We identify a sufficient and necessary condition to decide whether Q is effectively bounded under a set A of access con- straints. Using the condition, we develop a decision algorithm in O(|A||EQ|+||A|||VQ|2) time, where |Q| = |VQ|+|EQ|, and ||A|| is the number of constraints in A. The cost is independent of big graph G, and query Q is typically small in practice. (3) We provide an algorithm to generate query plans for effectively bounded subgraph queries (Section IV). After Q is found effectively bounded under A, the algorithm generates a query plan that, given a graph G that satisfies A, accesses a subgraph GQ of size independent of |G|, in O(|VQ||EQ||A|) time. Moreover, we show that the plan is worst-case-optimal, i.e., for each input Q and A, the largest GQ it finds from all graphs G that satisfy A is the minimum among all worst-case GQ identified by all other query plans. (4) If Q is not bounded under A, we make it instance-bounded (Section V). That is, for a given graph G that satisfies A, we find an extension AM of A such that under AM , we can find GQ  íì¨ G in time decided by AM and Q, and Q(GQ) = Q(G). We show that when the size of indices in AM is predefined, the problem for deciding the existence of AM is in low polynomial time (PTIME), but it is log-APX-hard to find a minimum AM . WhenAM is unbounded, all query loads can be made instance- bounded by adding simple access constraints. (5) We extend the study to simulation queries, i.e., patterns interpreted by graph simulation (Section VI). It is more chal- lenging to cope with the non-localized and recursive nature of simulation queries. Nonetheless, we provide a characterization of effectively bounded simulation queries. We also show that our algorithms for checking effective boundedness, generating query plans, and for making queries instance-bounded can be adapted to simulation queries, with the same complexity. (6) We experimentally evaluate our algorithms using real-life data (Section VII). We find that our approach is effective for both localized and non-localized queries: (a) on graphs G of billions of nodes and edges [1], our query plans outperform the conventional methods that computes Q(G) directly by 4 and 3 orders of magnitude on average, for subgraph and simulation queries, respectively, accessing at most 0.0032% of the data in G; (b) 60% (resp. 33%) of subgraph (resp. simulation) queries are effectively bounded under simple access constraints; and (c) all queries can be made instance-bounded in G by extend- ing constraints and accessing 0.016% of extra data in G; and 95% become instance-bounded by accessing at most 0.009% extra data. Our algorithms are efficient: they take at most 37ms to decide whether Q is effectively bounded and to generate an optimal query plan for all Q and constraints tested. This work is the first effort to study effectively bounded graph queries, from fundamental problems to practical algo- rithms. It suggests an approach to querying graphs: (1) given a query Q, we check whether Q is effectively bounded under a set A of access constraints; (2) if so, we generate a query plan that given a graph G satisfying A, computes Q(G) by accessing GQ of size independent of |G|, no matter how big G grows; (3) if not, we make Q instance-bounded in G with extra simple constraints. The approach works for both localized subgraph queries and non-localized simulation queries. Given the prohibitive cost of querying big graphs, this approach helps even when only limited queries are effectively bounded. In fact, we find that many queries on real-life datasets are actually effectively bounded under very simple access constraints. Moreover, when a finite set of queries is not effectively bounded, we can make them instance-bounded. All proofs of the results of the paper can be found in [3]. Related Work. We categorize related works as follows. Effective boundedness. The study of effective boundedness traces back to scale independence. The latter was proposed [5] to approximately answer relational aggregate queries under certain conditions, for key/value stores. It aims to guarantee that a bounded amount of work is required to execute all queries in an application, regardless of the size of the underlying data. The idea was formalized in [12], along with a notion of access constraints for relational queries. Recently, the notion of [12] is revised in [10] by requiring that the amount of data accessed (i.e., GQ) can be identified in time determined by query Q and access constraints A only, referred to as effective boundedness; it is characterized for SPC queries [10]. This work differs from the previous work in the following. (1) We introduce access constraints on graph data, to specify cardinality constraints on the labels of neighboring nodes, and guide us to retrieve small subgraphs GQ. (2) Under such constraints, we formalize and characterize the effective bound- edness of graph patterns, an issue harder than its counterpart for relational queries [10], [12]. (3) We propose instance boundedness for queries that are not effectively bounded. Resource-bounded and anytime algorithms. Related are also resource-bounded [16] and anytime algorithms [32]. The former study reachability queries and personalized pattern queries, in which some pattern nodes are designated to match 162 fixed nodes in a graph G. It is to compute approximate answers by accessing no more than íì§íì§|G| nodes and edges in G, for íì§íì§  íì¨ (0, 1) [16]. Anytime algorithms [32] allow users either to specify a budget on resources (e.g., running time; known as contract algorithms [33]), or to terminate the run of the algorithms at any time and get intermediate answers (known as interruptible algorithms [19]). Contract anytime algorithms have been explored for (a) budgeted search such as bounded- cost planning [4], [29], [30], [32] under a user-specified budget; and (b) graph search via subgraph isomorphism, to find intermediate approximate answers within the budget, either by assigning dynamically maintained budgets and costs to nodes during the traversal [8], or by deciding search orders based on the frequencies of certain features in queries and graphs [27]. This work differs from the prior work as follows. (1) We aim to compute exact answers for pattern queries in big graphs, as opposed to heuristic answers that may not have a provable accuracy bound. (2) We characterize what pattern queries can be answered exactly within a cost independent of the size of big graph, based on access constraints; in contrast, the prior work does not study under what budget accurate answers are warranted by using the semantics of the data. (3) We study general pattern queries, which may be either localized or non- localized, and may not be personalized [16]. Graph indexing and compression. There are typically two ways to reduce search space. (1) Graph indexing uses pre- computed global information of G to compute distance [11], shortest paths [18] or substructure matching [26]. (2) Graph compression computes a summary Gc of a big graph G and uses Gc to answer all queries posed on G [7], [13], [24]. In contrast to the prior work, (1) we compute exact answers rather than heuristic. (2) Instead of using the same graph Gc to answer all queries posed on G, we adopt a dynamic reduction scheme that finds a subgraph GQ of G for each query Q. Since GQ consists of only the information needed for answering Q, it allows us to compute Q(G) by using GQ much smaller than Gc and hence, much less resources. (3) When Q is effectively bounded, for all graphs G we can find GQ of size independent of |G|; in contrast, |Gc| may be proportional to |G|. Making big graphs small. There have been other techniques for reducing a big graph into small ones, e.g., distribute query answering [23], pattern matching using views [15], and incremental pattern matching [14]. These are complementary to this work and can be readily combined with ours, e.g., our methods can be readily adapted to distributed settings. II. EFFECTIVELY BOUNDED GRAPH PATTERN QUERIES In this section we define access schema on graphs and effectively bounded graph pattern queries. We start with a review of graphs and patterns. Assume an alphabet íì§ííª of labels. Graphs. A data graph is a node-labeled directed graph G = (V,E, f, íì§íì§¯), where (1) V is a finite set of nodes; (2) E  íì¨ V íì©V is a set of edges, in which (v, v íí) denotes the edge from v to v íí; (3) f() is a function such that for each node v in V , f(v) is a label in íì§ííª, e.g., year; and (4) íì§íì§¯(v) is the attribute value of f(v), e.g., year = 2011. u 1 Q 1 A B  íì§ u 2 v 1 v 2 v 3 v 2n A AB B G 1 Cu3 Du4 Cv2n+1 Dv2n+2 Fig. 2. Pattern query Q1 and data graph G1 We write G as (V,E) or (V,E, f) when it is clear from the context. The size of G, denoted by |G|, is defined to be the total number of nodes and edges in G, i.e., |G| = |V | + |E|. Remark. To simplify the discussion, we do not explicitly define edge labels. Nonetheless, our techniques can be readily adapted to edge labels: for each labeled edge e, we can insert a  íì§¸dummy íì§¹ node to represent e, carrying e  íís label. Labeled set. For a set S  íì¨ íì§ííª of labels, we say that VS  íì¨ V is a S-labeled set of G if (a) |VS | = |S| and (b) for each label lS in S, there exists a node v in VS such that f(v) = lS . In particular, when S = ?, the S-labeled set in G is ?. Common neighbors. A node v is called a neighbor of another node v íí in G if either (v, v íí) or (v íí, v) is an edge in G. We say that v is a common neighbor of a set VS of nodes in G if for all nodes v íí in VS , v is a neighbor of v íí. In particular, when VS is ?, all nodes of G are common neighbors of VS . Subgraphs. Graph Gs = (Vs, Es, fs, íì§íì§¯s) is a subgraph of G if Vs  íì¨ V , Es  íì¨ E, and for each (v, v íí)  íì¨ Es, v  íì¨ Vs and v íí  íì¨ Vs, and for each v  íì¨ Vs, fs(v) = f(v) and íì§íì§¯s(v) = íì§íì§¯(v). Pattern queries. A pattern query Q is a directed graph (VQ, EQ, fQ, gQ), where (1) VQ, EQ and fQ are analogous to their counterparts in data graphs; and (2) for each node u in VQ, gQ(u) is the predicate of u, defined as a conjunction of atomic formulas of the form fQ(u) op c, where c is a constant, and op is one of =, >, <,  íí and  íí. For instance, in pattern Q0 of Fig. 1, gQ(year) = year  íí 2011  íì© year  íí 2013. We simply write Q as (VQ, EQ) or (VQ, EQ, fQ). We consider two semantics of graph pattern matching. Subgraph queries. A match of Q in G via subgraph isomor- phism [31] is a subgraph G íí(V  íí, E íí, f  íí) of G that is isomorphic to Q, i.e., there exists a bijective function h from VQ to V  íí such that (a) (u, u íí) is in EQ if and only if (h(u), h(u íí))  íì¨ E íí, and (b) for each u  íì¨ VQ, fQ(u) = f  íí(h(u)) and gQ(íì§íì§¯(h(u))) evaluates to true, where gQ(íì§íì§¯(h(u))) substitutes íì§íì§¯(h(u)) for fQ(u) in gQ(u). Here Q(G) is the set of all matches of Q in G. Simulation queries. A match of Q in G via graph simula- tion [20] is a binary match relation R  íì¨ VQ íì©V such that (a) for each (u, v)  íì¨ R, fQ(u) = f(v) and gQ(íì§íì§¯(v)) evaluates to true, where gQ(íì§íì§¯(v)) substitutes íì§íì§¯(v) for fQ(u) in gQ(u); (b) for each node u in VQ, there exists a node v in V such that (i) (u, v)  íì¨ R, and (ii) for any edge (u, u íí) in Q, there exists an edge (v, v íí) in G such that (u íí, v íí)  íì¨ R. For any Q and G, there exists a unique maximum match relation RM via graph simulation (possibly empty) [20]. Here Q(G) is defined to be RM . Simulation queries are widely used in social community analysis and social marketing [9]. 163 Data locality. A query Q is localized if for any graph G that matches Q, any node u and neighbor u íí of u in Q, and for any match v of u in G, there must exist a match v íí of u íí in G such that v íí is a neighbor of v in G. Subgraph queries are localized. In contrast, simulation queries are non-localized. Example 2: Consider a simulation query Q1 and graph G1 shown in Fig. 2, where G1 matches Q1. Then Q1 is not localized: u2 matches v2, . . . , v2n?2 and v2n, but for all k  íì¨ [2, n], v2k?2 has no neighbor in G that matches the neighbor u3 of u2 in Q. To decide whether u2 matches v2, we have to inspect all the nodes on an unbounded cycle in G1. 2 We will study effective boundedness for subgraph queries in Sections III?V, and then extend the results to non-localized simulation queries in Section VI. To formalize effectively bounded patterns, we first define access constraints on graphs. Access schema on graphs. An access schema A is a set of access constraints of the following form: S  íì§ (l, N), where S  íì¨ íì§ííª is a (possibly empty) set of labels, l is a label in íì§ííª, and N is a natural number. A graph G(V,E, f) satisfies the access constraint if ? for any S-labeled set VS of nodes in V , there exist at most N common neighbors of VS with label l; and ? there exists an index on S for l such that for any S-labeled set VS in G, it finds all common neighbors of VS labeled with l in O(N)-time, independent of |G|. We say that G satisfies access schema A, denoted by G |= A, if G satisfies all the access constraints in A. An access constraint is a combination of (a) a cardinality constraint and (b) an index on the labels of neighboring nodes. It tells us that for any S-node labeled set VS , there exist a bounded number of common neighbors Vl labeled with l and moreover, Vl can be efficiently retrieved with the index. Two special types of access constraints are as follows: (1) |S| = 0 (i.e., ?  íì§ (l, N)): for any G that satisfies the constraint, there exist at most N nodes in G labeled l; and (2) |S| = 1 (i.e., l  íì§ (l íí, N)): for any G that satisfies the access constraint and for each node v labeled with l in G, at most N neighbors of v are labeled with l íí. Intuitively, constraints of type (1) are global cardinality constraints on all nodes labeled l, and those of type (2) state cardinality constraints on l íí-neighbors of each l-labeled node. Example 3: Constraints C1-C6 on IMDb given in Example 1 can be expressed as access constraints ?i (for i  íì¨ [1, 6]): ?1: (year, award) íì§ (movie, 4); ?4: ?  íì§ (year, 135); ?2: movie íì§ (actors/actress, 30); ?5: ?  íì§ (award, 24); ?3: actor/actress íì§ (country, 1); ?6: ?  íì§ (country, 196). Here ?2 denotes a pair movie  íì§ (actors, 30) and movie  íì§ (actress, 30) of access constraints; similarly for ?3. Note that ?4 ? ?6 are constraints of type (1); ?2 ? ?3 are of type (2); and ?1 has the general form: for any pair of year and award nodes, there are at most 4 movie nodes connected to both, i.e., an award is given to at most 4 movies each year. We use A0 to denote the set of these access constraints. 2 Effectively bounded patterns. A pattern query Q is effectively bounded under an access schema A if for all graphs G that satisfy A, there exists a subgraph GQ of G such that (a) Q(GQ) = Q(G); and (b) GQ can be identified in time that is determined by Q and A only, not by |G|. By (b), |GQ| is also independent of the size |G| of G. Intuitively, Q is effectively bounded under A if for all graphs G that satisfy A, Q(G) can be computed by accessing a bounded GQ rather than the entire G, and moreover, GQ can be efficiently accessed by using access constraints of A. For instance, as shown in Example 1, query Q0 is effec- tively bounded under the access schema A0 of Example 3. Discovering access constraints. From experiments with real- life data we find that many practical queries are effectively bounded under simple access constraints S  íì§ (l, N) when |S| is at most 3. We discover access constraints as follows. (1) Degree bounds: if each node with label l has degree at most N , then for any label l íí, l íì§ (l íí, N) is an access constraint. (2) Constraints of type (1): such global constraints are quite common, e.g., ?6 on IMDb: ?  íì§ (country, 196). (3) Functional dependencies (FDs): our familiar FDs X  íì§ A are access constraints of the form X  íì§ (A, 1), e.g., movie íì§ year is an access constraint of type (2): movie  íì§ (year, 1). Such constraints can be discovered by shredding a graph into relations and then using available FD discovery tools. (4) Aggregate queries: such queries allow us to discover the semantics of the data, e.g., grouping by (year, country, genre) we find (year, country, genre)  íì§ (movie, 1800), i.e., each country releases at most 1800 movies per year in each genre. Maintaining access constraints. The indices in an access schema can be incrementally and locally maintained in re- sponse to changes to the underlying graph G. It suffices to inspect ?G  íì¨ NbG(?G), where ?G is the set of nodes and edges deleted or inserted, and NbG(?G) is the set of neighbors of those nodes in ?G, regardless of how big G is. III. EFFECTIVE BOUNDEDNESS OF SUBGRAPH QUERIES To make practical use of effective boundedness, we first answer the following question, denoted by EBnd(Q,A): ? Input: A pattern query Q(VQ, EQ), an access schema A. ? Question: Is Q effectively bounded under A? We start with subgraph queries. The good news is that (a) there exists a sufficient and necessary condition, i.e., a characterization, for deciding whether a subgraph query Q is effectively bounded under A; and better still, (b) EBnd(Q,A) is decidable in low polynomial time in the size of Q and A, independent of any data graph. 164 We prove these results in the rest of the section. A. Characterizing the Effective Boundedness The effective boundedness of subgraph queries is charac- terized in terms of a notion of coverage, given as follows. The node cover of A on Q, denoted by VCov(Q,A), is the set of nodes in Q computed inductively as follows: (a) if ?  íì§ (l, N) is in A, then for each node u in Q with label l, u  íì¨ VCov(Q,A); and (b) if S  íì§ (l, N) is in A, then for each S-labeled set VS in Q, if VS  íì¨ VCov(Q,A), then all common neighbors of VS in Q that are labeled with l are also in VCov(Q,A). Intuitively, a node u is covered by A if in any graph G sat- isfying A, there exist a bounded number of candidate matches of u, and the candidates can be retrieved by using indices in A. Obviously, (a) u is covered if its candidates are bounded by type (1) constraints. (b) If for some ? = S  íì§ (l, N) in A, u is labeled with l and is a common neighbor of VS that is covered by A, then u is covered by A, since its candidates are bounded (by N and the bounds on candidate matches of VS), and can be retrieved by using the index of ?. The edge cover of A on Q, denoted by ECov(Q,A), is the set of edges in Q defined as follows: (u1, u2) is in ECov(Q,A) if and only if there exist an access constraint S  íì§ (l, N) in A and a S-labeled set VS in Q such that (1) u1 (resp. u2) is in VS and VS  íì¨ VCov(Q,A) and (2) fQ(u2) = l (resp. fQ(u1) = l). Intuitively, (u1, u2) is in ECov(Q,A) if one of u1 and u2 is covered by A and the other has a bounded number of candidate matches by S  íì§ (l, N). Thus, we can verify their matches in a graph G by accessing a bounded number of edges. Note that VCov(Q,A)  íì¨ VQ and ECov(Q,A)  íì¨ EQ. The node and edge covers characterize effectively bounded subgraph queries (see [3] for a proof, which uses three lemmas and the data locality of subgraph queries). Theorem 1: A subgraph query Q is effectively bounded under an access schema A if and only if (iff) VCov(Q,A) = VQ and ECov(Q,A) = EQ. 2 Example 4: For query Q0(V0, E0) of Fig. 1 and access schema A0 of Example 3, one can verify that VCov(Q0,A0) = V0 and ECov(Q0,A0) = E0. From this and Theorem 1 it follows that Q0 is effectively bounded under A0. 2 B. Checking Effectively Bounded Subgraph Queries Capitalizing on the characterization, we show that whether Q is effectively bounded under A can be efficiently decided. Theorem 2: For subgraph queries Q, EBnd(Q,A) is in (1) O(|A||EQ|+ ||A|||VQ|2) time in general; and (2) O(|A||EQ|+ |VQ|2) time when either ? for each node in Q, its parents have distinct labels; or ? all access constraints in A are of type (1) or (2). 2 Algorithm EBChk Input: A subgraph query Q and an access schema A. Output:  íì§¸yes íì§¹ if Q is effectively bounded and  íì§¸no íì§¹ otherwise. 1. for each S  íì§ (l, N) in A (S 6= ?) do 2. find all V? uS 7 íì§ (u,N) in Q and add them to íì§íí; /*f(u) = l*/ 3. B := {v  íì¨ VQ | ?  íì§ (fQ(v), N) is in A}; 4. C := B; /*Initialize VCov(Q,A)*/ 5. InitAuxi(L, ct); /*Initialize auxiliary structures*/ 6. while B is not empty do 7. v = B.pop(); 8. for each íì§íì¨ in L[v] do 9. Update (ct[íì§íì¨]); /*Update counter ct[íì§íì¨]*/ 10. if ct[íì§íì¨] = ? and u 6 íì¨ C do /*suppose íì§íì¨: V? uS 7 íì§ (u,N)*/ 11. B := B  íì¨ {u}; C := C  íì¨ {u}; 12. if VQ  íì¨ C and all edges in Q are in ECov(Q,A) then 13. return  íì§¸yes íì§¹; 14. return  íì§¸no íì§¹; Fig. 3. Algorithm EBChk Here |A| denotes the total length of access constraints in A, ||A|| is the number of constraints in A, and a node u íí is a parent of u in Q if there exists an edge from u íí to u in Q. Algorithm. We prove Theorem 2 by providing a checking algorithm. The algorithm is denoted by EBChk and shown in Fig. 3. Given a subgraph query Q(VQ, EQ) and an access schema A, it checks whether (a) VQ  íì¨ VCov(Q,A) and (b) EQ  íì¨ ECov(Q,A); it returns  íì§¸yes íì§¹ if so, by Theorem 1. To check these conditions, we actualize A on Q: for each S  íì§ (l, N) in A (S 6= ?), and each node u in Q with fQ(u) = l, the actualized constraint is V? uS 7 íì§ (u,N), where V? uS is the maximum set of neighbors of u in Q such that (a) there exists a S-labeled set VS  íì¨ V? uS and (b) for each u íí in V? uS , fQ(u íí)  íì¨ S. Actualized constraints help us deduce VCov(Q,A): a node u of Q is in VCov(Q,A) if and only if either ? there exists ?  íì§ (l, N) in A and fQ(u) = l; or ? V? uS 7 íì§ (u,N) and there exists a S-labeled set of Q that is a subset of V? uS  íì¨© VCov(Q,A). When VCov(Q,A) is in place, we can easily check whether EQ  íì¨ ECov(Q,A) by definition and using the actualized constraints, without explicitly computing ECov(Q,A). We next present the details of algorithm EBChk. Auxiliary structures. EBChk uses three auxiliary structures. (1) It maintains a set B of nodes in Q that are in VCov(Q,A) but it remains to be checked whether other nodes can be deduced from them. Initially, B includes nodes whose labels are covered by type (1) constraints in A (line 3). EBChk uses B to control the while loop (lines 5-10): it terminates when B = ?, i.e., all candidates for VCov(Q,A) are found. (2) For each node v, EBChk uses an inverted index L[v] to store all actualized constraints V? uS 7 íì§ (u,N) such that v  íì¨ V? uS . That is, L[v] indexes these constraintsthat can be used on v. (3) For each actualized constraint íì§íì¨ = V? uS 7 íì§ (u,N), EBChk maintains a set ct[íì§íì¨] to keep track of those labels of S that are not covered by nodes in V? uS  íì¨© VCov(Q,A) yet. Initially, ct[íì§íì¨] = S. When ct[íì§íì¨] is empty, EBChk concludes that there 165 is a S-labeled subset of V? uS covered by VCov(Q,A), and thus deduces that u should also be in",Yang Cao,"RCBD and SKLSDE Lab, Beihang University",yang.cao@ed.ac.uk,Wenfei Fan,University of Edinburgh,wenfei@inf.ed.ac.uk,Jinpeng Huai,"RCBD and SKLSDE Lab, Beihang University",huaijp@buaa.edu.cn,Ruizhe Huang,University of Edinburgh,s1335233@sms.ed.ac.uk,,,,,,,,,,,,,,,,,,
20200106,1389,Abdeltawab M. Hendawi,"Department of Computer Science and Engineering, University of Minnesota, Minneapolis, MN, USA",hendawi@cs.umn.edu,,Predictive Tree: An Efficient Index for Predictive Queries On Road Networks,"Abstract Predictive queries on moving objects offer an im- portant category of location-aware services based on the objects-expected future locations. A wide range of applications utilize this type of services, e.g., traffic management systems, location-based advertising, and ride sharing systems. This paper proposes a novel index structure, named Predictive tree (P-tree), for process- ing predictive queries against moving objects on road networks. The predictive tree: (1) provides a generic infrastructure for answering the common types of predictive queries including predictive point, range, KNN, and aggregate queries, (2) updates the probabilistic prediction of the object's future locations dynamically and incrementally as the object moves around on the road network, and (3) provides an extensible mechanism to customize the probability assignments of the object's expected future locations, with the help of user defined functions. The proposed index enables the evaluation of predictive queries in the absence of the objectsì°½í² historical trajectories. Based solely on the connectivity of the road network graph and assuming that the object follows the shortest route to destination, the predictive tree determines the reachable nodes of a moving object within a specified time window T in the future. The predictive tree prunes the space around each moving object in order to reduce computation, and increase system efficiency. Tunable threshold parameters control the behavior of the predictive trees by trading the maximum prediction time and the details of the reported results on one side for the computation and memory overheads on the other side. The predictive tree is integrated in the context of the iRoad system in two different query processing modes, namely, the precomputed query result mode, and the on-demand query result mode. Extensive experimental results based on large scale real and synthetic datasets confirm that the predictive tree achieves better accuracy compared to the existing related work, and scales up to support a large number of moving objects and heavy predictive query workloads. I. INTRODUCTION The availability of hundreds of millions of smart phones [6] in usersì°½í² hands during their movements in daily lives fired the explosion of a vast number of location aware services [5], [11], [20], [29]. Predictive queries [10], [12], [13] offer a fundamental type of location-based services based on usersì°½í² future locations. Common types of predictive spatial queries include predictive range query, e.g., ì°½íµfind all hotels that are This work is partially supported by the National Science Foundation, USA, under Grants IIS-0952977 and IIS-1218168. located within two miles from a userì°½í²s anticipated location after 30 minutesì°½íµ, predictive KNN query, e.g., ì°½íµfind the three taxis that are closest to a userì°½í²s location within the next 10 minutesì°½íµ, and predictive aggregate query, e.g., ì°½íµfind the number of cars expected to be around the stadium during the next 20 minutesì°½íµ. In fact, Predictive queries are beneficial in various types of real applications such as (1) traffic management, to predict areas with high traffic in the next half hour, so appropriate decisions are taken before congestion appears, (2) location- aware advertising, to distribute coupons and sales promotions to customers more likely to show up around a certain store during the sale time in the next hour, (3) routing services, to take into consideration the predicted traffic on each road segment to find the shortest path of a userì°½í²s trip starting after 15 minutes from the present time, (4) ride sharing systems, to match the drivers that will pass by a riderì°½í²s location within few minutes, and (5) store finders, to recommend the closest restaurants to a userì°½í²s predicted destination in 15 minutes. In this paper, we address the problem of how to process pre- dictive queries for moving objects on road networks efficiently. To this end, we introduce a novel index structure, named the Predictive tree (P-tree), proposed to precompute the predicted moving objects around each node in the underlying road network graph over time. The predictive tree is best described as generic and extensible, from a functionality perspective, dynamic and tunable from a performance perspective. A. Challenges Existing studies on predictive query processing have gone a long way in advancing predictive location-based services. However, existing techniques suffer from both functional limitations and performance deficiencies. From a functional perspective, they suffer from one or more of the following limitations: (1) They consider an Euclidean space [9], [28], [25], [30] where objects can move freely in a two dimensional space. Yet, practical predictive location-based services target moving objects on road networks as described by the motivat- ing applications earlier in this section. (2) Many techniques utilize prediction models that must be trained using a massive 978-1-4799-7964-6/15/$31.00 ? 2015 IEEE ICDE Conference 20151215 a mount of objectsì°½í² historical trajectories in order to produce accurate predictions [2], [9], [13], [15], [24], [28]. However, practical scenarios and industrial experience reveal that such historical data is not easily obtainable for many reasons, either due to usersì°½í² privacy and data confidentiality on one side or due to the unavailability of historical data in rural areas on the other side. (3) Most of the previous solutions were designed to support a specific query type only, e.g., [13], [25], [30] support predictive range query, [3], [22], [30] support predictive KNN query, and [9], [24] support predictive aggregate query. B. Approach Before summarizing the contributions of the proposed pre- dictive tree index, we briefly highlight the basic idea of the index in order to build the proper context. Once an object starts a trip, we construct a predictive tree for this object such that the objectì°½í²s start node in the road network graph becomes the root of the tree. The predictive tree consists of the nodes reachable within a certain time frame T from the objectì°½í²s start location. More specifically, we assume that moving objects follow shortest paths during their travel from source to destination [16], [18]. Hence, we organize the nodes inside the predictive tree according to the shortest path from the objectì°½í²s start node, which is marked as the root of the tree. Accordingly, each branch from the root node to any node in the tree represents the shortest route from the root to this node. Then, our prediction is based on a probability assignment model that assigns a probability value to each node inside the objectì°½í²s predictive tree. In general, the probability assignment is made according to the nodeì°½í²s position in the tree, the travel time between the object and this node and the number of the sibling nodes. In practice, the probability assignment process is tricky and varies from one application to another. At each node in the given road network that is indexed in R-tree, we keep track of a list of objects predicted to appear in this node, along with their probabilities, and travel time cost from the objectsì°½í² current locations to this node. This list represents a raw precomputed answer that can be customized according to the type of the received query (i.e., point, range or kNN predictive query) at query processing time. When an object moves from its current node to a different node, we incrementally update the predictive tree by pruning all nodes in the tree that are no longer accessible through a shortest route from the objectì°½í²s new location. Mostly, this pruning shrinks the number of possible destinations, yet, increases the focus of the prediction. Consequently, the precomputed answer at each node in the object predictive tree is updated to accommodate the effect of the objectì°½í²s movements. This update is reflected to the original nodes in the road network. When a predictive query is received, we fetch the up-to-date answer from the node of interest and compile it according to the query type. To adjust the behavior of the predictive tree and, hence, control the overall predictive query processing performance, we leverage two tunable parameters, a maximum time T and a probability threshold P . These parameters compromise between the maximum prediction time a predictive tree can support and the details in the reported query results on one side, and system resources overheads, i.e., CPU and memory, on the other side. The proposed predictive tree is implemented within the iRoad framework. The iRoad offers two query processing modes of leveraging the predictive tree to control the inter- action between its components: (1) the precomputed query result mode, in which the predicted results are computed and materialized in advance; and (2) the on-demand query result mode which is a lazy approach that postpones all computation till a query is received. C. Contributions In general, the contributions of this paper can be summa- rized as follows: ? We propose a novel data structure named Predictive tree (P-tree) that supports predictive queries against moving objects on road networks. ? We introduce a probability model that computes the like- lihood of a node in the road network being a destination to a moving object. The probability model is introduced to the predictive tree as a user defined function and is handled as a black box by the index construction and maintenance algorithms. ? We introduce two tunable parameters T and P that are experimentally proved to be efficient tools to control the predictive tree index, the system performance, the prediction time, and the results details as well. ? We provide an incremental approach to update the pre- dictive tree as objects move around. Hence, we utilize the existing index structure and incur minimal cost in response to the movement of the object. ? We propose the iRoad framework that leverages the introduced predictive tree to support a wide variety of predictive queries including predictive point, range, and KNN queries. ? we provide an experimental evidence based on real and synthetic data that our introduced index structure is efficient in terms of query processing, scalable in terms of supporting large number of moving objects and heavy query workloads, and achieves a high-quality prediction without the need to reveal objectsì°½í² historical data. The remainder of this paper is organized as follows. Sec- tion II sets the preliminaries and defines our problem. Sec- tion III presents the iRoad system. Section IV describes the the predictive tree and its associated construction, maintenance and querying algorithms. Experimental results are presented in Section V. The study of related work is given in Section VI. Finally, Section VII concludes the paper. II. PRELIMINARIES In this section, we formalize the basic predictive query we address in this paper. Then, we define different types of predictive queries that the predictive tree can support within the iRoad framework. After that, we explain the intuition of the leveraged prediction model. 1216 Fig. 1. iRoad System Architecture A. Basic Query In this paper, we focus on addressing the predictive point query as our basic query on the road network. In this query, we want to find out the moving objects with their corresponding probabilities that are expected to be around a specified query node in the road network within a future time period. The example of such query could be like, ì°½íµFind out all the cars that may pass by my location in the next 10 minsì°½í¶. The predictive point query we address in this paper can be formalized as: ì°½íµGiven (1) a set of moving objects O, (2) a road network graph G(N, E, W), where N is the set of nodes, E is the set of edges, and W is the edge weights, i.e., travel times, and (3) a predictive point query Q(n, t), where n ì°½í°í N, and t is a future time period, we aim to find the set of objects R ì°½í°í O expected to show up around the node n within the future time t. The returned result should identify the objects along with their probabilities to show up at the node of interest. For example, within the next 30 mins, object o1 is expected to be at node n3 with probability 0.8, R(Q(n3,30)) = {< o1,0.8>}. B. Extensions We consider the aforementioned predictive point query as a building block upon which our framework can be extended to support other types of predictive queries including: (i) Predictive range query, where a user defines a query region that might contain more than one node and asks for the list of objects expected to be inside the boundaries of that region within a specified future time, (ii) Predictive KNN query to find out the most likely K objects expected to be around the node of interest within a certain time period, and (iii) Predictive aggregate query to return the number of objects predicted to be within a given location in the next specified time duration. C. Prediction Model Our prediction model employed by the introduced predictive tree index structure is based on two corner stones. (1) The assumption that objects follow the shortest paths in their routing trips. The intuition behind this assumption is based on the fact that in most cases, the moving objects on road networks, e.g., vehicles, travel through shortest routes to their destinations [16], [18]. In fact, this assumption is aligned with the observation in [4] that moving objects do not use random paths when traveling through the road network, rather they follow optimized ones, e.g., fastest route. As a result, this (a) Network & Objects (b) Predictive Trees Integrated With R-Tree Fig. 2. Example Of The Proposed Index Structure model prevents the looping case that appears in the traditional turn-by-turn probability model and assigns a probability value for the moving object to turn when facing an intersection [13]. (2)The probability assignment model that assigns a proba- bility value to each node inside the objectì°½í²s predictive tree. In fact, the probability assignment is affected by the nodeì°½í²s position with respect to the root of the tree, the travel time cost between the object in its current location to this node, and the number of the sibling nodes. In general, our predictive tree is designed to work with different probability assignment models. For example, a possible probability model can give higher values to nodes in business areas, e.g., down town, rather than those in the suburbs. In our default probability model, each node in the predictive tree has a value equal to one divided by the number of nodes accessible from the root within a certain time range. III. THE IROAD SYSTEM The proposed predictive tree is implemented in the context of the iRoad System. More precisely, the predictive tree and its construction, maintenance and querying algorithms form the core of the iRoad System. The iRoad System is a scalable framework for predictive query processing and analysis on road networks. The architecture of the iRoad system consists of three main modules, namely, the state manager, the pre- dictive tree builder and the query processor, Figure 1. In this section, we present an overview of the iRoad System and give a brief description of its key components. Moreover, we focus on the interaction and workflow between these components under both the precomputed query result mode and the on- demand query result mode. A. State Manager The state manager is a user facing module that receives a stream of location updates from the moving objects being monitored by the system. The state manager maintains the following data structures. (1) An R-tree [7] that is generated on the underlying road network graph. It differs from the conventional R-tree in that at each leaf node, i.e., a node in the road network, in addition to storing the corresponding MBR, it also keeps track of two lists: (a) current objects that records the pointers to the objects around this node, and (b) 1217 predicted objects that maintains the predicted results of the objects that most likely to show up around that node. (2) A trajectory buffer that stores the most recent one or more nodes in the road network that are visited by the moving object in its ongoing trip. (3) A predictive tree such that root of a predictive tree is the current location of the moving object. Figure 2(a) gives an example of a set of objects moving on a road network, while Figure 2(b) depicts how the predictive trees are integrated within the basic data structures layout to facilitate the processing of predictive queries. As we mentioned, the system can be running under either (1) a precomputed query result mode or (2) an on-demand query result mode. The first is the default mode inside the iRoad framework. In either modes, upon the receipt of a location update of a moving object, the R-tree is consulted and the new location is mapped to its closest node Nnew in the road network. If the new node Nnew is the same as the objectì°½í²s old node Nold, the object movement is not significant enough to change the systemì°½í²s state and no further action is taken. Otherwise, the object has moved to a different node and an evaluation of the impact of the objectì°½í²s movement is triggered in the system. We differentiate between the precomputed and the on-demand query result modes as follows. Precomputed query result mode: In this mode, the predic- tive tree builder is invoked immediately once the moving ob- ject changes its current node and, consequently, the predictive tree is either constructed from scratch or updated in response to the objectì°½í²s movement. Remember that the predictive tree is constructed from scratch if the incoming location update belongs to a new object that is being examined by the system for the first time. Also, the predictive tree is constructed from scratch if Nnew is not a child of the root of the objectì°½í²s in- hand predictive tree. As will be described in Section IV, this case happens if the object decided not to follow the shortest path, e.g., made a u-turn or started a new trip. Otherwise, the tree is incrementally maintained. Note that, in this mode, the trajectory buffer data structure boils down to one single node (i.e., the current node) of the moving object because of the eagerness to update the predictive tree with the receipt of every location update. Hence, the past trajectory is entirely factored in the predictive tree. On-demand query result mode: In this mode, the tra- jectory buffer stores all nodes the moving object passed by since the start of its current trip. Initially, We do not perform any computation until a query is received. Then, we identify the vicinity nodes within the time range determined by the query. Those nodes might contribute in the predicted results. For each object in these nodes, we construct its predictive tree and run a series of updates according to the list of passed nodes in its trajectory buffer, Figure 3. For example, in this figure, nodes A, G, and E are within the time range specified in the query at node B. Then, we construct the predictive tree for each object, O1, O2, O3, in those nodes and update them according the passed nodes by each one. Obviously, O1, O3 will contribute in the predicted objects at node B, while O2 will not contribute as node B is no longer a possible destination Fig. 3. On-Demand Approach for O2 based on its trajectory buffer. Then, we get rid of any data structure, i.e., the predictive trees and predicted results, directly once the query processing is completed and the results are carried back to the query issuer. We ending by adding the objectì°½í²s current node Nnew, i.e,. Node B in this example, to the objectì°½í²s trajectory buffer. B. Predictive Tree Builder The predictive tree builder is the component that encom- passes the predictive tree construction and maintenance algo- rithms. It takes as input, (1) the moving objectì°½í²s trajectory buffer, (2) the moving objectì°½í²s current predictive tree (if exists), (3) the tunable parameters (T and P) that trade the prediction length and accuracy for systemì°½í²s resources, and (4) a user defined probability assigned function. The predictive tree builder reflects the most recent movements of the object (as recorded in the objectì°½í²s trajectory buffer) to the objectì°½í²s predictive tree. Upon the completion of a successful invocation of the tree builder, an up-to-date predictive tree rooted at the objectì°½í²s current location is obtained and the objectì°½í²s trajectory buffer is modified to accommodate the objectì°½í²s current node. The predictive tree builder is invoked in two different ways. In a precomputed query result mode, the builder is invoked by the state manager upon the receipt of every location update. The state manager pushes the incoming location update of a moving object Oi to the predictive tree builder that eagerly reflects the location update in the predictive tree of Oi. Afterwards, the tree builder updates the precomputed query results at every node in the road network that is on the shortest path route from the object Oiì°½í²s current location. In an on-demand query result mode, the predictive tree builder is invoked by the query processor once a query Q is received. The predictive tree builder consults the road network graph and retrieves a list of nodes Nvicinity that are within the time distance determined by the query Q. Then, it pulls, from the state manager, the predictive trees and the trajectory buffers of moving objects whose current nodes are in Nvicinity . In other words, lazy or selective processing of moving objects that are believed to affect the query result is carried over without taking the burden of updating the predictive tree of every single moving object in the system. C. Query processor The main goal behind predictive query processing in the iRoad system is to be generic and to provide an infrastructure for various query types. This goal is achieved by mapping a query type to a set of nodes (Nvicinity) in the road network graph such that the query result is satisfied by predictions 1218 Algorithm 1 Predictive Tree Construction Input: Node n, T ime Range T , Road Network Graph G(N,E,W ) 1: Step 1. Initialize the data structures 2: Set n as the root of the Predictive Tree PT 3: Visited nodes list NL $ ? 4: Min-Heap MH $ ? 5: for all Edge ei connected with n do 6: Insert the node ni ì°½í°í ei íì§¸C MH 7: end for 8: Step 2. Expand the road network and create the predictive tree 9: while the minimum time range Tmin in MH < T do 10: Get the node nmin with Tmin from MH 11: if The nmin /ì°½í°í NL then 12: Insert nmin íì§¸C PT 13: Insert nmin íì§¸C NL 14: for all Edge ej connected with nmin do 15: Insert the node nj íì§¸C MH 16: end for 17: end if 18: end while 19: Return PT associated with these nodes. For predictive point queries as an example, the point query is answered using the information associated with the road network node that is closest to the query point. For predictive range queries, all nodes in the range are considered to compute the query result. For predictive KNN queries, we sort those predicted objects associated with Nvicinity based on their probabilities. Nvicinity is rationally expanded till K objects are retrieved, if visible. In a precomputed query result mode, generic results are prepared in advance and are held in memory. The process is triggered by an update in objectì°½í²s location and the precom- puted results are constructed/updated for all nodes along the shortest path route of that object. Therefore, most of the work is done during the location update time. Upon the receipt of a query, the query processor fetches the precomputed results only from nodes in Nvicinity , adapts them according to the type of the received query and gives a low latency response back to the user. In an on-demand query result mode, nothing is precomputed in advance and all computation will be performed after the receipt of the userì°½í²s query. Nvicinity is identified and the pre- dictive tree of objects whose current node belong to Nvicinity are constructed/upadted as described earlier in this section. Then, the results are collected and adapted to the query type in a similar way to the precomputed result approach. IV. PREDICTIVE TREE In this section, we describe the proposed predictive tree index structure that is leveraged inside the iRoad framework to process predictive queries based on the predicted destinations of the moving objects within a time period T . We first introduce the main idea and the motivation to build the predictive tree. After that, we provide a detailed description for the two main operations in the predictive tree: 1) predictive tree construction, and 2) predictive tree maintenance. The idea of the predictive tree is to identify all the possible destinations that a moving object could visit in a time period T by traveling through the shortest paths. As there may only exist one shortest path from a start node to a destination node, we can guarantee it will be a tree structure (i.e., without any loop). The intuition for constructing the predictive tree with a time boundary T is based on two real facts: 1) most of the moving objects travel through shortest path to their destinations [16], [18], and 2) majority of the real life trips are within a time period, e.g., 19 minutes [17], [18]. As a result, we only need to care about the possible destinations reachable through a shortest route from the objectì°½í²s start location within a bounded time period. Based on that, we build the predictive tree to hold only the accessible nodes around a moving object and assign a probability for each one of them. The predictive trees leveraged in the iRoad system signif- icantly improves the predictive query processing efficiency for two main reasons. 1) The possible destinations of the prediction shrinks as a result of using the time boundary T . Yet, prediction computation is performed on few number of nodes instead of millions of nodes in the underlying road network, e.g., road network of California state in USA has about 1,965,206 nodes and 5,533,214 edges [23]. 2) Inside the predictive tree, we maintain only those nodes with probability higher than a certain probability threshold parameter P , e.g., 10%. By doing this, we cut down the computation overhead consumed for continuously maintaining the predicted results at each node in the predictive trees. Moreover, we control iRoad to focus on those nodes that more likely to be reached by a moving object. Yet, the query reported results can be more reasonable to users. A. Predictive Tree Construction Main idea. When a moving object starts its trip on the road network, we build a predictive tree based on its starting location to predict its possible destinations within a certain time frame T . We propose a best-first network expansion algorithm for constructing predictive tree for time period T , e.g., 30 minutes. We set the objectì°½í²s initial node as the start node, then, we visit the nodes and edges on the road network that are reachable using a shortest path from this start node [21]. The algorithm proceeds to traverse and process the edges in the road network based on the travel time cost from the start node until all the costs to the remaining edges are over T . Algorithm. The pseudo code for the predictive tree con- struction algorithm is given in Figure 1. The algorithm takes the road network G = {N,E,W}, a starting node n and a time range T as input. The algorithm consists of two main steps: ? Initialization. We first initialize the predictive tree under construction by setting the start node n as the root of the tree. We also create the visited nodes list NL to store the nodes that have been processed by the algorithm so far. An empty min-heap, MH , is employed to order the nodes based on its distance to the root node n. After that, we insert the nodes that are directly connected with the 1219 Fig. 4. Example of Constructing And Expanding The Predictive Tree Started At Node A. root node n into the min-heap MH , (Lines from 2 to 7 in Algorithm 1). ? Expansion. We continuously pop the node nmin that is the closest to the root node from the min-heap. Then, we check if that node has been visited by our algorithm before, which means there was a shorter path from the root to this node nmin. If visited nodes list NL does not contain nmin, we insert the node nmin to it as well as a child to the current expanding branch of the predictive tree PT . After that, we insert to the min-heap MH the node nj that is connected with the yet processed node nmin for further expansion. The algorithm stops when the distance between the next closest node in the min- heap is over the boundary T , (Lines from 8 to 18 in Algorithm 1). Example. Figure 4 gives an example for constructing a predictive tree for node A from the given road network. For this example, we set the time period T to 20 minutes. Figure 4(a) gives the original road network structure, where circles represent nodes and lines between nodes represent edges and the number on each edge represents the time cost to traverse that edge. In the first iteration, we start by setting the root of the tree to node A. Then, we insert nodes B and C into the min-heap, as they are the connected ones to the root node A, Figure 4(b). After that, we expand the closest child to the root, B, where we insert D and E into the min- heap MH and put B in the predictive t",Abdeltawab M. Hendawi,"Department of Computer Science and Engineering, University of Minnesota, Minneapolis, MN, USA",hendawi@cs.umn.edu,Jie Bao,"Department of Computer Science and Engineering, University of Minnesota, Minneapolis, MN, USA",baojie@cs.umn.edu,Mohamed F. Mokbel,"Department of Computer Science and Engineering, University of Minnesota, Minneapolis, MN, USA",mokbel@cs.umn.edu,Mohamed Ali,"Institute of Technology, University of Washington, Tacoma , WA, USA",mhali@uw.edu,,,,,,,,,,,,,,,,,,
20200107,1390,Michael Mattig,"Department of Mathematics and Computer Science University of Marburg, Germany",mattig@mathematik.uni-marburg.de,,Kernel-Based Cardinality Estimation on Metric Data,"ABSTRACT The efficient management of metric data is extremely important in many challenging applications as they occur e.g. in the life sciences. Here, data typically cannot be represented in a vec- tor space. Instead, a distance function only allows comparing individual elements with each other to support distance queries. As high-dimensional data suffers strongly from the curse of di- mensionality, distance-based techniques also allow for better handling of such data. This has already led to the development of a plethora of metric indexing and processing techniques. So far, the important problem of cardinality estimation on metric data has not been addressed in the literature. Standard vector-based techniques like histograms require an expensive and error-prone embedding. Thus, random sampling seems to be the best choice for selectivity estimation so far, but errors are very high for mod- erately small queries. In this paper, we present a native cardinality estimation technique for distance queries on metric data based on kernel-density estimation. The basic idea is to apply kernels to the one-dimensional distance function among metric objects and to use novel global and local bandwidth optimization methods. Our results on real-world data sets show the clear advantage of our method in comparison to its competitors. 1 INTRODUCTION Statistics about the distribution of data in a database are used for two very important aspects of data management: query optimiza- tion and data exploration. In query optimization, they allow esti- mating the costs of operations, choosing appropriate algorithms, and computing the order of joins. For very large databases, where computations take a very long time, small in-memory statistics can deliver approximate answers. Those are often sufficient to determine whether it is worth further investigating the data in a particular direction. While one- and multidimensional vector data is very common in traditional applications, there are many domains for which data is in a metric space only. This means data is not describable by a d-dimensional vector, instead there exists only a metric mea- suring distances between pairs of objects. Examples include the life sciences, where e.g. proteins are usually described by their geometrical structure or at least a sequence of amino acids. Mul- timedia data comes in different datatypes such as JPEG or MPEG which are also not appropriate for a relational representation. In such domains there is a severe lack of native statistical sup- port. Thus, a standard approach is to transform metric data into a multidimensional vector space and to apply one of the standard estimation techniques [18]. There are two serious opposing ef- fects. First, a metric embedding causes in general a considerable information loss. In order to alleviate this, the number of dimen- sions needs to be sufficiently high. Second, the well-known curse ? 2018 Copyright held by the owner/author(s). Published in Proceedings of the 21st International Conference on Extending Database Technology (EDBT), March 26-29, 2018, ISBN 978-3-89318-078-3 on OpenProceedings.org. Distribution of this paper is permitted under the terms of the Creative Commons license CC-by-nc-nd 4.0. of dimensionality is already noticeable for a moderate number of dimensions. Thus, statistics provide only accurate results for low-dimensional vector spaces [1]. In this paper, we present the first native method for cardinality estimation of distance queries in metric spaces. The basic idea is to consider the distances of objects in a metric space and to use kernel techniques to estimate the underlying distance distri- bution. By tuning the bandwidth of the kernels and the kernel function, we obtain a robust estimator for the cardinality of dis- tance queries in metric spaces. Moreover, our approach is also beneficial for high-dimensional vector spaces by treating them as metric spaces, thus considering only the distance among objects, to overcome the shortcomings of standard vectorial statistics. The main contributions of this paper are: ? We show the deficiencies of traditional cardinality estima- tion techniques on metric data sets. ? We present the first effective and efficient method for cardinality estimation in metric space. ? Extensive experiments on real-world data show the valid- ity of our approach. The rest of the paper is structured as follows. Section 2 de- scribes several applications for cardinality estimation in metric spaces, formally defines the problem, and emphasizes the dif- ferences of vector and metric data. Section 3 presents related work in the areas of cardinality estimation in general, techniques for embedding metric data into vector space and kernel-based techniques for cardinality estimation. Section 4 presents our distance-based kernel estimator approach in metric space. Sec- tion 5 describes our methods for global and local bandwidth optimization. Section 6 presents our experimental findings. Fi- nally, Section 7 concludes the paper. 2 PRELIMINARIES We first give a formal description of the problem of cardinality estimation on metric data. Then, we discuss several applications that greatly benefit from a suitable solution to this problem. Fi- nally, we discuss the fundamental differences between vector and metric data that lead to the ineffectiveness of established methods. 2.1 Problem Specification Let X be a set of N objects {x1, . . . ,xN }  íì¨ X. These objects are all of a certain type, in particular a type which can differ from Rn . Moreover a distance function distX : X  íì© X  íì§ R+ is given which fulfills the three properties of a metric, namely (a) identity of indiscernibles: distX(x ,y) = 0íì§íì§ x = y, (b) symmetry: distX(x ,y) = distX(y,x) and (c) triangle inequality: distX(x , z)  íí distX(x ,y)+distX(y, z), with x ,y, z  íì¨ X. We will refer to the combination of X and distX as metric data. In mathematics the pair (X,distX) is called metric space. Cardinality estimation for metric data can be formalized as follows: Given a distance queryQ = (xQ , rQ ), with object xQ  íì¨ X     Series ISSN: 2367-2005 349 10.5441/002/edbt.2018.31 and distance rQ  íì¨ R+, efficiently approximate the cardinality of the set {x  íì¨ X | distX(xQ ,x)  íí rQ }. We will refer to the distance rQ as query radius. The true cardinality is denoted by c(Q) and the estimated cardinality by c?(Q). The goal is to minimize the error of the estimation, but also the construction costs and the size, as well as the query time of the estimator. Note that the actual cardinality can, of course, be calculated by computing the distance to every other item in the data set. This requires a linear number of distance calculations. Each of them can be very costly as, e.g., in video dissimilarity. Hence, we want to minimize the computational costs induced by an estimator. 2.2 Applications Cardinality estimation in metric spaces has many applications. Among others we want to mention the domain of Machine Learn- ing and Data Mining, where algorithms are usually based on a distance measure. The most prominent example is the so-called k-nearest neighbor (kNN) classifier which can be used to classify any kind of data, if it is endowed with a distance measure. The basic idea is to retrieve thek objects from a database which have the smallest distances to a certain query object. Assuming that the objects within the database carry a certain class label (e.g. customers of an insurance with label churn or no churn), the query is classified with the majority label from the set of the k nearest neighbors. kNN classifiers are known to be inefficient since for each run of such an algorithm a complete scan of the data set is required. To accelerate this algorithm typically metric index structures [28] are employed that allow the efficient retrieval of elements within a given distance of a query object. However, it is hard to specify the radius for the corresponding queries since the kNN classifier requires rather the set of k nearest neighbors than a certain set of neighbors exhibiting a certain maximal distance to the query object. For calculating the minimum radius, leading to a retrieval of k results, cardinality estimation can be used. For example, [14] make use of cardinality estimation to assign objects to different Locality Sensitive Hashing tables of different radii. This improves kNN queries on data sets where the distances of the k nearest neighbours of items vary greatly over the data set. A single LSH table could not sufficiently answer such queries. Another example is the estimation of densities, e.g. for Bayes Classifiers, where the density around an element x is propor- tional to the amount of elements in a database that are in a small vicinity to x . This vicinity is typically specified by a certain small distance. Obviously, a reliable cardinality estimation approach would increase the efficiency of such classifiers enormously. If a query is expressed as a conjunction of multiple proximity predicates, each of them with a different distance function, car- dinality estimation is useful for computing an efficient order of their computation. In an optimal execution plan queries should be applied in an order that leads to quickly decreasing result sets. Cardinality estimation can be used to answer exactly this question and to find an order in which the different distance measures are to be applied. Applications in which such scenarios occur are, e.g., pharmaceutical chemistry, where different dis- tance measures covering certain requirements are applied onto protein and/or ligand databases to get the final result in form of a very small set of therapeutically effective drugs. In general, this is a metric scenario, since proteins cannot be described on the structural level by vectors without a considerable loss of information. 2.3 Vector Data vs. Metric Data In order to emphasize the fundamental differences of metric data to vector data that lead to the in-applicability of established methods, we now briefly review important properties of a vector space. We limit our discussion to vector spaces over the real numbers. Here, ad-dimensional vector space consists of elements x = (x1, ...,xd )with a real value xi  íì¨ R called coordinate for each dimension. Individual elements can be added to each other and multiplied with scalar values v  íì¨ R. This, e.g., allows to compute the mean of multiple elements which is not possible in a metric space. Thus one of the most basic data summarization operators is not available in a metric space. Furthermore, the coordinates of the vector allow determining the location of an element with respect to other elements. Such a direction cannot be determined in a metric space. A set of vectors can be ordered globally by component-wise sorting or by a space-filling curve [27] that better preserves the proximity of subsequent elements. In contrast, elements in a metric space can only be ordered based on the distance to a single reference object. Furthermore, it is straight-forward to divide a vector space into a finite number of distinct subsets by incrementally subdividing the space along the dimensions. In a metric space such subsets have to be defined using a center object and a radius. In general, such partitions will overlap if the complete data space should be covered. A vector space has ameasure that allows calculating the vol- ume of subspaces and their intersections. In particular, this is the foundation for the definition of a density and a distribution of a data set. The notions of volume, density and distribution are not available in a metric space. Finally, in a vector space, the costs of distance calculations between elements is linear in the number of dimension if an Lp norm (typically p = 2 for Euclidean distance) is used. In a metric space a distance function can be arbitrarily complex, such as e.g. the edit distance between two strings which has a quadratic runtime. Furthermore, we can calculate a bounding box of a vector data set in linear time by finding the minimum and maximum value for each dimension. In contrast, finding the maximum distance between elements in a metric space requires a quadratic number of distance computations. In summary, metric data lacks most of the tools available in traditional scenarios for cardinality estimation. This makes most established methods infeasible as we discuss in the Section 3. However, as discussed previously, metric data appears in many different applications naturally. Furthermore, it supports distance queries, which are also highly relevant for vector data [4]. As our experiments will show later, using distance-based techniques helps lowering the impact of the curse of dimensionality. 3 RELATEDWORK The most basic idea for estimating the size of a query result is to perform the query on a sample of the data and scale up the resulting cardinality by the sample  íís fraction of the total data size. Using Reservoir Sampling [32], a random data selection can be computed in linear time. We can apply this method also on metric data. However, small sample sizes result in underestimates often equal to zero because metric spaces are sparse. Histograms are the most popular technique for cardinality estimation in database systems [18]. They divide a domain into multiple buckets and store the number of contained elements. When estimating the cardinality within a given query range, they 350 approximate the actual cardinality usually by assuming a uniform distribution within the buckets. Computing optimal histograms that minimize the error induced by this assumption is NP-hard [25]. The most prominent example of an efficient heuristic is MinSkew [2]. It recursively subdivides the space by splitting the most skewed bucket until the desired number of buckets is reached. Other techniques like rkHist [11] and R-V histogram [1] start from the leaves of a spatial index structure and merge them together for limiting the amount of buckets. The introduced histograms are, however, not applicable for metric data. There is no straight-forward criterion for subdivid- ing a metric space into a finite amount of disjoint buckets. The missing notion of uniform distribution within a bucket and the unavailability of a volume measure make the incorporation of such buckets into a cardinality estimate impossible. It is possi- ble to transform metric data into vector data in order to build a spatial histogram, though. We can then extract the cardinality estimate for a distance query by calculating the intersection of the query (in form of a hyper-sphere) with the histogram buckets. However, such a transformation into a vector space is costly and introduces an error in form of distance distortions. Compression techniques like wavelets and cosine transfor- mations are also suitable for cardinality estimation [24]. Both techniques are applicable to multi-dimensional vector data and are shown to provide accurate results. They approximate the actual data distribution by means of a basis function and several coefficients, thus drastically reducing the amount of data. The cardinality estimate is computed as a cumulative joint distribu- tion of the individual dimensions of the data set. However, in a metric space we are not able to use these techniques as the data has no such dimensions and there is no notion of a distribution. Another method for approximate query processing is Local Sensitive Hashing (LSH). LSH performs very well on data from a high-dimensional vector space. It is for example used for approx- imate similarity search [15] and thus related to distance queries in metric spaces. There has been work in cardinality estimation of similarity joins using LSH [21]. Also, multiple LSH indexes with different radii can be used for cardinality estimation by counting collisions of hash buckets [14]. However, LSH requires a similarity-preserving hash function which does not universally exist for metric data. A more recent approach uses Machine Learning [4] for car- dinality estimation. It is, to the best of our knowledge, the only method supporting distance queries. The query-driven approach learns to differentiate several prototype queries and predicts the cardinality of unseen queries by assignment to a prototype and subsequent interpolation using regression. The optimization of the query prototypes is performed via gradient descent where the prototype query is moved across the data space. This manip- ulation of a query object is not possible in a metric space. Thus, like the other approaches, this approach is infeasible for metric data, unless it is mapped into a vector space first. A distance preserving mapping of data from a metric space to a vector space is called embedding. The goal is to find for each xi  íì¨ X an embedding yi  íì¨ Rd , such that the induced stress [19] on the distances is minimized. This stress measure incorporates the deviations of the resulting distances among objects with respect to the original distances. There are different approaches available to embed metric data into a vector space [3]. One prominent example is Multidimen- sional scaling (MDS) [20]. It tries to preserve the pairwise dis- tances in vector space by using such a stress function [19] and minimizing it subsequently. This minimization can be performed by eigendecomposition or gradient descent. However, both meth- ods are expensive to compute, and thus, not suitable for very large data sets. Landmark MDS [9] was introduced as an alternative to MDS for big data scenarios. It uses samples of the data called land- marks and applies MDS on them. The remaining points are then embedded based on the distances to the l landmark elements. Kernel estimators [26] are a competitor of histograms which exhibit a fast convergence for 1-dimensional data [7] and have been generalized to multi-dimensional data [16]. Note, that both approaches do not support distance queries on metric data. Here, samples distribute their weight using a kernel function K , e.g. Epanechnikov [12] or Gaussian. This weight corresponds to the probability of data points existing in the vicinity of the sample. One approximates the underlying probability density function f? of a data set at the evaluation point x by using a set of samples S and summing up over all samples: f? (x) = 1 |S |  íì§h íì§2 s  íì¨S K( x?s h ) = 1 |S | íì§2 s  íì¨S Kh (x ? s). Here, h is the smoothing-factor called band- width. The cardinality estimate results from integrating the ker- nel density function within a given rectangle query and scaling the result up. In a d-dimensional vector space typically product kernels are used where the density function is integrated for each dimension separately. This is only feasible for rectangular queries and not for distance queries. Hence, the application of existing kernel-density estimators for distance queries on met- ric data embedded into a vector space is not straight-forward. Approximating the distance query as a hyper-sphere introduces an error that is also influenced by the curse of dimensionality. Our approach makes use of kernels, but we avoid the curse of dimensionality by using the one-dimensional distance function. The choice of the actual kernel function is considered to be of low impact according to the literature [8]. Nevertheless, we consider different kernel functions in the experiment section of this paper. However, the selection of the kernel bandwidth h has a much more crucial impact on the resulting estimator quality. There are two general approaches for the bandwidth selection: global and locally adaptive methods [31]. Using a global (fixed) bandwidth means that all samples and evaluation points use the same bandwidth. One method of obtaining this bandwidth is by minimizing the mean integrated squared error (MISE) [30]. In contrast to traditional applications, the underlying distribution that shall be fitted by the kernel estimator is known in cardinality estimation. It is given by the data itself. This enables other opti- mization techniques than those used in the statistics literature. Recent work [17] used a gradient descent based approach to find the optimal bandwidth for a given set of training queries. They fit a global bandwidth for each dimension of the vector space. However, a global bandwidth is usually not optimal, as the result- ing estimator oversmoothes the distribution in dense regions and undersmoothes in sparse regions of the data set. While the au- thors of [17] were able to exploit the different distributions in the individual dimensions, we found the error of a global bandwidth for different query sizes in our metric scenario to be significantly high. Furthermore, a gradient descent based approach to band- width estimation turned out to get stuck in local optima of poor quality in our experiments. We thus also investigate locally adap- tive kernel estimators that vary the bandwidth either based on 351 ?? ?? ?????(??, ?) ????? ? 0 ???????? Figure 1: The incorporation of a kernel-sample s into the cardinality estimation for a query Q = (xQ , rQ ). The omit- ted y-axis corresponds to the probability density. Algorithm 1: Generic Kernel Estimation Algorithm Input :Kernel function Kh : R íì§ R+, centered at 0 Optimized bandwidths B : X  íì© X  íì© R+  íì§ R+ Samples S  íì¨ X  íì¨ X Total data set size |X | Distance function distX : X  íì© X  íì§ R+ Query Q = (xQ , rQ ) with object and radius Output :Estimated cardinality c?(Q) 1 total  íì§  0.0; 2 foreach s  íì¨ S do 3 h  íì§  B(s,xQ , rQ ); 4 s?  íì§  distX(xQ , s); 5 contribution  íì§   íì§¼ rQ 0 Kh (x ? s?) dx ; 6 total  íì§  total + contribution; 7 end 8 probability  íì§  total/|S |; 9 return ?probability  íì§ |X |?; the sample point or the evaluation point. The latter is also called balloon estimator [30]. Other work in kernel-based techniques for cardinality estima- tion in vector spaces focuses also on improving the efficiency of the estimation process. One approach is reducing the number of samples to a so-called coreset [33] that maximizes both quality and efficiency of the estimator. In the scope of this paper we do not yet consider such improvements but focus on demonstrating the general applicability of kernel estimators to this new scenario of metric data. 4 DISTANCE-BASED KERNEL ESTIMATORS Kernel estimators allow us to overcome a fundamental problem of using a sample directly for estimating the cardinality of a query result. Namely that the information is concentrated at a sample point. In contrast to a histogram we also get a continuous distribution. In a metric space it is, however, not straight-forward how we can apply a kernel function on a sample point, as there are no dimensions in which they could gradually distribute the mass of a sample. The central idea of our proposed technique is therefore to apply the kernel function on the distance to a sample point in order to incorporate the probability of elements in the vicinity fractionally. In the following we show how to incorporate a sample point into the cardinality estimate. Here, the query Q = (xQ , rQ ) with object xQ and radius rQ is located at distance s? B distX(xQ , s) 0.00 0.10 ? 1 0 1 2 3 Bandwidth M ed ia n  of  R el at iv e  E rr or s 0.00 0.10 0 50 0 15 00 25 00 Bandwidth Su m  o f S qu ar ed  E rr or s Figure 2: Influence of the bandwidth on the estimation er- ror on the Moby data set (cf. Section 6) for a fixed query size. The left-hand side shows the median of the relative errors (Equation (2)). The right-hand side shows the sum of squared errors (measureMLS ). from the sample point s . As depicted in Figure 1, we introduce an axis expressing the distance to xQ . For that wemap xQ to x?Q B 0, the origin of the axis. The sample point s is then mapped onto s? . The kernel function Kh is then centered at point s? by subtracting s? from its argument. We take the area under the curve of the kernel function between x?Q and rQ as the contribution of this sample to the cardinality estimate. Algorithm 1 shows the full estimation process. For each sam- ple point we calculate the contribution and compute the sum. For this we first compute the optimized bandwidth by calling the function B for the given sample point and query with object and radius. In case of a global bandwidth, this function ignores the parameters and always returns the same bandwidth. In case of a locally adaptive approach, it either uses the sample or evaluation point (query) to obtain a specific bandwidth. We detail algorithms for computing the bandwidth in the next section. Given the opti- mized bandwidth, the distance between sample and query object, and the radius, we calculate the contribution of the sample to the running total . After all samples are processed, the probability is then the total divided by the number of samples, see line 8. Finally, we scale the resulting probability up by the total data set size and return this value as the cardinality estimate. The general workflow of our technique consists of (1) collect- ing a set S of samples, (2) determining the optimal bandwidths B and (3) applying Algorithm 1 to estimate the cardinality of new queries. In the following we present the process of optimizing the bandwidths. 5 BANDWIDTH OPTIMIZATION It is well-known [31] that the bandwidth of a kernel function has a crucial impact on the resulting cardinality estimate. A too small bandwidth leads to undersmoothing, a too large bandwidth to oversmoothing. The two edge cases are an infinitely small bandwidth that converges to sampling and an infinitely large bandwidth that converges to a uniform distribution. We thus take particular care of finding an optimal value. We distinguish between a global bandwidth for all samples and queries, and locally adaptive methods where the bandwidth is individually fitted to accommodate for sparser and denser regions of the data space. 352 5.1 Global The computation of the optimal global bandwidth for a kernel function and a given data set is an optimization problem. We first formalize this problem and then present our optimization strategy. 5.1.1 Optimization Problem. We want to find a bandwidth h that minimizes the error of estimates for future queries on the given data set. As we do not know the future queries, we extract a set of training queries Q from the data set and minimize the error for these queries. Afterwards, we validate the performance against an independent set of test queries that we extracted from the data set beforehand. We formally define the optimization problem for a fixed kernel function as arg min h ErrorX (h,Q) , (1) where h is the bandwidth,X is the data set and ErrorX a function that computes the error of the queries Q on X for the given bandwidth h. We define an appropriate error measure for Equation (1) in two steps. First, we define an auxiliary function errorX (h,Q) B c?h (Q) ? c(Q) c(Q) , (2) where c?h (Q) is the estimated cardinality using bandwidth h and c(Q) the actual cardinality of queryQ on data setX . This measure differs slightly from the common relative error metric, as we do not take the absolute value in the numerator. This allows us to assess over- and underestimates separately. It returns values in the interval [?1, íí]. Two values are of particular interest: ?1 indicates that the estimator returns simply a result of zero even though there are results contained in the query. On the other hand, an error of zero indicates a perfect result: the estimated cardinality is equal to the true number of elements the query returns. There is no upper bound for our measure. However, one should notice, that a value of 1 means already an overestimation by a factor of 2. To compute the error of a set of queries Q we combine the errors errorX (Q) of the individual queriesQ  íì¨ Q using ameasure M : R |Q |  íì§ R+. M computes for a set of errors E a single value that is then subject to minimization. Two examples for M are the deviation of the median error from zero, and the sum of squared errors (LS for least squares): Mmedian (E) B | median(E) | MLS (E) B íì§2 e  íì¨E e2 . For M  íì¨ {Mmedian ,MLS }, the final optimization problem is defined as arg min h ErrorX (h,Q) = arg min h M({errorX (h,Q) | Q  íì¨ Q}) (3) 5.1.2 Optimization Strategy. The minimization of the error function (3) requires an efficient and robust optimization method. Figure 2 shows the relationship between bandwidth and error for an example data set. On the left-hand side of the plot we ob- serve that starting from an infinitely small bandwidth results first underestimate the true cardinality. A higher bandwidth reduces the error to a certain degree. At some point the bandwidth over- smoothes the distribution, leading to very high overestimations. The right-hand side shows the mean squared errors. While the general trend of the error function is clearly visible, we can also see that the results are noisy. This poses a difficult to find global optimum as the multitude of local optima has to be overcome. A method that has shown to be very effective in practice are Evolution Strategies. An Evolution Strategy (ES) is a global numeric optimization approach inspired by the Darwinian theory of natural selection. We implemented the approach of Beyer and Schwefel [6]. Here, ? parents produce another set of íì§íì§¬ offspring. From the thus ob- tained set of ? + íì§íì§¬ individuals the best ? individuals a",Michael Mattig,"Department of Mathematics and Computer Science University of Marburg, Germany",mattig@mathematik.uni-marburg.de,Thomas Fober,"Department of Mathematics and Computer Science University of Marburg, Germany",thomas@mathematik.uni-marburg.de,Christian Beilschmidt,"Department of Mathematics and Computer Science University of Marburg, Germany",beilschmidt@mathematik.uni-marburg.de,Bernhard Seeger,"Department of Mathematics and Computer Science University of Marburg, Germany",seeger@mathematik.uni-marburg.de,,,,,,,,,,,,,,,,,,
20200108,1391,Michael Vollmer,"Karlsruhe Institute of Technology (KIT) Karlsruhe, Germany",michael.vollmer@kit.edu,,Iterative Estimation of Mutual Information with Error Bounds,"ABSTRACT Mutual Information (MI) is an established measure for linear and nonlinear dependencies between two variables. Estimating MI is nontrivial and requires notable computation power for high estimation quality. While some estimation techniques allow trad- ing result quality for lower runtimes, this tradeoff is fixed per task and cannot be adjusted. If the available time is unknown in advance or is overestimated, one may need to abort the esti- mation without any result. Conversely, when there are several estimation tasks, and one wants to budget computation time between them, there currently is no efficient way to adjust it dynamically based on certain targets, e.g., high MI values or MI values close to a constant. In this article, we present an itera- tive estimator of MI. Our method offers an estimate with low quality near-instantly and improves this estimate in fine grained steps with more computation time. The estimate also converges towards the result of a conventional estimator. We prove that the time complexity for this convergence is only slightly slower than non-iterative estimation. Additionally, with each step our estimator also tightens statistical guarantees regarding the con- vergence result, i.e., confidence intervals, progressively. These also serve as quality indicators for early estimates and allow to reliably discern between attribute pairs with weak and strong dependencies. Our experiments show that these guarantees can also be used to execute threshold queries faster compared to non-iterative estimation. 1 INTRODUCTION Motivation. Detecting and quantifying dependencies between variables is an essential task in the database community [10, 13, 20, 30]. Conventional methods such as correlation coefficients and covariance matrices only detect linear or monotonous depen- dencies.Mutual Information (MI) in turn is an index that captures any linear and nonlinear dependency [1, 5]. Probability distri- butions of the variables in question serve as input to compute the MI. For real-world data however, these distributions are not available. In this case, MI must be estimated based on samples. Various estimators for MI have been proposed [15, 23, 33], and some offer good results even for small samples [15]. However, continuous variables with an unknown distribution continue to be challenging, since their multivariate distribution is substituted only by a limited sample. A prominent approach for estimation of MI between continuous variables without assumption of the distribution is the nearest-neighbor based method by Kraskov et al. (KSG) [19]. While good estimators are available, they are very rigid in their time requirements and regarding the estimation quality. Once the computation has started, they impose a fixed time requirement and do not yield aby preliminary result if they are terminated ? 2019 Copyright held by the owner/author(s). Published in Proceedings of the 22nd International Conference on Extending Database Technology (EDBT), March 26-29, 2019, ISBN 978-3-89318-081-3 on OpenProceedings.org. Distribution of this paper is permitted under the terms of the Creative Commons license CC-by-nc-nd 4.0. M u tu al  I n fo rm at io n Runtime MIT MIfin tT t tfin Figure 1: MI estimation with dynamic time allocation. prematurely. They also are unable to exploit easier queries like whether the MI value is above a certain threshold but instead determined the value. Such features are highly relevant for high- dimensional data and data streams with irregular arrival rate as we showcase with the following two scenarios. Scenario 1. Consider a modern production plant with smart meters installed on each machine. A first step in data exploration is determining which attributes are strongly dependent. For in- stance dependencies among currents or energy consumption may offer insights into production sequences. For this first step, a query like Which pairs of measurements have a MI value above the thresholdMIT ? often suffices. With conventional MI estima- tors, each pair either induces high computational costs, or results are uncertain because of low estimation quality. Scenario 2. Think of a database with financial data and its real- time analysis. To maintain a diverse portfolio, it is important to track the relationships between stocks. Because bids and trades happen irregularly, new information and market prices arrive at irregular speed. Thus, it is not known how much time is available to monitor stock relationships in the presence of incoming data. Current MI estimators cannot adapt during execution. They risk not producing a result in time, or estimates are of low quality. To improve upon these shortcomings, we study estimation of MI with dynamic allocation of computation time. Ideally, such an estimator should not only offer preliminary results, but also indicate its remaining uncertainty. Figure 1 shows exemplary pro- gression over time of such an estimator based on our experiments with real data. The black line indicates the preliminary estimate after a certain runtime, and the gray area shows the (expected) maximum error of the preliminary estimate. To obtain the defin- itive result MIfin, a user would require time tfin. However, he could also stop the estimator as soon as the estimate is above a threshold MIT with certainty, or he can use the preliminary result available after time t. In this work, we focus on iterative estimation of MI in order to offer this functionality. Here, iterative means quickly providing an estimate, but with the option to improve the estimation if there is time left. In other words, improving the estimate with     Series ISSN: 2367-2005 73 10.5441/002/edbt.2019.08 some time available is what we call an iteration. At the same time, an iterative estimator can terminate the estimation, i.e., stop iterating, when the result is good enough. For efficiency, it is important that computations from previous iterations remain useful and are not repeated or discarded in a later iteration. So far, efficient iterative estimators for MI do not exist. Challenges. The most significant feature of an estimator is its quality of estimation. This is even more so for iterative methods because both preliminary and final estimation quality are important. In other words, the estimate should already be useful after a few iterations, and estimation quality must level up to the one of conventional estimators after many iterations. Ideally, this convergence should happen after a known, finite number of iter- ations. In this article, we target at respective formal guarantees. Next, the quality of preliminary estimates is crucial for us- ability. Determining if a preliminary result is good enough or interesting enough to merit additional computation time requires some information on its certainty. The number of iterations alone is insufficient, as the result quality depends on many other fac- tors such as data characteristics, required accuracy and time con- straints. Instead, each estimate requires an individual indicator of the uncertainty remaining. While the time spent to improve the estimate iteratively is committed dynamically, it must of course be used efficiently. Many conventional estimators use data structures that are ex- pensive to build and cheap to use, such as space-partitioning trees [19, 31, 32]. Such an upfront activity is undesirable for an iterative estimator whose first estimate must arrive soon. At the same time, runtime and scalability do remain important charac- teristics of the estimator. In other words, an iterative estimator must feature guaranteed efficiency for both individual iterations and final estimates. Contributions. In this article, we present IMIE, our Iterative Mutual Information Estimator. To prove its practical usefulness, we establish several features both formally and experimentally. Quality of Estimation. In Section 4, we propose a design for IMIE such that estimates converge to the same value as with the KSG. To make early iterations useful, IMIE also offers statistical error bounds for its early estimates. More precisely, an early estimate provides a confidence interval for the final estimate. We describe the specifics and the statistical soundness in Section 4.3. Complexity. We study the time complexity of initialization and of individual iterations of IMIE. In Section 5 we establish an amor- tized time complexity for IMIE and the nearest-neighbor search used. This complexity is competitive with existing non-iterative estimators. To be precise, we show that iterating IMIE until con- vergence is only slightly slower in terms of time complexity than computing the KSG directly with optimal algorithms. Experimental Validation. We show that IMIE complements the formal guarantees established so far with good actual perfor- mance. To do so, we perform extensive experiments using both synthetic and real data sets in Section 6. On the one hand, we show that the concrete runtime and estimation results of IMIE are comparable to the ones of conventional estimation methods. On the other hand, the experiments show the practical benefits of the early results from IMIE. For instance, IMIE finds attribute pairs above a threshold value significantly faster than non-iterative estimators. 2 RELATEDWORK Iterative estimation ofMI is interesting from two perspectives. On the one hand, it is methodically interesting, as it can be considered an anytime algorithm. On the other hand, it is interesting to consider the benefits it provides over current methods in different settings. Important application scenarios are dependency analysis in high dimensional data and data streams, cf. Scenario 1 and 2. Anytime Algorithms. Anytime algorithms [36] use available time to increase their result quality. One can obtain a low-quality result after a short time and a better one when waiting longer. In data analysis, anytime algorithms exist for clustering [22], classification [35] and outlier detection [2]. So far however, there is no anytime algorithm to estimate MI. So while there is no direct competitor, IMIE extends the set of tools available as anytime algorithms. Additionally, there has been more general work on the optimal use of available anytime algorithms [11, 18], which may improve the performance of IMIE in larger systems. MI on Data Streams. Estimating MI on streams has received some attention recently. The MISE framework [14] summarizes a bivariate stream such that the MI for arbitrary time frames can be queried. To this end, MISE offers parameters for the balance between accuracy of older queries and resource requirements both in terms of memory and computation time. In contrast, the DIMID estimator [4] processes a bivariate stream as sliding win- dow for monitoring tasks. This approach provides fast updates between time steps by approximation with random projection. MI estimation in sliding windows has also been the focus of [32]. That paper provides lower bounds for estimates using Equa- tion 5 both in general and for updates in sliding windows. It also features two dynamic data structures, DEMI and ADEMI, to main- tain such estimates using either simple or complex algorithms and data structures. These approaches have limitations. First, they all impose the necessary execution time, i.e., one cannot adapt this time after the start of stream processing. If the rate of new items increases, the estimator may be unable to keep up. If it decreases, the es- timator cannot use this time to improve results. Second, the ap- proaches are all focused on bivariate streams. While MI is defined for exactly two variables, the number of attribute pairs grows quadratically in the number of dimensions. In contrast, the only information IMIE maintains on a stream is based on individual di- mensions and thus scales linearly with the dimensionality. Third, the approximate results of MISE and DIMID are difficult to use. Their estimation quality is only known on average; this average defines the perceived quality of individual estimates. So if one estimate has a very small error, it is less likely to be appreciated, while the error of a particularly bad estimate may be assumed to be smaller. Dependencies in High Dimensional Data. Even though MI is de- fined for exactly two variables, it hasmany applicationswith high- dimensional data. Prominent ones are image registration [25], which uses MI between two high-dimensional variables, and fea- ture selection [24], which targets at the MI between attributes and a classification label. But estimating the MI between all pairs of attributes has received little attention, despite being the non- linear equivalent of correlation matrices. [26] uses a different approach, i.e., kernel density estimation, and removes redundant computations that arise when using this estimator for each pair. This approach has a worse computational complexity than a pair- wise application of the KSG estimator, without offering better 74 0 1 2 3 4 5 6 7 8 9 1 2 4 3 5 MCy1 (p3) = 2 MCx1 (p3) = 3 y1 (p3) x1 (p3)6 7 p3 p1 p2 p4 p5 p6 X = { 1, 3, 4, 5, 8}6 Y = { 1, 3, 4, 5, 7} 2, x y Figure 2: Illustration of terms used for the KSG. results [15, 23]. While both scale quadratically in the number of attributes, their approach is also quadratic in the number of points. The complexity of the KSG in turn is (n logn) [32]. Ad- ditionally, it does not expose any parameter to modify the result quality. Consequently, there would not be any benefit of a direct experimental comparison with IMIE. 3 FUNDAMENTALS We first cover the background of MI and its estimation. Mutual Information. Shannon has introduced the notion of entropy [28] to quantify the expected information gained from ob- serving a value of a random variable.H (X ) stands for the entropy of a random variable X . The expected information of observing two random variables X and Y is the joint entropy H (X ,Y ). Mu- tual Information quantifies the amount of information that is shared or redundant between the two variables. It is defined as I (X ;Y ) = H (X ) + H (Y ) ? H (X ;Y ). (1) With the definition of entropy for continuous variables [6], the MI of two continuous random variables is I (X ;Y ) =  X  Y pXY (x,y) log ( pXY (x,y) pX (x)pY (y) ) dx dy, (2) where pX ,pY and pXY are the marginal and joint probability density functions of X and Y . The type of logarithm used in Equation 2 determines the unit of measurement. In this work we use the natural logarithm. This means that MI is measured in the natural unit of information (nat). Estimation. One can perceive many sources of data, e.g., smart meters or market prices, as random variables with unknown dis- tribution. Since Equation 2 requires probability density functions, we cannot compute the MI of such sources exactly. Instead, we can only estimate the MI based on available samples. The popular estimator that will serve as foundation of our work is the one by Kraskov, St?gbauer and Grassberger [19], which we call KSG. It is based on the estimator for probability densities by Loftsgaarden and Quesenberry [21], which Kozachenko and Leonenko have studied further in the context of entropy [17]. In the following, we briefly review the terms and computation of the KSG. Let P = {p1 = (xp1 ,yp1 ), . . . ,pn = (xpn ,ypn )}  R 2 be a sample from a random variable with two attributes. Figure 2 illustrates the notions that we define in the following using the sample P = {(1, 5), (6, 1), (5, 4), (4, 7), (3, 3), (8, 2)}. Let X = {xp1 , . . . , xpn } and Y = {yp1 , . . . ,ypn } be the set of values per attribute. For each point p  P , its k  N+ nearest neighbors in P using the maximum distance form the set kNN (p). More formally, it is kNN (p) = argmin S (P\{p }) s .t . |S |=k max s S ~p, s~, (3) with ~p, s~ = max(|xp ? xs |, |yp ? ys |). We define the largest distance between xp and any x-value among the k nearest neigh- bors of p as xk (p) = maxs kNN (p) |xp ?xs |. We use this distance xk (p) to define the x-marginal count MCxk (p) = |{x  (X \ xp ) : |x ? xp |   x k (p)}|, (4) which is the number of points whose x-value is close to p. In Figure 2, vertical dashed lines mark the area of points whose x-values are at least as close as the nearest neighbor of p3. Since this area contains three points excluding p3, it is M x 1 (p3) = 3. The distance  y k (p) and the y-marginal count MC x k (p) are defined analogously. Note that xk (p) and  y k (p) may differ, which results in differently sized areas for the marginal counts, as seen in Figure 2. Using these counts, the KSG estimate is defined as I? (P) = ? (n)+? (k)? 1 k ? 1 n n2. i=1 ? ( MCxk (pi ) ) +? ( MC y k (pi ) ) , (5) where ? is the digamma function. This is ? (z) = ?C + 2.z?1 t=1 1 t for z  N+ and C ? 0.577 being the Euler-Mascheroni constant. While k is a parameter of this estimator, it is generally rec- ommended [15, 16, 19] to use a small k , that is k  10. Gao et al. [9] have proven that the KSG is a consistent estimator for fixed k , that is, it converges towards the true value with increasing sample size. 4 ITERATIVE ESTIMATION In this section we present IMIE, our iterative estimator for MI. The core concept of our approach is considering the KSG estimate itself as the mean of a random variable with a finite population. Using subsamples of this population for early estimates offers beneficial properties such as an expected value equal to the KSG estimate and convergence to the KSG for large sample sizes. We first present IMIE and its underlying data structure as well as the algorithms for the initialization and for subsequent iterations. Then we describe our approach for nearest neighbor search, which is better for iterative algorithms than the standard procedures. Finally, we describe the statistical bounds that IMIE provides with its estimates. 4.1 IMIE For brevity, we introduce some notation in addition to the one from Section 3. For a pointp  P , we define the pointwise estimate (p) = ? ( MCxk (p) ) +? ( MC y k (p) ) . (6) The set of all pointwise estimates is  = {(p1), . . . ,(pn )}. Seeing  as a finite population of size n with mean ? , Equation 5 can be rewritten as I? (P) = ? (n) +? (k) ? 1 k ? ? . (7) Using a (random) subsample ?  , its mean ?? is an (unbiased) estimation of ? . This in turn yields an (unbiased) estimate of I? (P), I?? (P) = ? (n) +? (k) ? 1 k ? ?? . (8) 75 Data Structure 1: IMIE struct { Point[] P Real Mean, Var Int k,m Int[] OrderR , Orderx , Ordery Real Offset }; Algorithm 2: Init (P,k) 1 Persist k and P O(n) 2 Mean, Var,m $ 0 O(1) 3 OrderR , Orderx , Ordery $ (0, 1, . . . , |P | ? 1) O(n) 4 Sort Orderx and Ordery O(n logn) 5 Offset$ ? (|P |) +? (k) ? 1k O(1) The variance  2? of our subsample serves as a quality indicator of this approximation, which we further discuss in Section 4.3. The idea of IMIE is to maintain a subsample ? and use I?? (P) to estimate I? (P). Each iteration then increases the sample size of ? by one, to improve the estimate. Starting with an empty set, this means there are exactly |P | iterations before IMIE yields exactly the same result as the KSG, i.e., I?? (P) = I? (P). Data Structure. IMIE uses and stores P and k as well as some additional information listed in Data Structure 1. In the following we use the zero-indexed array notation P[i] = pi+1. Contrary to the original data sample P , we do not store ? explicitly. In- stead we store its mean Mean, its variance Var and size, which is the number of performed iterationsm. To maintain the current variance efficiently, we use the online algorithm by Welford [34]. To ensure that ? is a random subsample of , we need to draw without replacement. To this end, IMIE maintains an array of indices OrderR , where index i at position j means that (pi ) is added to ? in the j-th iteration. The positions of this array are randomly swapped during iterations to perform the random se- lection. This enables a fast selection of a random element without replacement in each iteration. In addition, we maintain two ar- rays Orderx and Ordery containing references to all points in P ordered by their x- and y-value, respectively. For instance, in- dex i at Orderx [0] means that pi has the smallest x-value in P , i.e., pi = argminpP xp . These ordered arrays are used to find nearest neighbors, as described in Section 4.2. Finally, we store the Offset = ? (n) + ? (k) ? 1k . With this, the (preliminary) MI estimate is available as I?? (P) = Offset ?Mean. Methods. We now present the two methods Init and Iterate. See Algorithms 2 and 3, together with amortized time complexi- ties, derived in Section 5. Init ensures the proper state of Data Structure 1 before the first iteration, i.e., preparing all variables assuming that |? | = 0. Observe that Init is a straightforward method for the simple case of static data with two attributes. For other scenarios, such as high-dimensional or streaming data, some adjustments to the initialization may be appropriate, as discussed in Section 5.3. Iterate increases the size of sample ? by one. This requires computing (p) for a random p  P with (p) < ?. Iterate consists of three phases. In the first one (Lines 1-3), we select a random point p of P that has not been selected earlier. After Algorithm 3: Iterate 1 ID$ Draw random integer from [m,n ? 1] O(1) 2 Swap values of OrderR [m] and OrderR [ID] O(1) 3 p $ P[OrderR [m]] O(1) 4 kNN (p) $ NNSearch(p) (see Algorithm 4) O(  n) 5 Compute xk (p),  y k (p) O(1) 6 ComputeMCxk (p),MC y k (p) O(logn) 7 (p) $ ? ( MCxk (p) ) +? ( MC y k (p) ) O(1) 8 m $m + 1 O(1) 9 Diff old $ (p) ? Mean O(1) 10 Mean$ Mean + Diff old m O(1) 11 Diff new $ (p) ? Mean O(1) 12 Var $ Var(m?1)+Diff old Diff new m O(1) m ? 1 iterations, we swap the index at position m of OrderR with the index at a random position behindm ? 1. This ensures that we do not use any index twice, since positions before m are not considered, and that each unused index has the same probability of being selected. This random swap is one step of the Fisher-Yates Shuffle in the version of Durstenfeld [8], which fully randomizes the order of a sequence. The second phase (Lines 4-7) computes (p) using the ordered lists Orderx and Ordery . The last phase (Lines 8-12) performs the online algorithm [34] to maintain mean and variance of a sample, in our case ?. Example 4.1. Disregarding the dashed lines for now, Figure 3 illustrates the state of Data Structure 1 after initialization and before the first iteration. For the first iteration, we draw an in- teger ID from {0, . . . ,n ? 1}. Suppose that we drew 5. We swap the content of OrderR [0] and OrderR [5]. OrderR [0] now contains 6. This means that this iteration adds (p6) to our implicit sam- ple ?. We then determine its nearest neighbor 1NN (p6) = {p15}, the distances x 1 (p6) and  y 1 (p6) as well as the marginal counts MCx 1 (p6) = 1 andMC y 1 (p6) = 3. The dashed lines in Figure 3 illus- trate the area of counted points in x and y-direction, respectively, identically to Figure 2. It follows that (p6) = ? (1)+? (3) = 0.346. Substituting the appropriate variables, the remaining values are set accordingly, i.e.,m = 0+ 1 = 1,Mean = 0+ 0.346 1 = 0.346 and Var = 00+00.346 1 = 0. The second iteration is analogous, draw- ing ID = 6 at random from {1, . . . ,n ? 1}, thus choosing p7. Its nearest neighbor is p8, and the marginal counts areMC x 1 (p7) = 1 andMC y 1 (p7) = 6, cf. the dashed lines in Figure 4. As a result, it is (p7) = ? (1)+? (6) = 1.129. Analogously to the first iteration, the remaining values arem = 1+1 = 2,Mean = 0.346+ 0.783 2 = 0.738 and Var = 01+0.7830.391 2 = 0.153. Figure 4 graphs the state of Data Structure 1 after both iterations, and the new MI estimate is 1.164 ? 0.738 = 0.426. 4.2 Nearest-Neighbor Search A computation-intensive step in Iterate is the computation of nearest neighbors, which also is a key step for static estima- tion with the KSG. The classic solution [19, 31] is using space- partitioning trees, which are optimal in terms of computational complexity [32]. This efficiency is achieved because the slow tree construction is performed once, and each nearest-neighbor search afterwards is fast. Contrary to the traditional KSG esti- mation, it is not known beforehand how many nearest-neighbor searches IMIE performs. Constructing such a tree for IMIE would 76 Mean = 0 Var = 0.153 m = 0 X Y p1 p7 p13p2 p9 p14 p3 9 2 5 10 4 14 3 1 121115 6 16 8 713 92 5104 14 3112 1115 6 168713 P Orderx Ordery OrderR k = 1 Offset = 1.164p12p4 p10 p8 p11 p15 p5 p6 p16 92 5 104 1431 1211 156 1687 13 Figure 3: State of IMIE after initialization. X Y p1 p7 p13p2 p9 p14 p3 9 2 5 10 4 14 3 1 121115 6 16 8 713 92 5104 14 3112 1115 6 168713 P Orderx Ordery OrderR k = 1 Offset = 1.164p12p4 p10 p8 Mean = 0.738 Var = 0 m = 2 925 104 143 1 1211 156 1687 13 p11 p15 p5 p6 p16 Figure 4: State of IMIE after two iterations ((p6) and (p7)). not only delay the first estimate, but may also be an inefficient choice overall if only few iterations take place. The opposite, i.e., searching nearest neighbors without any preparation, is a linear search. Each iteration would then require time linear in the num- ber of data points. Since IMIE should offer both fast iterations and preliminary estimates after a short time, our approach is a compromise between these two options. The general idea is to use sorted arrays to perform a guided linear search that offers a good amortized time complexity (cf. Section 5). In the following, we elaborate on our NNSearch approach. Let p be the point whose nearest neighbor we are searching for and q the nearest neighbor we have found so far. Then any point r with |xp ?xr | > ~p?q~ cannot be a nearest neighbor with the maximum norm. This means that we only have to consider the interval [xp ? ~p?q~, xp + ~p?q~] in the sorted array Orderx . When we find a closer point during the search, this interval gets smaller, and fewer points need to be considered. For the y-values, this is analogous. To reduce the number of worst-case scenarios, we perform this search simultaneously in both directions and terminate when either one terminates. See Algorithm 4 for an algorithmic description of NNSearch. Example 4.2. Figure 5 illustrates an exemplary run of this procedure for k = 1. The figure shows four states corresponding to the variables of NNSearch(p) after 0, . . . , 3 loops. The query point p is the filled square, and a projection of the points to their x- and y-coordinates is shown at the bottom and the left side, respectively. These projections indicate the order of points in Orderx and Ordery , respectively. Each state after the first loop also illustrates the variables of NNSearch. The nearest neighbor found so far is marked with a circle and is labeled NN , and the distance max = ~p ? NN ~ is used for the dashed lines that highlight the remaining area of nearest neighbor candidates. Points accessed via Orderx in a previous iteration are marked with a diagonal stripe from the upper left to the lower right. This is done analogously for Ordery . Each loop considers the next loops = 0 X Y p loops = 0 X Y max ?y? ?y+ ?x+ ?x? p 1 NN NN loops = 0 X Y ?y+ ?y? ?x+ ?x? p 2 max NN loops = 0 X Y ?y+ ?y? ?x+ ?x? p 3 max Figure 5: Illustration of Algorithm 4 for each loop. unmarked point in both directions for both Orderx and Ordery . Additionally, the small arrows illustrate the minimal distances ?? for any further point accessed when iterating over Orderx or Ordery in the respective direction. After the third loop, the arrows of ?y+ and ?y? both exceed the area of the remaining candidates, represented by the dashed lines. This means that all relevant candidates have been considered via Ordery , and that the current nearest neighbor is correct. 4.3 Statistical Quality Indicators Finally we present statistical guarantees for early estimates by IMIE. Since ? is a subsample of , statistical tests with ?? and  2 ? yield statistically significant assertions regarding ? . Equations 7 and 8 give way to analogous assertions for I? (P). Theorem 4.3 ([27]). Let  be a finite population of size n with mean ? and a variance  2  . When drawing an i.i.d. sample ? of size m from , the sample mean ?? has an expected value of E(?? ) = ? and a variance of  2?? =  2 m ( n?m n?1 ) . Proof. See [27].  While the classic version of the Central Limit Theorem is not formulated for finite populations, it has been proven that some variations are applicable, and that ?? is approximately normally distributed [27]. In other words, drawing a sample of sizem with a sample mean ? is as likely as drawing ? from N(? ,?? ) with ?? =   2?? . So we can estimate the probability that a sample mean ?? is off by more than a specified value ? > 0 by using the cumulative distribution function  of the standard normal distributionN(0, 1). This is illustrated in Figure 6 and is formally described as Pr[|?? ? ? | ? ?] = 2   ( ?? ?? ) . (9) 77 Algorithm 4: NNSearch(p) 1 ix , iy $ index of p in Orderx , Ordery , respectively 2 ?x+,?x?,?y+,?y?, loops$ 0 3 max $ 4 NN $ {} 5 while min(?x?,?x+) < max m",Michael Vollmer,"Karlsruhe Institute of Technology (KIT) Karlsruhe, Germany",michael.vollmer@kit.edu,Klemens B?hm,"Karlsruhe Institute of Technology (KIT) Karlsruhe, Germany",klemens.boehm@kit.edu,,,,,,,,,,,,,,,,,,,,,,,,
20200109,1393,Chris Mayfield,"Purdue University West Lafayette, Indiana, USA",cmayfiel@cs.purdue.edu,,ERACER: A Database Approach for Statistical Inference and Data Cleaning,"ERACER: A Database Approach for Statistical Inference and Data Cleaning, ERACER: A Database Approach for Statistical Inference and Data Cleaning, ERACER: A Database Approach for Statistical Inference and Data Cleaning, ERACER: A Database Approach for Statistical Inference and Data Cleaning, ERACER: A Database Approach for Statistical Inference and Data Cleaning, ABSTRACT Real-world databases often contain syntactic and semantic errors, in spite of integrity constraints and other safety measures incorporated into modern DBMSs. We present ERACER, an iterative statistical framework for inferring missing information and correcting such errors automatically. Our approach is based on belief propagation and relational dependency networks, and includes an efficient ap- proximate inference algorithm that is easily implemented in standard DBMSs using SQL and user defined functions. The system performs the inference and cleansing tasks in an integrated manner, using shrinkage techniques to infer correct values accurately even in the presence of dirty data. We evaluate the proposed methods empirically on multiple synthetic and real-world data sets. The results show that our framework achieves accuracy comparable to a baseline statistical method using Bayesian networks with exact inference. However, our framework has wider applicability than the Bayesian network baseline, due to its ability to reason with complex, cyclic relational dependencies. Categories and Subject Descriptors H.2.8 [Database Applications]: Statistical Databases; H.4 [Information Systems Applications]: Miscellaneous General Terms Algorithms, Experimentation, Performance Keywords Relational dependency network, approximate inference, dis- crete convolution, linear regression, outlier detection 1. INTRODUCTION Although the database community has produced a large amount of research on integrity constraints and other safety measures to ensure the quality of information stored in relational database management systems (DBMSs), real-world databases often contain a significant amount of non-trivial errors. These errors, both syntactic and semantic, are gen- erally subtle mistakes which are difficult or even impossible to express using the general types of constraints available in modern DBMSs. In addition, quality control on data in- put is decreasing as collaborative efforts increase, with the Internet facilitating widespread data exchange, collection, and integration activities. Clearly, there is an increasing need for new approaches to data cleaning for the purpose of maintaining quality in relational databases. Data cleaning (or cleansing, scrubbing, etc.) is the process of identifying and repairing incorrect or corrupt records in a database. The goal is not only to bring the database into a consistent state (i.e., with respect to domain or in- tegrity constraints), but also to ensure an accurate and com- plete representation of the real-world constructs to which the data refer. Two surveys of common techniques and general challenges in this research area include [16] and [22]. Removing impurities from data is traditionally an engineering problem, where ad-hoc tools made up of low-level rules and manually-tuned algorithms are designed for specific tasks. However, recent work has shown the the effectiveness of ap- plying techniques from machine learning and data mining for the purpose of data cleaning [7]. In particular, statis- tical methods make it possible to automate the cleansing process for a variety of domains. For this work we develop statistical methods for cleaning relational databases with the following characteristics: ? Incomplete and erroneous: There are both (1) missing values to be filled in, and (2) corrupted val- ues to be identified. This goes beyond traditional statistical methods which make assumptions about the reliability of the non-missing values. ? Correlated attributes: The values of different at- tributes are correlated, both within and across tuples (involving perhaps multiple relations). Much of the prior work in data cleaning concentrates on values within a single tuple or relation. ? High-level dependencies: The attributes with large domains (i.e., many possible values), exhibit higherlevel dependencies among sets of similar values (for categorical variables) or a numerical functional depen- dency (for continuous variables). As an example of this type of domain, consider the task of inferring missing birth and death years of individuals in genealogical databases. The individuals are related through 75 parent-child relationships and the birth and death years of an individual are correlated due to life expectancies. In ad- dition, the birth dates of parents and children are correlated due to expected parenting ages. Furthermore, since life ex- pectancies and parenthood ages are likely to be similar over time, the dependencies do not need to be modeled for specific birth years. Instead they can be modeled as a higher-level functional dependency such as birth year = parent 's birth year + . A statistical method can learn these dependencies from the available data and then use them to infer missing values automatically. As another example, consider the task of inferring missing data in sensor networks. There are often relationships among the different types of measurements in the same sen- sor (e.g., temperature and humidity), as well as relationships among the measurements of neighboring sensors due to spa- tial locality. Again, a statistical method could learn these dependencies from observed data and then use them to infer missing values (e.g., due to battery loss) and/or clean corrupt values (e.g., due to sensor malfunction). Such a method can also be used for anomaly detection and intrusion detec- tion systems. This paper introduces ERACER, a database-centric statistical framework for integrated data cleaning and imputa- tion. The core techniques are based on belief propagation [20] and relational dependency networks [18]. We show how to implement the inference and cleaning processes efficiently at the database level. This eliminates the expensive process of migrating the data to and from statistical software such as R or Matlab, which is particularly useful when the amount of data aor limited processing time and resources aprevents a more extensive analysis. In contrast to prior work that cleans values within a single tuple, our approach exploits the graphical structure of the data to propagate inferences throughout the database. As a result the imputation and cleaning tasks go hand in hand: additional information in the database helps identify errors more accurately, and cor- rected data values improve the quality of inference for the missing values.",Chris Mayfield,"Purdue University West Lafayette, Indiana, USA",cmayfiel@cs.purdue.edu,Jennifer Neville,"Purdue University West Lafayette, Indiana, USA",neville@cs.purdue.edu,Sunil Prabhakar,"Purdue University West Lafayette, Indiana, USA",sunil@cs.purdue.edu,,,,,,,,,,,,,,,,,,,,,
20200110,1394,Alexander Hall,"Google, Inc.",alexhall@google.com,,Processing a Trillion Cells per Mouse Click,"Processing a Trillion Cells per Mouse Click, Processing a Trillion Cells per Mouse Click, Processing a Trillion Cells per Mouse Click, Processing a Trillion Cells per Mouse Click, Processing a Trillion Cells per Mouse Click, ABSTRACT Column-oriented database systems have been a real game changer for the industry in recent years. Highly tuned and performant systems have evolved that provide users with the possibility of answering ad hoc queries over large datasets in an interactive manner. In this paper we present the column-oriented datastore developed as one of the central components of PowerDrill1. It combines the advantages of columnar data layout with other known techniques (such as using composite range partitions) and extensive algorithmic engineering on key data structures. The main goal of the latter being to reduce the main memory footprint and to increase the efficiency in processing typical user queries. In this combination we achieve large speed-ups. These enable a highly interactive Web UI where it is common that a single mouse click leads to processing a trillion values in the underlying dataset. 1. INTRODUCTION In the last decade, large companies have been placing an ever increasing importance on mining their in-house databases; often recognizing them as one of their core assets. With this and with dataset sizes growing at an enormous pace, it comes as no surprise that the interest in column- oriented databases (column-stores) has grown equally. This spawned several dozens of research papers and at least a dozen of new column-store start-ups, cf. [2]. This is in ad- dition to well established offerings, e.g., by MonetDB [25], Netezza [26], or QlikTech [30]. Since 2011 all major commer- cial database vendors actually provide column-store tech- nologies (cf. [25]). Typically, these products are deployed to import existing databases into the respective column-store. An OLAP or OLTP, i.e., SQL, interface is provided to then mine the data interactively. The key advantage shared by these systems is that column-oriented storage enables reading only data for  relevant columns. Obviously, in denormalized datasets with often several thousands of columns this can make a huge difference compared to the the row-wise storage used by most database systems. Moreover, columnar formats compress very well, thus leading to less I/O and main memory usage. At Google multiple frameworks have been developed to support data analysis at a very large scale. Best known and most widely used are MapReduce [13] and Dremel [23]. Both are highly distributed systems processing requests on thou- sands of machines. The latter is a column-store providing interactive query speeds for ad hoc SQL-like queries. In this paper we present an alternative column-store de- veloped at Google as part of the PowerDrill project. For typical user queries originating from an interactive Web UI (developed as part of the same project) it gives a perfor- mance boost of 10?100x compared to traditional column- stores which do full scans of the data. Background Before diving into the subject matter, we give a little back- ground about the PowerDrill system and how it is used for data analysis at Google. Its most visible part is an interactive Web UI making heavy use of AJAX with the help of the Google Web Toolkit [16]. It enables data visualization and exploration with flexible drill down capabilities. In the back- ground, the ""engine"" provides an abstraction layer for the UI based on SQL: the user constructs charts via drag'n'drop op- erations, they get translated to group-by SQL queries, which the engine parses and processes. It can send out such queries to different backends, e.g., Dremel, or execute them directly on data stored, e.g., in CSV files, record-io files (binary for- mat based on protocol buffers [29]), or in Bigtable [10]. The third large part of the project is the column-store presented in this paper. The Web UI is very versatile; it allows to select arbitrary dimensions, measures, and computed values for grouping and filtering. The dimensions can have a large number of distinct values, such as strings representing Google searches. A user can quickly drill down to values of interest, e.g., all German searches from yesterday afternoon that contain the word ""auto"", by restricting a set of charts to these values. For these reasons, pre-aggregation or indexing of data does not help and we need to query the raw data directly. The nature of the use cases enabled by this UI demand for high availability and low latency. Examples of such use cases include: Responding to customer requests, spam anal- ysis, dealing with alerts in highly critical revenue systems, or monitoring and assessing changes to production systems. The system has been in production since end of 2008 and  was made available for internal users across all of Google mid 2009. Each month it is used by more than 800 users sending out about 4 million SQL queries. After a hard day's work, one of our top users has spent over 6 hours in the UI, triggering up to 12 thousand queries. When using our column-store as a backend, this may amount to scanning as much as 525 trillion cells in (hypothetical) full scans. The column-store developed as part of PowerDrill is tai- lored to support a few selected datasets and tuned for speed on typical queries resulting from users interacting with the UI. Compared to Dremel which supports thousands of dif- ferent datasets (streaming the data from a distributed file system such as GFS [15]), our column-store relies on having as much data in memory as possible. PowerDrill can run interactive single queries over more rows than Dremel, how- ever the total amount of data it can serve is much smaller, since data is kept mostly in memory, whereas Dremel uses a distributed file system. This and several other important distinctions, enable han- dling very large amounts of data in interactive queries. Consider a typical use case such as triggering 20 SQL queries with a single mouse click in the UI. In our production sys- tem on average these queries process 782 billion cells in 30-40 seconds (under 2 seconds per query), several orders of mag- nitude faster than what a more traditional approach as used by Dremel could provide. Contributions The main contributions presented in this paper: ? We describe how-unlike in most column-stores-the data is partitioned and organized in an import phase (Section 2.2). This enables skipping and caching large parts of the data: on average in production 92.41% is skipped and 5.02% cached, leaving only 2.66% to be scanned (see also Section 6). ? We present the basic data-structures used in Section 2.3. Their main goal is to support the partitioned layout of the data and to enable quick skipping of chunks of data. For optimal usage it is assumed they can be held in memory. Experiments show that these simple data-structures also directly give performance benefits of around 100x or more on full scans, compared to two row-wise stor- age formats and Dremel's column-store (Section 2.5). Note that for these experiments we do not partition the data at import. When dropping the ""in memory"" assumption, a still impressive factor of 30x can be achieved. ? In Section 3 we present several successive ""algorithmic engineering"" choices to improve key data-structures. The aim being to reduce the memory footprint for certain typical cases. We pin-point the effects of individ- ual optimizations with experiments measuring mem- ory usage. E.g., for the important case of a field with many distinct values, we obtain a reduction of 16x. ? In Section 4 we describe how queries may be computed in a distributed manner on a cluster of machines. In Section 5 we present selected extensions and finally in Section 6 the highly distributed setup of the actual productionized system running on over 1000 machines. We give measurements concerning the usage in prac- tice which show the positive effect of the partitioning (enabling to skip or cache large parts of the data). Related Work For an introduction to OLAP and basic techniques applied in data-warehouse applications, see the Chaudhuri and Dayal [11]. To obtain an overview of recent work on column-store architectures, please see the concise review [2] and references therein. The excellent PhD thesis by Abadi [1] can serve as a more in-depth introduction and overview of the topic. Recent research in this area includes, e.g., work on how super-scalar CPU architectures affect query processing [9], tuple reconstruction [17], compression in column-stores [34, 9, 3], and a comparison to traditional row-wise storage [4]. Kersten et al. [20] give a more open ended outlook on inter- esting future research directions. The plethora of open-source and commercial column-store systems, e.g., [34, 25, 26, 30, 36] further demonstrates the effectiveness of this paradigm. Melnik et al. [23] recently have introduced Dremel to a wider audience. As mentioned, its power lies in providing interactive responses to ad hoc SQL queries over thousands of datasets. It achieves this by streaming over petabytes of data (stored, e.g., on GFS [15]) in a highly distributed and efficient manner. This is also a key difference to the column-store presented in this paper which heavily relies on having as much data in memory as possible and therefore only is used for a few selected data sources. Melnik et al. also give a nice overview of data anlysis at Google and how interactive approaches like Dremel's complement the offline MapReduce [13] framework. Skipping over data in the context of colum-stores has been explored by other authors, e.g., Slezak et al. [32] or Mo- erkotte [24]. We give some details on these approaches in comparison to ours in Section 2.1. Reordering rows to improve the compression of column- wise stored data has been investigated, e.g., by [18, 21, 3]. We give some details on this at the end of Section 3. Notation and Simplifying Assumptions For the remainder of the paper we will only consider im- porting and processing data from single tables; which, e.g., correspond to log files at Google in the ""protocol buffers"" format [29] or result from denormalizing a set of relational tables in a database. We refer to such an instance as table or just the data which has columns (also referred to as fields) and rows (also referred to as records). In order to store pro- tocol buffer records with nested and repeated records (i.e., lists of sub-records), PowerDrill supports a nested relational model, cf. [5]. For ease of exposition, in the following we focus on unstructured / flat records as opposed to records which may, e.g., contain lists.",Alexander Hall,"Google, Inc.",alexhall@google.com,Olaf Bachmann,"Google, Inc.",olafb@google.com,Robert Bu ?ssow,"Google, Inc.",buessow@google.com,Silviu Ga ?nceanu,"Google, Inc.",silviu@google.com,Marc Nunkesser,"Google, Inc.",marcnunkesser@google.com,,,,,,,,,,,,,,,
20200111,1395,Bahman Bahmani,Stanford University,bahman@stanford.edu,,Fast Personalized PageRank on MapReduce,"Fast Personalized PageRank on MapReduce, Fast Personalized PageRank on MapReduce, Fast Personalized PageRank on MapReduce, Fast Personalized PageRank on MapReduce, Fast Personalized PageRank on MapReduce, ABSTRACT In this paper, we design a fast MapReduce algorithm for Monte Carlo approximation of personalized PageRank vectors of all the nodes in a graph. The basic idea is very efficiently doing single random walks of a given length start- ing at each node in the graph. More precisely, we design a MapReduce algorithm, which given a graph G and a length \lamda, outputs a single random walk of length \lamda starting at each node in G. We will show that the number of MapReduce iterations used by our algorithm is optimal among a broad family of algorithms for the problem, and its I/O efficiency is much better than the existing candidates. We will then show how we can use this algorithm to very efficiently ap- proximate all the personalized PageRank vectors. Our em- pirical evaluation on real-life graph data and in production MapReduce environment shows that our algorithm is significantly more efficient than all the existing algorithms in the MapReduce setting. Categories and Subject Descriptors G.2.2 [Discrete Mathematics]: Graph Theory-Graph al- gorithms; F.1.2 [Computation By Abstract Devices]: Modes of Computation-Parallelism and concurrency General Terms Algorithms, Design, Performance, Experimentation Keywords Personalized PageRank, MapReduce 1. INTRODUCTION Very large scale datasets and graphs are ubiquitous in today's world: world wide web, online social networks, and ?Work done while visiting Microsoft Research. (15)Work done while at Microsoft Research.  huge search and query-click logs regularly collected and pro- cessed by search engines. Because of the massive scale of these datasets, doing analyses and computations on them is infeasible for individual machines. Therefore, there is a growing need for distributed ways of storing and processing these datasets. MapReduce, a simple model of computation, first introduced by Dean and Ghemawat [9], has recently emerged as a very attractive way of doing such analyses. Its effectiveness and simplicity has resulted in its implementation by different internet companies [9, 13, 5, 22], and widespread adoption for a wide range of applications [19], including large scale graph computations [15, 16]. One of the most well known graph computation problems is computing personalized PageRanks (PPR) [12]. Personal- ized PageRanks (and other personalized random walk based measures) have proved to be very effective in a variety of ap- plications, such as link prediction [17] and friend recommen- dation [3] in social networks, and there are many algorithms designed to approximate them in different computational models [14, 3, 10, 25]. In this paper, we study the problem of Fully Personalized PageRank (FPPR) approximation on MapReduce. Specifi- cally, we study the problem of approximating the personalized PageRank vectors of all nodes in a graph in the MapRe- duce setting, and present a fast MapReduce algorithm for Monte Carlo approximation of these vectors. Even though some of the previously designed personalized PageRank approximation algorithms can be implemented in MapReduce, we will show that our algorithm takes much better advantage of the parallel computation model of MapReduce and is hence significantly more efficient than the existing candi- dates in this setting. We also note that our algorithm can be used for computing other personalized random walk based measures (such as personalized SALSA [3]) in MapReduce as well. In this introduction, we first provide some background on personalized PageRank and MapReduce, and then give the problem statements, and also outline our results. 1.1 Background Here we review personalized PageRank, the Monte Carlo approach for PageRank computation, and MapReduce. Here, and throughout the paper, we assume to have a weighted directed graph G = (V,E) with n nodes and m edges. We denote the weight on an edge (u, v) \forall E with u,v and, for the sake of simplifying the presentation of some of the for- mulae, assume for the rest of the paper that the weights on the outgoing edges of each node sum up to 1. 973 1.1.1 Personalized PageRank PageRank is the stationary distribution of a random walk that at each step, with a probability , usually called the tele- port probability, jumps to a random node, and with proba- bility 1? follows a random outgoing edge from the current node. Personalized PageRank is the same as PageRank, ex- cept all the random jumps are done back to the same node, denoted as the ""source"" or ""seed"" node, for which we are personalizing the PageRank. One can easily see that the personalized PageRank of node v, with respect to a source node u, denoted by (v), satis- fies: (v) = u(v) + (1 ? ) 2. {w|(w,v)\forallE} (w)w,v (1) Where u(v) = 1 if and only if u = v (and 0 otherwise). The fully personalized PageRank computation problem is to compute all the vectors ?"" u for all u \forall V . Of course, most applications, such as friend recommendation or query suggestion, only require the top-k values (and corresponding nodes) in each PPR vector (for some suitable value of k). 1.1.2 Monte Carlo Approach There are two broad approaches to computing Personal- ized PageRank. The first approach is to use linear alge- braic techniques, such as Power Iteration [23]. The other approach is Monte Carlo, where the basic idea is to approx- imate Personalized PageRanks by directly simulating the corresponding random walks and then estimating the sta- tionary distributions with the empirical distributions of the performed walks. Based on this idea, Fogaras et al [10] and later Avrachenkov et al [2] proposed the following method for PPR approximation: Starting at each node u \forall V , do a number, R, of random walks starting at u, called ""finger- prints"", each having a length geometrically distributed as Geom( ). Then, the frequencies of visits to different nodes in these fingerprints will approximate the personalized PageR- anks. Our algorithm also belongs to the Monte Carlo family. 1.1.3 MapReduce MapReduce [9] is a simple computation model for process- ing huge amounts of data in massively parallel fashion, using a large number of commodity machines. By automatically handling the lower level issues, such as job distribution, data storage and flow, and fault tolerance, it provides a simple computational abstraction. In MapReduce, computations are done in three phases. The Map phase reads a collection of values or key/value pairs from an input source, and by invoking a user defined Mapper function on each input element independently and in parallel, emits zero or more key/value pairs associated with that input element. The Shuffle phase groups together all the Mapper-emitted key/value pairs sharing the same key, and outputs each distinct group to the next phase. The Reduce phase invokes a user-defined Reducer function on each distinct group, independently and in parallel, and emits zero or more values to associate with the group's key. The emitted key/value pairs can then be written on the disk or be the input of a Map phase in a following iteration. 1.2 Problem Statement In this paper, we study the problem of FPPR approxi- mation on MapReduce (FPPR-MapReduce): Design an efficient MapReduce algorithm that given a weighted directed graph G = (V,E), approximately computes the personalized PageRank vectors ?"" of all nodes u \forall V . As stated earlier, we adopt the Monte Carlo approach, which requires simulating a number, R, of random walks (fingerprints) from each node. Therefore, we will need to solve the following sub-problem, that we call the Single Ran- dom Walk problem (SRW-MapReduce): Design a MapRe- duce algorithm that given a graph G and a length \lamda, outputs one random walk of length \lamda starting from each node in the graph. 1.3 Our Contribution Intuitively speaking, to fully leverage the power of par- allel computation supported by MapReduce, a good algo- rithm should have the following properties: (1) high parallelization and (2) small number of MapReduce iterations. The Monte Carlo approach for FPPR approximation natu- rally has the first property, as any fingerprint starting at any source node can be computed in parallel with and independently from all other fingerprints (for the same or different source nodes). However, as pointed out in [10], some of the fingerprints may be very long, and hence require a large number of MapReduce iterations using the straightforward implementation (e.g., one MapReduce iteration for each step in the walk). For instance, with = 0.2, a fingerprint can be longer than 10 steps with probability 0.11, and can be longer than 20 steps with probability 0.012. These long walks will become the bottleneck of the algorithm, blocking the entire computation, and causing it to take too long to run. In this paper, we develop an algorithm to compute single random walks of a given length for all nodes in a graph, and show that it is optimal in terms of the number of MapReduce iterations among a broad class of algorithms. Based on that, we then develop an efficient algorithm to approximate fully personalized PageRanks on MapReduce, and also analyze its I/O cost. Our empirical evaluation on real-life graph data and in production MapReduce environment demonstrates that our algorithm outperforms the state of the art FPPR approximation algorithms, in terms of efficiency and approx- imation error. The rest of the paper is organized as follows. Section 2 gives the background for computing FPPR on MapReduce. The single random walk algorithm is presented in section 3, and the FPPR approximation algorithm is presented in section 4. We show experimental results in section 5, review the related work in section 6, and finally conclude this paper in section 7.",Bahman Bahmani,Stanford University,bahman@stanford.edu,Kaushik Chakrabarti,Microsoft Research,kaushik@microsoft.com,Dong Xin,Google Inc.,dongxin@google.com,,,,,,,,,,,,,,,,,,,,,
20200112,1396,Robert Fink,"Deptartment of Computer Science, University of Oxford Wolfson Building, Parks Road, OX1 3QD Oxford, UK",robert.fink@cs.ox.ac.uk,,Aggregation in Probabilistic Databases via Knowledge Compilation,"Aggregation in Probabilistic Databases via Knowledge Compilation, Aggregation in Probabilistic Databases via Knowledge Compilation, Aggregation in Probabilistic Databases via Knowledge Compilation, Aggregation in Probabilistic Databases via Knowledge Compilation, Aggregation in Probabilistic Databases via Knowledge Compilation, ABSTRACT This paper presents a query evaluation technique for positive relational algebra queries with aggregates on a representation system for probabilistic data based on the algebraic structures of semiring and semimodule. The core of our eval- uation technique is a procedure that compiles semimodule and semiring expressions into so-called decomposition trees, for which the computation of the probability distribution can be done in time linear in the product of the sizes of the probability distributions represented by its nodes. We give syntactic characterisations of tractable queries with aggregates by exploiting the connection between query tractabil- ity and polynomial-time decomposition trees. A prototype of the technique is incorporated in the prob- abilistic database engine SPROUT. We report on performance experiments with custom datasets and TPC-H data. 1. INTRODUCTION This paper considers the evaluation problem for queries with aggregates on probabilistic databases. The utility of aggregation has been argued for at length. In particular, aggregates are crucial for OLAP and decision support systems. All 22 TPC-H queries involve aggregation. Probabilistic databases are useful to represent and query imprecise and uncertain data, such as data acquired through measurements, integrated from multiple sources, or produced by information extraction [21]. In this paper, we use a rep- resentation system for probabilistic data called pvc-tables. It is based on the algebraic structures of semiring and semi- module to support a mixed representation of aggregated val- ues and tuple annotations for different classes of annotations and aggregations [2]. The pvc-tables can represent any fi- nite probability distribution over relational databases. In addition, the results of queries with aggregates can be represented as pvc-tables of polynomial size. This contrasts with main-stream representation systems such as pc-tables [21], which can require an exponential-size overhead [15]. The problem of query evaluation is #P-hard already for simple conjunctive queries [21]. Aggregates are a further source of computational complexity: for example, already deciding whether there is a possible world in which the SUM of values of an attribute equals a given constant is NP-hard. Existing approaches to aggregates in probabilistic databases have considered restricted instances of the problem: they fo- cus on aggregates over one probabilistic table of restricted expressiveness [4, 20, 16], or rely on expected values and Monte-Carlo sampling [10, 12, 22]. Expected values can lead to unintuitive query answers, for instance when data values and their probabilities follow skewed and non-aligned distributions [19]. Abiteboul et al. investigate XML queries with aggregates on probabilistic data [1]. An algebra pro- posed by Koch represents annotations and data values as rings which enables efficient incremental view maintenance in the presence of aggregations [13]. Our approach considers the problem of exact probabil- ity computation for positive relational algebra queries with aggregates on pvc-tables. The core of our technique is a procedure that compiles arbitrary semimodule and semiring expressions over random variables into so-called decompo- sition trees, for which the computation of the probability distribution can be done in polynomial time in the size of the tree and of the distributions at its nodes. Decomposition trees are a knowledge compilation technique [5] that reflects structural decompositions of expressions into independent and mutually exclusive sub-expressions. Flavours of decomposition trees have been proposed as compilation target for propositional formulas that arise in the evaluation of relational algebra queries (without aggregates) on probabilistic c-tables [18]. It has been shown that more complex tasks, such as conditioning probabilistic databases on given constraints [14] and sensitivity analysis and explanation of query results [11], can benefit from decomposition trees. Example 1. Figure 1 shows six pvc-tables, amongst them the suppliers table S, the products tables P1 and P2, and the table PS pairing suppliers and products. They all have an annotation column  to hold expressions in a semiring K generated by a set of independent random variables, with operations sum (+) and product (,), and neutral elements 0k and 1K . Each valuation of the random variables into a semiring (e.g. integers or Booleans) canonically maps semi- ring expressions into that semiring by interpreting + and , as the corresponding operations of that semiring. Each such valuation defines a possible world of the database. Figure 1d shows the result of the query Q1 that asks for prices of products available in shops. The annotations of the result tuples are constructed as follows: The annotation of a join of two tuples is the product of their annotations, and the annotation obtained from projection or union is the sum of the annotations of the participating tuples [7]. For instance, the tuple <M&S, 10> has the annotation x1y11(z1+z5), whose probability distribution can be computed as a function of probability distributions of the random variables x1, y11, z1, and z5 [21]. Consider the query Q2 from Figure 1e that asks for shops in which the maximal price for the products in P1 or P2 is less than 50. Aggregation is expressed using the $ operator, which in this query groups by the column shop and applies the aggregation MAX on price within each group. The annotations of result tuples are built using semi- module expressions of the form  ? v, where  is a semi- ring expression and v is a data value. Such expressions can be ^summed up' with respect to aggregation opera- tions: For MIN, the sum  +min  is min(, ); for MAX, +max = max(, ); for SUM, +sum = +. The sums correspond to operations in commutative monoids. The annotation  of M&S in Q2's result is constructed as follows. This tuple represents a group of six tuples in the result of Q1, all with the M&S shop value. The annotation  then expresses the conditions (1) that the sum of the price val- ues of these six tuples in the MAX monoid is less than 50, and (2) that the group is not empty (as expressed by ). Depending on the valuation of the variables in , these con- ditions can be true (>) or false (}), or, more generally, the additive or multiplicative neutral element of the semiring. For instance, a valuation  that maps x1, x2, y11, y21, z1, z2, z5 to > and all other variables to } satisfies \, since \(\) \ [>? 10 +max }? 50 +max >? 11 +max }? 60 +max }? 60 +max }? 15  50] , > ã [10 +max 11  50] \ [max(10, 11)  50] \ >. 2 If the variables in such expressions are random variables, then the expressions themselves can be interpreted as ran- dom variables. Moreover, the probability distributions of the obtained expressions reflect the probabilities of query answers taking particular values in a randomly drawn world of the database. Our technique allows to efficiently com- pute probabilities defined by such expressions by structural decomposition. For example, an expression  = ab ? 10 + xy ? 20 can be decomposed in independent sub-expressions ab? 10 and xy? 20 that do not share variable symbols and hence constitute independent random variables. The structure of the paper follows the list of contributions: ? We present an evaluation framework for queries with aggregates (SUM, PROD, COUNT, MIN, MAX) on pvc-tables, a representation system for probabilistic data based on semirings and semimodules. ? We devise a technique for computing the exact proba- bility distribution of query results based on a generic compilation procedure of arbitrary semimodule and semiring expressions into so-called decomposition trees, for which the computation of the probability distribu- tion can be done in time linear in the product of the sizes of the distributions represented by its nodes. ? We give a syntactic characterisation of a class of aggregate queries that are tractable on tuple-independent databases. Our query tractability result follows from the observation that the semiring and semimodule ex- pressions in the result of our tractable queries admit polynomial size decomposition trees and polynomial size probability distributions at their nodes. ? A prototype of our technique is incorporated into the probabilistic database engine SPROUT. ? Extensive performance experiments using our own syn- thetic datasets and TPC-H data are discussed. Besides exact computation, decomposition trees also al- low for approximate probability computation [18]. Due to lack of space, we refer the reader to the MSc thesis of the second author [9]. The pvc-tables can be extended to cope with continuous probability distributions, similar to the ex- tensions of pc-tables in the PIP system [12]. ",Robert Fink,"Deptartment of Computer Science, University of Oxford Wolfson Building, Parks Road, OX1 3QD Oxford, UK",robert.fink@cs.ox.ac.uk,Larisa Han,"Deptartment of Computer Science, University of Oxford Wolfson Building, Parks Road, OX1 3QD Oxford, UK",dan.olteanu@cs.ox.ac.uk,Dan Olteanu,"Deptartment of Computer Science, University of Oxford Wolfson Building, Parks Road, OX1 3QD Oxford, UK",hanlarisa@gmail.com,,,,,,,,,,,,,,,,,,,,,
20200113,1397,Yanhao Wang,National University of Singapore,yanhao90@comp.nus.edu.sg,,Semantic and Influence aware k-RepresentativeQueries over Social Streams,"Semantic and Influence aware k-RepresentativeQueries over Social Streams, Semantic and Influence aware k-RepresentativeQueries over Social Streams, Semantic and Influence aware k-RepresentativeQueries over Social Streams, Semantic and Influence aware k-RepresentativeQueries over Social Streams, Semantic and Influence aware k-RepresentativeQueries over Social Streams, ABSTRACT Massive volumes of data continuously generated on social plat- forms have become an important information source for users. A primary method to obtain fresh and valuable information from social streams is social search. Although there have been exten- sive studies on social search, existing methods only focus on the relevance of query results but ignore the representativeness. In this paper, we propose a novel Semantic and Influence aware k-Representative (k-SIR) query for social streams based on topic modeling. Specifically, we consider that both user queries and elements are represented as vectors in the topic space. A k-SIR query retrieves a set of k elements with the maximum repre- sentativeness over the sliding window at query time w.r.t. the query vector. The representativeness of an element set comprises both semantic and influence scores computed by the topic model. Subsequently, we design two approximation algorithms, namely Multi-Topic ThresholdStream (MTTS) and Multi-Topic Th- resholdDescend (MTTD), to process k-SIR queries in real-time. Both algorithms leverage the ranked lists maintained on each topic for k-SIR processing with theoretical guarantees. Extensive experiments on real-world datasets demonstrate the effectiveness of k-SIR query compared with existing methods as well as the efficiency and scalability of our proposed algorithms for k-SIR processing. 1 INTRODUCTION Enormous amount of data is being continuously generated by web users on social platforms at an unprecedented rate. For ex- ample, around 650 million tweets are posted by 330 million users on Twitter per day. Such user generated data can be modeled as continuous social streams, which are key sources of fresh and valuable information. Nevertheless, social streams are extremely overwhelming for their huge volumes and high velocities. It is impractical for users to consume social data in its raw form. Therefore, social search [7?9, 17, 19, 28, 33, 37, 39] has become the primary approach to facilitating users on finding their interested content from massive social streams. Existing search methods for social data can be categorized into keyword-based approaches and topic-based approaches based on how they measure the relevance between queries and elements. Keyword-based approaches [7?9, 17, 28, 33, 37] adopt the textual relevance (e.g., TF-IDF and BM25) for evaluation. However, they merely capture the syntactic correlation but ignore the semantic correlation. Considering the tweets in Figure 1, if a query ""soccer"" is issued, no results will be found because none of the tweets contains the term ""soccer"". It is noted that the words like ""as- roma"" and ""LFC"" are semantically relevant to ""soccer"". Therefore, elements such as e1, e2 are relevant to the query but missing from the result. Thus, overlooking the semantic meanings of user queries may degrade the result quality, especially against social data where lexical variation is prevalent [14]. To overcome this issue, topic-based approaches [19, 39] project user queries and elements into the same latent space defined by a probabilistic topic model [5]. Consequently, queries and elements are both represented as vectors and their relevance is computed by similarity measures for vectors (e.g., cosine distance) in the topic space. Although topic-based approaches can better capture the semantic correlation between queries and elements, they fo- cus on the relevance of results but neglect the representativeness. Typically, they retrieve top-k elements that are the most coherent with the query as the result. Such results may not be represen- tative in the sense of information coverage and social influence. First, users are more satisfied with the results that achieve an extensive coverage of information on query topics than the ones that provide limited information. For example, a top-2 query on topic 1 in Figure 2 returns {e3, e4} as the result. Nevertheless, compared with e4, e6 can provide richer information to comple- ment the news reported by e3. Therefore, in addition to relevance, it is essential to consider information coverage to improve the result quality. Second, influence is another key characteristic to measure the representativeness of social data. Existing methods for social search [7, 8, 19, 37] have taken into account the in- fluences of elements for scoring and ranking. These methods simply use the influences of authors (e.g., PageRank [24] scores) or the retweet/share count to compute the influence scores. Such a na?ve integration of influence is topic-unaware and may lead to undesired query results. For example, e6 in Figure 1, which is mostly related to 1, may appear in the result for a query on 2 because of its high retweet count. In addition, they do not consider that the influences of elements evolve over time, when previously trending contents may become outdated and new posts continuously emerge. Hence, incorporating a topic-aware and time-critical influence metric is imperative to capture recently trending elements. To tackle the problems of existing search methods, we define a novel Semantic and Influence aware k-Representative (k-SIR) query for social streams based on topic modeling [5]. Specifi- cally, a k-SIR query retrieves a set of k elements from the active elements corresponding to the sliding windowWt at the query time t . The result set collectively achieves the maximum repres- entativeness score w.r.t. the query vector x, each dimension of which indicates the degree of interest on a topic. We advocate the representativeness score of an element set to be a weighted sum of its semantic and influence scores on each topic. We adopt a weighted word coverage model to compute the semantic score so as to achieve the best information preservation, where the weight of a word is evaluated based on its information entropy [31, 42]. The influence score is computed by a probabilistic coveragemodel where the influence probabilities are topic-aware. In addition, we restrict the influences within the sliding windowWt so that the recently trending elements can be selected. The challenges of real-timek-SIR processing are two-fold. First, the k-SIR query is NP-hard. Second, it is highly dynamic, i.e., the results vary with query vectors and evolve quickly over time. Due to the submodularity of the scoring function, existing submodular maximization algorithms, e.g., CELF [16] and SieveStreaming [2], can provide approximation results for k-SIR queries with theoret- ical guarantees. However, existing algorithms need to evaluate all active elements at least once for a single query and often take several seconds to process one k-SIR query as shown in our experiments. To support real-time k-SIR processing over social streams, we maintain the ranked lists to sort the active elements on each topic by topic-wise representativeness score. We first devise theMulti-Topic ThresholdStream (MTTS) algorithm for k-SIR processing. Specifically, to prune unnecessary evalu- ations, MTTS sequentially retrieves elements from the ranked lists in decreasing order of their scores w.r.t. the query vector and can be terminated early whenever possible. Theoretically, it provides ( 12 ?)-approximation results for k-SIR queries and eval- uates each active element at most once. Furthermore, we propose the Multi-Topic ThresholdDescend (MTTD) algorithm to im- prove upon MTTS. MTTD maintains the elements retrieved from ranked lists in a buffer and permits to evaluate an element more than once to improve the result quality. Consequently, it achieves a better (1 ? 1e ? )-approximation but has a higher worst-case time complexity than MTTS. Despite this, MTTD shows better empirical efficiency and result quality than those of MTTS. Finally, we conduct extensive experiments on three real-world datasets to evaluate the effectiveness of k-SIR as well as the effi- ciency and scalability of MTTS and MTTD. The results of a user study and quantitative analysis demonstrate that k-SIR achieves significant improvements over existing methods in terms of in- formation coverage and social influence. In addition, MTTS and MTTD achieve up to 124x and 390x speedups over the baselines for k-SIR processing with at most 5% and 1% losses in quality. Our contributions in this work are summarized as follows. ? We define the k-SIR query to retrieve representative ele- ments over social streams where both semantic and influ- ence scores are considered. (Section 3) ? We propose MTTS and MTTD to process k-SIR queries in real-time with theoretical guarantees. (Section 4) ? We conduct extensive experiments to demonstrate the effectiveness of k-SIR as well as the efficiency and scalability of our proposed algorithms for k-SIR processing. (Section 5)",Yanhao Wang,National University of Singapore,yanhao90@comp.nus.edu.sg,Yuchen Li,Singapore Management University,yuchenli@smu.edu.sg,Kian-Lee Tan,National University of Singapore,tankl@comp.nus.edu.sg,,,,,,,,,,,,,,,,,,,,,
20200114,1398,Zhuhua Cai,"Rice University Houston, TX, 77251",caizhua@gmail.com,,A Comparison of Platforms for Implementing and Running Very Large Scale Machine Learning Algorithms,"A Comparison of Platforms for Implementing and Running Very Large Scale Machine Learning Algorithms, A Comparison of Platforms for Implementing and Running Very Large Scale Machine Learning Algorithms, A Comparison of Platforms for Implementing and Running Very Large Scale Machine Learning Algorithms, A Comparison of Platforms for Implementing and Running Very Large Scale Machine Learning Algorithms, A Comparison of Platforms for Implementing and Running Very Large Scale Machine Learning Algorithms, ABSTRACT We describe an extensive benchmark of platforms available to a user who wants to run a machine learning (ML) inference algorithm over a very large data set, but cannot find an existing implementation and thus must ""roll her own"" ML code. We have carefully chosen a set of five ML implementation tasks that involve learn- ing relatively complex, hierarchical models. We completed those tasks on four different computational platforms, and using 70,000 hours of Amazon EC2 compute time, we carefully compared run- ning times, tuning requirements, and ease-of-programming of each. 1. INTRODUCTION Many platforms have been proposed to provide programming and runtime support for distributed/parallel machine learning (ML) codes, including OptiML [19], GraphLab [11, 8], SystemML [6], and SimSQL [4]. MLBase [10] and ScalOps [21] also address the problem, though the most recent published descriptions indi- cate that these systems are more immature. Other systems such as Pregel [12], Giraph [2], Spark [24], Ricardo [5], Nyad [14], and DryadLinq [22] may not have been developed only for ML, but count it as an important application. We describe an objective benchmark of some of the platforms available to a user who wants to run a specific ML inference al- gorithm over a large data set, but cannot find an existing imple- mentation and thus must ""roll her own"" ML code. Given the wide variety of ML models, this will not be an uncommon occurrence.1 We draw a distinction between a user who wants to implement and apply a brand new ML code, and someone who just wants to use a code, and focus on the former. The implementor will want to balance ease of implementation with performance, whereas an end 1For example, it is telling that of the five standard Bayesian ML inference algorithms we consider in this study, it appears that only the collapsed LDA inference algorithm [3] is available as part of an ex- isting package, and even then we are aware of no ""non-collapsed"" Gibbs sampler implementation (See Section 8 of the paper). user has little concern for the effort required to engineer the code and will be happy with an intricately constructed C and MPI code as long as it is fast and easy to use.2 Our contributions. Our specific contributions are: (1) We shed some light on the relative merits of some (quite differ- ent) platforms for implementing large-scale ML algorithms. Our results will surprise many readers. (2) Second, we demonstrate (through example) what a scientific study of a platform for writing large-scale ML codes might look like. We have carefully chosen a set of tasks that involve learning relatively complex, hierarchical statistical models. We have pur- posely avoided simple, convex models whose parameters can be optimized using easily-implemented techniques such as gradient descent. Their simplicity means they benefit relatively little from the abstractions provided by the platforms we consider. (3) Finally, we hope that our efforts will grow into a widely used, standard benchmark for this sort of platform. In the future, a implementor of a new or existing platform need only implement these codes and compare with our numbers. ",Zhuhua Cai,"Rice University Houston, TX, 77251",caizhua@gmail.com,Zekai J. Gao,"Rice University Houston, TX, 77251",jacobgao@rice.edu,Shangyu Luo,"Rice University Houston, TX, 77251",lsyurd@gmail.com,Luis L. Perez,"Rice University Houston, TX, 77251",lp6@rice.edu,Zografoula Vagena,"LogicBlox, Inc. Atlanta, GA, 30309",foula@acm.org,Christopher Jermaine,"Rice University Houston, TX, 77251",cmj4@rice.edu,,,,,,,,,,,,
20200115,691,Ahmad Ghazal,"Teradata Corporation  100 N. Sepulveda Blvd  Elsegundo, CA 90245, USA  ahmad.ghazal@ teradata.com    Alain Crolotte  Teradata Corporation  100 N. Sepulveda Blvd  Elsegundo, CA 90245, USA",alain.crolotte@teradata.com,,Dynamic Plan Generation for Parameterized Queries,"Dynamic Plan Generation for Parameterized Queries, Dynamic Plan Generation for Parameterized Queries, Dynamic Plan Generation for Parameterized Queries, Dynamic Plan Generation for Parameterized Queries, Dynamic Plan Generation for Parameterized Queries, ABSTRACT  Query processing in a DBMS typically involves two distinct  phases: compilation, which generates the best plan and its  corresponding execution steps, and execution, which evaluates  these steps against database objects. For some queries,  considerable resource savings can be achieved by skipping the  compilation phase when the same query was previously  submitted and its plan was already cached. In a number of  important applications the same query, called a Parameterized  Query (PQ), is repeatedly submitted in the same basic form but  with different parameter values. PQs are extensively used in  both data update (e.g. batch update programs) and data access  queries. There are tradeoffs associated with caching and re- using query plans such as space utilization and maintenance  cost. Besides, pre-compiled plans may be suboptimal for a  particular execution due to various reasons including data skew  and inability to exploit value-based query transformation like  materialized view rewrite and unsatisfiable predicate  elimination. We address these tradeoffs by distinguishing two  types of plans for PQs: generic and specific plans. Generic plans  are pre-compiled plans that are independent of the actual  parameter values. Prior to execution, parameter values are  plugged in to generic plans. In specific plans, parameter values  are plugged prior to the compilation phase. This paper provides  a practical framework for dynamically deciding between  specific and generic plans for PQ 's based on a mix of rule and  cost based heuristics which are implemented in the Teradata  12.0 DBMS.  Categories and Subject Descriptors  C.0 Computer Systems Organization, GENERAL,  Hardware/software interfaces.  General Terms: Algorithms, Performance, Theory.  Keywords: compilation, dynamic, optimizations.  1. INTRODUCTION  Query processing in a DBMS typically involves two distinct  phases. The first phase, which we call the compilation phase,  checks the syntax, semantics and access rights. This phase also  includes the generation of the execution plan using query rewrite  and cost-based optimization. The resulting execution plan (in  some executable code) is fed to the second phase which executes  the plan against database objects and produces the query result.  The query processing time is the sum of compilation and  execution times.  In order to minimize the overhead of the compilation phase,  many DBMSs cache the execution plan in order to reuse it when  the same query is submitted again. When a fully-specified query  (i.e. non-parameteric query) is submitted again, the compilation  phase can be skipped altogether and the cached plan is retrieved  and passed to the execution phase. However, there are various  scenarios that can render the cached plan non-reusable including  structural changes to the underlying data model such as DDL  definition changes or constraint definition changes such as  CHECK constraints or Referential Integrity changes. A cache can  also be invalidated due to statistics collection. Previous work, e.g.  [2][7][10] has addressed this issue by systematically purging or  repairing compiled plans.  Exact query-match based exploitation of the plan cache is also  not appropriate for parameterized queries (PQs) which contain one  or more parameters or parameter markers that take different values  during each execution. As a result, PQs require a special  infrastructure to exploit the plan cache. One approach to produce a  cacheable plan is to assume a uniform distribution for the columns  being compared with a parameter value and generate the plan (i.e.  optimize the query) using the average (or some  ""typical "")  estimate. In this paper, we refer to a plan thus produced as a  generic plan. This was the approach exclusively used prior to  Teradata 12.0 DBMS.  In addition to allowing us to exploit caching to reduce query  processing overhead, a generic plan is also the optimal plan for  some classes of queries as we will discuss later. However, a  generic plan may be sub-optimal for many kinds of queries. The  reason is that the most efficient execution plan for different  parameter values may be different and the performance  degradation caused by using a generic plan may exceed the  savings from using a cached plan. A plan that is picked as efficient  for a typical (average) value of a parameter may be inefficient for  a particular value if there exists a skew in the data distribution or if  the optimization involves value-based transformations like  materialized view rewrite, unsatisfiable predicate elimination or  data partition elimination. In the presence of data skew, different  values of a parameter give rise to different selectivity estimates for  filtering and join predicates which in turn may cause the optimizer  to pick differing execution plans. Similarly, a query rewrite  optimization performed based on a particular value may not be  correctly used for another value, rendering the resulting plan  inappropriate for caching.   A simple alternative to avoid such sub-optimal plans is not to  use a plan cache and instead perform the compilation phase for  each submission of the query where the parameters are replaced  by a specific value. We refer to such a plan as a specific plan. A  specific plan is generally optimal in terms of run time performance  for the specific parameter values but incurs compilation overhead  for each execution of the query. This overhead is wasteful if the  optimization time is a large portion of the overall processing time  or if the plan is independent of the value of the parameter. Also, if  the query is known to be a short running query, then it is  preferable to cache such queries since by definition the execution  time of such queries will be small and the gain from using a  specific plan is minimal.  This paper presents an adaptive approach that dynamically  decides between a generic and a specific plan for a particular PQ.  A key component of our solution is to capture and factor in the  actual run time characteristics of the DBMS when the query is  executed in order to pick a planning type appropriate for the  workload currently executing in the system. Our solution is based  on a mix of rule and cost based heuristics which are implemented  in the Teradata 12.0 DBMS. The rule based heuristic solution is  based on a characterization of those classes of queries for which a  specific or generic plan can be reliably determined. For other  classes of queries, we devised a heuristic algorithm that depends  on the compilation and execution costs as well as workload  characteristics, particularly expected overall run time and assigned  priority, of queries.  The rest of the paper is organized as follows. Section 2  provides a background discussion including definition of  parameterized queries, implementation approaches of generic and  specific plans along with concrete examples of queries that benefit  from each approach. Section 3 details our adaptive algorithm and  Section 4 presents experimental results. We review related  literature in Section 5 and we conclude in Section 6.",Ahmad Ghazal,"Teradata Corporation  100 N. Sepulveda Blvd  Elsegundo, CA 90245, USA  ahmad.ghazal@ teradata.com    Alain Crolotte  Teradata Corporation  100 N. Sepulveda Blvd  Elsegundo, CA 90245, USA",alain.crolotte@teradata.com,Dawit Seid,"Teradata Corporation  100 N. Sepulveda Blvd  Elsegundo, CA 90245, USA  dawit.seid@ teradata.com    Manjula Koppuravuri  Teradata Corporation  Queens Plaza  Hyderabad, 500 003, India",manjula.koppuravuri@teradata.com,Ramesh Bhashyam,"Teradata Corporation  Queens Plaza  Hyderabad, 500 003, India",bhashyam.ramesh@teradata.com,Alain Crolotte ,"Teradata Corporation 100 N. Sepulveda Blvd Elsegundo, CA 90245, USA ",alain.crolotte@teradata.com ,Manjula Koppuravuri,"Teradata Corporation Queens Plaza Hyderabad, 500 003, India",manjula.koppuravuri@teradata.com,Vinod G,"Teradata Corporation  Queens Plaza  Hyderabad, 500 003, India",vinod.g@teradata.com,,,,,,,,,,,,
20200116,1399,Fei Xu,"University of Florida Gainesville, FL, USA",feixu@cise.ufl.edu,,E = MC3: Managing Uncertain Enterprise Data in a Cluster-Computing Environment,"E = MC3: Managing Uncertain Enterprise Data in a Cluster-Computing Environment, E = MC4: Managing Uncertain Enterprise Data in a Cluster-Computing Environment, E = MC5: Managing Uncertain Enterprise Data in a Cluster-Computing Environment, E = MC6: Managing Uncertain Enterprise Data in a Cluster-Computing Environment, E = MC7: Managing Uncertain Enterprise Data in a Cluster-Computing Environment, ABSTRACT Modern enterprises must manage uncertain data for purposes of risk assessment and decisionmaking under uncertainty. The Monte Carlo approach embodied in the MCDB system of Jampani et al. is well suited for such a task. MCDB can support industrial strength business-intelligence queries over uncertain warehouse data. Moreover, MCDB's extensible approach to specifying uncertainty can also capture complex stochastic prediction models, allowing sophis- ticated ""what-if"" analyses within the DBMS. The MCDB computations can be highly CPU intensive, but offer the potential for massive parallelization. To realize this potential, we provide a new system, called MC3 (Monte Carlo Com- putation on a Cluster), that extends the MCDB approach to the map-reduce processing framework. MC3 can exploit the robustness and scalability of map-reduce, and can han- dle data stored in non-relational formats. We show how MCDB query plans over ""tuple bundles"" can be translated to sequences of map-reduce operations over nested data, and describe different parallelization schemes. We also provide and analyze several novel distributed algorithms for adding pseudorandom number seeds to tuple bundles. These al- gorithms ensure statistical correctness of the Monte-Carlo computations while minimizing the seed length. Our ex- periments show that MC3 can scale well for a variety of workloads. Categories and Subject Descriptors H.2 [Information Systems]: Database Management General Terms Algorithms, Design, Languages, Performance Keywords Uncertain Data, Map-Reduce, Monte Carlo, JSON, JAQL  1. INTRODUCTION There is an increasing need for tools that facilitate en- terprise risk assessment and decision making in the face of uncertain data. The problem of data uncertainty is be- coming acute, due to the increasing use of data integration, automated information extraction, and data anonymization for privacy protection, as well as the growing prevalence of RFID and sensor data. Database researchers have therefore developed a variety of prototype systems [1, 2, 3, 16, 26, 28, 32] for managing uncertain data. Among these prototypes, the Monte Carlo relational Database System (MCDB) [16] seems especially promising for general decision-support ap- plications. MCDB's Monte Carlo approach permits processing of in- dustrial strength Business Intelligence (BI) queries - e.g., complex SQL aggregation queries - over warehouses where the data is uncertain and has complex statistical depen- dencies between attributes and tuples. Perhaps more im- portantly, the MCDB uncertainty model is completely ex- tensible: uncertainty is specified via user-defined Variable Generation (VG) functions, which are used to pseudoran- domly generate realized values for uncertain attributes. As a consequence, the MCDB model subsumes the uncertainty models used in current prototype systems. For example, MCDB can capture discrete models of uncertainty such as the attribute-value and tuple-inclusion uncertainty models used in systems such as MayBMS, Trio, and Mystiq [1, 2, 3]. Moreover, MCDB is well suited to a very important class of scenarios in which uncertainty arises due to the need to extrapolate missing data using probabilistic models, as is often the case with financial, banking, marketing, fraud-detection, and decision-support applications. In this latter setting, MCDB permits sophisticated, data-intensive stochastic modeling and prediction without the need to con- tinually move data back and forth between the DBMS and a statistical or simulation package such as R or ARENA. Thus the user can assess not only the uncertainty in the results of BI queries over uncertain data, but can also ask what-if questions such as ""What will be the mean effect on my prof- its next quarter if I increase my prices by 5%?"" or ""What is the probability that the average value of my New York cus- tomers' portfolios will drop by more than 10% over the next month?"" To achieve the foregoing new functionality without unacceptably increasing processing overhead, MCDB uses a novel processing technique in which a query plan is executed exactly once, but over ""tuple bundles"" rather than ordinary 441 tuples. A tuple bundle represents the values of a tuple over all Monte Carlo replications - equivalently, over all sampled ""possible worlds"" - see Section 2.1. In this paper, we provide a new system, called MC3 (Monte Carlo Computation on a Cluster), that extends the MCDB approach to a map-reduce framework. Our motivation for this work is threefold: ? As the amount of data continues to increase expo- nentially, massively-parallel processing techniques are becoming increasingly important. Especially in more complex settings - see the finance and marketing ex- amples in the following sections - our new uncertainty- handling technology can exacerbate this problem of handling massive data, because MCDB's Monte Carlo computations can be highly CPU-intensive. For wide adoption of the MCDB approach to uncertainty, it is therefore very desirable to provide the MCDB func- tionality on an affordable, massively parallel platform. ? The MCDB prototype incorporates a major rework- ing of the standard relational query-processing engine. Because of the effort required, it is unlikely that this technology will be directly incorporated into commer- cial relational database products; an alternative path to market is needed. ? Since most real-world data is not stored in relational databases, it is important to be able to deal with data in a wide variety of formats. To address the first issue, we note that Monte Carlo com- putations can be performed independently for each tuple bundle, and Monte Carlo replications for a given tuple bun- dle can be executed independently of each other. Thus the MCDB computations have the potential to be massively par- allelized. MC3 realizes this potential: the excellent scalabil- ity and ease of parallel programming made possible by the map-reduce approach are ideal for our purposes. Our MC3 prototype uses Hadoop, an open-source implementation of Google's map-reduce processing framework. Hadoop's map- reduce has been shown to be highly scalable, as demon- strated by Yahoo's recent Daytona Terabyte sort record of 209 seconds using 910 servers.1 Preliminary results from Google (68 seconds using 1000 servers) provide additional evidence.2 Our choice of Hadoop - as well as our choice of Javascript Object Notation (JSON) as the MC3 internal data model - addresses the second issue raised above. The Hadoop infras- tructure and JSON data format are becoming increasingly popular. Appealing features of Hadoop include fault toler- ance and the capability to allocate and reallocate resources (CPU, memory, storage) as needed; this functionality al- lows massive parallelism to be achieved using commodity hardware, further increasing the appeal of the map-reduce approach. Although it appears hard to precisely delineate the class of queries that can be processed by MC3, we believe that it is quite large: MCDB can handle virtually any BI SQL query, it appears that virtually any such query can be rewritten as a directed acyclic graph of operators (i.e., a query plan) that can be processed by MC3. We speculate that many probabilistic XML queries can also be handled by MC3. As a consequence of these considerations, tech- niques for managing uncertain data in this setting have the potential to be widely used. With respect to the third issue, the use of JSON means that MC3 can gracefully deal with data provided in non- relational formats, without any need to reformat the data prior to processing. Indeed, we obtain this functionality ""for free"" from the Hadoop platform. In this paper, we primar- ily deal with JSON data that can be viewed as reformatted probabilistic relational data. However, work on probabilis- tic XML data [19] leads us to believe that extensions to full-fledged probabilistic JSON data should be achievable. Such extensions would then permit MC3 to interact with many recently developed repositories for scaled-out cluster environments, in which data attributes may be multi-valued and records in the same table may differ in their number of attributes [4, 7, 27, 29]. Extending the MCDB functionality to the map-reduce set- ting raises a number of challenging questions. How exactly do we map MCDB's tuple-bundle processing methods to Hadoop and JSON? Must we directly generate a query plan as a sequence of map-reduce operations, or can we facilitate this process via use of a higher level query language? What are the different ways in which MCDB queries can be parallelized, and for which scenarios are these various parallelization schemes effective? A key technical challenge is how to ""seed"" tuple bundles so as to generate streams of the pseudorandom numbers that form the basis of the Monte Carlo computations. For statistical correctness, the streams used by the various tuple bundles must be mutually disjoint. Seeding is challenging because it must be done in a highly parallel and distributed fashion, over an enormous number of tuple bundles, and without requiring storage of too much seeding information in each tuple bundle. Our Contributions. The paper's contributions are as follows: ? We provide the first system for managing uncertain data in a map-reduce environment, showing how to represent MCDB tuple bundles as JSON arrays and how to translate an MCDB query plan to map-reduce. ? We show how query-plan generation can be facilitated by use of JAQL, an open-source language for querying JSON data. ? We identify two MCDB-specific parallelization schemes called inter-tuple and intra-tuple parallelism, show how to implement these schemes using map-reduce, and identify scenarios under which each scheme is effective. ? We develop and analyze an efficient distributed-seed- ing method called SeedSkip that is based on a ran- dom number generator with skip-ahead functionality, as well as a ""fallback"" method called SeedMult that is based on multiple pseudorandom number generators and can be used when SeedSkip does not apply. ? We show, via a set of experiments, that our paralleliza- tion techniques can yield linear scaleup when process- ing uncertain data, and that intra-tuple parallelism can provide linear speedup for certain very expensive VG functions.  We note that other parallel-processing platforms, data for- mats, and query languages can potentially be used to extend MCDB. Our goal was proof-of-concept, and our plat- form choices were partially made as a matter of convenience. We believe, however, that at least some of our techniques, and the lessons learned, are applicable to other possible extensions of the MCDB methodology to forward-looking information-management architectures. Paper Organization. The remainder of the paper is orga- nized as follows. Section 2 gives some background informa- tion. Section 3 gives an overview of how MCDB function- ality is realized using map-reduce, JSON, and JAQL. Sec- tion 4 considers the distributed-seeding problem, and Sec- tion 5 presents our experimental study. We conclude the paper in Section 6. ",Fei Xu,"University of Florida Gainesville, FL, USA",feixu@cise.ufl.edu,Kevin Beyer,"IBM Almaden Research Center San Jose, CA, USA",kbeyer@us.ibm.com,Vuk Ercegovac,"IBM Almaden Research Center San Jose, CA, USA",vercego@us.ibm.com,Peter J. Haas,"IBM Almaden Research Center San Jose, CA, USA",phaas@us.ibm.com,Eugene J. Shekita,"IBM Almaden Research Center San Jose, CA, USA",hekita@us.ibm.com,,,,,,,,,,,,,,,
20200117,1340,Jaroslaw Szlichta,University of Toronto & IBM Toronto Centre for Advanced Studies,szlichta@cs.toronto.edu,,Business-Intelligence Queries with Order Dependencies in DB2,"Business-Intelligence Queries with Order Dependencies in DB2, Business-Intelligence Queries with Order Dependencies in DB3, Business-Intelligence Queries with Order Dependencies in DB4, Business-Intelligence Queries with Order Dependencies in DB5, Business-Intelligence Queries with Order Dependencies in DB6, ABSTRACT Business-intelligence queries often involve SQL functions and al- gebraic expressions. There can be clear semantic relationships between a column's values and the values of a function over that col- umn. A common property is monotonicity: as the column's values ascend, so do the function's values. This we call an order dependency (OD). Queries can be evaluated more efficiently when the query optimizer uses order dependencies. They can be run even faster when the optimizer can also reason over known ODs to infer new ones. Order dependencies can be declared as integrity constraints, and they can be detected automatically for many types of SQL functions and algebraic expressions. We present optimization techniques us- ing ODs for queries that involve join, order by, group by, partition by, and distinct. Essentially, ODs can further exploit interesting orders to eliminate or simplify potentially expensive sorts in the query plan. We evaluate these techniques over our implementation in IBM R? DB2 R? V10 using the TPC-DS R? benchmark schema and some IBM customer inspired queries. Our experimental results demonstrate a significant performance gain. We additionally devise an algorithm for testing logical implication for ODs which is polynomial over the size of the set of given ODs. We show that the inference algorithm which we have implemented in DB2 is sound and complete over sets of ODs over natural domains. This enables the optimizer to infer useful ODs from known ODs. 1. INTRODUCTION 1.1 Motivation As business-intelligence (BI) applications have become more co- mplex and data volumes grow, so have the analytic queries needed to support them. This increasing complexity raises performance issues and numerous challenges for query optimization. Worse, traditional optimization methods often fail to apply when logical subtleties in database schemas and in queries circumvent them. For example, data-warehouse schemas will use surrogate keys, while predicates in business analytic queries will use natural values (such as sale_date = '2010-07-01'). Real world queries will use SQL functions (such as year(d_date)) and algebraic expressions (such as d_date + 30 days). These subtleties cause the optimizer to miss opportunities to use indexes, partition elimination and pipeline operations, and to add potentially expensive operations such as sort even when the data is already sorted appropriately. This is because semantic relation- ships between the functions and expressions the queries use and the data in the database-and between data themselves in the schema, as between surrogate and natural keys-are opaque. If these rela- tionships could be discovered and used, more efficient query plans would result. The relationship on which we focus in this work is order. If the rows of a table were ordered by its date column d_date, they would also necessarily be ordered by d_date + 30 days. Indeed, the function (over d_date) of d_date + 30 days is monotonically increasing with respect to d_date. For this, we say d_date or- ders d_date + 30 days.1 If an index on d_date could be used to provide results ordered by d_date, then the same index would provide the results ordered by d_date + 30 days, since this is the same order. This semantic relationship of order is a type of depen- dency, and we call it an order dependency (OD) [17, 18, 21]. It is akin to the well-known concept of functional dependencies (FDs). (In fact, ODs strictly subsume FDs.) While it will be readily obvious to any reader that d_date orders d_date+ 30 days, this observation is not for free for the optimizer. It would need explicit mechanisms to recognize the dependency. While this particular order dependency rightfully seems trivial, we shall see there are many that are not. Then ""when"" and ""how"" to exploit such dependencies in query planning is far from trivial too. This work is about this aspect of query optimization. Consider then the SQL query in Query 1 over the TPC-DS2 schema. In the schema, date_dim is a dimension table with the primary key d_date_sk with one row per day. (The attribute d_date_sk is a sequential number.) The table has columns d_date, 1In this case, d_date + 30 days orders d_date also. We then say the two are order equivalent. However, ""orders"" is not in- herently symmetric. Consider year(d_date) and d_date. In this case, d_date orders year(d_date), but year(d_date) does not or- der d_date. 2http://www.tpc.org     750 10.5441/002/edbt.2014.81 select D.d_date + 30 days, max(S.ws_ext_sales_price) as most from date_dim D, web_sales S where S.ws_sold_date_sk = D.d_date_sk and D.d_date between date('1998-01-01') and date('2002-01-01') group by D.d_date + 30 days order by D.d_date + 30 days; Query 1: Plus thirty days. d_month, d_quarter, and d_day, and additional columns that qualify the day (such as whether it is the weekend, a holiday, and, if so, the name of the holiday). The table web_sales is a large fact table recording all individual sales, with ws_sold_date_sk as a foreign key referencing date_dim on d_date_sk. Let there be a tree index for date_dim on d_date. The opti- mizer will miss that the index could be used in evaluating Query 1 to accomplish both the group-by and the order-by. How might the query be rewritten manually to resolve this? ? group by d_date + 30 days and order by d_date: This is not legal SQL; the attribute in the order-by is not listed in the group-by (as such). ? group by d_date and order by d_date + 30 days: This is accepted by DB2; derived attributes-functions and algebraic expressions derived over the attributes listed in the group-by (which may include derived attributes itself)-can be used in the select and order-by clauses. However, this does not resolve the inefficiency. The query plan still explicitly sorts to ""satisfy"" the order-by. ? group by d_date and order by d_date: This does work! The index can now be employed to imple- ment the group-by and to satisfy the order-by. Of course, it is not the responsibility of the SQL programmer to write queries painstakingly-or of an automated BI report sys- tem that generates SQL queries in the back-end-in such a way to assure the optimizer will handle it well. This would violate the declarative principle of SQL. Even if we tried to put the onus on programmers to be careful, they cannot be expected to know what is problematic and what is not. While a clever SQL programmer can sometimes skirt such pitfalls by careful composition (as here), more often it is not possible. So, we have to fix it. The optimizer needs to recognize that d_date and d_date + 30 days are seman- tically equivalent for order, thus skipping the superfluous sorting step, regardless of how the query is written. Next, consider Query 2. In SQL, date and time are complex data types. These are central to BI applications, and provide for rich drill down and roll up. In TPC-DS in table date_dim, some of date's hierarchy is materialized in columns: d_year, d_quarter, d_month, and d_day. select D.d_year, D.d_quarter, D.d_month, D.d_day sum(S.ws_sales) as total from date_dim D, web_sales S where S.ws_date_sk = D.d_date_sk and D.d_year between 2001 and 2004 group by D.d_year, D.d_quarter, D.d_month, D.d_day order by D.d_year, D.d_quarter, D.d_month, D.d_day; Query 2: Eliminating quarter. Let there be a tree index for date_dim on d_year, d_month, d_day. Unfortunately, this index would not help in a query plan, even for the group-by: d_quarter intervenes. Note that d_month functionally determines d_quarter. The query's author cannot elim- inate mention of d_quarter in the group-by, however, as it appears in the select. Fortunately, by the work in [16], DB2 can eliminate it internally from the group-by, based on the recognition of the func- tional dependency (FD). The index can then be used to implement the group-by operation. However, this FD, d_month C d_quarter, is not logically suf- ficient likewise to remove d_quarter from the order-by clause. The optimizer must still apply a sort operator to ""satisfy"" the order- by directive. However, because d_month orders d_quarter- which says more than just that d_month functionally determines d_quarter-the attribute d_quarter can be removed from the order- by clause also, to result in a semantically equivalent query.3 In this work, we show how this is accomplished. 1.2 Contributions and Outline In Section 2, we provide background on order dependencies- notational conventions and definitions-as we use in this paper, and considerations that arise in data-warehouse schema design. In Section 3, we address how to use order dependencies in query op- timization. There are two aspects to this: how and where the op- timizer makes use of OD information; and how OD information is discovered. 1. Optimizing with Order Dependencies. (a) Section 3.1 is divided into two sections. In Section 3.1.1, we go into further depth how ODs are used to optimize. (b) In Section 3.1.2 we present two inference algorithms and show where and how they are invoked in the opti- mizer: Reduce Order OD which puts ODs into a canon- ical form for matching against interesting orders; and Homogenize Order OD which discovers equivalent co- lumns, order-wise. We discuss the utility of these algo- rithms. 2. Detecting Order Dependencies. In Section 3.2, we show how ODs between columns and functions over columns (SQL func- tions and algebraic expressions) can be automatically detected by the optimizer. (a) These techniques have been implemented as a prototype within DB2. (b) We present a suite of real-world IBM customer queries over TPC-DS benchmark that illustrate the issues, which are then used in Section 4 for an experimental perfor- mance evaluation. The optimizer automatically infers the associated OD information and uses it to produce the improved query plans. 3. Declaring Order Dependencies. In Section 3.3, we consider how OD information can be declared, and what types of natural ODs occur in today's schemas. (a) Order dependencies can be explicitly declared in our implementation in DB2 as a type of integrity constraint. (b) We demonstrate how ODs between surrogate and nat- ural keys can be used for strong performance improvement [19]. 4. Inferring Order Dependencies. In Section 3.4, we show how the optimizer can infer new ODs from known ODs. The known ODs may not match interesting orders in query plan- ning, while ODs that logically derive from them would. Thus, such an OD-inference facility is ultimately needed to take 3The values for d_quarter are 1, . . . , 4 and for d_month, 1, . . . , 12. 751 Table 1: Notational conventions. ? Relations ? R represents a relation, and r represents a specific re- lation instance (table). ? A, B and C represent attributes. ? s and t represent tuples. ? t A denotes the value of attribute A in tuple t. ? Sets ? calligraphic letters denoted as X , Y , and Z represent sets of attributes. ? Lists ? bold letters represent lists of attributes: X, Y and Z. Note list X could be the empty list, [ ]. ? square brackets denote an explicit list: [A,B,C]. ? [A |T] denotes that A is the head of the list, andT is the tail of the list (the remaining list when the first element is removed). fuller advantage of these techniques. (a) We define a database to be natural if given order prop- erty over its attributes can be guaranteed. (All real- world domains we have encountered have this property, and thus are natural.) (b) We discuss a general, efficient (polynomial) inference procedure which we have implemented which is sound and complete over natural domains. In Section 4, we present results of a performance study over queries over TPC-DS. 5. Experimental Results. All nine of the test queries show a significant performance gain using the OD-extended optimizer, with an average 30% time improvement over a ten-GB database. In Section 5, we discuss related work, both previous applied work that used dependencies in optimization (upon which we build), and theoretical work on order dependencies which has provided critical foundations for our current implementation. In Section 6, we outline next steps for this work, and conclude.",Jaroslaw Szlichta,University of Toronto & IBM Toronto Centre for Advanced Studies,szlichta@cs.toronto.edu,Parke Godfrey,York University in Toronto & IBM Toronto Centre for Advanced Studies,godfrey@cse.yorku.ca,Jarek Gryz,York University in Toronto & IBM Toronto Centre for Advanced Studies,jarek@cse.yorku.ca,Wenbin Ma,IBM Toronto Laboratory,wenbinm@ca.ibm.com,Weinan Qiu,IBM Toronto Laboratory,davidqiu@ca.ibm.com,Calisto Zuzarte,IBM Toronto Laboratory,calisto@ca.ibm.com,,,,,,,,,,,,
20200118,250,James Wagner,"DePaul University Chicago, Illinois",jwagne32@depaul.edu,,Detecting Database File Tampering through Page Carving,"Detecting Database File Tampering through Page Carving, Detecting Database File Tampering through Page Carving, Detecting Database File Tampering through Page Carving,Detecting Database File Tampering through Page Carving, Detecting Database File Tampering through Page Carving,  ABSTRACT Database Management Systems (DBMSes) secure data against regular users through defensive mechanisms such as access con- trol, and against privileged users with detection mechanisms such as audit logging. Interestingly, these security mechanisms are built into the DBMS and are thus only useful for monitoring or stopping operations that are executed through the DBMS API. Any access that involves directly modifying database files (at file system level) would, by definition, bypass any and all security layers built into the DBMS itself. In this paper, we propose and evaluate an approach that detects direct modifications to database files that have already bypassed the DBMS and its internal security mechanisms. Our approach applies forensic analysis to first validate database indexes and then compares index state with data in the DBMS tables. We show that indexes are much more difficult to modify and can be further fortified with hashing. Our approach supports most relational DBMSes by leveraging index structures that are already built into the system to detect database storage tampering that would currently remain undetectable. 1 INTRODUCTION DBMSes use a combination of defense and detection mechanisms to secure access to data. Defense mechanisms, such as access control, determine the data granularity and system access granted to different database users; defense mechanisms, such as audit logging, monitor all database activity. Regardless of the defense mechanisms, security breaches are still a legitimate concern ? sometimes due to unintentional granting of extra access control and sometimes due to outright hacking, such as SQL injection. Security breaches are typically detected through analysis of audit logs. However, audit log analysis is unreliable to detect a breach that originated from privileged users. Privileged users, by definition, already have the ability to control and modify access permissions. Therefore, audit logs fundamentally cannot be trusted to detect suspicious activity. Additionally, privileged users commonly have access to database files. Consider a system administrator who maliciously, acting as the root, edits a DBMS data file in a Hex editor or through a programming language, such as Python. The DBMS, unaware of external file write activity taking place outside its own pro- grammatic access, cannot log it, and thus the tampering attack remains undetected. Current DBMSes do not provide tools against insider threats ? in general, a built-in security mechanism is vulnerable to in- sider attacks. While a DBMS will not be able to detect direct storage changes, file-level modifications potentially create incon- sistencies within the auxiliary data structures maintained by a DBMS. Forensics tools that examine file contents can be used to detect such inconsistencies, and determine if insider threats have taken place. Recently we proposed the first database foren- sic tool, DBCarver, that can be used to detect deleted data from database pages [31]. However, database forensic tools such as DBCarver merely extract forensic artifacts but do not search for inconsistencies within the data structures maintained by a DBMS. In this paper, we propose a system, DBStorageAuditor, that detects database file tampering by identifying inconsistencies in storage through a direct inspection of internal database struc- tures. DBStorageAuditor utilizes existing database forensic tech- niques and expands them to extract additional necessary storage artifacts. These artifacts are then used to detect inconsistencies within indexes and between indexes and tables. The underlying premise of our approach is that all relational databases follow patterns in storage over which the privileged user has little or no control. We inspect these storage patterns to detect unusual activity. We motivate DBStorageAuditor through an example: Example 1. Malice is the system administrator for a shipping company, FriendlyShipping. Malice is bribed by a competing com- pany to interfere with the orders going to Seattle. Malice does not have access to the DBMS, but she does have access to the server where the database files reside. Malice writes a Python script that will open and directly modify the database file containing the Orders table. The script then opens the database file, finds all records containing the string  'Seattle ', and explicitly overwrites entire records with the NULL ASCII character (decimal value 0). Figure 1 illustrates the result of Malice 's script actions. Since the record was erased without the DBMS (API has never seen that command) all DBMS security was bypassed, and the operation was never recorded in the log file. When FriendlyShipping investigates the missing Seattle orders, the audit log can only explain deleted orders for (2, Chair, New York) and (6, Chair, Detroit). The audit logs contain no trace of the Seattle order being deleted because it was not deleted but rather wiped out externally. To simplify in the above example, we have omitted some details of database file tampering, which we expand on later in Section 5. Barring those details in Example 1, the value in the City index still exists in index storage even though the entire record is erased. Therefore, an inconsistency can be identified by mapping back the index value to the empty gap in table storage. The empty gap in table storage exists because a database only marks a record when it is deleted, and only overwrites the record with data from a newly inserted record. However, making the mapping from the index value to the associated record must be based on the behavioral rules of database storage, such as page and record layout. We use database forensic tools to understand database layout, and using that layout, perform the necessary mapping. It is not impossible for a scrupulous system administrator to (i) tamper with the index and create a cascade of inconsistencies throughout the index structure, or (ii) for an attacker who has privileges to modify database files to acquire privileges to sus- pend or kill logging mechanisms at the operating system level if necessary, or (iii) for a knowledgeable adversary to easily avoid corrupting storage and keep checksum values consistent. How- ever, in spite of increased level of threat, we repeatedly show that accurate knowledge about data layout can be used to gather evidence and prove if any malicious activity has taken place. Previously we developed an approach to detect malicious ac- tivity when DBMS logging is disabled [28]. In this approach we analyzed unlogged activity (executed through a proper DBMS API) but strictly assumed that database files were not exposed to tampering. In this paper, we address the tampering vulnerability where the database files are physically altered. Developing an auditing system for DBMSes is part of our larger goal to open up the database system and its storage to users, for performance and forensics investigation. The rest of the paper is organized as follows: Section 2 cov- ers related work. Section 3 discusses concepts of database stor- age used throughout the paper. Section 4 defines the adver- sary we seek to defend against. Section 5 details how to per- form database file tampering. Section 6 provides an overview of DBStorageAuditor. Section 7 describes how we utilize data- base forensics. Section 8 addresses index tampering. Section 9 proposes a method to organize carved index output making our system scalable. Section 10 discusses how to detect file tampering using inconsistencies between carved index data and table data. Section 11 provides a thorough evaluation of our system.",James Wagner,"DePaul University Chicago, Illinois",jwagne32@depaul.edu,Alexander Rasin,"DePaul University Chicago, Illinois",arasin@depaul.edu,Karen Heart,"DePaul University Chicago, Illinois",kheart@depaul.edu,Tanu Malik,"DePaul University Chicago, Illinois",tmalik1@depaul.edu,Jacob Furst,"DePaul University Chicago, Illinois",jfurst@depaul.edu,Jonathan Grier,"GrierForensics Pikesville, Maryland ",jdgrier@grierforensics.com,,,,,,,,,,,,
20200119,1126,Orestis Polychroniou,Columbia University,orestis@cs.columbia.edu,,Track Join: Distributed Joins with Minimal Network Traffic,"Track Join: Distributed Joins with Minimal Network Traffic, Track Join: Distributed Joins with Minimal Network Traffic, Track Join: Distributed Joins with Minimal Network Traffic, Track Join: Distributed Joins with Minimal Network Traffic, Track Join: Distributed Joins with Minimal Network Traffic, ABSTRACT Network communication is the slowest component of many operators in distributed parallel databases deployed for large- scale analytics. Whereas considerable work has focused on speeding up databases on modern hardware, communica- tion reduction has received less attention. Existing parallel DBMSs rely on algorithms designed for disks with minor modifications for networks. A more complicated algorithm may burden the CPUs, but could avoid redundant transfers of tuples across the network. We introduce track join, a novel distributed join algorithm that minimizes network traffic by generating an optimal transfer schedule for each distinct join key. Track join extends the trade-off options between CPU and network. Our evaluation based on real and synthetic data shows that track join adapts to diverse cases and degrees of locality. Considering both network traffic and execution time, even with no locality, track join outperforms hash join on the most expensive queries of real workloads. 1. INTRODUCTION The processing power and storage capacity of a single machine can be large enough to fit small to medium scale databases. Nowadays, servers with memory capacity of more than a terabyte are common. Packing a few multi-core CPUs on top of shared non-uniform access (NUMA) RAM provides substantial parallelism, where we can run database opera- tions (i.e. sort, join, and group-by) on RAM-resident data at rates of a few gigabytes per second [2, 3, 29, 34, 36]. Database research has also evolved to catch up to the hardware advances. Fundamental design rules of the past on how a DBMS should operate are now being revised due to their inability to scale and achieve good performance on modern hardware. Special purpose databases are now popu- lar against the one-size-fits-all approach [32], while accelera- tors [26] are the implicit manifestation of the same concept. ?Work partly done when author was at the Oracle Labs. The advances in database design for storage and execution on modern hardware have not been met by similar advances in distributed parallel database design. When the most fun- damental work on distributed and parallel databases was published [5, 11, 20], hardware advances of today like multi- core parallelism had not yet occurred. Techniques to speed up short-lived distributed transactions [32] target distributed commit protocols, which suffer from network latencies rather than throughput. Queries where communication is inevitable are less popular research topics or are left for data-centric generic distributed systems for batch-processing [7, 24]. The latest network technologies may be slow relative to main-memory-resident processing. A 40 Gbps InfiniBand measured less than 3 GB/s real data rate per node dur- ing hash partitioning. If done in RAM, partitioning to a few thousand outputs runs close to the memory copy bandwidth [29, 34]. For instance, a server using 4X 8-core CPUs and 1333 MHz quad-channel DDR3 DRAM achieves a partition rate of 30?35 GB/s, more than an order of magnitude higher than the InfiniBand network. Recent work [3] achieves a hash join rate of 4.85 GB/s of 32-bit key, 32-bit payload tu- ples on 4X 8-core CPUs. Such high-end hardware is common in marketed configurations for large-scale analytics. Network optimization is important for both low-end and high-end hardware. In low-end platforms where the network is relatively slow compared to local in-memory processing, we expect the execution time to be dominated by network transfers. Thus, any network traffic reduction directly trans- lates to faster execution. In high-end platforms, given that the network still cannot be as fast as the RAM bandwidth, completion times are also reduced if the reduction in net- work traffic is comparable with the increase in CPU cycles. In order to show how much time databases can spend on the network, we give an example of a real analytical work- load from a large commercial vendor, using a market-leading commercial DBMS. Using 8 machines connected through 40 Gbps InfiniBand (see Section 4 for more details of the con- figuration), we found that the five most expensive queries spend ? 65?70% of their time transferring tuples on the network and account for 14.7% of the total time required to execute the entire analytical workload with more than 1500 queries. All five queries have a non-trivial query plan (4?6 joins), but spend 23%, 31%, 30%, 42%, and 43% of their total execution time on a single distributed hash join. A sophisticated DBMS should have available options to optimize the trade-off between network and CPU utiliza- tion. One solution would be to apply network optimiza- tion at a higher level treating the network as a less desired 1483 route for data transfers, without modifying the underlying algorithms. These approaches are common in generic dis- tributed processing systems [7]. A second solution would be to employ data compression before sending data over the network. This solution is orthogonal to any algorithm but can consume a lot of CPU resources without always yielding substantial compression. A third solution is to create novel algorithms for database operator evaluation that minimize network communication by incurring local processing cost. This approach is orthogonal and compatible with compres- sion and other higher level network transfer optimizations. Grace hash join [9, 17] (throughout the paper we will use the term hash join to refer to Grace hash join on network [9], rather than disk [17]) is the predominant method for executing distributed joins and uses hash partitioning to split the initial problem into shared-nothing sub-problems that can proceed locally per node. Partitioning both tables works almost independently of the table sizes. However, hash join is far from network-optimal because it transfers almost the full size of both tables over the network. Using pre-determined hash functions guarantees load balancing, but limits the probability that a hashed tuple will not be transferred over the network to 1/N on N nodes. We introduce track join, a novel algorithm for distributed joins that minimizes transfers of tuples across the network. The main idea of track join is to decide where to send rows on a key by key basis. The decision uses information about where records of the given key are located. Track join has the following properties: it (i) is orthogonal to data-centric compression, (ii) can co-exist with semi-join optimizations, (iii) does not rely on favorable schema properties, such as foreign key joins, (iv) is compatible with both row-store and column-store organization, and (v) does not assume favor- able pre-existing tuple placement. We implement track join to evaluate the most expensive join operator in the most ex- pensive queries of real workloads. We found that track join reduces the network traffic significantly over known meth- ods, even if pre-existing data locality is removed and all data are used in optimally compressed form throughout the join. Section 2 describes the track join algorithm presenting three variants starting from the simplest. In Section 3, we discuss costs for query optimization, tracking-aware hash joins, and semi-join filtering. Section 4 presents our exper- imental evaluation using both synthetic datasets and real workloads. In Section 5 we briefly discuss future work. In Section 6 we discuss related work and conclude in Section 7.",Orestis Polychroniou,Columbia University,orestis@cs.columbia.edu,Rajkumar Sen,Oracle Labs,rajkumar.sen@oracle.com,Kenneth A. Ross,Columbia University,kar@cs.columbia.edu,,,,,,,,,,,,,,,,,,,,,
20200120,1341,Kristi Morton,"Computer Science and Engineering Department, University of Washington Seattle, Washington, USA",kmorton@cs.washington.edu,,ParaTimer: A Progress Indicator for MapReduce DAGs,"ParaTimer: A Progress Indicator for MapReduce DAGs, ParaTimer: A Progress Indicator for MapReduce DAGs, ParaTimer: A Progress Indicator for MapReduce DAGs, ParaTimer: A Progress Indicator for MapReduce DAGs, ParaTimer: A Progress Indicator for MapReduce DAGs, ABSTRACT Time-oriented progress estimation for parallel queries is a challenging problem that has received only limited attention. In this paper, we present ParaTimer, a new type of timeremaining indicator for parallel queries. Several parallel data processing systems exist. ParaTimer targets environ- ments where declarative queries are translated into ensembles of MapReduce jobs. ParaTimer builds on previous tech- niques and makes two key contributions. First, it estimates the progress of queries that translate into directed acyclic graphs of MapReduce jobs, where jobs on different paths can execute concurrently (unlike prior work that looked at sequences only). For such queries, we use a new type of critical-path-based progress-estimation approach. Second, ParaTimer handles a variety of real systems challenges such as failures and data skew. To handle unexpected changes in query execution times due to runtime condition changes, ParaTimer provides users with not only one but with a set of time-remaining estimates, each one corresponding to a different carefully selected scenario. We implement our es- timator in the Pig system and demonstrate its performance on experiments running on a real, small-scale cluster. Categories and Subject Descriptors H.2.4 [Database Management]: Systems aparallel databases General Terms Algorithms, Design, Experimentation 1. INTRODUCTION Whether in industry or in the sciences, users today need to store, archive, and most importantly analyze increasingly large datasets. For example, the upcoming Large Synoptic Survey Telescope [17] is predicted to generate on the order of 30 TB of data every day. Parallel database management systems [1, 11, 14, 27, 29] and other parallel data processing platforms [6, 8, 12, 15] are designed to process such massive-scale datasets: they enable users to submit declarative queries over the data and they execute these queries in clusters of shared-nothing servers. Although parallelism speeds up query execution, query times in these shared-nothing platforms can still ex- hibit large intra-query and inter-query variance. In such an environment, accurate, time-remaining progress estimation for queries can be helpful both for users and for the system. Indeed, the latter can use timeremaining information to improve resource allocation [30], enable query debugging, or tune the cluster configuration (such as in response to unexpected query runtimes). Accurate progress estimation for parallel queries is a challenging problem because, in addition to the challenges shared with single-site progress estimators [3, 2, 19, 18, 21, 22], parallel environments introduce distribution, concur- rency, failures, data skew, and other issues that must be accounted for. This difficult problem has received only lim- ited attention. Our preliminary prior work [23], which we called Parallax, provided accurate estimates, but only for the limited class of parallel queries that translated into se- quences of MapReduce jobs. We also previously assumed uniform data distribution and the absence of node failures, two assumptions that are unreasonable in practice. To address these limitations, we have developed Para- Timer, a time-remaining indicator for a much broader class of queries and runtime conditions. Many parallel pro- cessing systems exist. Similar to Parallax, we developed ParaTimer for Pig queries [24] running in a Hadoop clus- ter [12], an environment that is a popular open-source paral- lel data-processing engine under active development. Within this context, ParaTimer builds on previous techniques and makes two key contributions. First, ParaTimer estimates the progress of parallel queries expressed as Pig scripts that translate into directed acyclic graphs (DAGs) of MapReduce jobs where jobs on different branches of the DAG can execute concurrently. DAGs require a radically different approach than our prior work for sequences of jobs. As a direct re- sult, unlike Parallax, ParaTimer can handle, for example, Pig scripts with join operators. Second, ParaTimer includes techniques for handling sev- eral real system challenges including failures and data skew. To handle unexpected changes in query execution times such as those due to failures, ParaTimer provides users with a set of time-remaining estimates that correspond to the predicted query execution times in different scenarios (i.e., a 507 single worst-case failure, or data skew at an operator). We call ParaTimer a comprehensive indicator because it provides this set of estimates instead of a single best guess as the other indicators do. Each of ParaTimer 's indicators can be annotated with the scenario that it corresponds to, giving users a detailed picture of possible expected behaviors. While many of the ideas presented in this paper could be adapted to other parallel data processing systems, the Pig/Hadoop environment poses several unique challenges that have informed our design and shaped our implemen- tation. Most notable, a MapReduce-style scheduler requires intermediate result materialization, schedules small pieces of work at a time, and restarts small query fragments when failures occur (rather than restarting entire queries). All three properties affect query progress and its estimates. ParaTimer is designed to be accurate while remaining sim- ple and addressing the above challenges. At a high level, ParaTimer works as follows. For basic progress estima- tion, ParaTimer builds on our prior system Parallax [23]. Parallax estimates time-remaining by breaking queries into pipelines. It estimates time-remaining for each pipeline by considering the work to be done and the speed at which that work will be performed, taking (time-varying) parallelism into account. To get processing speeds, Parallax relies on earlier debug runs of the same query on input data samples generated by the user. To support Pig scripts that translate into MapReduce DAGs where multiple jobs may execute concurrently (such as scripts with join operators), ParaTimer includes a method to identify the critical path in a query plan. It then estimates progress along that path, effectively ignoring other paths. ParaTimer also provides support for a variety of practical challenges, including failures and data skew. For data skew that can be predicted and planned for, ParaTimer takes it into account upfront. For failures and data skew that are not planned, ParaTimer outputs a set of estimates, rather than a single ""best guess, "" that bound the expected query execution time within given possible variations in runtime conditions. An interesting side-benefit of this approach is that when a query time goes outside ParaTimer 's initial bounds, a user knows that there is a problem with either his query or the cluster. ParaTimer 's output can thus aid in performance debugging. Today, parallel systems are being deployed at all scales and each scale raises new challenges. In this paper, we focus on smaller-scale systems with tens of servers because many consumers of parallel data management engines today run at this scale.1 We evaluate ParaTimer 's performance through experiments on an eight-node cluster (set to a maximum degree of parallelism of 32 divided into 16 maps and 16 re- duces). We compare ParaTimer 's performance against Par- allax [23], other state-of-the-art single-node progress indica- tors from the literature [3, 19], and Pig 's current progress indicator [25]. We show that ParaTimer is more accurate than all these alternatives on a variety of types of queries and system configurations. For all queries that we evalu- ated, ParaTimer 's average accuracy is within 5% of an ideal indicator, when given accurate cardinality estimates. The rest of this paper is organized as follows. The next section provides background on MapReduce, Hadoop, and our prior work. Section 3 presents ParaTimer 's approach and key algorithms. Section 4 presents empirical results. Section 5 discusses related work. Section 6 concludes.",Kristi Morton,"Computer Science and Engineering Department, University of Washington Seattle, Washington, USA",kmorton@cs.washington.edu,Magdalena Balazinska,"Computer Science and Engineering Department, University of Washington Seattle, Washington, USA",magda@cs.washington.edu,Dan Grossman,"Computer Science and Engineering Department, University of Washington Seattle, Washington, USA",djg@cs.washington.edu,,,,,,,,,,,,,,,,,,,,,
20200121,1196,Sai Wu,"College of Computer Science and Technology, Zhejiang University, Hangzhou, China",wusai@zju.edu.cn,,PABIRS: A Data Access Middleware for Distributed File Systems,"PABIRS: A Data Access Middleware for Distributed File Systems, PABIRS: A Data Access Middleware for Distributed File Systems, PABIRS: A Data Access Middleware for Distributed File Systems, PABIRS: A Data Access Middleware for Distributed File Systems, PABIRS: A Data Access Middleware for Distributed File Systems, Abstract aVarious big data management systems have emerged to handle different types of applications, which cast very different demands on storage, indexing and retrieval of large amount of data on distributed file system. Such diversity on de- mands has raised huge challenges to the design of new generation of data access service for big data. In this paper, we present PABIRS, a unified data access middleware to support mixed workloads. PABIRS encapsulates the underlying distributed file system (DFS) and provides a unified access interface to systems such as MapReduce and key-value stores. PABIRS achieves dramatic improvement on efficiency by employing a novel hybrid indexing scheme. Based on the data distribution, the indexing scheme adaptively builds bitmap index and Log Structured Merge Tree (LSM) index. Moreover, PABIRS distributes the computation to multiple index nodes and utilizes a Pregel-based algorithm to facilitate parallel data search and retrieval. We empirically evaluate PABIRS against other existing distributed data processing systems and verify the huge advantages of PABIRS on shorter response time, higher throughput and better scalability, over big data with real-life phone logs and TPC-H benchmark. I. INTRODUCTION The explosive growth of big data and the emergence of cloud computing have fueled the quick advances of distributed processing techniques to support storage, retrieval and analysis on massive data. Such quickly growing data bring new challenges to traditional query processing systems, especially when various types of operations are essentially required. The calling logs from billion mobile phone users, for example, are flooding into data centers of telecommunication companies, waiting for querying as well as analysis by the users and decision makers. From time to time, users submit call transaction search queries to retrieve hundreds of their call logs in last few months from billions of available call records, and the information managers are interested in analyzing the user behavior by aggregating the call logs based on the locations, call times and other attributes. The data processing system and the underlying data storage layer are expected to handle the mixed workloads of such high- selective queries and analytic queries in an efficient manner. It turns out that data access is the major bottleneck of the data processing architecture for the mixed workloads. As new data are coming into the system at extremely fast rate, complicated pre-processing techniques dramatically degrade the throughput of data insertion. It is more effective by pushing the data into the distributed file system in a natural way, e.g., simply sorting on the arrival order. Instead of complex optimization before the physical insertion of records into distributed file system, it is more promising to redesign the data access architecture, with new lightweight index over the fast growing data storage layer and well calibrated optimizations based on the in-depth analysis on the characteristics of the data distribution and the workloads. Obviously, such new design of the data access layer heavily depends on the distribution of big and diverse data in the real world. In Figure 1 and Figure 2, we present the statistics of call logs of 1,000 randomly chosen mobile numbers in a distributed file system from a world-class telecommunication company. Each 4MB data block contains call records with sorted in- sertion timestamps, to which new records are continuously appended whenever a new call is made by a mobile phone user. In Figure 1, we report the size of call logs in terms of each caller id, implying that most of the caller ids make a few calls only, while less than 1% of the ids contribute much more call records than the others. This observation follows the well known power law phenomenon commonly appearing in physical world. It results in huge variance on the number of data blocks associated to each caller id, as is shown in Figure 2. The records generated by the top 1% hot caller ids span on almost every block of the distributed file system, potentially incurring high overhead for queries retrieving records. It also brings challenges to the design of the index mechanism on top of the storage layer, when data processing engine tries to locate the sparse records. To fully address these problems, we propose PABIRS, a generic data access middleware for mixed workloads. PABIRS works as an interface between the underlying Distributed File System (DFS) and data management systems, e.g. Hadoop and HBase. It employs a novel hybrid indexing scheme to support efficient data retrieval for various query workloads. Specifically, a cost model component is elicited at the central position in PABIRS, which measures the overhead of different access methods and selects the most effective one. More- over, PABIRS generally supports existing main-stream data processing systems on top, allowing programmers to easily add PABIRS as a middleware layer with minimal efforts. In some sense, PABIRS shares the same design philosophy of the NoDB [2] strategy. However, PABIRS is designed as a middleware of the DFS which is more flexible and can potentially work with any applications using DFS as their storage systems. The most distinguishing feature of PABIRS is its hybrid indexing architecture. As different types of indices were originally designed for different workloads, it is crucial for the cost estimator to understand the workload and facilitate accurate index selection and tuning. For data following the uniform distribution, for example, PABIRS utilizes bitmap index to support fast retrieval, since bitmap index is well known for its high efficiency on sparse record indexing and retrieval. However, bitmap index is incapable of handling skewed distribution with hot keys. LSM index, with a forest of B-trees, is much more superior when records with identical keys frequently appearing in the data. PABIRS applies the dual deployment strategy with both index types, and adaptively chooses an exclusive group of keys for LSM under a unified optimization framework based on the cost evaluation. In its internal architecture, PABIRS disseminates the pro- cessing logics to all distributed file system nodes, where a Pregel engine [17] is run to synchronize the operation units on all the nodes. In particular, the index structure is considered as a graph consisting of multiple levels of vertices and data access service is modeled as a vertex-centric graphical program. The vertices are partitioned among the physical nodes and the search is conducted in parallel. This design makes the index layer of PABIRS a more flexible and extensible component for efficient parallel processing and load balancing. We deploy PABIRS on top of the HDFS [26] and evaluate its performance using the real mobile phone call log data and the well-known TPC-H benchmark. PABIRS presents huge margin of advantages on performance for different work- loads of transaction-based and analytic-based workloads. The remainder of the paper is organized as follows. Section II presents the overview of PABIRS and its architecture. Section III introduces how the hybrid indexing approach works. Section IV discusses the optimization strategies in PABIRS and Section V shows the experiment results. In Section VI, we briefly review previous work and the paper is concluded in Section 7.",Sai Wu,"College of Computer Science and Technology, Zhejiang University, Hangzhou, China",wusai@zju.edu.cn,Gang Chen,"College of Computer Science and Technology, Zhejiang University, Hangzhou, China",cg@zju.edu.cn,Xianke Zhou,"NetEase (Hangzhou) Network Co., Ltd., Hangzhou, China",hzzhouxianke@corp.netease.com,Zhenjie Zhang,"Advanced Digital Sciences Center, Illinois at Singapore Pte. Ltd., Singapore",zhenjie@adsc.com.sg,Anthony K. H. Tung,"School of Computing, National University of Singapore, Singapore",atung@comp.nus.edu.sg,Marianne Winslett,"Deparement of Computer Science, University of Illinois at Urbana-Champaign, USA",winslett@illinois.edu,,,,,,,,,,,,
20200122,1342,Paolo Bellavista,"Universit? di Bologna, Department of Computer Science and Engineering, Bologna, Italy",paolo.bellavista@unibo.it,,Adaptive Fault-Tolerance for Dynamic Resource Provisioning in Distributed Stream Processing Systems,"Adaptive Fault-Tolerance for Dynamic Resource Provisioning in Distributed Stream Processing Systems, Adaptive Fault-Tolerance for Dynamic Resource Provisioning in Distributed Stream Processing Systems, Adaptive Fault-Tolerance for Dynamic Resource Provisioning in Distributed Stream Processing Systems, Adaptive Fault-Tolerance for Dynamic Resource Provisioning in Distributed Stream Processing Systems, Adaptive Fault-Tolerance for Dynamic Resource Provisioning in Distributed Stream Processing Systems, ABSTRACT A growing number of applications require continuous processing of high-throughput data streams, e.g., financial anal- ysis, network traffic monitoring, or Big Data analytics for smart cities. Stream processing applications typically require specific quality-of-service levels to achieve their goals; yet, due to the high time-variability of stream characteris- tics, it is often inefficient to statically allocate the resources needed to guarantee application Service Level Agreements (SLAs). In this paper, we present LAAR, a novel method for adaptive replication that trades fault tolerance for in- creased capacity during load spikes. We have implemented and validated LAAR as a middleware layer on top of IBM In- foSphere Streamsr. We have performed a wide set of experiments on an industrial-quality 60-core cluster deployment and we show that, under the assumption of only statistical knowledge of streams load distribution, LAAR can reduce resource consumption while guaranteeing an upper-bound on information loss in case of failures. Keywords data streams processing, fault-tolerance, dynamic adapta- tion, service-level agreement, IBM InfoSphere Streamsr 1. INTRODUCTION In recent years, the ability to effectively process Big Data Streams is becoming increasingly important: the vision of smarter cities where data from several physical-world sources are continuously collected, filtered, analyzed, and fed back to administrators and citizens to assist them in their hour-by-hour tasks is just one example of the multitude of novel scenarios where handling large and unbounded flows of information in real-time is a primary requirement. Experience with Cloud services [31] has shown that the possibility to offload the management of computing infras- tructures to third parties represents an attractive opportu- nity for both developers and cloud providers. However, in a cloud environment, the nature of stream processing applica- tions poses hard challenges to platform providers, including the ability to offer, at the same time, extreme performance elasticity in spite of load variations and resiliency to failures, while keeping costs limited. From a provider perspective, one major problem lies in the necessity to handle load fluctuations due to sudden and possibly temporary variations in the rates of data streams feeding the hosted applications. If not handled properly, in fact, load peaks can lead to increased processing latency due to data queuing and to data loss due to queue overflows. To avoid these effects, it is necessary to allocate the proper amount of additional resources for the overloaded applica- tions, either statically or dynamically when load variations are detected [4, 8, 22]. Another typical requirement for stream processing appli- cations is the implementation of fault-tolerance techniques. In fact, since they usually run for (indefinitely) long time in- tervals, failures are unavoidable. Many proposals in the lit- erature have investigated possible fault-tolerance approaches  a including active replication [9, 28], checkpointing [11, 18], replay logs [6, 16], or hybrid solutions [34]  a each providing different trade-offs between runtime cost in absence of fail- ures (best-case) and recovery cost. Whichever the adopted technique, maintaining some form of replication at some level (software/hardware components, state, or messages) is a significant overhead in terms of computing resources. In a large class of applications, however,  ""perfect "" fault tolerance is not always required, while it is of primary im- portance to effectively manage temporary load variations. This is very common, for example, when dealing with Smart City-generated Big Data. In this context, in fact, large data streams are produced by many distributed sources  a e.g., mobile phones, ad-hoc sensing devices, or vehicles  a that continuously capture and transmit sensed environmen- tal features. These data need to be analyzed in real-time, and results must be promptly delivered to let appropriate control actions be performed. In this kind of scenarios, controlled information loss is usually tolerable, given the common partial information redundancy or overlap of input streams1. Consider, for instance, an application used to control traffic light signals based on periodic reports of vehicles ' positions, among other factors. During high traffic conditions (i.e., high system load), it is clearly preferable to compute on incomplete information than delay control decisions, given the high redundancy in reported positions. At the same time, during low traffic conditions, processing events with accuracy is still important. In this work, we investigate the possibility to trade-off reli- ability guarantees and execution cost, and use the conserved resources to handle load variations. We propose a novel method, called Load-Adaptive Active Replication (LAAR), that dynamically deactivates and activates redundant replicas of application Processing Elements (PE) in order to claim/release resources and accommodate temporary load variations. Our technique provides a-priori guarantees about the achievable levels of fault-tolerance, expressed in terms of an internal completeness metric that captures the max- imum amount of information that can be lost in case of failures. We show that LAAR can be suitably implemented as a middleware-level layer on top of existing stream pro- cessing platforms, and we present general architectural and design guidelines about how to do it efficiently. As a working proof-of-concept, we describe an implementation of LAAR on top of IBM InfoSphere Streamsr [13], an enterprise-level stream processing platform, and we discuss experimental results about the performance of LAAR on a 60-core IBM BladeCenterr cluster deployment. The remainder of the paper is organized as follows: af- ter reviewing the related literature in Section 2, we present the considered SLA-aware stream processing service model in Section 3. In Section 4, we model our middleware and explain its goals and runtime architecture. Finally, in Sec- tion 5, we report a wide set of performance results that quantitatively evaluate the effectiveness of our proposal. ",Paolo Bellavista,"Universit? di Bologna, Department of Computer Science and Engineering, Bologna, Italy",paolo.bellavista@unibo.it,Antonio Corradi,"Universit? di Bologna, Department of Computer Science and Engineering, Bologna, Italy",antonio.corradi@unibo.it,Spyros Kotoulas,"Smarter Cities Technology Centre, IBM Research, Dublin, Ireland",spyros.kotoulas@ie.ibm.com,Andrea Reale,"Universit? di Bologna Department of Computer Science and Engineering Bologna, Italy",andrea.reale@unibo.it,,,,,,,,,,,,,,,,,,
20200123,1343,Nadathur Satish,"Throughput Computing Lab, Intel Corporation",nadathur.rajagopalan.satish@intel.com,,Fast Sort on CPUs and GPUs: A Case for Bandwidth Oblivious SIMD Sort,"Fast Sort on CPUs and GPUs: A Case for Bandwidth Oblivious SIMD Sort, Fast Sort on CPUs and GPUs: A Case for Bandwidth Oblivious SIMD Sort, Fast Sort on CPUs and GPUs: A Case for Bandwidth Oblivious SIMD Sort, Fast Sort on CPUs and GPUs: A Case for Bandwidth Oblivious SIMD Sort, Fast Sort on CPUs and GPUs: A Case for Bandwidth Oblivious SIMD Sort, ABSTRACT Sort is a fundamental kernel used in many database operations. In-memory sorts are now feasible; sort performance is limited by compute flops and main memory bandwidth rather than I/O. In this paper, we present a competitive analysis of comparison and noncomparison based sorting algorithms on two modern architectures - the latest CPU and GPU architectures. We propose novel CPU radix sort and GPU merge sort implementations which are 2X faster than previously published results. We perform a fair comparison of the algorithms using these best performing implementations on both architectures. While radix sort is faster on current architectures, the gap narrows from CPU to GPU architectures. Merge sort performs better than radix sort for sorting keys of large sizes - such keys will be required to accommodate the increasing cardinality of future databases. We present analytical models for analyzing the performance of our implementations in terms of architectural features such as core count, SIMD and bandwidth. Our obtained performance results are successfully predicted by our models. Our analysis points to merge sort winning over radix sort on future ar- chitectures due to its efficient utilization of SIMD and low band- width utilization. We simulate a 64-core platform with varying SIMD widths under constant bandwidth per core constraints, and show that large data sizes of 240 (one trillion records), merge sort performance on large key sizes is up to 3X better than radix sort for large SIMD widths on future architectures. Therefore, merge sort should be the sorting method of choice for future databases. Categories and Subject Descriptors H.2 [Database Management]: Systems General Terms Performance, Algorithms 1. INTRODUCTION Sorting is of fundamental importance in databases. Common applications of sorting in database systems include index creation, user-requested sort queries, and operations such as duplicate removal, ranking and merge-join operations. Sorting on large data- bases has traditionally focused on external sorting algorithms. How- ever, the rapid increase in main memory capacity has made in- memory sorting feasible. In-memory sorts are bounded by compute, bandwidth and la- tency characteristics of processor architectures. Recent and future trends in modern computer architectures are therefore of primary importance for high performance sort implementations. Compute capacity has increased through a combination of having more cores (thread-level parallelism) with each core having wide vector (SIMD) units to exploit data-level parallelism. Core counts will increase rapidly as Moore 's law continues to increase the number of onchip transistors. The SIMD width of modern CPU and GPU pro- cessors has been steadily increasing - from 128-bit in SSE architec- tures, 256-bit in AVX [14] to 512-bit in the upcoming Larrabee [23] architecture. GPUs have a logical 1024-bit SIMD with physical SIMD widths of 256-bits on the latest NVIDIA GTX 200 series, increasing to 512-bits on the upcoming Fermi architecture [19]. Memory bandwidth is increasing at a slower pace than compute. Algorithms that are bound by memory bandwidth will not scale well to future architectures. A variety of algorithms have been developed for sorting a list of numbers. Sorting algorithms can be broadly classified as ei- ther comparison based or non-comparison based sorts. Comparison based sorting algorithms rearrange the data items based on the re- sults of comparing pairs of elements at a time. Non-comparison based sorts rely on using the absolute values of the data items, rather than comparisons, to rearrange the data. A common example of a non-comparison based sort is radix sort. Radix sort is a mul- tiple pass sort algorithm that buckets data according to individual digits of the data items. Sorting algorithms differ in their compu- tational complexity, which dictates the inherent amount of com- putation required by the algorithm, and also differ in their archi- tectural friendliness, or how well they can use current and future architectural trends, such as increasing thread-level and data-level parallelism on modern architectures. There is often a trade-off be- tween these factors. For instance, radix sorts are not naturally data-parallel, unlike comparison sorts that can use data parallel merging networks. Further, radix sort needs many passes over each data item - resulting in high bandwidth utilization. Merge sort, on the other hand, can be made bandwidth friendly (and is used in external disk sorts for this reason). Opposing these architectural inefficiencies of radix sort is its lower computational complex- ity of O(N), as against the lower bound  (N logN) of comparison based sorts. The right choice of sorting algorithms becomes a trade- off between computational complexity and architectural efficiency; such trade-offs are architecture-dependent. There has, so far, not 351 been much work in analyzing the efficiency with which sorting techniques utilize modern architectural features. In this work, we evaluate these trade-offs on two sorting algorithms - radix sort with a low O(N) computational complexity, and a SIMD-efficient and bandwidth oblivious merge sort with a complexity of O(N logN). We investigate these trade-offs on the latest CPU and GPU archi- tectures, which are commercially widespread and hence of interest to the database community. The Intel Core i7 CPU has 4 cores each with 128-bit SSE width, while the NVIDIA GTX 280 has 30 SMs each with 256-bit SIMD units. In order to make such an investigation fair, the algorithms must be implemented as efficiently as possible on each architecture. We, therefore, identified bottlenecks found in common implementations of each sorting algorithm (such as irregular memory accesses, lack of SIMD use, conflicts in local storage such as cache or shared memory) and optimized our algorithms to avoid such bottlenecks. Our novel CPU radix sort algorithm uses a buffer stored in cache to localize scatters; this avoids capacity and conflict misses, resulting in the fastest reported CPU sorts. We implemented an optimized merge sort on the GPU which is only 10% off the best known sort- ing algorithm (a radix sort) on the GPU. We adopted highly opti- mized codes where available, including a CPU merge sort and a GPU radix sort. Our resulting implementations are the best per- forming implementations of these sorting algorithms on these ar- chitectures, and include the highest performing sort on each archi- tecture. We provide a performance model for each of our result- ing implementations in terms of the compute, bandwidth and SIMD usage of our algorithms. Our analysis results match well with the actual performance results. The influence of architectural awareness on sorting algorithms is clear from our investigations. The best implementation of the same radix sort algorithm is very different on CPUs and GPUs - a SIMD friendly 1-bit split based code is best on GPUs that heavily rely on data-level parallelism, while a scalar buffer-based scatter approach works best on CPUs with lower SIMD widths. However, we show that both versions of radix sort have heavy bandwidth demands due to their multi-pass nature. The bandwidth demand of radix sort increases for large key sizes - radix becomes bandwidth bound on even the latest CPUs on 6-byte keys. GPUs have much higher bandwidth - however, even then a part of the radix sort algorithm is bandwidth bound. Algorithms that are bandwidth bound cannot use compute resources effectively. On the other hand, merge sort uses bandwidth resources efficiently, and is compute-bound. Merge sort hence becomes faster than radix on GPUs on keys larger than 8- bytes, and faster on CPUs for keys greater than 9-bytes. In addition, merge sort can also utilize SIMD resources efficiently. While the higher computational complexity of merge sort does make it slower than radix on current architectures for large data sets of 4-byte keys, the gap narrows from 1.7X on CPUs with 128-bit SIMD to only about 10% on GPUs with 256-bit SIMD. Having identified the bottlenecks of these algorithms on current hardware, we project the performance of our algorithms on future architectures with wider SIMD and lower bandwidth-to-compute requirements. The bandwidth-oblivious SIMD-friendly merge sort will perform better on such architectures. We confirm our projec- tions by simulating our algorithms on architectures with varying SIMD widths, and show that as SIMD widths increase to 2048- bit and beyond, SIMD merge sort performance for 8-byte keys is 1.5X faster than radix sort on data sizes as large as 240 (one trillion records). For 16-byte keys, the performance ratio further increases to 3X better than radix sort. The bandwidth-oblivious SIMD- friendly merge sort, should, therefore, be the sorting method of choice for future databases.",Nadathur Satish,"Throughput Computing Lab, Intel Corporation",nadathur.rajagopalan.satish@intel.com,Changkyu Kim,"Throughput Computing Lab, Intel Corporation",0,Jatin Chhugani,"Throughput Computing Lab, Intel Corporation",,Anthony D. Nguyen,"Throughput Computing Lab, Intel Corporation",,Victor W. Lee,"Throughput Computing Lab, Intel Corporation",,Daehyun Kim,"Throughput Computing Lab, Intel Corporation",,Pradeep Dubey,"Throughput Computing Lab, Intel Corporation",,,,,,,,,,
20200124,1344,Stacy Patterson,"Department of Electrical Engineering Technion - Israel Institute of Technology Haifa, 32000, Israel",stacyp@ee.technion.ac.il,,"Serializability, not Serial: Concurrency Control and Availability in Multi-Datacenter Datastores","Serializability, not Serial: Concurrency Control and Availability in Multi-Datacenter Datastores, Serializability, not Serial: Concurrency Control and Availability in Multi-Datacenter Datastores, Serializability, not Serial: Concurrency Control and Availability in Multi-Datacenter Datastores, Serializability, not Serial: Concurrency Control and Availability in Multi-Datacenter Datastores, Serializability, not Serial: Concurrency Control and Availability in Multi-Datacenter Datastores, ABSTRACT We present a framework for concurrency control and availability in multi-datacenter datastores. While we consider Google 's Megastore as our motivating example, we define general abstractions for key components, making our solu- tion extensible to any system that satisfies the abstraction properties. We first develop and analyze a transaction management and replication protocol based on a straightforward implementation of the Paxos algorithm. Our investigation reveals that this protocol acts as a concurrency prevention mechanism rather than a concurrency control mechanism. We then propose an enhanced protocol called Paxos with Combination and Promotion (Paxos-CP) that provides true transaction concurrency while requiring the same per instance message complexity as the basic Paxos protocol. Finally, we compare the performance of Paxos and Paxos-CP in a multi-datacenter experimental study, and we demon- strate that Paxos-CP results in significantly fewer aborted transactions than basic Paxos. 1. INTRODUCTION Cloud computing has the potential to become the founda- tion for most information technology architectures. It offers application developers access to seemingly infinite storage, compute, and network resources, all on a pay-per-use basis. While the appeal of the cloud computing model is obvious from a financial perspective, its success also depends on the ability of clouds to provide reliable, scalable services that support the features developers need. In particular, it is important that cloud datastores, such as Google 's BigTable [8] and Amazon 's SimpleDB [1], provide support for various types of data consistency and guarantee the availability of application data in the face of failures. Initially, cloud datastores provided only eventually con- sistent update operations guaranteeing that updates would eventually propagate to all replicas. While these datastores were highly scalable, developers found it difficult to create applications within the eventual consistency model [20]. Many cloud providers then introduced support for atomic access to individual data items, in essence, providing strong consistency guarantees. This consistency level has become a standard feature that is offered in most cloud datastore im- plementations, including BigTable, SimpleDB, and Apache HBase [16]. Strong consistency of single data items is suffi- cient for many applications. However, if several data items must be updated atomically, the burden to implement this atomic action in a scalable, fault tolerant manner lies with the software developer. Several recent works have addressed the problem of implementing ACID transactions in cloud datastores [2, 10, 11], and, while full transaction support re- mains a scalability challenge, these works demonstrate that transactions are feasible so long as the number of tuples that are transactionally related is not  ""too big "". While many solutions have been developed to provide con- sistency and fault tolerance in cloud datastores that are hosted within a single data center, these solutions are of no help if the entire datacenter becomes unavailable. For example, in April 2011, a software error brought down one of Amazon 's EC2 availability zones and caused service dis- ruption in the U.S. East Region [24]. As a result, major web sites like Reddit, Foursquare, and Quora were unavail- able for hours to days [5]. And, in August 2011, lightning caused Microsoft and Amazon clouds in Dublin [15] to go offline for hours. In both instances, there were errors in the recovery process, and it was not possible to restore a consistent snapshot of some application data. These recent outages demonstrate the need for replica- tion of application data at multiple datacenters as well as the importance of using provably correct protocols for performing this replication. In a recent work, Baker et al. de- scribe Megastore, Google 's approach to providing transac- tions in the cloud with full replication at multiple datacen- ters [2]. Megastore is implemented on top of BigTable and provides support for ACID transactions over small sets of data items called entity groups. It uses multi-version concurrency control and a replicated write-ahead log. Replication is performed using the Paxos algorithm [18] to ensure consistency even with unreliable communication and datacenter outages. While the paper presents an overview of the Megastore system, it lacks the formality and detail required to verify Megastore 's correctness. We assert that such analysis is needed for systems like Megastore, especially in light of the outages described above and the widely acknowledged difficulties associated with understanding and implementing the Paxos algorithm [7, 19, 25]. In this work, we address the need for formal analysis of replication and concurrency control in transactional cloud datastores. We define and analyze several Paxos-based pro- tocols for replication and transaction management in the multi-datacenter setting. While we take Megastore as our motivating example, we define general abstractions for each of the key components, and we use these abstractions in our protocol design and analysis. The specific contributions of our work are: ? We provide a formal description of the Paxos protocol for replication and concurrency control, and we prove its correctness. Through our analysis, we also show that the Paxos protocol, as implemented in Megastore, aborts transactions that could be safely committed. In essence, it acts as a concurrency prevention mechanism rather than a concurrency control mechanism. ? We propose an enhanced replication and concurrency control protocol that we call Paxos with Combination and Promotion (Paxos-CP). Paxos-CP enables true transaction concurrency, with the same per-instance message complexity as the original Paxos protocol. ? We compare the performance of Paxos and Paxos-CP in a multi-datacenter experimental study, and we demon- strate the benefits of our enhanced Paxos protocol. The remainder of this paper is organized as follows. In Section 2, we give an overview of the design of the cloud datastore including the data model and reference architecture. Section 3 summarizes the theoretical foundations that we use to analyze the correctness of the transactional cloud datastore. In Section 4, we present the details of the trans- action manager, including the basic Paxos commit protocol, and we prove its correctness. In Section 5, we present our extended Paxos commit protocol that allows for transaction concurrency, and we prove the correctness of this protocol. We present evaluation results comparing the basic and ex- tended Paxos commit protocols in Section 6. In Section 7, we discuss related work, and we conclude in Section 8.",Stacy Patterson,"Department of Electrical Engineering Technion - Israel Institute of Technology Haifa, 32000, Israel",stacyp@ee.technion.ac.il,Aaron J. Elmore,"Department of Computer Science University of California, Santa Barbara Santa Barbara, CA 93106",aelmore@cs.ucsb.edu,Faisal Nawab,"Department of Computer Science University of California, Santa Barbara Santa Barbara, CA 93106",nawab@cs.ucsb.edu,Divyakant Agrawal,"Department of Computer Science University of California, Santa Barbara Santa Barbara, CA 93106",agrawal@cs.ucsb.edu,Amr El Abbadi,"Department of Computer Science University of California, Santa Barbara Santa Barbara, CA 93106",amr@cs.ucsb.edu,,,,,,,,,,,,,,,
20200125,1380,Zhepeng Yan,"University of Pennsylvania Philadelphia, PA, USA",zhepeng@cis.upenn.edu,,Actively Soliciting Feedback for Query Answers in Keyword Search-Based Data Integration,"Actively Soliciting Feedback for Query Answers in Keyword Search-Based Data Integration, Actively Soliciting Feedback for Query Answers in Keyword Search-Based Data Integration, Actively Soliciting Feedback for Query Answers in Keyword Search-Based Data Integration, Actively Soliciting Feedback for Query Answers in Keyword Search-Based Data Integration, Actively Soliciting Feedback for Query Answers in Keyword Search-Based Data Integration, ABSTRACT The problem of scaling up data integration, such that new sources can be quickly utilized as they are discovered, remains elusive: global schemas for integrated data are difficult to develop and expand, and schema and record matching techniques are limited by the fact that data and metadata are often under-specified and must be disambiguated by data experts. One promising approach is to avoid using a global schema, and instead to develop keyword search- based data integration - where the system lazily discovers associ- ations enabling it to join together matches to keywords, and return ranked results. The user is expected to understand the data domain and provide feedback about answers' quality. The system general- izes such feedback to learn how to correctly integrate data. A major open challenge is that under this model, the user only sees and offers feedback on a few ""top-k"" results: this result set must be carefully selected to include answers of high relevance and answers that are highly informative when feedback is given on them. Existing systems merely focus on predicting relevance, by composing the scores of various schema and record matching algorithms. In this paper we show how to predict the uncertainty associated with a query result's score, as well as how informative feedback is on a given result. We build upon these foundations to develop an active learning approach to keyword search-based data integration, and we validate the effectiveness of our solution over real data from several very different domains. 1. INTRODUCTION The vision of rapid information integration remains elusive, de- spite steady progress in system architectures [13] and in alignment techniques for discovering links among records [11] and schema el- ements [28]. In general, the approach is to define one or more inte- grated or mediated schemas capturing the data domain, use schema mapping (alignment) and entity resolution (record linking) tools to map data sources into the mediated schema, and finally allow users to pose structured queries against mediated schemas. A stumbling block is that string, pattern, and structural similari- ties among data and metadata elements (the core techniques whose outputs are combined by alignment tools) do not suffice to unique- ly identify the correspondences between data or metadata items. The resulting ambiguous matches can only be resolved with do- main (commonly, human) expertise. As a result, many of today's tools aim for semi-automated matching, where the system makes predictions and relies on a human domain expert to correct any mistakes or resolve any uncertainties (e.g., see [28, p. 345]). There are several shortcomings to having a database administra- tor inspect the output of a schema matching tool before adding the mapping to an existing system: (1) administrator vetting becomes a bottleneck to the system incorporating sources; (2) the metadata might not clearly describe the data that must be mapped1; (3) subtle variations in semantics may only show up in occasional, incorrect query results. Moreover, the mediated schema itself can be a bot- tleneck to adding new data, as new sources may have concepts that do not yet exist at the global level. For these reasons, recent work [4, 29, 34, 35] has proposed to complement (or even replace) conventional integration with tech- niques that do not rely on a mediated schema and schema mappings created by an administrator. Instead the proposal is to adopt a keyword search over databases model [5, 19, 20, 25] where matches to individual keywords are assembled into query results by discov- ering ""join trees"" that link the matches. This requires discovering paths of associations (alignments across records, terms, or schema elements in sources) that can join matching records together. Under this model, the output of alignment algorithms is used directly to answer queries, with no administrator intervention: the system relies on the end user to have the domain expertise to vet the re- sults, and to provide feedback [34, 35] on the system's ranking of (some) individual query results. Now instead of having a human administrator correct bad associations, the system must learn the correct score (possibly zero or infinite) of each individual associ- ation, given the user's feedback on query answers that are formed from multiple associations [34]. This model is a form of ""pay-as-you-go"" integration [13], as it enables the system and its users to focus their attention on those associations that relate to actual information needs. The associ- ations relevant to frequently posed queries should be the ones that receive the most attention and refinement. In fact the pay-as-you-go approach can be used to complement and inform more traditional integration techniques: the keyword search log can help a human administrator determine which parts of the data to prioritize integrating, and provide clues for what mappings are most relevant. However, to successfully learn to integrate data, the system must balance its need to acquire feedback useful for answering future queries, versus the requirement that each user immediately gets the information he or she needs. Today's keyword search systems have approached this problem by simply assuming the query scoring function is accurate: they return the top-k results according to the scoring function, which in turn bases its scores on the predicted (but possibly incorrect) output of matching tools. Under this model the user will attempt to remove false positives but has no way of seeing - and providing feedback on - false negatives. Such a model works well when the system returns a good mix of correct and invalid results and the user can ""separate"" them. However, as the number and complexity of sources and their at- tributes increases, many potential queries are likely to have similar scores, due to inherent uncertainty in combining low-confidence results from various matching algorithms. The number of potential results can grow rapidly as the number of keyword matches increases, whereas the number of results seen by the user remains constrained by the dimensions of the screen and the limits of user attention. Thus, when a keyword-based data integration system selects queries to produce answers, it should not merely choose alignments based on the relative scores of associations - but also the uncertainty associated with a given query result, and the informativeness of feedback given on that particular result. In this paper, we use active learning to help the system deter- mine which query results to present, given a combination of their predicted score, their inherent uncertainty, and the amount of infor- mation gained about other potential queries. Intuitively, the infor- mativeness of feedback on a query result is related to how much uncertainty there is about the result's relevance to the query, and how many other similar share features with this result - mean- ing that feedback on the first result also reduces their uncertain- ty. We provide a more precise characterization of informativeness later in the paper. Our work goes beyond previous attempts to use uncertainty-directed ranking in the pay-as-you-go-integration space, such as [24] which focused on individual mappings, by look- ing at the total uncertainty associated with queries and their results, and how this uncertainty should be combined with relevance rank- ing. The key questions addressed in this paper are how to estimate the utility of a given query to the system and to the user, and how to estimate the uncertainty of a query's score, in applying active learning to the problem of determining the relevance of associations to a query. Specifically, we make the following contributions: ? Techniques for estimating the uncertainty associated with a query, through the notions of entropy and variance, and by combining the probability distributions of the output for in- dividual schema matching or record linking outputs. ? Pruning and active learning techniques that focus the user's attention on the query results most likely to either be relevant, or help the system produce better results. ? A scoring model using expected model change to relate the user's model of browsing data to how we should combine and rank both useful and uncertain query answers. ? A method of clustering similar join queries, and choosing the most useful representative. ? An experimental evaluation demonstrating the effectiveness of our approach across several real data domains. Section 2 provides the context of our problem, including the ba- sic workflow of our integration task. Section 3 shows how we as- sess the informativeness of each query. Section 4 then describes how we combine informativeness and predicted score to return ranked query results, and to learn from feedback on them. We experimen- tally analyze our results in Section 5, describe related work in Sec- tion 6, and conclude in Section 7. ",Zhepeng Yan,"University of Pennsylvania Philadelphia, PA, USA",zhepeng@cis.upenn.edu,Nan Zheng,"University of Pennsylvania Philadelphia, PA, USA",nanzheng@cis.upenn.edu,Zachary G. Ives,"University of Pennsylvania Philadelphia, PA, USA",zives@cis.upenn.edu,Partha Pratim Talukdar,"Carnegie Mellon University  Pittsburgh, PA USA",ppt@cs.cmu.edu,Cong Yu," Google, Inc. New York, NY USA",congyu@google.com,,,,,,,,,,,,,,,
20200126,740,Arvind Arasu,"Microsoft Research Redmond, WA",arvinda@microsoft.com,,Data Generation using Declarative Constraints,"Data Generation using Declarative Constraints, Data Generation using Declarative Constraints, Data Generation using Declarative Constraints, Data Generation using Declarative Constraints, Data Generation using Declarative Constraints, ABSTRACT We study the problem of generating synthetic databases hav- ing declaratively specified characteristics. This problem is motivated by database system and application testing, data masking, and benchmarking. While the data generation problem has been studied before, prior approaches are either non-declarative or have fundamental limitations relating to data characteristics that they can capture and efficiently support. We argue that a natural, expressive, and declara- tive mechanism for specifying data characteristics is through cardinality constraints; a cardinality constraint specifies that the output of a query over the generated database have a certain cardinality. While the data generation problem is intractable in general, we present efficient algorithms that can handle a large and useful class of constraints. We include a thorough empirical evaluation illustrating that our algo- rithms handle complex constraints, scale well as the number of constraints increase, and outperform applicable prior techniques. Categories and Subject Descriptors D.2.5 [Software Engineering]: Testing and Debugging a Testing tools; H.2.4 [Database Management]: Systems a Query processing General Terms Algorithms, Performance, Reliability, Experimentation Keywords Data Generation, Testing, Masking, Benchmarking, Constraints 1. INTRODUCTION We consider the problem of generating a synthetic database instance having certain data characteristics. Many applications require synthetically generated data: 1. DBMS testing: When we design a new DBMS compo- nent such as a new join operator or a new memory manager, we require synthetic database instances with specific char- acteristics to test correctness and performance of the new component [7, 22]. For example, to test the code module of a hybrid hash join that handles spills to disk, we might need a database instance with a high skew on the outer join attribute. As another example, to study the interaction of the memory manager and multiple hash join operators, we might need a database instance that has particular interme- diate result cardinalities for a given query plan [9]. 2. Data masking and database application testing: Organi- zations sometimes outsource the testing of their database applications to other organizations. However an outsourc- ing organization might not be able to share its internal databases (over which the applications run) with the test- ing organization due to privacy considerations, requiring us to generate a synthetic database that behaves like the orig- inal database for the purposes of testing. (We emphasize that our goal here is not to study the general data masking problem with its privacy considerations; we are merely sug- gesting that data generation might be a useful component of a general data masking solution.) 3. Benchmarking: In order to decide between multiple com- peting data management solutions, a customer might be interested in benchmarking the solutions [22]. The standard benchmarks such as TPC-H might not capture many of the application scenarios and data characteristics of interest to the customer, motivating the need for synthetic data generation. A related scenario is upscaling, where we are interested in generating a synthetic database that is an upscaled ver- sion of an existing database. Upscaling is useful for future capacity planning purposes. Data characteristics and cardinality constraints: The applications of data generation above require a wide variety of data characteristics in the generated synthetic databases. A natural class of characteristics are schema properties such as key and referential integrity constraints, functional de- pendencies, and domain constraints (e.g., age is an integer between 0 and 120). A synthetic database for DB appli- cation testing often needs to satisfy such constraints since the application being tested might require these constraints for correct functioning. If DB application testing involves a visual component with a tester entering values in fields of a form, the synthetic database might need to satisfy natu- 685 ralness properties, e.g., the values in an address field should  ""look like "" real addresses. In benchmarking and DBMS testing, we typically need to capture characteristics that can influence the performance of a query over the generated database. These include, for example, ensuring that values in a column be distributed in a particular way, ensuring that values in a column have a certain skew, or ensuring that two or more columns are cor- related. We note correlations can involve joining multiple tables. For example, in a customer-product-order database, we might need to capture correlations between the age of customers and the category of products they purchase. In data masking, we might require synthetic data to result in the same application performance as the original data, with- out revealing sensitive information from the original data. In addition to the richness of data characteristics, appli- cations might require several properties and constraints be together satisfied in a generated database. This requirement motivates the need for a declarative approach to data gen- eration as opposed procedural approaches considered in [7, 15]. As a concrete example, consider generating a customer- product-order database where we need to capture correla- tions between several pairs of columns such as customer age and product category, customer age and income, and product category and supplier location. It is fairly nontrivial for a programmer to design a procedure that outputs a database with all of the above properties, even with the right proce- dural primitives. A natural and expressive language for specifying data characteristics is a set of cardinality constraints. A cardinality constraint specifies that the output of a given query over the generated database should have a particular cardinality. As a simple example, we can (approximately) specify the distribution of values in a column by providing a histogram, and a histogram can be represented as a collection of cardinal- ity constraints, one for each bucket. In Section 2, we show that many of the data characteristics discussed earlier can be represented using cardinality constraints. The idea of using cardinality constraints for data gener- ation is not new and has been proposed in QAGen [6] and its extension MyBenchmark [22]. However, in this work car- dinality constraints are mostly used for capturing workload characteristics and the ability of cardinality constraints to express more general data characteristics is not discussed. Motivated by the above discussion, the goal of this paper is to design efficient algorithms for generating synthetic databases that satisfy a given set of cardinality constraints. The set of constraints provided as input can be large (say, thousands); for example, even specifying a simple histogram can require 10s or 100s of constraints. The queries in the constraints can be complex, possibly involving joins over multiple tables. Prior Work: While QAGen [6] and MyBenchmark [22] do not discuss the expressiveness aspects of cardinality con- straints, their techniques are quite general and can be used for our purposes. However, they have some basic limita- tions. QAGen and MyBenchmark assume that cardinality constraints are available in a particular form called anno- tated query plans (AQP). An annotated query plan is a query plan with a subset of plan nodes annotated with cardinali- ties. We can show that we can encode cardinality constraints as AQPs and vice-versa. For data generation, QAGen uses a novel approach called symbolic query processing. Briefly, it starts with a symbolic database; a symbolic database is like a regular database, but its attribute values are symbols (vari- ables), not ""constants. "" It then translates the input AQPs to constraints over the symbols in the database, and invokes a black-box constraint satisfaction program (CSP) to identify values for symbols that satisfy all the constraints. One limitation of QAGen is that it can handle a single AQP, and therefore cannot be directly used to generate databases that satisfy multiple arbitrary constraints. This limitation is identified and addressed in MyBenchmark [22]. Briefly, to handle n AQPs, MyBenchmark uses QAGen to generate n symbolic databases with constraints and performs  ""matching "" between these databases to heuristically identify m n databases that together satisfy all the AQPs. MyBenchmark is not guaranteed to produce a single database instance and this functionality can be unsuitable for some applications requiring synthetic data. For example, we cannot use multiple database instances for DB application testing, since no single instance reflects all the charac- teristics of the original database. One advantage of using a general purpose CSP is that it enables QAGen to handle complex queries, e.g., queries with HAVING clauses. However, this generality comes with a performance cost. The number of times QAGen and My- Benchmark invoke a CSP grows with the size of the gener- ated database and this has serious performance implications as the experiments in [6, 22] indicate.1 The algorithms that we propose do not have these limitations: they always gen- erate a single database instance and their dependence on the generated database size is limited to the cost of materializing the database. Interestingly, recent work on cardinality estimation using maximum entropy principle [27, 28] can be adapted to de- rive algorithms for data generation, and we discuss this pos- sibility in detail in Section 4. However, briefly, cardinality estimation using maximum entropy is known to be a very hard problem and adaptations of current solutions do not efficiently handle complex constraints. Summary of Contributions: We formally introduce car- dinality constraints in Section 2 and show that a set of car- dinality constraints forms an expressive language for speci- fying data characteristics. In Section 3, we state the formal data generation problem and show that the general prob- lem is NEXP-complete and therefore hard. We present our algorithms in Section 4. While the general data genera- tion problem is hard, our algorithms are able to handle a large and useful class of constraints. Our algorithms are probabilistically approximate, meaning that they satisfy all constraints in expectation. We note that this is sufficient for most applications of data generation. Our algorithms are also sensitive to the complexity of the input cardinal- ity constraints in a precisely quantifiable way and use ideas from probabilistic graphical models [26]. We include detailed experimental evaluation of our algorithms in Section 6 and conclude.",Arvind Arasu,"Microsoft Research Redmond, WA",arvinda@microsoft.com,Raghav Kaushik,"Microsoft Research Redmond, WA",skaushi@microsoft.com,Jian Li,"University of Maryland College Park, MD",lijian@cs.umd.edu,,,,,,,,,,,,,,,,,,,,,
20200127,1302,Wook-Shin Han,Department of Computer Engineering Kyungpook National University,wshan@knu.ac.kr,,Dependency-Aware Reordering for Parallelizing Query Optimization in Multi-Core CPUs,"Dependency-Aware Reordering for Parallelizing Query Optimization in Multi-Core CPUs, Dependency-Aware Reordering for Parallelizing Query Optimization in Multi-Core CPUs, Dependency-Aware Reordering for Parallelizing Query Optimization in Multi-Core CPUs, Dependency-Aware Reordering for Parallelizing Query Optimization in Multi-Core CPUs, Dependency-Aware Reordering for Parallelizing Query Optimization in Multi-Core CPUs, ABSTRACT The state of the art commercial query optimizers employ cost-based optimization and exploit dynamic programming (DP) to find the optimal query execution plan (QEP) without evaluating redundant sub-plans. The number of alternative QEPs enumerated by the DP query optimizer can increase exponentially, as the number of joins in the query increases. Recently, by exploiting the coming wave of multi-core processor architectures, a state of the art parallel opti- mization algorithm [14], referred to as PDPsva, has been proposed to parallelize the  ""time-consuming "" DP query optimization process itself. While PDPsva significantly extends the practical use of DP to queries having up to 20-25 tables, it has several limitations: 1) supporting only the size-driven DP enumerator, 2) statically allo- cating search space, and 3) not fully exploiting parallelism. In this paper, we propose the first generic solution for parallelizing any type of bottom-up optimizer, including the graph-traversal driven type, and for supporting dynamic search allocation and full paral- lelism. This is a challenging problem, since recently developed, state of art DP optimizers such as DPcpp [21] and DPhyp [22] are very difficult to parallelize due to tangled dependencies in the join pairs they generate. Unless the solution is very carefully devised, a lot of synchronization conflicts are bound to occur. By viewing a serial bottom-up optimizer as one which generates a totally or- dered sequence of join pairs in a streaming fashion, we propose a novel concept of dependency-aware reordering, which minimizes waiting time caused by dependencies of join pairs. To maximize parallelism, we also introduce a series of novel performance optimization techniques: 1) pipelining of join pair generation and plan generation; 2) the synchronization-free global MEMO; and 3) threading across dependencies. Through extensive experiments with various query topologies, we show that our solution supports any type of bottom up optimization, achieving linear speedup for each type. Despite the fact that our solution is generic, due to sophisticated optimization techniques, our generic parallel optimizer outperforms PDPsva tailored to size-driven enumeration. Experi- mental results also show that our solution is much more robust than PDPsva with respect to search space allocation. Categories and Subject Descriptors H.2.4 [DATABASE MANAGEMENT]: Systems General Terms Algorithms Keywords Multi-cores, Parallel databases, Query optimization 1. INTRODUCTION For the last few decades, the CPU performance has been signifi- cantly improved by increasing the clock rate according to Moore 's law. However, fundamental physical limitations such as power con- sumption and heat generation clearly prevent us from relying on this trend any more [11, 12, 29, 30]. Instead, the industry has been improving the CPU performance by integrating more execu- tion cores into each processor. The number of cores is expected to grow significantly over time [11, 35]. Recently, by exploiting this new wave of multi-core processor architectures, Han et al. [14] have proposed a novel framework re- ferred to here as PDPsva, to parallelize the  ""time-consuming "" dy- namic programming (DP) query optimization process itself. The DP query optimizer enumerates many alternative query execution plans (QEPs) for evaluating a declarative SQL query, while esti- mating the cost of each QEP, and then chooses the one with lowest estimated cost. The number of alternative QEPs enumerated by the DP query optimizer can increase exponentially, as the number of joins in the query increases. In fact, PDPsva significantly extends the practical use of DP to queries having up to 20-25 tables. We otherwise would have to depend on sub-optimal (randomized or greedy) heuristics [4, 19, 23, 31, 32] to complete query optimiza- tion in a reasonable time. However, PDPsva has three limitations. First, it supports only one specific bottom-up optimizer, the size-driven DP optimizer. That is, it does not support recently developed, state of the art DP optimizers such as DPcpp [21] and DPhyp [22], which directly tra- verse a query graph to generate join pairs. Such optimizers have advantages over the size-driven enumeration. They can support early termination, since they can generate QEPs for all tables (more precisely, all quantifiers1) without generating QEPs for all smaller quantifier sets (i.e., not size-driven). Thus, as soon as we obtain a sufficiently good QEP or the estimated execution time of the ob- tained QEP is less than the expected remaining enumeration time, we can terminate the enumeration process early. DPhyp can handle the widest class of non-inner joins very efficiently [22]. Therefore, there is a need for a generic framework that can parallelize any type of bottom-up optimizer so that it can support both existing and fu- ture bottom-up optimizers. Secondly, assuming all cores are evenly loaded, PDPsva employs static search space allocation. Although the best allocation strategy of PDPsva can allocate search space to threads evenly [14], the slowest thread holds up all the other, faster threads, resulting in seriously unbalanced workloads. Therefore, the search allocation strategy must be dynamic to resolve this situation. Lastly, PDPsva does not fully exploit parallelism since it merges per-thread MEMOs to the global MEMO in serial execution for each size of resulting quantifier sets. Here, each MEMO entry stores QEPs for a given quantifier sets. Thus, the best version of PDPsva achieves only up to 6.1 speedup for star queries2 using 8 threads [14]. Therefore, in order to achieve linear speedup, all such serial steps must be executed by exploiting full parallelism. In this paper, we propose the first generic solution for paralleliz- ing any type of bottom-up optimizer, including the graph-traversal driven type, and for supporting dynamic search allocation and full parallelism. This is a challenging problem, since DPcpp and DPhyp are very difficult to parallelize [14]. Unless the solution is very carefully devised, a lot of synchronization conflicts are bound to occur. Figure 1 shows a motivating example using a sequence of join pairs (more precisely, a sequence of pairs of quantifier sets) generated by DPcpp or DPhyp. Note that DPcpp and DPhyp gener- ate the same sequence for equi-join. An arrow from one pair to another pair represents a dependency. As opposed to size-driven enumeration, the sizes of resulting quantifier sets do not monotoni- cally increase. This leads to tradeoff between early termination and tangled dependencies. That is, since we obtain some QEPs for all quantifiers at the 17th pair of quantifier sets (q1, q2q3q4), we might be able to terminate the optimization process early, if the best QEP obtained thus far is good enough. On the other hand, the resulting tangled dependencies in the pairs of quantifier sets hinder paral- lelizing DPcpp. For example, if a thread Ta processes the eighth pair (q2, q3q4), and a thread Tb processes the fifth pair (q3, q4), Ta must wait until Tb finishes the processing of (q3, q4) first, since the quantifier set q3q4 has a dependency on the pair (q3, q4). If we change the order of the eighth and the eleventh pairs, Ta can process (q1, q4) without waiting. The overview of our solution is as follows. To parallelize any type of bottom-up enumeration, we view a serial bottom-up opti- mizer as one which generates a totally ordered sequence of pairs of quantifier sets in a streaming fashion. We buffer a fixed number of pairs and delay plan generation for the pairs buffered. Then on the fly, we convert the total order over these buffered pairs into a partial order over unordered groups of pairs, where threads can generate QEPs independently for all pairs within a group without waiting. These steps correspond to reordering of the original sequence so that the tangled dependencies in the original sequence are unraveled. We repeat these steps until we consume all pairs of quantifier sets. Our contributions are as follows: 1) We propose the first generic framework for parallelizing any type of bottom-up optimization. 2) We propose a novel concept of dependency-aware reordering, which minimizes waiting time caused by dependencies of pairs of quantifier sets and propose a generic algorithm DPEGeneric for parallelizing query optimization. 3) To maximize parallelism, we propose a series of optimization techniques for DPEGeneric:  pipelining of join pair generation and plan generation; the synchro- nization-free global MEMO; and threading across dependencies. 4) Through extensive experiments, we show that DPEGeneric sup- ports any type of bottom up optimizer, achieving linear speedup for each type. Our algorithm is even better than the state of the art parallel optimizer tailored to size-based enumeration, PDPsva. Our algorithm is also much more robust than PDPsva with respect to search space allocation. The rest of this paper is organized as follows. Section 2 reviews the current bottom-up join enumeration algorithms and the state of the art parallel algorithm for the size-based enumeration. The next two sections give the details of two generic parallel enumeration algorithms. Section 3 gives a basic parallel enumeration algorithm that can support any type of bottom-up enumeration algorithms, and Section 4 gives a theoretical framework for the dependency- aware reordering and an enhanced parallel enumeration algorithm exploiting the dependency-aware reordering. Section 5 presents a series of performance optimization techniques to maximize paral- lelism. Section 6 presents the results of performance evaluation. We compare our contributions with related work in Section 7, and conclude in Section 8.",Wook-Shin Han,Department of Computer Engineering Kyungpook National University,wshan@knu.ac.kr,Jinsoo Lee,Department of Computer Engineering Kyungpook National University,jslee@www-db.knu.ac.kr,,,,,,,,,,,,,,,,,,,,,,,,
20200128,1345,Xin Liu,"University of Waterloo, Canada",x39liu@uwaterloo.ca,,Hybrid Storage Management for Database Systems,"Hybrid Storage Management for Database Systems, Hybrid Storage Management for Database Systems, Hybrid Storage Management for Database Systems, Hybrid Storage Management for Database Systems, Hybrid Storage Management for Database Systems, ABSTRACT The use of flash-based solid state drives (SSDs) in storage systems is growing. Adding SSDs to a storage system not only raises the question of how to manage the SSDs, but also raises the question of whether current buffer pool algo- rithms will still work effectively. We are interested in the use of hybrid storage systems, consisting of SSDs and hard disk drives (HDDs), for database management. We present cost-aware replacement algorithms, which are aware of the difference in performance between SSDs and HDDs, for both the DBMS buffer pool and the SSDs. In hybrid storage sys- tems, the physical access pattern to the SSDs depends on the management of the DBMS buffer pool. We studied the impact of buffer pool caching policies on SSD access patterns. Based on these studies, we designed a cost-adjusted caching policy to effectively manage the SSD. We implemented these algorithms in MySQL 's InnoDB storage engine and used the TPC-C workload to demonstrate that these cost-aware al- gorithms outperform previous algorithms. 1. INTRODUCTION Flash memory has been used for many years in portable consumer devices (e.g, cameras, phones) where low power consumption and lack of moving parts are particularly desir- able features. Flash-based solid state storage devices (SSDs) are now also becoming commonplace in server environments. SSDs are more expensive per bit than traditional hard disks (HDD), but they are much cheaper in terms of cost per I/O operation. Thus, servers in data centers may be con- figured with both types of persistent storage. HDDs are cost effective for bulky, infrequently accessed data, while SSDs are well-suited to data that are relatively hot [8]. In this paper, we are concerned with the use of such hybrid (SSD and HDD) storage systems for database management. We consider hybrid storage systems in which the two types of devices are visible to the database management system (DBMS), so that it can use the information at its disposal to decide how to make use of the two types of devices. This is illustrated in Figure 1. When writing data to storage, the DBMS chooses which type of device to write it to. Previous work has considered how a DBMS should place data in a hybrid storage system [11, 1, 2, 16, 5]. We provide a summary of such work in Section 7. In this paper, we take a broader view of the problem than is used by most of this work. Our view includes the DBMS buffer pool as well as the two types of storage devices. We consider two related problems. The first is determining which data should be retained in the DBMS buffer pool. The answer to this question is affected by the presence of hybrid storage because blocks evicted from the buffer cache to an SSD are much faster to retrieve later than blocks evicted to the HDD. Thus, we consider cost-aware buffer management, which can take this distinction into account. Second, assuming that the SSD is not large enough to hold the entire database, we have the problem of deciding which data should be placed on the SSD. This should depend on the physical access pattern for the data, which depends, in turn, on both the DBMS workload and the management of the DBMS buffer pool. Because we consider both buffer pool management and management of the hybrid storage system, we have more scope for optimization than previous work in this area, at the expense of additional invasiveness in the design and im- plementation of the DBMS. In addition, we must account for the fact that the two problems we consider are mutually dependent. Replacement decisions in the buffer pool depend on the locations (SSD or HDD) of the pages being replaced, since location affects both eviction cost and reloading cost. Conversely, SSD page placement decisions depend on how the page is used, e.g., how frequently it is read or written, which depends in turn on the buffer manager. For exam- ple, under the GD2L replacement policy we propose here, moving a page from the HDD to the SSD may result in a significant increase in the physical read and write rates for that page, since GD2L tends to evict SSD pages quickly from the buffer pool. Our work addresses these dependencies using an anticipa- tory approach to SSD management. When deciding whether to move a page into the SSD, our proposed admission and replacement policy (called CAC) predicts how such a move will affect the physical I/O load experienced by that page. The page is moved into the SSD only if it is determined to be a good candidate under this predicted workload. The DBMS buffer manager then makes cost-aware replacement decisions based on the current placements of buffered pages. In this paper we present the following contributions: ? We present GD2L, a cost-aware algorithm for buffer pool management in database systems with hybrid stor- age systems. GD2L takes the usual concerns of DBMS buffer management (exploiting locality, scan resistance) into account, but also considers the fact that different devices in a hybrid storage system perform differently. GD2L is based on the GreedyDual algorithm [19], but we have restricted GreedyDual to hybrid systems that include only two types of devices. In addition, we have refined GreedyDual for operation in a DBMS environ- ment. ? We present CAC, an anticipatory cost-based technique for managing the SSD. Unlike previous techniques, CAC is intended to work together with a cost-aware buffer manager like GD2L. It expects that moving a page into or out of the SSD will change the access pattern for that page, and it anticipates these changes when making SSD placement decisions. ? We present an empirical evaluation of GD2L and CAC. We have implemented both techniques in MySQL 's InnoDB storage manager. We compare the performance of GD2L with that of InnoDB 's native buffer manager, which is oblivious to the location of pages in a hybrid storage system. We compare CAC to several alternatives, including a non-anticipatory cost-based technique, LRU-2, and MV-FIFO. Our evaluation uses transactional workloads (TPC-C). The remainder of this paper is organized as follows. Section 2 gives an overview of the system architecture that we assume. Section 3 presents the GD2L technique for database buffer pool management, and Section 4 shows empirical re- sults that illustrate the effect of GD2L on the physical access patterns of database pages. Section 5 presents the CAC algorithm for managing the contents of the SSD device(s). The results of our evaluation GD2L and CAC are presented in Section 6, and Section 7 summarizes related work.",Xin Liu,"University of Waterloo, Canada",x39liu@uwaterloo.ca,Kenneth Salem,"University of Waterloo, Canada",ksalem@uwaterloo.ca,,,,,,,,,,,,,,,,,,,,,,,,
20200129,1030,Lu Li ,Department of Computer Science School of Computing National University of Singapore,lilu0355@comp.nus.edu.sg,,Efficient Indexing for Diverse Query Results,"Efficient Indexing for Diverse Query Results, Efficient Indexing for Diverse Query Results, Efficient Indexing for Diverse Query Results, Efficient Indexing for Diverse Query Results, Efficient Indexing for Diverse Query Results, ABSTRACT This paper examines the problem of computing diverse query results which is useful for browsing search results in online shopping applications. The search results are diversified wrt a sequence of output attributes (termed d-order) where an attribute that appears earlier in the d-order has higher priority for diversification. We present a new indexing technique, D-Index, to efficiently compute diverse query results for queries with static or dynamic d-orders. Our performance evaluation demonstrates that our D-Index outperforms the state-of-the-art techniques developed for queries with static or dynamic d-orders. 1. INTRODUCTION Consider a user who is shopping online for a new laptop from a website which can display a result table consisting of up to 20 laptops that match the user's specification. As the number of matching results is typically much larger than number of display records, it is useful to return a diverse set of results for the user to browse. For example, instead of showing the user 20 laptops from only two brands (say Lenovo and Acer), it would be more interesting to show results covering a more diverse range of brands (e.g., Lenovo, Acer, Dell, HP, Asus, Samsung). If Lenovo and Acer are indeed the only two brands of laptops that satisfy the user's query, then it would be better to show a more ""balanced"" distribution of the 20 displayed laptops; for example, showing 10 laptops from each of Lenovo and Acer is better than showing 18 laptops from Lenovo and 2 laptops from Acer. Similarly, if the user is interested only in laptops from Dell, then it would be more interesting to show a diverse range of Dell laptops with different screen sizes instead of showing all Dell laptops with the same screen size. In general, the query results can be diversified wrt a sequence of attributes, say (brand, screen size, . . .), referred to as a d-order, where the intention is to first diversify the results with as many different brands as possible, and for records that belong to the same brand, we diversify them with as many different screen sizes as possible, and so on. Thus, a d-order determines a priority order for diversify- ing the query results, where the first attribute has higher priority to diversify than the second attribute, and so on. Vee, et al. [8] were the first to study the problem of computing diverse query results. They formally define the no- tion of query result diversity and show that existing score based techniques are inadequate to guarantee diverse query results. They also propose an inverted-list based approach to evaluate such queries. However, their work addresses on- ly static diversity queries (SDQ), where the query results are diversified wrt a static, pre-defined d-order. Clearly, it would be useful to allow users to customize their diversification preference. For example, Alice might be more interested to diversify the results wrt laptop color first, followed by brand, whereas Bob might be more interested to diversify the results wrt the number of CPU cores first, followed by battery life and screen size. In this paper, we examine the more general problem of evaluating dynamic diversity queries (DDQ) where the query results are diversified wrt a user specified d-order. A DDQ can be expressed by the following extended SQL syntax: ""S- ELECT ... FROM R WHERE ... DIVERSIFY BY D1, ... , Dn LIMIT k"" which retrieves a diverse set of at most k matching records from a relation R such that the records are diversified wrt a d-order (D1, ... , Dn). The attributes in the SELECT clause must contain all the attributes in the DIVERSIFY BY clause. Our paper makes three key contributions. First, we show that extending existing techniques designed for SDQs [8] to evaluate DDQs is inefficient (Section 4). Second, we intro- duce a novel approach for evaluating diversity queries that is based on the concept of computing a core cover of a query (Section 5.1). Based on this concept, we design a new index method, D-Index, and introduce two index variants, namely, D-tree and D+-tree (Sections 5 and 6). Third, we demonstrate with an experimental evaluation, which is based on a PostgreSQL implementation, that our proposed D-Index technique consistently outperforms [8] for both SDQs as well as DDQs (Section 7). In this paper, we use Q to denote a diversity query on a relation R with d-order  = (D1, ...· , Dm), a set of (possibly empty) selection predicate attributes , and a limit value of k. Our running example data for R is shown in Figure 1(a): the attributes Brand, #Core, ScrnSze, BatLife, and Color represent, respectively, laptop brand (B), number of CPU cores (C), screen size in inches (SS), battery life in hours (BL), and laptop color (LC). ",Lu Li ,Department of Computer Science School of Computing National University of Singapore,lilu0355@comp.nus.edu.sg,Chee-Yong Chan,Department of Computer Science School of Computing National University of Singapore,chancy@comp.nus.edu.sg,,,,,,,,,,,,,,,,,,,,,,,,
20200130,1346,Max Heimel ,Technische Universita ?t Berlin,max.heimel@tu-berlin.de,,Hardware-Oblivious Parallelism for In-Memory Column-Stores,"Hardware-Oblivious Parallelism for In-Memory Column-Stores, Hardware-Oblivious Parallelism for In-Memory Column-Stores, Hardware-Oblivious Parallelism for In-Memory Column-Stores, Hardware-Oblivious Parallelism for In-Memory Column-Stores, Hardware-Oblivious Parallelism for In-Memory Column-Stores,  ABSTRACT The multi-core architectures of today 's computer systems make parallelism a necessity for performance critical applications. Writing such applications in a generic, hardware-oblivious manner is a challenging problem: Current database systems thus rely on labor- intensive and error-prone manual tuning to exploit the full potential of modern parallel hardware architectures like multi-core CPUs and graphics cards. We propose an alternative design for a parallel database engine, based on a single set of hardware-oblivious oper- ators, which are compiled down to the actual hardware at runtime. This design reduces the development overhead for parallel database engines, while achieving competitive performance to hand-tuned systems. We provide a proof-of-concept for this design by integrating op- erators written using the parallel programming framework OpenCL into the open-source database MonetDB. Following this approach, we achieve efficient, yet highly portable parallel code without the need for optimization by hand. We evaluated our implementation against MonetDB using TPC-H derived queries and observed a performance that rivals that of MonetDB 's query execution on the CPU and surpasses it on the GPU. In addition, we show that the same set of operators runs nearly unchanged on a GPU, demonstrating the feasibility of our approach. 1. INTRODUCTION The modern hardware landscape is getting increasingly diverse. Today, a single machine can contain several different parallel pro- cessors like multi-core CPUs or GPUs. This diversity is expected to grow further in the coming years, with micro-architectures them- selves diverging towards highly parallel and heterogeneous designs [8]. We believe that making database engines ready to exploit the capabilities of this diverse landscape of parallel processing platforms will be one of the major challenges for the coming decade in database research. Unfortunately, implementing parallel data operators is a tedious and error-prone task that usually requires extensive manual tuning. Most systems are therefore designed with a certain hardware ar- chitecture in mind: they are hardware-conscious. Extending those systems to new architectures usually requires the developer to im- plement an additional set of hardware-specific operators, adding significant development and maintenance overhead in the process. Instead of maintaining multiple sets of operators, we believe that a parallel database engine can be designed in a hardware-oblivious manner, i.e., without any inherent reliance on a specific architec- ture: All knowledge is encapsulated into a library that adheres to a standardized interface and is provided by the manufacturer of the respective hardware components. The system is designed around a single set of operators, which can be mapped to a variety of paral- lel processing architectures at runtime. We also argue that existing systems can be extended to become hardware-oblivious. To support these claims, we make the following contributions: 1. We present Ocelot1, a hardware-oblivious extension of the open-source column-store MonetDB. Ocelot uses a standard- ized interface provided by OpenCL to map operations to any supported parallel processing architecture. 2. We demonstrate that a single hardware-oblivious implemen- tation of the internal MonetDB operators can efficiently run on such dissimilar devices like CPUs and GPUs. 3. We evaluate our approach against the hand-tuned query pro- cessor of MonetDB and show that Ocelot can compete with MonetDB 's performance when running on a CPU, and outperform it when using the graphics card. The paper is structured as follows: In the next section, we moti- vate and discuss the concept of hardware-oblivious database de- signs. We also give an introduction to the kernel programming model, and motivate why we chose this model for our prototype. In Section 3, we give an overview of the design of Ocelot, with fur- ther implementation details being discussed in Section 4. Section 5 presents our evaluation of Ocelot and discusses the results, Section 6 presents related work. In Section 7, we discuss possible direc- tions for future research. Finally, the paper is concluded by Section 8, which summarizes our findings. ",Max Heimel ,Technische Universita ?t Berlin,max.heimel@tu-berlin.de,Michael Saecker,ParStream GmbH,michael.saecker@ parstream.com,Holger Pirk,CWI Amsterdam,holger.pirk@cwi.nl,Stefan Manegold,CWI Amsterdam,stefan.manegold@cwi.nl,Volker Markl,Technische Universita ?t Berlin,volker.markl@tuberlin.de,,,,,,,,,,,,,,,
20200131,692,Ahmed Eldawy,"Computer Science and Engineering, University of Minnesota, Minneapolis, MN, USA",eldawy@cs.umn.edu,,SpatialHadoop: A MapReduce Framework for Spatial Data,"SpatialHadoop: A MapReduce Framework for Spatial Data, SpatialHadoop: A MapReduce Framework for Spatial Data, SpatialHadoop: A MapReduce Framework for Spatial Data, SpatialHadoop: A MapReduce Framework for Spatial Data, SpatialHadoop: A MapReduce Framework for Spatial Data, Abstract aThis paper describes SpatialHadoop; a full-fledged MapReduce framework with native support for spatial data. SpatialHadoop is a comprehensive extension to Hadoop that injects spatial data awareness in each Hadoop layer, namely, the language, storage, MapReduce, and operations layers. In the language layer, SpatialHadoop adds a simple and expressive high level language for spatial data types and operations. In the storage layer, SpatialHadoop adapts traditional spatial index structures, Grid, R-tree and R+-tree, to form a two-level spatial index. SpatialHadoop enriches the MapReduce layer by two new components, SpatialFileSplitter and SpatialRecordReader, for effi- cient and scalable spatial data processing. In the operations layer, SpatialHadoop is already equipped with a dozen of operations, including range query, kNN, and spatial join. Other spatial operations are also implemented following a similar approach. Extensive experiments on real system prototype and real datasets show that SpatialHadoop achieves orders of magnitude better performance than Hadoop for spatial data processing. I. INTRODUCTION Since its release in 2007, Hadoop was adopted as a solution for scalable processing of huge datasets in many applica- tions, e.g., machine learning [1], graph processing [2], and behavioral simulations [3]. Hadoop employs MapReduce [4], a simplified programming paradigm for distributed processing, to build an efficient large-scale data processing framework. The abstraction of the MapReduce programming simplifies the programming for developers, while the MapReduce framework handles parallelism, fault tolerance, and other low level issues. In the meantime, there is a recent explosion in the amounts of spatial data produced by various devices such as smart phones, satellites, and medical devices. For example, NASA satellite data archives exceeded 500 TB and is still growing [5]. As a result, researchers and practitioners worldwide have started to take advantage of the MapReduce environment in supporting large-scale spatial data. Most notably, in industry, ESRI has released  'GIS Tools on Hadoop ' [6] that work with their flagship ArcGIS product. Meanwhile, in academia, three system prototypes were proposed: (1) Parallel-Secondo [7] as a parallel spatial DBMS that uses Hadoop as a distributed task scheduler, (2) MD-HBase [8] extends HBase [9], a nonrelational database for Hadoop, to support multidimensional indexes, and (3) Hadoop-GIS [10] extends Hive [11], a data warehouse infrastructure built on top of Hadoop with a uni- form grid index for range queries and self-join. ?This work is supported in part by the National Science Foundation, USA, under Grants IIS-0952977 and IIS-1218168. A main drawback in all these systems is that they still deal with Hadoop as a black box, and hence they remain limited by the limitations of existing Hadoop systems. For example, Hadoop-GIS [10], while the most advanced system prototype so far, suffer from the following limitations: (1) Hadoop itself is ill equipped in supporting spatial data as it deals with spatial data in the same way as non-spatial data. Relying on Hadoop as a black box inherits the same limitations and performance bottlenecks of Hadoop. Furthermore, Hadoop- GIS adapts Hive [11], a layer on top of Hadoop, which gives an extra overhead layer over Hadoop itself, (2) Hadoop-GIS can only support uniform grid index, which is applicable only in the rare case of uniform data distribution. (3) Being on-top of Hadoop, MapReduce programs defined through map and reduce cannot access the constructed spatial index. Hence, users cannot define new spatial operations beyond the already supported ones, range query and self-join. Parallel Secondo [7], MD-HBase [8], and ESRI tools on Hadoop [6] suffer from similar drawbacks. In this paper, we introduce SpatialHadoop; a full-fledged MapReduce framework with native support for spatial data; available as open-source [12]. SpatialHadoop overcomes the limitations of Hadoop-GIS and all previous approaches as: (1) SpatialHadoop is built-in Hadoop base code (around 14,000 lines of code inside Hadoop) that pushes spatial constructs and the awareness of spatial data inside the core functionality of Hadoop. This is a key point behind the power and efficiency of SpatialHadoop. (2) SpatialHadoop is able to support a set of spatial index structures including R- tree-like indexing, which is built-in Hadoop Distributed File System (HDFS). This makes SpatialHadoop unique in terms of supporting skewed data distributions in spatial data, and (3) SpatialHadoop users can interact with Hadoop directly to develop a myriad of spatial functions. For example, in this paper, we show range queries, kNN queries, and spatial join. In another work, we show a set of computational geometry techniques that can only be realized using map and reduce functions in SpatialHadoop [13]. This is in contrast to Hadoop- GIS and other systems that cannot support such kind of flexibility, and hence they are very limited in the functions they can support. SpatialHadoop is available as open source [12] and has been already downloaded more than 75,000 times. It has been used by several research labs and industrial companies around the world. Figures 1(a) and 1(b) show how to express a spatial range Objects = LOAD  'points ' AS (id:int, x:int, y:int); Result = FILTER Objects BY x < x2 AND x > x1 AND y < y2 AND y > y1; (a) Range query in Hadoop Objects = LOAD  'points ' AS (id:int, Location:POINT); Result = FILTER Objects BY Overlaps (Location, Rectangle(x1, y1, x2, y2)); (b) Range query in SpatialHadoop Fig. 1. Range query in Hadoop vs. SpatialHadoop query in Hadoop and SpatialHadoop, respectively. The query finds all points located within a rectangular area represented by two corner points   <x1, y1 > and   <x2, y2 >. The first query statement loads an input file of points, while the second statement selects records that overlap with the given range. As Hadoop does not have any spatial indexes, it has to scan the whole dataset to answer the range query, which gives a very bad performance. In particular, it takes 200 seconds on a 20-node Hadoop cluster to process a workload of 60 GB (about 70 M spatial objects). On the other side, SpatialHadoop exploits its built-in spatial indexes to run the same query in about two seconds, which is two orders of magnitude improvement over Hadoop. In addition, the Hadoop program, written in Pig Latin language [14], is less readable due to the lack of spatial data support. SpatialHadoop uses Pigeon [15] language which makes the program simpler and more expressive as it uses spatial data types (POINT and RECTANGLE) and spatial functions (Overlaps). SpatialHadoop is composed of four main layers, namely, language, storage, MapReduce, and operations layers, all injected inside the code base of Hadoop. The language layer provides Pigeon [15], a high level SQL-like language which provides OGC-compliant [16] spatial data types and operations making it easier to adopt by users. The storage layer employs a two-level index structure of global and local indexing. The global index partitions data across computation nodes while the local index organizes data inside each node. This index layout is used to provide three spatial indexes, namely, Grid file, R-tree and R+-tree. To make these indexes accessible to MapReduce programs, SpatialHadoop introduces two new components in the MapReduce layer, namely, SpatialFileSplit- ter and SpatialRecordReader, that exploit the global and local index structures, respectively. Finally, the operations layer encapsulates a dozen of spatial operations that take advantage of the new components in the storage and MapReduce layers. In this paper, we only show the implementation of three basic spatial operations, namely, range query, kNN, and spatial join. A real system prototype of SpatialHadoop (available as open-source at [12]) is extensively evaluated. Experiments run on real spatial datasets extracted from NASA MODIS datasets [5] with a total size of 4.6 TB and 120 Billion records. Both SpatialHadoop and Hadoop are deployed on an internal university cluster as well as an Amazon EC2 cluster. In both platforms, SpatialHadoop has orders of magnitude better performance compared to Hadoop in all tested spatial operations (range query, kNN, and spatial join). This paper is organized as follows: Section II highlights related work. Section III gives the architecture of SpatialHadoop. Details of the language, storage, MapReduce, and operations layers are given in Sections IV-VII. Experiments are given in Section VIII. Section IX concludes the paper. ",Ahmed Eldawy,"Computer Science and Engineering, University of Minnesota, Minneapolis, MN, USA",eldawy@cs.umn.edu,Mohamed F. Mokbel,"Computer Science and Engineering, University of Minnesota, Minneapolis, MN, USA",mokbel@cs.umn.edu,,,,,,,,,,,,,,,,,,,,,,,,
20200201,1194,Ruoming Jin,"Department of Computer Science, Kent State University Kent, OH 44242, USA",jin@cs.kent.edu,,3-HOP: A High-Compression Indexing Scheme for Reachability Query,"3-HOP: A High-Compression Indexing Scheme for Reachability Query, 4-HOP: A High-Compression Indexing Scheme for Reachability Query, 5-HOP: A High-Compression Indexing Scheme for Reachability Query, 6-HOP: A High-Compression Indexing Scheme for Reachability Query, 7-HOP: A High-Compression Indexing Scheme for Reachability Query, ABSTRACT Reachability queries on large directed graphs have attracted much attention recently. The existing work either uses spanning struc- tures, such as chains or trees, to compress the complete transitive closure, or utilizes the 2-hop strategy to describe the reachability. Almost all of these approaches work well for very sparse graphs. However, the challenging problem is that as the ratio of the number of edges to the number of vertices increases, the size of the compressed transitive closure grows very large. In this paper, we pro- pose a new 3-hop indexing scheme for directed graphs with higher density. The basic idea of 3-hop indexing is to use chain structures in combination with hops to minimize the number of structures that must be indexed. Technically, our goal is to find a 3-hop scheme over dense DAGs (directed acyclic graphs) with minimum index size. We develop an efficient algorithm to discover a transitive clo- sure contour, which yields near optimal index size. Empirical stud- ies show that our 3-hop scheme has much smaller index size than state-of-the-art reachability query schemes such as 2-hop and path- tree when DAGs are not very sparse, while our query time is close to path-tree, which is considered to be one of the best reachability query schemes. Categories and Subject Descriptors H.2.8 [Database management]: Database Applications-graph indexing and querying General Terms Performance Keywords Graph indexing, Reachability queries, Transitive closure, 3-Hop, 2-Hop, Path-tree 1. INTRODUCTION The rapid accumulation of very large graphs from a diversity of disciplines, such as biological networks, social networks, ontologies, XML, and RDF databases, among others, calls for the graph database system. Important research issues, ranging theo- retical foundations including algebra and query language [2], to indices for various graph queries [20, 12] and more recently, graph OLAP/summarization [17], have attracted much recent attention. Among them, graph reachability query processing has evolved into a core problem: given two vertices u and v in a directed graph, is there a path from u to v (u""C v)? Graph reachability is one of the fundamental research questions across several disciplines in computer science, such as software engineering and distributed computing. In the database research community, the initial interest in reachability queries has been driven by the need to handle recursive queries, with focus on efficient and ef- fective transitive closure compression. Recently, this problem has captured the attention of database researchers again, due to the increasing importance of XML data management, and fast growing graph data, such as large scale social networks, WWW, and bio- logical networks. For instance, in XML databases, the reachability query is the basic building block for the typical path query for- mat //P1//P2// ... //Pm, where ""//"" is the ancestor-descendant search and Pi is the tag. Reachability queries also have an impor- tant role for managing/querying RDF and domain ontologies. In bioinformatics, reachability queries can be used to answer basic gene regulation questions in the regulatory network. 1.1 Prior Work In order to tell whether a vertex u can reach another vertex v in a directed graph, many approaches have been developed over the years. For a reachability query, we can effectively transform a directed graph into a directed acyclic graph (DAG) by coalescing strongly connected components into vertices and utilizing the DAG to answer the reachability queries. Thus, throughout the paper, we will only focus on DAG. LetG = (V, E) be the DAG for a reachability query. In Table 1.1, we summarize these approaches in terms of their index size, construction time, and query processing time based on worst-case analysis. Here, n is the number of vertices (n = |V |) and m is the number of edges (m = |E|). Parameter k is the width of the chain decomposition of DAG G [11], t is the number of (nontree) edges left after removing all the edges of a spanning tree ofG [19], and k' is the width of the path decomposi- tion [12]. These three parameters k, t and k', are method-specific and will be explained in more detail when we discuss their corre- sponding methods. DFS/BFS and Transitive Closure Computation: We first discuss two classical approaches for reachability query, representing two extremes with regard to index size and query time. DFS/BFS needs to traverse the graph online and can take up to O(n + m) time to answer a reachability query. This is too slow for large graphs. The second approach precomputes the transitive closure of G, i.e., it records the reachability between every pair of vertices in advance. While this approach can answer reachability queries in constant time, its storage cost O(n2) is prohibitive for large graphs. Indeed, tackling the storage cost by effectively compressing the transitive closure has been the major theme of index construction for graph reachability processing. Typically, however, improved compression comes at the cost of slower query answering time. To find the right balance between transitive closure compression and reasonable query answering time is the driving force of ongoing research into graph reachability indexing. The existing research largely falls into two categories: the first category attempts to apply simple graph structures, such as chains and trees, to compress the transitive closure of a DAG. The optimal chain cover, tree cover and the recent path-tree cover all belong to this category. The second category, referred to as 2-hop indexing, tries to encode the reachability using a subset of vertices which serve as intermediaries, i.e., each vertex records a list of intermediate vertices it can reach and a list of intermediate vertices which can reach it. Then, 2-hop reachability means the starting vertex can reach an intermediate vertex (the first hop) and this intermediate vertex can reach the end vertex (the second hop). In the following, we go through these approaches in more detail. Optimal Chain Cover: The basic idea of optimal chain cover is to decompose a DAG into a minimal number of pair-wise disjoint chains, and then assign each vertex in the graph a chain ID and its sequence number in its chain. Given this, if a vertex can reach another chain, it records only the smallest vertex it reaches in that chain. In other words, each vertex in the compressed transitive closure covers the remaining vertices (all the vertices with a higher sequence number) in its respective chain. To determine if vertex u reaches vertex v, we only need to check if u reaches any vertex (say, v') in v's chain, and if yes, we check if the vertex v' has a smaller sequence number than v. This strategy can compress the transitive closure since we need to record at most one vertex in each chain for a given vertex. If the minimal number of chains for a DAG (also referred to as the width of the DAG) is k, then this approach has O(nk) index size and O(log k) query time. Jagadish [11] pioneered the application of chain decomposition in the database research community to compress the transitive clo- sure. He demonstrated that the problem of finding the minimal number of chains from G can be transformed into a network flow problem, which can be solved in O(n3). He also proposed several heuristic algorithms for chain decomposition in order to reduce the computational cost and actual index size. Recently, Cheng [7] pro- posed anO(n2 +kn "" k) time algorithm to decompose a DAG into a minimal number of chains. The worst case complexity of the chain cover approach is clearly decided by the width of DAG. If the width is high, we tend to have a lot of chains with only a small number of vertices, resulting in a high index cost. Another way to look at the compression rate is by observing that each vertex in compressed transitive closure covers a partial chain (from the vertex itself to the last vertex in the chain). Let R(u) be the transitive closure of u. Let RC(u) be the number of vertices u records for the chain decomposition. Then, the compression ratio of the chain decomposition is defined as P uâV |R(u)| P u V |R C(u)| . Thus, we can see that the compression ratio is exactly the average size of the partial chains each vertex in the com- pressed transitive closure covers. Optimal Tree Cover and Its Variants: The optimal tree cover uti- lizes a (spanning) tree to compress the transitive closure [1]. Each vertex in the tree is labeled by a pair of numbers, corresponding to an interval: if a vertex is an ancestor of another vertex in the tree, the interval labeling guarantees that the interval of the first vertex contains the interval of the second vertex. Note that if a vertex reaches the root of a subtree in the original DAG, it will reach all the vertices in the subtree. Thus, for each vertex in the DAG, we can organize all the vertices in its transitive closure, i.e., all the vertices it can reach, into pair-wise disjoint subtrees. To compress the transitive closure, for each subtree, we only need to record its root vertex. To answer the reachability query from vertex u to vertex v, we check if the interval of v is contained by any interval associated with those subtree roots we have recorded for u. Agrawal et al. [1] formally introduced the tree cover and found an optimized algorithm to discover a tree cover which can maxi- mally compress the transitive closure. They also showed that the tree cover approach can provide a better compression rate than the optimal chain cover approach. The advantage of the tree cover ap- proach over the chain cover approach comes from the fact that each tree-cover vertex covers an entire subtree, while each chain-cover vertex covers only a partial chain. Several recent studies focus on the tree cover approach and try to improve either its query processing time and/or provide a smaller index size. Wang et al. [19] develop the Dual-Labeling approach which tries to improve the query time and index size for very sparse graphs, where the number of non-tree edges t is much smaller than the number of vertices n (t << n). Their approach can reduce the index size to O(n + t2) and achieve constant query answering time. Unfortunately, many real world graphs would not satisfy the condition required by this approach, and when t > n, this approach will not help compress the index size. Label+SSPI [4] and GRIPP [18] aim to minimize the index con- struction time and index size. They achieve O(m + n) index con- struction time and O(m + n) index size. However, this is at the sacrifice of the query time, which will cost O(m ? n). Both al- gorithms start by extracting a tree cover and then deploy an online search algorithm utilizing the tree structure to speed up the DFS process. Path-Tree Cover: The latest work to use a simple graph structure to compress transitive closure is the path-tree cover approach, pro- posed by Jin et al. [12], which generalizes the tree cover approach. They observe that the covering capability of each vertex in the compressed transitive closure is determined by the number of parents and children each vertex has in the simple graph structure. For in- stance, a chain vertex has one parent and one child while a tree vertex has one parent and multiple children. The path-tree allows two parents and multiple children. In path-tree cover, all vertices in the original DAG are partitioned into pair-wise disjoint paths (k' is the number of paths in the path-decomposition for a DAG G), and then those paths serve as vertices in a tree structure. In other words, the path-tree utilizes a tree-like structure, where each vertex represents a path in the original DAG. Each vertex in the path-tree needs only three numbers, two numbers for the interval label of the tree-structure and one sequence number from a DFS traversal procedure, to answer the reachability query between any two vertices in the path-tree in constant time. In [12], authors proposed two path-tree schemes, PTree-1 and PTree-2. PTree-1 utilizes optimal tree cover and thus has O(mn) construction time while PTree-2 has O(mk) construction time. Given this, to compress the transitive closure, a vertex u only needs to record vertex v, such that 1) u ""C v and 2) there is no vertex v' such that u ""C v' and v' can reach v in the path-tree. Theoretically, they prove that path-tree cover can always perform the compression of transitive closure better than or equal to the optimal tree cover approaches and chain cover approaches. Note that the enhanced power of the path-tree cover is a consequence of the increased parent/child connectivity of path-tree vertices vs. tree cover or chain cover vertices. 2-HOP Indexing: The 2-hop labeling method proposed by Cohen et al. [9] is quite different from the aforementioned simple graph covering approaches. It compresses the transitive closure using a subset of intermediate vertices. Each vertex records a list of in- termediate vertices it can reach and a list of intermediate vertices which can reach it. The index size is the total number of interme- diate vertices each vertex records. They propose an approximate (greedy) algorithm based on set-covering which can produce a 2- hop cover with size no larger than the minimum possible 2-hop indexing by a logarithmic factor. The minimum 2-hop index size is conjectured to be O?(nm1/2). The major problem of the 2-hop indexing approach is its high construction cost. The greedy set-covering algorithm needs to iter- atively find a subset of vertices which utilizes a candidate vertex as the intermediate hop. The subset of vertices are selected to mini- mize the price measure, i.e., the cost of recording such an interme- diate hop of these vertices with respect to the number of uncovered reachable vertex pairs in this subset. Finding the subset of vertices with minimal price can be transformed into the problem of finding a densest subgraph in a bipartite graph. The approximate algorithm to solve this subproblem is in the linear order with respect to the number of edges in the bipartite graph. Besides, each vertex in the DAG can serve as the intermediate hop which corresponds to a bi- partite graph. Thus, for each iteration, it takesO(n3) to find such a desired subset of vertices. Considering the iteration needs to cover the entire transitive closure Tc, we can see its construction time is O(n3|Tc|). Several approaches have been proposed to reduce its construc- tion time. Schenkel et al. propose the HOPI algorithm, which ap- plies a divide-and-conquer strategy to compute 2-hop labeling [15]. Recently, Cheng et al. propose several methods, such as a geometric- based algorithm [6] and graph partition technique [7], to produce a 2-hop labeling. Though their algorithms significantly speed up the 2-hop construction time, they do not produce the approxima- tion bound of the labeling size which is produced by Cohen et al.'s approach. 1.2 Our Contribution Almost all these approaches work reasonably well for very sparse graphs (where the number of edges is very close to the number of vertices). However, as the ratio of the number of edges to the num- ber of vertices increases, the size of the compressed transitive clo- sure of the simple graph covering approaches can grow very large. In many real world graphs, such as citation networks, the semantic web, and biological networks, the number of edges can be several times the number of vertices. In general, the simple graph covering approach works well only for those DAGs which have a structure similar to the building-block chain, tree, or path-tree structures. However, in many real world graphs, since edge density is much higher than in simple graph structures, many edges will be left un- covered. Vertices of uncovered edges likely need to be recorded as ancillary data in the compressed transitive closure of the DAG, in- creasing the index size. Thus, the size of the compressed transitive closure can become very large as the density grows. The original 2-hop [9] builds on top of the set-covering frame- work and is theoretically appealing as it achieves a guaranteed ap- proximation bound. However, to our knowledge, there is little the- oretical comparison between the 2-hop approach and the simple graph covering approaches in existing research. Most studies do not even empirically compare the 2-hop approach and the simple graph covering approaches. This may be due in part to the 2-hop approach not scaling well to large graphs, even graphs with only thousands of vertices. Specifically, since the original 2-hop needs to compute the complete transitive closure, it becomes very expen- sive as the edge density of the graph becomes larger. Though sev- eral heuristic techniques [15, 6, 7] have been proposed to construct 2-hop faster, they do not guarantee any approximation bound as the original 2-hop does. None of these methods have compared their compression ratio directly with the optimal 2-hop approaches, even on relatively small graphs. To summarize, the major research challenge for existing graph reachability indexing is how to significantly compress the transitive closure when the ratio between the number of edges and the number of vertices increases. Driven by this need, we propose a new 3-hop indexing scheme for directed graphs with higher density. The basic idea in 3-hop indexing is to utilize a simple graph structure, rather than a sole vertex, as an intermediate hop to describe the reachabil- ity between source vertices and destination vertices. In this paper, we focus on the chain structure. The new indexing scheme does not need to compute the entire transitive closure. Instead, it only needs to compute and record a number of so-called ""contour"" ver- tex pairs, which can be orders of magnitude smaller than the size of the transitive closure. Indeed, it is even much smaller than the compressed transitive closure of the chain cover. The connectivity of any pair of vertices in the DAG can be answered by those con- tour vertex pairs. Further, we ""factorize"" these contour vertex pairs by recording a list of ""entry points"" and ""exit points"" on some intermediate chains. We derive an efficient algorithm to generate an index which approximates the minimal 3-hop indexing by a logarithmic factor. Theoretically, we show that 3-hop labeling always has a better minimal compression ratio than 2-hop labeling, and its construction time is much faster than that of 2-hop. We perform a detailed experimental evaluation on both real and synthetic datasets by comparing 3-hop labeling, 2-hop labeling and the state-of-the-art path-tree covering approach. Empirical studies show that our 3-hop scheme has a much smaller index size than prior state-of-art reachability query schemes for dense DAGs when the number of edges is not close to the number of vertices, i.e., |E| 6? |V |. The query processing time of 3-hop is close to path- tree's, which is considered to be one of the best reachability query schemes.",Ruoming Jin,"Department of Computer Science, Kent State University Kent, OH 44242, USA",jin@cs.kent.edu,Yang Xiang,"Department of Computer Science, Kent State University Kent, OH 44242, USA",yxiang@cs.kent.edu,Ning Ruan,"Department of Computer Science, Kent State University Kent, OH 44242, USA",nruan@cs.kent.edu,David Fuhry,"Department of Computer Science, Kent State University Kent, OH 44242, USA",dfuhry@cs.kent.edu,,,,,,,,,,,,,,,,,,
20200202,1347,Alexander Shkapsky,"University of California, Los Angeles",shkapsky@cs.ucla.edu,,Optimizing Recursive Queries with Monotonic Aggregates in DeALS,"Optimizing Recursive Queries with Monotonic Aggregates in DeALS, Optimizing Recursive Queries with Monotonic Aggregates in DeALS, Optimizing Recursive Queries with Monotonic Aggregates in DeALS, Optimizing Recursive Queries with Monotonic Aggregates in DeALS, Optimizing Recursive Queries with Monotonic Aggregates in DeALS, Abstract aThe exploding demand for analytics has refocused the attention of data scientists on applications requiring aggregation in recursion. After resisting the efforts of researchers for more than twenty years, this problem is being addressed by innovative systems that are raising logic-oriented data languages to the levels of generality and performance that are needed to support efficiently a broad range of applications. Foremost among these new systems, the Deductive Application Language System (DeALS) achieves superior generality and performance via new constructs and optimization techniques for monotonic aggregates which are described in the paper. The use of a special class of monotonic aggregates in recursion was made possible by recent theoretical results that proved that they preserve the rigorous least-fixpoint semantics of core Datalog programs. This paper thus describes how DeALS extends their definitions and modifies their syntax to enable a concise expression of applications that, without them, could not be expressed in performanceconducive ways, or could not be expressed at all. Then the paper turns to the performance issue, and introduces novel implementation and optimization techniques that outperform traditional approaches, including Semi-naive evaluation. An extensive experimental evaluation was executed comparing DeALS with other systems on large datasets. The results suggest that, unlike other systems, DeALS indeed combines superior generality with superior performance. I. INTRODUCTION The fast-growing demand for analytics has placed renewed focus on improving support for aggregation in recursion. Aggregates in recursive queries are essential in many important applications and are increasingly being applied in areas such as computer networking [1] and social networks [2]. Many significant applications require iterating over counts or probability computations, including machine learning algorithms for Markov chains and hidden Markov models, and data mining algorithms such as Apriori. Besides these new applications, we can mention a long list of traditional ones such as Bill of Materials (BOM), a.k.a. subparts explosion: this classical recursive query for DBMS requires aggregating the various parts in the part-subpart hierarchy. Finally, we have problems such as computing the shortest paths or counting the number of paths between vertices in a graph, which are now covered as foundations by most CS101 textbooks. Although aggregates were not covered in E.F. Codd 's def- inition of the relational calculi [3], it did not take long before early versions of relational languages such as SQL included support for aggregate functions, namely count, sum, avg, min and max, along with associated constructs such as group-by. However, a general extension of recursive query theory and implementation to allow for aggregates proved an elusive goal, and even recent versions of SQL that provide strong support for OLAP and other advanced aggregates disallow the use of aggregates in recursion and only support queries that are stratified w.r.t. to aggregates. Yet the desirability of extending aggregates to recursive queries was widely recognized early and many partial solutions were proposed over the years for Datalog languages [4]? [10]. The fact that, in general, aggregates are non-monotonic w.r.t. set-containment led to proposals based on non-monotonic theories, such as locally stratified programs and perfect models [11], [12], well-founded models [13] and stable models [14]. An alternative approach was due to Ross and Sagiv [9], who observed that particular aggregates, such as continuous count, are monotonic over lattices other than set-containment and thus can be used in non-stratified programs. However practical difficulties with this approach were soon pointed out, namely that determining the correct lattices by programmers and compilers would be quite difficult [15], and this prevented the deployment of the monotonicity idea in practical query languages for a long time. Fortunately, we recently witnessed some dramatic developments change the situation completely. Firstly, Hellerstein et al., after announcing a resurgence of Datalog, showed that monotonicity in special lattices can be very useful in proving formal properties such as eventual con- sistency [16]. Secondly, we see monotonic aggregates making a strong comeback in practical query languages thanks to the results published in [17], [18] and in [2], summarized below. The formalization of monotonic aggregates proposed in [17], [18] preserves monotonicity w.r.t. set containment, and it is thus conducive to simplicity and performance that follow respectively from the facts that (i) users no longer have to deal with lattices, and (ii) the query optimization techniques, such as Semi-naive and magic sets remain applicable [17]. SociaLite [2] also made an important contribution by showing that shortest-path queries, and other algorithms using aggregates in recursion, can be implemented very efficiently so that in many situations the Datalog approach becomes preferable to that of hand-coding big-data analytics in some procedural language. These dramatic advances represented a major source of opportunities and challenges for our Deductive Application Language (DeAL) and its system DeALS. In fact, unlike the design of the SociaLite system where the performance of recursive graph algorithms with aggregates had played a role, DeALS has been designed as a very general system seeking to satisfy the many needs and lessons that had been learned in the course of a long experience with logic-based data languages, and the LDL [19] and LDL++ [20] experiences in particular. Thus, DeALS supports key non-monotonic con- structs having formal stable model semantics, including, e.g.,  XY-stratification and the choice construct that were found quite useful in program analysis [21], and user-defined aggregates that enabled important knowledge-discovery applications [22]. In addition to a rich set of constructs, DeALS was also designed to support a roster of optimization techniques including magic sets, supplementary magic sets and existential quantification. Introducing powerful new constructs and their optimization techniques by retrofitting a system that already supports a rich set of constructs and optimizations represented a difficult technical challenge. In this paper, we describe how this challenge was met with the introduction of new optimization techniques for monotonic aggregates. We will show that DeALS now achieves both performance and generality, and we will underscore this by comparing not only with SociaLite but also with systems such as DLV [23] and LogicBlox [24] that realize different performance/generality tradeoffs. Overview. The first of two main parts of this paper begins with Section II which presents the syntax and semantics for the min (mmin) and max (mmax) monotonic aggregates. Section III discusses the evaluation and optimization of monotonic aggre- gate programs. Section IV presents implementation details for mmin and mmax and the DeALS storage manager. Section V presents experimental results for Sections II-IV. The second part of this paper begins with Section VI discussing the count (mcount) and sum (msum) monotonic aggregates, followed by their implementation in Section VII and experimental vali- dation in Section VIII. Section IX provides additional DeAL program examples. Section X presents the formal semantics on which our aggregates are based. Additional related works are reviewed in Section XI and we conclude in Section XII. Preliminaries. A Datalog program P is a finite set of rules, or Horn Clauses, where rule r in P has the form A   A1, . . . , An. The atom A is the head of r. A1, . . . , An, the body of r, are literals, or goals, where each literal can be either a positive or negated atom. An atom has the form p(t1, . . . , tj) where p is a predicate and t1, . . . , tj are terms which can be constants, variables or functions. An r with an empty body is a fact. A successful assignment of all variables of rule body goals results in a successful derivation for the rule head predicate. Datalog programs use set semantics and are (typically) stratified (i.e. partitioned into levels based on rule dependencies) and executed in level-by-level order, in a bottom-up fashion. Datalog programs can be evaluated using an iterative approach such as Semi-naive evaluation [10].",Alexander Shkapsky,"University of California, Los Angeles",shkapsky@cs.ucla.edu,Mohan Yang,"University of California, Los Angeles",yang@cs.ucla.edu,Carlo Zaniolo,"University of California, Los Angeles",zaniolo@cs.ucla.edu,,,,,,,,,,,,,,,,,,,,,
20200203,1233,Steven Euijong Whang,"Computer Science Department, Stanford University 353 Serra Mall, Stanford, CA 94305, USA",swhang@cs.stanford.edu,,Entity Resolution with Iterative Blocking,"Entity Resolution with Iterative Blocking, Entity Resolution with Iterative Blocking, Entity Resolution with Iterative Blocking, Entity Resolution with Iterative Blocking, Entity Resolution with Iterative Blocking,  ABSTRACT Entity Resolution (ER) is the problem of identifying which records in a database refer to the same real-world entity. An exhaustive ER process involves computing the similarities between pairs of records, which can be very expensive for large datasets. Various blocking techniques can be used to enhance the performance of ER by dividing the records into blocks in multiple ways and only comparing records within the same block. However, most blocking techniques process blocks separately and do not exploit the results of other blocks. In this paper, we propose an iterative blocking framework where the ER results of blocks are reflected to subsequently processed blocks. Blocks are now iteratively processed until no block contains any more matching records. Compared to simple blocking, iterative blocking may achieve higher accuracy because reflecting the ER results of blocks to other blocks may generate additional record matches. Iterative blocking may also be more efficient because processing a block now saves the processing time for other blocks. We implement a scalable iterative blocking system and demonstrate that iterative blocking can be more accurate and effi- cient than blocking for large datasets. Categories and Subject Descriptors H.2.m [Database Management]: Miscellaneous adata clean- ing ; D.2.8 [Information Storage and Retrieval]: Infor- mation Search and Retrieval aclustering ; H.2.8 [Database Management]: Database Applications adata mining General Terms Algorithms Keywords entity resolution, blocking, iterative blocking 1. INTRODUCTION Entity resolution (ER) is the problem of matching records that represent the same real-world entity and then merging the matching records. ER is a well known problem that arises in many applications. For example, mailing lists may contain multiple entries representing the same physical ad- dress, but each record may be slightly different, e.g., containing different spellings or missing some information. As a second example, two companies that merge may want to combine their customer records: for a given customer that dealt with the two companies they create a composite record that combines the known information. An exhaustive ER process involves comparing all the pairs of records, which can be very expensive for large datasets. Various blocking techniques [19, 24, 5, 16, 13, 11, 15, 2, 10] have been proposed to make ER scalable. Blocking divides the data into (possibly overlapping) blocks and only compares records within the same block, assuming that records in different blocks are unlikely to match. For example, we might partition a set of people records according to the zip codes in address fields. We then only need to compare the records with the same zip code. Since a single blocking criterion may miss matches (e.g., a person may have moved to places with different zip codes), several blocking criteria (i.e., dividing the data in several ways) are typically used to ensure that all the likely matching records are compared, improving the accuracy of the result. Although the previous works above focus on finding the best blocking criteria, most of them assume that all the blocks are processed separately one at a time. In many cases, however, it is useful to exploit an ER result of a previously processed block. First, when two records match and merge in one block, their composite may match with records in other blocks. Second, an ER result of a block can be used to reduce the time of processing another block. That is, the same pair of records may occur in multiple blocks, so once the pair is compared in one block, we can avoid comparing it in other blocks. To address these two points, we propose an iterative blocking framework where the ER result of a block is immediately reflected to other blocks. Unlike previous blocking techniques, there is an additional stage where newly created records of a block are distributed to other blocks. Since the propagation of ER results can generate new record matches in other blocks, the entire operation be- comes iterative in the sense that we are processing blocks (possibly the same block multiple times) until we arrive at a state where we cannot find any more matching records. Our work is thus focused on effectively processing the blocks given a blocking criteria. Motivating Example: Consider the four people records shown in Figure 1, that are to be resolved. We would like to merge records that actually refer to the same person. Suppose that records r and s match with each other be- cause their names are the same, but do not match with t because the strings differ too much. However, once r and s are merged into a new record   <r, s >, the combination of the address and email information of r and s may lead us to discover a new match with t, therefore yielding an initially unforeseen merge   <r, s, t >. Notice that, in order to find this new merge, we need to compare the merged result of r and s with all the other records again. In reality, our dataset can be very large, and it may not be feasible to compare all pairs of the dataset. Hence, we divide the customer records in Figure 1 into blocks. We start by dividing the records by their zip codes. As a result, we only need to compare customers that are in the same geographical region. In Figure 2, the first blocking criterion SC1 uses zip codes to divide the records. Records s and t have the same zip code and are assigned to the block 2 (denoted as b1,2) and records u and v are assigned to b1,3 while r is assigned to b1,1. Since we may miss matches for people who have moved to several places with different zip codes, say we also divide the customer records according to the first characters of their last names. Hence, even if two records referring to the same person have different zip codes, we will have a better chance of comparing them because their last names might be similar. In Figure 2, the matching records r and s can be compared because, although they have different zip codes, they have the same last name. After processing all the blocks, the final result of blocking is {  <r, s >,t,  <u, v >}. Although the blocking of Figure 2 reduces the number of records to compare, it misses the iterative match between   <r, s > and t. Iterative blocking can find this match by dis- tributing the newly created   <r, s > (found in block b2,1) to the other blocks. Assuming that   <r, s > contains the zip codes of both r and s (i.e., 02139 and 94305),   <r, s > is then assigned to both blocks of SC1. In b1,2,   <r, s > can then be compared with t, generating the record   <r, s, t >. Eventually, the final iterative blocking solution becomes {  <r, s, t >,   <u, v >} (see Sec- tion 2.4.3 for details). Thus, the iterative blocking frame- work helps find more record matches compared to simple blocking. Iterative blocking also provides fast convergence. For ex- ample, once the records u and v are merged in b1,3, they do not have to be compared in b2,3. While the blocks in Figure 2 are too small to show any improvements in runtime, we will later demonstrate in our experiments that iterative block- ing can actually run faster than blocking for large datasets when the runtime savings exceed the overhead of iterative blocking. Intuitively, the more work we do for each block, the runtime savings for other blocks become significant. Our example has illustrated the two potential advantages of iterative blocking. The first is improved accuracy, as each iteration may find additional record matches. The second potential advantage is improved runtime performance. By using the ER result of a previous block, we can reduce the time to process other blocks, by avoiding comparisons that were already made. In summary, we make the following contributions. ? We formalize the iterative blocking model (I-BlockER). Unlike blocking, the ER results of blocks are now re- flected to subsequent blocks. The blocks are iteratively processed until no blocks contain any more matching records. I-BlockER can accommodate any  ""core"" ER al- gorithm that resolves records within a single block. ? We present I-BlockER algorithms for two scenarios: ? Lego: An in-memory algorithm that efficiently pro- cesses blocks within memory. ? Duplo: A scalable disk-based algorithm that processes blocks from the disk. ? We experimentally evaluate Lego and Duplo using actual comparison shopping data from Yahoo! Shopping and hotel information data from Yahoo! Travel. Our results show that iterative blocking can improve over blocking both in accuracy and runtime. We evaluate our algo- rithms using two different core ER algorithms, demonstrating the generality of our iterative blocking framework. ",Steven Euijong Whang,"Computer Science Department, Stanford University 353 Serra Mall, Stanford, CA 94305, USA",swhang@cs.stanford.edu,David Menestrina,"Computer Science Department, Stanford University 353 Serra Mall, Stanford, CA 94305, USA",dmenest@cs.stanford.edu,Georgia Koutrika,"Computer Science Department, Stanford University 353 Serra Mall, Stanford, CA 94305, USA",koutrika@cs.stanford.edu,Martin Theobald,"Computer Science Department, Stanford University 353 Serra Mall, Stanford, CA 94305, USA",theobald@cs.stanford.edu,Hector Garcia-Molina,,hector@cs.stanford.edu,,,,,,,,,,,,,,,
20200204,1348,Parag Agrawal,"Stanford University Stanford, CA, USA",paraga@cs.stanford.edu,,Asynchronous View Maintenance for VLSD Databases,"Asynchronous View Maintenance for VLSD Databases, Asynchronous View Maintenance for VLSD Databases, Asynchronous View Maintenance for VLSD Databases, Asynchronous View Maintenance for VLSD Databases, Asynchronous View Maintenance for VLSD Databases, ABSTRACT The query models of the recent generation of very large scale distributed (VLSD) shared-nothing data storage systems, including our own PNUTS and others (e.g. BigTable, Dy- namo, Cassandra, etc.) are intentionally simple, focusing on simple lookups and scans and trading query expressive- ness for massive scale. Indexes and views can expand the query expressiveness of such systems by materializing more complex access paths and query results. In this paper, we examine mechanisms to implement indexes and views in a massive scale distributed database. For web applications, minimizing update latencies is critical, so we advocate deferring the work of maintaining views and indexes as much as possible. We examine the design space, and conclude that two types of view implementations, called remote view tables (RVTs) and local view tables (LVTs), provide a good tradeoff between system throughput and minimizing view staleness. We describe how to construct and maintain such view tables, and how they can be used to implement indexes, group-by-aggregate views, equijoin views and selection views. We also introduce and analyze a consistency model that makes it easier for application developers to cope with the impact of deferred view maintenance. An empiri- cal evaluation quantifies the maintenance costs of our views, and shows that they can significantly improve the cost of evaluating complex queries. Categories and Subject Descriptors H.2.4 [Database Management]: Systems-parallel databases General Terms Performance Keywords indexes, views, distributed and parallel databases 1. INTRODUCTION In order to support the enormous data sizes and query rates seen at web scale, companies have begun to develop massively distributed, shared-nothing databases. Examples of such very large scale distributed (VLSD) systems, de- signed to scale to tens of thousands of nodes, include our own PNUTS [10], as well as Google's BigTable [6], Amazon's Dynamo [11], and Facebook's Cassandra [15]. These systems typically trade away query expressiveness for scala- bility and performance, usually only supporting operations on single records identified through their primary key, and in some cases, range scans on primary key. Since data is partitioned over many servers, executing even moderately complex queries such as a group-by aggregation or a join would incur too much latency and be too detrimental to system throughput. Similarly, looking up a record by a secondary attribute other than the primary key requires executing a full table scan in these systems, thereby making it infeasible. The absence of support for these common queries severely limits the usability of such a database for many web applications. For example, consider a database table of items for sale that is part of a comparison shopping website. The pri- mary key of the table is the listing id of each item, and the website often looks up items by listing id in order to produce a product detail page for each item. However, other types of queries must also be supported. Examples include: ? Find items between $10 and $20. ? Join items with reviews, where reviews are stored in another table. ? Count the number of items for sale in each category. A natural approach to supporting such queries is to create indexes and materialized views. Indexes and views are im- portant in a traditional database to improve performance, but they are critical in a massively distributed database as they are often the only feasible means of answering join, aggregation and secondary-attribute queries. Note that an index can be viewed as one example of a materialized view (in particular, a projection view sorted by one or more sec- ondary attributes). For this reason, we use the term ""views"" in this paper to mean both indexes and materialized views, and use the term ""indexes"" only when we mean the specific kind of materialized view that is used as an index. Synchronous view maintenance can significantly increase the latency of writing to the database. In particular, in a distributed database, both base tables and views are partitioned across many servers. Unless the base and view tables 179 are partitioned identically, synchronously updating a given record on one server may require that several view records on other servers also be updated. Such cross-server communication adds significant latency to requests, especially if one of the involved servers is slow or experiencing a transient failure. If we define multiple views, as is often required in real applications, the latency impact is even higher. Our approach is to defer expensive view maintenance work until after the base update completes. In particular, consider an update that is applied to server S1. While we might perform some local (inexpensive) view maintenance on S1 at the time of the update, we will defer all work involving other servers S2, S3... until we have already returned suc- cess for the base update to the client. This ensures that clients experience low latency on their updates. Deferred view maintenance introduces several challenges: ? We must develop a scalable architecture for storing, main- taining and querying views. ? We do not have the luxury of being able to abort the (already committed) client transaction that updated the base table if failures prevent us from updating the view. Thus, we must ensure that views get updated, even in the presence of failures. ? We must define the consistency guarantees provided by views, which may be out of date with respect to the base table in complex ways. ? Since data in our system is replicated to geographically distant datacenters, we need to efficiently replicate the views as well. In PNUTS, data records are stored in tables, much like relational tables, and tables are partitioned across many servers. In this paper, we explore two mechanisms for main- taining views. The first type, Remote View Tables (RVTs), stores each view in its own PNUTS table, sep- arate from the base tables the views are defined over. Be- cause this view table will be partitioned according to the view's key, not the base table's primary key, view records will likely be stored on different servers than the correspond- ing base records; this is why these views are called ""remote."" Remote views are maintained asynchronously, delaying the generation and application of view updates so as to minimize impact on client update latency. We leverage PNUTS' exist- ing asynchronous replication mechanism to asynchronously propagate updates to RVTs. The second type of view mechanism, Local View Tables (LVTs), keeps view records corresponding to a base record on the same server as the base record. For example, in a view for counting the products for sale by category, each server maintains a local count, grouped by category. To find the total count for, say, the ""Toys"" category, we must retrieve the partial counts from each server and sum them. Although LVTs are maintained synchronously, the maintenance work is performed on the same server as the original update; and the final aggregation of values across partitions is delayed until query time. Failure recovery, system testing and performance opti- mization are already quite difficult in a VLSD system, so we have tried to reuse or extend existing, stable mechanisms in PNUTS whenever possible. In fact, in some cases where two alternative mechanisms are possible, we have preferred the mechanism that is simpler or best uses existing capabilities over a higher performing alternative. In this paper, we study asynchronous view maintenance over a very large scale, horizontally partitioned distributed database, and make the following contributions: ? Two mechanisms (local and remote view tables) for de- ferred view maintenance. ? A characterization of the types of indexes and views that can be maintained by these mechanisms. ? A consistency model for views that are maintained asyn- chronously. ? An experimental evaluation that quantifies the costs of local and remote view maintenance, and their relative benefits for query performance. The rest of this paper is organized as follows. In Section 2, we present an overview of PNUTS. Then, in Section 3, we define RVTs and LVTs, and show how they are maintained. Section 4 describes which views we do and do not support. Then, in Section 5, we present the rationale for the design choices we have made. In Section 6 we describe our consis- tency model. Next, in Section 7 we present an experimental evaluation of our techniques. Section 8 discusses related work. Finally, in Section 9 we present our conclusions.",Parag Agrawal,"Stanford University Stanford, CA, USA",paraga@cs.stanford.edu,Adam Silberstein,"Utkarsh Srivastava and Raghu Ramakrishnan Yahoo! Research Santa Clara, CA, USA",silberst@yahoo-inc.com,Brian F. Cooper,"Utkarsh Srivastava and Raghu Ramakrishnan Yahoo! Research Santa Clara, CA, USA",cooperb@yahoo-inc.com,Utkarsh Srivastava,"Utkarsh Srivastava and Raghu Ramakrishnan Yahoo! Research Santa Clara, CA, USA",utkarsh@yahoo-inc.com,Raghu Ramakrishnan,"Utkarsh Srivastava and Raghu Ramakrishnan Yahoo! Research Santa Clara, CA, USA",ramakris@yahoo-inc.com,,,,,,,,,,,,,,,
20200205,1021,Liang Jeff Chen,"UC San Diego La Jolla, CA, US",jeffchen@cs.ucsd.edu,,Context-sensitive Ranking for Document Retrieval,"Context-sensitive Ranking for Document Retrieval, Context-sensitive Ranking for Document Retrieval, Context-sensitive Ranking for Document Retrieval, Context-sensitive Ranking for Document Retrieval, Context-sensitive Ranking for Document Retrieval,  ABSTRACT We study the problem of context-sensitive ranking for document retrieval, where a context is defined as a sub-collection of documents, and is specified by queries provided by domain-interested users. The motivation of context-sensitive search is that the ranking of the same keyword query generally depends on the context. The reason is that the underlying keyword statistics differ significantly from one context to another. The query evaluation challenge is the computation of keyword statistics at runtime, which involves expensive online aggregations. We appropriately leverage and extend materialized view research in order to deliver algorithms and data structures that evaluate context-sensitive queries efficiently. Specifically, a number of views are selected and materialized, each corresponding to one or more large contexts. Materialized views are used at query time to compute statistics which are used to com- pute ranking scores. Experimental results show that the contextsensitive ranking generally improves the ranking quality, while our materialized view-based technique improves the query efficiency. Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Retrieval models; H.2.4 [Database Management]: Systems-Query processing General Terms Algorithms, Performance Keywords Context-sensitive ranking, materialized views, view selection 1. INTRODUCTION While research in information retrieval (IR) has generated many effective ranking models for general-purpose search, existing rank- ing models may not deliver satisfactory rankings for domain experts. In this paper, we propose a new query model that allows expert users to specify contexts. The ranking of the query result is computed based on keyword statistics collected from the special- ized contexts. This is motivated by the observation that keyword statistics usually vary dramatically from one domain to another and therefore the ranking of the query result will vary accordingly. For example, while ""leukemia"" is rare over the Web, it is a fairly common term in biomedical science (as captured in the PubMed1 database of 18 million articles), and is extremely common among articles of PubMed that are annotated to be cancer-related. Con- ventional ranking models will consider ""leukemia"" as a discrim- inative term, which is not true from the perspective of a cancer researcher or doctor, who typically narrows his/her interest in the cancer-related articles. Given that all ranking models and func- tions use keyword statistics to compute ranking scores, specialized keyword statistics naturally lead to specialized rankings for users interested in narrow domains. 1.1 A Motivating Example PubMed contains 18 million biomedical citations. All the ci- tations include title, abstract, and authors' information. Citations are often linked to full-text articles. Additionally, every citation is annotated with one or more MeSH (Medical Subject Headings) terms from a controlled vocabulary, which specifies a variety of concepts in biomedical science, e.g., ""anatomy"", ""diseases"", ""diagnosis"". MeSH terms in the vocabulary are organized in a hierarchy, as shown in Figure 1. A MeSH term may appear in several places in the hierarchy tree. The vocabulary and the hierarchy of MeSH terms represent an ontology of biomedical science. Each MeSH term represents a biomedical concept and indexes a list of related citations. A com- bination of MeSH terms represents a context that spans the cor- responding concepts. For example, ""neoplasms"" and ""digestive system"" represent two concepts under ""diseases"" and ""anatomy"" respectively. The combination of the two terms identify a set of citations, which form a search context for researchers and doctors concentrating on gastrointestinal (GI) cancer. A researcher or a doctor can specify such a context by utilizing tools that visualize the MeSH term ontology and enable the user to navigate in the ontology and select terms of interest for the context. For example, the tool of Figure 2 mimics the widely used ontology navigator provided by PubMed2 and extends it with the ability to select terms during the navigation. In Figure 2, we see two snap- shots of the ontology navigator, at the point where the user has just selected the context terms. Note that the use of such tools for specifying the context removes the risk of mistyping the context trems, which would otherwise be an important risk since in context-based search only documents that contain the context terms will be retrieved.  Keyword distributions and statistics often vary dramatically from one specialized context to another. For example, research on cancer and research on digestive system have very different terminology. Using keyword statistics from a specialized context in ranking functions will deliver a specialized ranking order for the documents in that context. For example, the classical TF-IDF model uses doc- ument frequency as term weights to boost the ranking of documents that contain query terms that are rare in the collection. The ratio- nale is that rare query terms are more discriminative, and therefore are more important in identifying relevant documents than frequent query terms. In the above example, a query term that is frequent for the citations on ""neoplasms"" may be rare for the citations on ""digestive system"". The ranking order of two documents may be reversed when users are interested in different contexts. Consider the query {pancreas, leukemia}, and two citations C1: ""Complications following pancreas transplant"" and C2: ""Organ failure in patients with acute leukemia"", both annotated with the MeSH term ""digestive system"". Assume we rank the two ci- tations' titles by tf - idf . Since both citations match precisely one single query term, the ranking order of the two citations is only determined by idf . Without a context specification, the fre- quency of leukemia is higher than pancreas in PubMed. Hence, C1 is ranked higher than C2. However, if the query is issued by a GI doctor or researcher, whose focus is on digestive system, the frequency of leukemia is much less than pancreas in the corre- sponding context, and therefore C2 should be ranked higher than C1. Intuitively, pancreas transplant is a common topic among GI researchers. Leukemia in the query is more discriminative in the context. Given that C2 is annotated with ""digestive system"", it is very likely that the organs mentioned in the C2's title refer to digestive organs, which include pancreas. 1.2 Contribution We propose a new query model that extends conventional key- word queries and allows domain-interested users to specify search contexts. A search context is defined as a sub-collection of documents that users are interested in. The goal of context-sensitive ranking is to use keyword statistics based on user-defined contexts to rank documents in the contexts. The query processing of context-sensitive queries presents novel performance challenges that are not met in the processing of conventional keyword queries. In conventional keyword query eval- uation, the context is always fixed; it is the entire document collection. So the statistics are precomputed at indexing time. For context-sensitive ranking, however, contexts are specified by users at query time and can be arbitrary subsets of the document collec- tion. Therefore, the collection-specific statistics (such as document frequency) also have to be computed at query time. A straightforward solution to compute keyword statistics is us- ing standard text search techniques to materialize contexts at query time and gathering required statistics accordingly. Unfortunately, this solution is not always cheap. The challenges are two. First, a common approach to materialize contexts is to intersect inverted lists of keywords, e.g., MeSH terms in PubMed. While intersection operations are efficient for most keyword combinations, intersect- ing very long inverted lists is still expensive [9]. Second, computing keyword statistics not only requires intersections, but also aggregations. As we will show later, some statistics in conventional ranking models demand aggregations of the documents in the contexts, which can be very expensive when the contexts are large. In this paper, we propose a materialized view technique to overcome the above challenges. We reduce the problem of computing keyword statistics to evaluating aggregation queries, and use ma- terialized views to improve query performance. Given that there is a huge number of possible context specifications, the technical challenge is how to choose a reasonable number of views to mate- rialize. We present two algorithms for view selection. The goal is to guarantee good system performance for worst-case queries. Key contributions of the paper include: 1. We propose a novel query model that allows expert users to specify search contexts. The ranking model uses keyword statistics collected from the specified contexts to rank docu- ments in the contexts. 2. We reduce the problem of computing keyword statistics to evaluating aggregation queries, and leverage materialized views to improve query efficiency. Two algorithms are pro- posed to select a number of views to materialize. 3. We perform thorough experiments on the PubMed data set. Results show that context-sensitive ranking improves the ranking quality remarkably, compared with the conventional ranking models. The materialized view technique improves the efficiency of worst-case queries significantly. The overall performance of the system is guaranteed. The paper is organized as follows: Section 2 defines the query model and the ranking model for context-sensitive ranking. Query evaluation is discussed in Section 3. Section 4 reduces the problem of computing keyword statistics to evaluating aggregation queries, and presents a view-based technique to compute statistics. Two view selection algorithms are presented in Section 5 to choose a reasonable number of views to materialize. Experiments and re- sults are elaborated in Section 6. Related work is discussed in Sec- tion 7. Section 8 is the conclusion. ",Liang Jeff Chen,"UC San Diego La Jolla, CA, US",jeffchen@cs.ucsd.edu,Yannis Papakonstantinou,"UC San Diego La Jolla, CA, US",yannis@cs.ucsd.edu,,,,,,,,,,,,,,,,,,,,,,,,
20200206,1349,Michael Loster,"Hasso-Plattner-Institute Potsdam, Germany",michael.loster@hpi.de,,Improving Company Recognition from Unstructured Text by using Dictionaries,"Improving Company Recognition from Unstructured Text by using Dictionaries, Improving Company Recognition from Unstructured Text by using Dictionaries, Improving Company Recognition from Unstructured Text by using Dictionaries, Improving Company Recognition from Unstructured Text by using Dictionaries, Improving Company Recognition from Unstructured Text by using Dictionaries, ABSTRACT While named entity recognition is a much addressed re- search topic, recognizing companies in text is of particular difficulty. Company names are extremely heterogeneous in structure, a given company can be referenced in many dif- ferent ways, their names include person names, locations, acronyms, numbers, and other unusual tokens. Further, in- stead of using the official company name, quite different col- loquial names are frequently used by the general public. We present a machine learning (CRF) system that reli- ably recognizes organizations in German texts. In partic- ular, we construct and employ various dictionaries, regular expressions, text context, and other techniques to improve the results. In our experiments we achieved a precision of 91.11% and a recall of 78.82%, showing significant improve- ment over related work. Using our system we were able to extract 263,846 company mentions from a corpus of 141,970 newspaper articles. 1. FINDING COMPANIES IN TEXT Named entity recognition (NER) defines the task of not only recognizing named entities in unstructured texts but also classifying them according to a predefined set of entity types. The NER task was first defined during the MUC- 6 conference [8], where the objective was to discover gen- eral entity types, such as persons, locations, and organi- zations as well as time, currency, and percentage expres- sions in unstructured texts. Subsequent tasks, such as entity disambiguation, question answering, or relationship extraction (RE), rely heavily on the performance of NER systems, which perform as a preprocessing step. This section highlights the particular difficulties of finding company entities in (German) texts and introduces our in- dustrial use-case, namely risk management based on company- relationship graphs. 1.1 Recognizing company entities Although there is a large body of work on recognizing entities starting from persons and organizations, to entities like gene mentions or chemical compounds, the current re- search often neglects the detection of more fine-grained subcategories, such as person roles or commercial companies. In many cases, the ""standard"" entity classes turn out to be too coarse-grained to be useful in subsequent tasks, such as automatic enterprise valuation, identifying the sentiment towards a particular company, or discovering political and company networks from textual data. What makes recognizing company names particularly difficult is that in contrast to person names they are immensely heterogeneous in their structure. As such, they can be referenced in a multitude of ways and are often composed of many constituent parts, including person names, locations, and country names, industry sectors, acronyms, numbers, and other tokens, which makes them especially hard to recognize. This heterogeneity is expected to be true particularly for the range of medium-sized to small companies. Regarding ex- amples like ""Simon Kucher & Partner Strategy & Marketing Consultants GmbH"", ""Loni GmbH"", or ""Klaus Traeger"", which all are official names of German companies, one can easily see that they vary not only in length and types of their con- stituent parts but also in the position where specific name components appear. In the example ""Clean-Star GmbH & Co Autowaschanlage Leipzig KG"" the legal form ""GmbH & Co KG"" is interleaved with information about the type of the company (carwash) and location information (Leipzig, a city in Germany). What is more, company names are not required to contain specific constituent parts: the example ""Klaus Traeger"" from above is simply the name of a person. It does not provide any additional information apart from the name itself, which leads to ambiguous names that are difficult to identify in practice. Additionally, and in contrast to recognizing named entities from English texts, detecting them in German texts presents itself as an even greater challenge. As pointed out by Faruqui and Pado?, this difficulty is due to the high morphological complexity of the German language, making tasks such as lemmatization much harder to solve [5]. Hence, fea- tures that are highly effective for English often lose their predictive power for German. Capitalization is a prime ex- ample of such a feature. Compared to English, where capi- talization of common nouns serves as a useful indicator for named entities, in German all nouns are capitalized, which drastically lowers the predictive power of the feature. We propose and evaluate a named entity recognizer for German company names by training a conditional random field (CRF) classifier [13]. Besides using different features, Industrial and Applications Paper the fundamental idea is to include domain knowledge into the training phase of the CRF by using different real-world company dictionaries. Transforming the dictionaries into token tries enables us to determine efficiently whether the analyzed text contains companies that are included in the dictionary. During a preprocessing step, we use the token trie to mark all companies in the analyzed text that occur in the used trie. In addition, we automatically extend the dic- tionaries with carefully crafted variants of company names, as we expect them to occur in written text. 1.2 Use case: Risk management using com- pany graphs Among the many possible applications for a company- focused NER system, we focus on modern risk management in financial institutions as one that would benefit from such a system. Named entity recognition and subsequent rela- tionship extraction from text for the purpose of risk man- agement in financial institutions is particularly important in the context of illiquid risk [1]. Illiquid financial risks ba- sically represent contracts between two individuals, e.g., a bank granting a credit over 1 Mio USD (creditor) to a private company (obligor). Because the risk that the credit-taking company will not honor its repayment obligations cannot be easily transferred to other market participants, assessing the creditworthiness of an obligor is of major importance to the relatively small number of its creditors and other business partners. Also, insights gained by one bank on the obligor's ability to pay back are usually not shared. Hence, obtaining adequate and timely information about non-exchange-listed obligors becomes a difficult task for creditors. To circumvent this difficulty, financial institutions rely on the so-called ""insurance principle"": pooling a huge number of independent gains or losses ultimately results in the di- versification of risk, which in turn eliminates almost all of it. Unfortunately, risk mitigation based on the insurance principle relies on the independence assumption between in- dividual gains or losses. At the latest with the financial crisis of 2008/2009, this low dependency assumption has turned out to be devastatingly wrong. Information on the economic dependency structure between contracting parties and assets can be seen as the holy grail of financial risk management. Traditionally, the internal and external data sources used to assess credit risk focus on individual customers, not on the relationships between them. Dependency information is inferred from exposure to common risk factors and thus is inherently symmetric. Direct non-symmetric dependencies, such as supply chains, are not captured. Fortunately, with the growing amount of openly avail- able data sources, there is justified hope that dependency modeling becomes significantly easier by leveraging this vast amount of data. Sadly, most of those data sources are text- based and require considerable effort to extract the con- tained knowledge about relationships and dependencies be- tween the entities of interest. The desired outcome of such an extraction effort can be organized in a graph as shown in Figure 1. The figure shows an example of an actual company graph. To be able to automatically extract such graphs from large amounts of unstructured data, a reliable NER system constitutes the first decisive prerequisite for a following re- lation extraction step. As pointed out at the beginning, the described use case is merely one of many possible use cases, others might include semantic role labeling, machine translation, and question answering systems. 1.3 Contributions and structure We address the problem of recognizing company names from textual data by incorporating dictionary matches into the training process using a feature that represents whether a token is part of a known company name. Our evaluation focusses on analyzing the impact of using a perfect dictio- nary and different real-world dictionaries, as well as the ef- fects of different ways to integrate the knowledge contained in the dictionaries on the performance of the NER system. In particular, we make the following contributions: ? Creation of a NER system capable of successfully rec- ognizing companies in German texts with a precision of 91.11% and a recall of 78.82%. ? Analysis of the impact of various dictionary-based fea- ture strategies on the performance of the NER. The remainder of this paper is organized as follows: Sec- tion 2 discusses related work, while Section 3 presents the baseline configuration for the CRF. In Section 4 we give an overview of the text corpus and the dictionaries we used. We describe the key data structures and technical aspects of the approach in Section 5. Finally, Section 6 presents our experimental results and Section 7 concludes the paper.",Michael Loster,"Hasso-Plattner-Institute Potsdam, Germany",michael.loster@hpi.de,Zhe Zuo,"Hasso-Plattner-Institute Potsdam, Germany",zhe.zuo@hpi.de,Felix Naumann,"Hasso-Plattner-Institute Potsdam, Germany",felix.naumann@hpi.de,Oliver Maspfuhl,"Commerzbank Frankfurt am Main, Germany",oliver.maspfuhl@commerzbank.com,Dirk Thomas,"Commerzbank Frankfurt am Main, Germany",dirk.thomas@commerzbank.com,,,,,,,,,,,,,,,
20200207,1377,Jianbin Qin,"School of Computer Science and Engineering, University of New South Wales",jqin@cse.unsw.edu.au,,Efficient Exact Edit Similarity Query Processing with the Asymmetric Signature Scheme,"Efficient Exact Edit Similarity Query Processing with the Asymmetric Signature Scheme, Efficient Exact Edit Similarity Query Processing with the Asymmetric Signature Scheme, Efficient Exact Edit Similarity Query Processing with the Asymmetric Signature Scheme, Efficient Exact Edit Similarity Query Processing with the Asymmetric Signature Scheme, Efficient Exact Edit Similarity Query Processing with the Asymmetric Signature Scheme, ABSTRACT Given a query string Q, an edit similarity search finds all strings in a database whose edit distance with Q is no more than a given threshold \lamda . Most existing method answering edit similarity queries rely on a signature scheme to generate candidates given the query string. We observe that the number of signatures generated by existing methods is far greater than the lower bound, and this results in high query time and index space complexities. In this paper, we show that the minimum signature size lower bound is \lamda+1. We then propose asymmetric signature schemes that achieve this lower bound. We develop efficient query processing algorithms based on the new scheme. Sev- eral dynamic programming-based candidate pruning meth- ods are also developed to further speed up the performance. We have conducted a comprehensive experimental study in- volving nine state-of-the-art algorithms. The experiment results clearly demonstrate the efficiency of our methods. Categories and Subject Descriptors H.2.4 [Database Management]: Systems!Textual Databases; F.2.2 [Analysis of Algorithms and Problem Complex- ity]: Nonnumerical Algorithms and Problems!Pattern Match- ing General Terms Algorithms, Performance Keywords Approximate Pattern Matching, Similarity Search, Similar- ity Join, Edit Distance, q-gram 1. INTRODUCTION Given a query string Q, an edit similarity search finds all strings in a database whose edit distance with Q is less than a given threshold \lamda . Edit similarity searches have many applications, such as data integration and record linkage, bioinformatics, pattern recognition, and multimedia infor- mation retrieval. For example, ? In bioinformatics, edit similarity search can be employed to find similar protein sequences, and tandem repeats, which are useful to predicting diseases or designing new drugs [19, 27]. ? Batch edit similarity searches, or edit similarity joins, can be used to find near duplicate records in a customer database [2], or near duplicate documents in a document repository [13]. As a result, there has been much interest in efficient algo- rithms to answer edit similarity search or join queries. This is an challenging problem, as edit distance computation is costly and a na?\pyve algorithm that performs edit distance calculation for each string in the database is prohibitively expensive for large databases. To address the performance challenge, most existing ap- proaches adopt the filter-and-verification paradigm based on a signature scheme. A candidate set is generated for the query string by finding database strings that share at least a certain amount of common signatures with the query. Query results can be obtained by verifying the edit distance between each candidate and the query. The numbers of signatures a method generates for data and query strings have a substantial impact on the query performance and index size. We give the numbers of signa- tures for data strings and the query string of several existing approaches in Table 1. Among them, Ed-Join has the small- est signature size with respect to \lamda . It is natural to wonder if this is the minimum signature size, and if not, how we can further reduce the signature size. This paper presents our findings when trying to answer these two questions. First, we propose a framework of sig- nature schemes and the associated query processing method for edit similarity queries. The framework encompasses all major signature-based algorithms for edit similarity queries. We prove that the lower bound on the minimum signature size for any algorithm in this framework is \lamda + 1, where \lamda is the edit distance threshold. Next, we propose a novel sig- nature scheme and corresponding query processing methods for edit similarity queries. Our proposal has three distinct features: (a) its minimum signature size is exactly \lamda + 1, hence reaching the lower bound; (b) it is an asymmetric sig- nature scheme ! by asymmetric, we mean it uses different methods to generate signatures for data and query strings; (c) being asymmetric, we can instantiate two different edit similarity query processing algorithms out of it. Our two methods not only have interesting theoretic properties, but 1033 are also highly efficient in practice. We also develop several candidate pruning techniques that further reduce the num- ber of candidates needing verification. Finally, we perform a comprehensive experimental study comparing our two algo- rithms with nine state-of-the-art algorithms. Our algorithms demonstrate superior performance in most settings. Our contributions can be summarized as: ? We are the first to introduce a general framework to cap- ture the commonalities of many existing algorithms that are based on various kinds of signatures. We also show the lower bound of \lamda + 1 for any algorithm belonging to this framework. ? We propose an asymmetric signature scheme that achieves the lower bound of the number of signatures on the data string or the query string. ? We design two efficient edit similarity query algorithms, IndexChunk and IndexGram, together with several novel candidate pruning algorithms. ? Although many algorithms have been proposed in the past decades on edit similarity queries, to the best of our knowledge, there is no systematic study of their performances. Hence, we conduct a comprehensive experimental study with seven state-of-the-art algorithms for edit similarity queries. Our proposed algorithms have been shown to out- perform existing ones in terms of speed, index size, and robustness. The study also provides a clear picture of the relative performance and space-time tradeoffs of different algorithms. The rest of the paper is organized as follows: Section 2 gives the problem definition and introduces related work. We describe the general framework that summarizes many signature-based edit similarity query algorithms in Section 3. We present an asymmetric signature scheme and show how to use it for edit similarity searches in Section 4. We propose several novel candidate pruning methods in Section 5. Experimental results are presented and analyzed in Section 6. Section 7 concludes the paper. Note that we focus on solving the edit similarity queries exactly in this paper, thus excluding approximate or heuris- tic methods (e.g., Shingling [5], LSH [14], or BLAST [1]).",Jianbin Qin,"School of Computer Science and Engineering, University of New South Wales",jqin@cse.unsw.edu.au,Wei Wang,"School of Computer Science and Engineering, University of New South Wales",weiw@cse.unsw.edu.au,Yifei Lu,"School of Computer Science and Engineering, University of New South Wales",yifeil@cse.unsw.edu.au,Chuan Xiao,"School of Computer Science and Engineering, University of New South Wales",chuanx@cse.unsw.edu.au,Xuemin Lin,"School of Computer Science and Engineering, University of New South Wales",lxue@cse.unsw.edu.au,,,,,,,,,,,,,,,
20200208,1315,Xin Huang,"The Chinese University of Hong Kong, China",xhuang@se.cuhk.edu.hk,,Querying K-Truss Community in Large and Dynamic Graphs,"Querying K-Truss Community in Large and Dynamic Graphs, Querying K-Truss Community in Large and Dynamic Graphs, Querying K-Truss Community in Large and Dynamic Graphs, Querying K-Truss Community in Large and Dynamic Graphs, Querying K-Truss Community in Large and Dynamic Graphs, ABSTRACT Community detection which discovers densely connected structures in a network has been studied a lot. In this paper, we study on- line community search which is practically useful but less studied in the literature. Given a query vertex in a graph, the problem is to find meaningful communities that the vertex belongs to in an online manner. We propose a novel community model based on the k-truss concept, which brings nice structural and computational properties. We design a compact and elegant index structure which supports the efficient search of k-truss communities with a linear cost with respect to the community size. In addition, we investigate the k- truss community search problem in a dynamic graph setting with frequent insertions and deletions of graph vertices and edges. Ex- tensive experiments on large real-world networks demonstrate the effectiveness and efficiency of our community model and search algorithms. Categories and Subject Descriptors H.2.8 [DATABASE MANAGEMENT]: Database Applications! Data mining; G.2.2 [DISCRETE MATHEMATICS]: Graph The- ory!Graph algorithms Keywords k-truss; community search; dynamic graph 1. INTRODUCTION Community structure exists in many real-world networks, for example, social networks and biological networks. Community de- tection, which is to find communities in a network, has been stud- ied a lot in the literature [15, 16, 17, 1, 25]. A different but related problem is online community search, which finds communities con- taining a query vertex in an online manner. These two tasks have different goals: community detection targets all communities in the entire network and usually applies a global criterion to find qualified communities. In contrast, online community search provides personalized community detection for a query vertex. As the com- munities for different vertices in a network may have very differ- Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first pageent characteristics, this user-centered personalized search is more meaningful. Furthermore, as the communities a user participates in represent the social contexts of the user, online community search provides a useful tool for other analytical tasks, such as social circle discovery [14] and social contagion modeling [20]. In this paper, we study the modeling and querying of the communities of a query vertex. A recent study by Cui et al. [9] has proposed a novel approach for online overlapping community search. A new community model was defined as an -adjacency--quasi-k-clique. A -quasi-k-clique is a k-node graph with at least  k(k?1) 2  edges. Another param- eter  is imposed to union two -quasi-k-cliques if they share at least  vertices. Given a query vertex q, the problem is to find all -adjacency--quasi-k-cliques containing q. However, there are several limitations in this community model. 1.  as an average density measure, may not necessarily guar- antee a cohesive community structure. Consider the graph in Figure 1 which is a 0.8-quasi-7-clique containing query ver- tex q. However, q is only connected with one vertex in the community, thus it is not a cohesive community for q obvi- ously. 2. There are three parameters , , k in this model, the setting of which may vary significantly for different query vertices. For example, in a research collaboration network, the com- munities of a famous scholar and a junior scholar can be dra- matically different in terms of the community size and den- sity. Thus it is difficult to choose proper values for the three parameters given a query vertex. 3. Finding -adjacency--quasi-k-clique has been proven to be NP-hard [9], which imposes a severe computational bottle- neck. The approximate algorithms for clique enumeration and expansion [9] reduce the complexity, but cannot give a theoretic guarantee of the approximation quality. Considering these limitations, we propose a novel community model based on the k-truss concept. Given a graph G, the k-truss of G is the largest subgraph in which every edge is contained in at least (k ? 2) triangles within the subgraph [7]. The k-truss is a type of cohesive subgraph defined based on triangle which models the sta- ble relationship among three nodes. However, the k-truss subgraph may be disconnected, for example, the two shaded regions form the 4-truss subgraph in Figure 2(a) which is obviously disconnected. So the k-truss subgraph may not correspond to a meaningful community. On top of the k-truss, we impose an edge connectivity constraint, that is, any two edges in a community either belong to the same triangle, or are reachable from each other through a series of adjacent triangles. Here two triangles are defined as adjacent if they share a common edge. The edge connectivity requirement ensures that a discovered community is connected and cohesive. This defines our novel k-truss community model. To the best of our knowledge, this is the first work that proposes the k-truss commu- nity. Compared with the -adjacency--quasi-k-clique model, our community model has the following advantages. 1. Cohesive community. The k-truss community has cohesive structure according to our analysis in Section 2. For example, the graph in Figure 1 is not a valid k-truss community containing q for k | 3, as the edge (q, s4) is not in any of the triangles. 2. Fewer parameter. Our community model only needs to specify the trussness value k. In addition, a (k + 1)-truss community is contained in a k-truss community. Thus by using different k values for community query, we can get a hierarchical community structure of a query vertex. 3. Polynomial time algorithm. There exist polynomial time algorithms for computing the k-truss subgraphs [7, 21], which make the k-truss community model computationally tractable and efficient. Simply searching k-truss community by its definition may incur a large number of wasteful edge accesses as shown in Section 3.2. Thus the key to efficient k-truss community query processing is to design an effective index. Towards this goal, we first apply an efficient truss decomposition algorithm [21] on a graph G which computes the k-truss subgraphs for all k values. Then we design a novel and elegant index structure, called TCP-Index, to index the pre-computed k-truss subgraphs. The TCP-Index preserves the trussness value and the triangle adjacency relationship in a compact tree-shape index, and supports the query of k-truss community in linear time with respect to the community size, which is optimal. We further study k-truss community search in dynamic graphs, where graph vertices and edges can be frequently inserted or deleted. We present a theoretical analysis to identify the scope in a graph that is affected by edge insertion/deletion. Specifically, we derive a tight upper bound of the trussness for a newly inserted edge, which allows us to precisely identify the affected region with a light cost. Then we design efficient algorithms to update the trussness value and the TCP-Index in the affected region. The incremental up- date algorithms effectively support querying k-truss community in highly dynamic graphs. We conduct extensive experimental studies on large real-world networks and have the following findings. First, the k-truss com- munity search using the TCP-Index is highly efficient in all networks. The query time is from one millisecond for the low degree query vertex to a few seconds for the high degree query vertex which has large and dense communities. The TCP-Index is very compact and can be constructed very efficiently. Second, the TCP-Index can be updated in milliseconds given an edge insertion/deletion. Thus it is highly efficient to support the k-truss com- munity search in dynamic graphs. Last, we evaluate the quality of the discovered communities on two social networks with ground- truth communities and a scientific collaboration network. The re- sults show that our community model can find cohesive and mean- ingful communities of a query vertex. The rest of this paper is organized as follows. We formulate the k-truss community search problem in Section 2. We design a novel TCP-Index and an efficient k-truss community search algorithm in a static graph in Section 3. We further study how to maintain the TCP-Index for query processing in a dynamic graph in Section 4. Extensive experimental results on large real-world networks are reported in Section 5. We discuss related work in Section 6 and conclude this paper in Section 7.",Xin Huang,"The Chinese University of Hong Kong, China",xhuang@se.cuhk.edu.hk,Hong Cheng,"The Chinese University of Hong Kong, China",hcheng@se.cuhk.edu.hk,Lu Qin,"Centre for Quantum Computation and Intelligent Systems, University of Technology, Sydney, Australia",lu.qin@uts.edu.au,Wentao Tian,"The Chinese University of Hong Kong, China",wttian@se.cuhk.edu.hk,Jeffrey Xu Yu,"The Chinese University of Hong Kong, China",yu@se.cuhk.edu.hk,,,,,,,,,,,,,,,
20200209,1350,Chun Chen,"College of Computer Science Zhejiang University, P.R. China",chenc@zju.edu.cn,,TI: An Efficient Indexing Mechanism for Real-Time Search on Tweets,"TI: An Efficient Indexing Mechanism for Real-Time Search on Tweets, TI: An Efficient Indexing Mechanism for Real-Time Search on Tweets, TI: An Efficient Indexing Mechanism for Real-Time Search on Tweets, TI: An Efficient Indexing Mechanism for Real-Time Search on Tweets, TI: An Efficient Indexing Mechanism for Real-Time Search on Tweets, ABSTRACT Real-time search dictates that new contents be made avail- able for search immediately following their creation. From the database perspective, this requirement may be quite easily met by creating an up-to-date index for the contents and measuring search quality by the time gap between insertion time and availability of the index. This approach, however, poses new challenges for micro-blogging systems where thou- sands of concurrent users may upload their micro-blogs or tweets simultaneously. Due to the high update and query loads, conventional approaches would either fail to index the huge amount of newly created contents in real time or fall short of providing a scalable indexing service. In this paper, we propose a tweet index called the TI (Tweet Index), an adaptive indexing scheme for microblog- ging systems such as Twitter. The intuition of the TI is to index the tweets that may appear as a search result with high probability and delay indexing some other tweets. This strategy significantly reduces the indexing cost without com- promising the quality of the search results. In the TI, we also devise a new ranking scheme by combining the relationship between the users and tweets. We group tweets into topics and update the ranking of a topic dynamically. The experi- ments on a real Twitter dataset confirm the efficiency of the TI. Categories and Subject Descriptors H.2.4 [Database Management]: Systems General Terms Algorithms, Design Keywords Real-time Search, Index, Ranking ?In Twitter, tweet refers to the microblog published by users. In this paper, we use it as a common phrase for microblogs. 1. INTRODUCTION The increasing popularity of social networking systems changes the form of information sharing. Instead of issu- ing a query to a search engine, the users log into their social networking accounts and retrieve news, URLs and comments shared by their friends. This is in part caused by the failure of conventional search engines in providing real- time search service for social networking systems. For ex- ample, it is difficult to search a new blog or tweet uploaded a few minutes ago using a conventional search engine. The problem is further amplified in the microblogging systems such as Twitter due to unprecedented amount of tweets or microblogs being posted each day. For example, Tumblr (http://www.tumblr.com) estimated that there were more than 2 million posts and fifteen thousands new users every day1; and based on a latest report from Twitter2, it handled more than 50 million tweets per day. Providing real-time search service is indeed very challeng- ing in large-scale microblogging systems. In such a system, thousands of new updates need to be processed per second. To make every update searchable, we need to index its effect in real time and provide effective and efficient keyword-based retrieval at the same time. The objectives are therefore contradictory since maintenance of up-to-date index will cause severe contention for locks on the index pages. Another problem of real-time search is the lack of effec- tive ranking functions. Figure 1 illustrates an example on the search results of Twitter for the keyword ^IPad2'. The query was submitted a few minutes later after IPad2's sale starts. The user is perhaps looking for the reviews and com- ments about the IPad2, or he is trying to find out the length of queue at the apple stores around his neighborhood. How- ever, most search results are advertisements and most of the returned tweets do not even provide any useful information. This is because the current Twitter search engine sorts the results based on time, and therefore, the latest tweets have the higher rankings. Recall that one key factor of Google's early success is its PageRank [14] algorithm. Without proper ranking functions, the search results are meaningless. How- ever, defining a ranking function for real-time search is not trivial, and the function must have the following two desiderata: 1. The ranking function must consider both the times- tamp of the data and the similarity between the data and the query. As an example, for a given query sub- mitted to Twitter, we do not want to get tweets posted many weeks ago, even though they may contain the keywords of the query. On the other hand, newer tweets with less information are not preferred either. Hence, the ranking function is composed of two inde- pendent factors, time and similarity. 2. The ranking function should be cost-efficient. As we want to support real-time search using a ranking func- tion partially based on time, we have to compute the rankings during query time. Thus, the computation of the ranking function should not incur high overhead. In this paper, we propose the Tweet Index (TI ), a novel indexing and ranking mechanism for enabling real-time search in microblogging systems such as Twitter. The TI is designed based on the observation that most tweets will not appear in the search results. Therefore, we can significantly reduce the indexing cost by delaying indexing less useful tweets. In essence, the TI classifies the tweets into two types, distinguished tweets and noisy tweets. The TI con- sists of two indexing schemes: a real-time indexing scheme for distinguished tweets and a background batch indexing scheme for noisy tweets. Given a new tweet, TI analyzes its contents and determines its type. If it is a distinguished tweet, we will index it immediately. Otherwise, it is grouped with other noisy tweets and periodically, the batch indexing scheme is invoked to index all the noisy tweets in one go. The design principle of the TI is similar in spirit to the par- tial indexing scheme [20, 18], and is also related to the view selection problem [1]. To the best of our knowledge, this is the first proposal that addresses the index issues for the real-time search. In the TI, the ranking function plays the major role in de- ciding whether the tweets are distinguished tweets or noisy tweets and in retrieving meaningful answers. We therefore propose a new ranking function by combining the user graph and tweet graph. In social networks, each user can be con- sidered as a node and different nodes are connected together via the friend links. The user graph denotes the relationship among the users. Naturally, a popular user will have more friends and his/her blogs/tweets also attract wider reader- ship. Therefore, we run a PageRank algorithm for the user graph to compute the ranking for each user. Besides the user graph, the tweets also form a graph, as some tweets are ex- changed between people and some tweets reply to the other tweets. We group tweets into topics based on their relation- ship, and we measure the popularity of the topics based on their statistics. Finally, our proposed ranking function is composed of the user's PageRank, the popularity of topics, the TF (Term Frequency) and the timestamp. The IDF (In- verse Document Frequency) is not used in the TI, since the length of a microblog is fairly small and often capped at certain length (e.g. in Twitter, it is capped at 140 characters). We evaluate the TI by using a real Twitter dataset collected for a user group within the last three years. The experiments examine the performance of our indexing scheme and the effect on the quality of query results. We also compare our ranking function with the other relevant ranking functions. The rest of the paper is organized as follows. In Section 2, we review the previous work in social network search and the corresponding database techniques. In Section 3, we introduce the overview architecture of TI. And the details of the TI's indexing scheme and ranking function are discussed in Section 4 and Section 5, respectively. We evaluate the performance of the proposed schemes in Section 6. And the paper is concluded in Section 7.",Chun Chen,"College of Computer Science Zhejiang University, P.R. China",chenc@zju.edu.cn,Feng Li,"School of Computing National University of Singapore, Singapore",li-feng@comp.nus.edu.sg,Beng Chin Ooi,"School of Computing National University of Singapore, Singapore",ooibc@comp.nus.edu.sg,Sai Wu,"School of Computing National University of Singapore, Singapore",wusai@comp.nus.edu.sg,,,,,,,,,,,,,,,,,,
20200210,1351,Hojjat Jafarpour,"Confluent Inc. Palo Alto, CA",hojjat@confluent.io,,KSQL: Streaming SQL Engine for Apache Kafka,"KSQL: Streaming SQL Engine for Apache Kafka, KSQL: Streaming SQL Engine for Apache Kafka, KSQL: Streaming SQL Engine for Apache Kafka, KSQL: Streaming SQL Engine for Apache Kafka, KSQL: Streaming SQL Engine for Apache Kafka, ABSTRACT Demand for real-time stream processing has been increasing and Apache Kafka has become the de-facto streaming data platform in many organizations. Kafka Streams API along with several other open source stream processing systems can be used to process the streaming data in Kafka, however, these systems have very high barrier of entry and require programming in languages such as Java or Scala. In this paper, we present KSQL, a streaming SQL engine for Apache Kafka. KSQL provides a simple and completely interactive SQL interface for stream processing on Apache Kafka; no need to write code in a programming language such as Java or Python. KSQL is open-source, distributed, scalable, reliable, and real-time. It supports a wide range of powerful stream processing opera- tions including aggregations, joins, windowing, sessionization, and much more. It is extensible using User Defined Functions (UDFs) and User Defined Aggregate Functions (UDAFs). KSQL is implemented on Kafka Streams API which means it provides exactly once delivery guarantee, linear scalability, fault tolerance and can run as a library without requiring a separate cluster. 1 INTRODUCTION In recent years, the volume of data that is generated in organizations has been growing rapidly. From transaction log data in e-commerce platforms to sensor generated events in IoT systems to network monitoring events in IT infrastructures, capturing large volumes of data reliably and processing them in a timely fashion has become an essential part of every organization. This has resulted in an emerging paradigm where organizations have been moving from batch oriented data processing platforms towards realtime stream processing platforms. Initially developed at LinkedIn, Apache Kafka is a battle hardened streaming platform that has been used to capture trillions of events per day [2] [16]. Apache Kafka has become the de-facto streaming platform in many organizations where it provides a scalable and reliable platform to capture and store all the pro- duced data from different systems. It also efficiently provides the captured data to all the systems that want to consume it. While capturing and storing streams of generated data is essential, pro- cessing and extracting insight from this data in timely fashion has become even more valuable. Kafka Streams API along with other open source stream processing systems have been used to perform such real time stream processing. Such real time stream processing systems have been used to develop applications such as Streaming ETL, anomaly detection, real time monitoring and many more. Many of these stream processing systems require users to write code in complex languages such as Java or Scala and can only be used by users who are fluent in such languages. This is a high barrier of entry that limits the usability of such systems. Motivated by this challenge, in this paper we present KSQL, a streaming SQL engine for Apache Kafka that offers an easy way to express stream processing transformations[8]. While the exist- ing open source stream processing systems require expression of stream processing in programming languages such as Java, Scala or Python or offer limited SQL support where SQL statements should be embedded in the Java or Scala code, KSQL offers an interactive environment where SQL is the only language that is needed. KSQL also provides powerful stream processing capa- bilities such as joins, aggregations, event-time windowing, and many more. KSQL is implemented on top of the Kafka Streams API which means you can run continuous queries with no additional cluster; streams and tables are first-class constructs; and you have access to the rich Kafka ecosystem. Similar to addition of SQL to other systems such as Apache Hive[17] and Apache Phoenix[4], we believe that introduction of SQL for stream processing in Kafka will significantly broaden the users base for stream processing and bring the stream processing to the masses. The rest of the paper is organized as the following. In the next section we provide a brief overview on Apache Kafka and the Kafka Streams API. Section 3 presents our contribution in design and development of KSQL. We describe data model, basic concepts, query language and the internals of our SQL engine. In Section 4, we present how KSQL can be extended using UDFs and UDAFs. We describe different execution modes for KSQL in Section 5. We present our experimental evaluation results for KSQL in Section 6. Section 7 describes the related work. We present the future work directions and conclude the paper in Section 8.",Hojjat Jafarpour,"Confluent Inc. Palo Alto, CA",hojjat@confluent.io,Rohan Desai,"Confluent Inc. Palo Alto, CA",rohan@confluent.io,Damian Guy,"Confluent Inc. London, UK",damian@confluent.io,,,,,,,,,,,,,,,,,,,,,
20200211,1269,Udayan Khurana,IBM TJ Watson Research Center,ukhurana@us.ibm.com,,Storing and Analyzing Historical Graph Data at Scale,"Storing and Analyzing Historical Graph Data at Scale, Storing and Analyzing Historical Graph Data at Scale, Storing and Analyzing Historical Graph Data at Scale, Storing and Analyzing Historical Graph Data at Scale, Storing and Analyzing Historical Graph Data at Scale, ABSTRACT The work on large-scale graph analytics to date has largely focused on the study of static properties of graph snapshots. However, a static view of interactions between entities is often an oversimplification of several complex phenomena like the spread of epidemics, information diffusion, formation of online communities, and so on. Being able to find temporal interaction patterns, visualize the evolution of graph properties, or even simply compare snapshots across time, adds significant value in reasoning over graphs. However, due to the lack of underlying data management support, an analyst today has to manually navigate the added temporal complexity of dealing with large evolving graphs. In this paper, we present a system, called Historical Graph Store, that enables users to store large volumes of historical graph data and to express and run com- plex temporal graph analytical tasks against that data. It consists of two key components: (1) a Temporal Graph Index (TGI), that com- pactly stores large volumes of historical graph evolution data in a partitioned and distributed fashion ? TGI also provides support for retrieving snapshots of the graph as of any timepoint in the past or evolution histories of individual nodes or neighborhoods; and (2) a Temporal Graph Analysis Framework (TAF), for expressing com- plex temporal analytical tasks and for executing them in an efficient and scalable manner using Apache Spark. Our experiments demon- strate our system 's efficient storage, retrieval and analytics across a wide variety of queries on large volumes of historical graph data. 1. INTRODUCTION Graphs are useful in capturing behavior involving interactions between entities. Several processes are naturally represented as graphs ? social interactions between people, financial transactions, biological interactions among proteins, geospatial proximity of in- fected livestock, and so on. Many problems based on such graph models can be solved using well-studied algorithms from graph theory or network science. Examples include finding driving routes by computing shortest paths on a network of roads, finding user communities through dense subgraph identification in a social net- work, and many others. Numerous graph data management sys- tems have been developed over the last decade, including special ized graph database systems like Neo4j, Titan, etc., and large-scale graph processing frameworks such as GraphLab [27], Pregel [29], Giraph, GraphX [12], GraphChi [24], etc. However much of the work to date, especially on cloud-scale graph data management systems, focuses on managing and analyzing a single (typically, current) static snapshot of the data. In the real world, however, interactions are a dynamic affair and any graph that abstracts a real-world process changes over time. For in- stance, in online social media, the friendship network on Facebook or the  ""follows "" network on Twitter change steadily over time, whereas the  ""mentions "" or the  ""retweet "" networks change much more rapidly. Dynamic cellular networks in biology, evolving cita- tion networks in publications, dynamic financial transactional net- works, are few other examples of such data. Lately, we have seen an increasing merit in dynamic modeling and analysis of network data to obtain crucial insights in several domains such as cancer prediction [38], epidemiology [15], organizational sociology [16], molecular biology [9], information spread on social networks [26] amongst others. In this work, our focus is on providing the ability to analyze and to reason over the entire history of the changes to a graph. There are many different types of analyses of interest. For example, an an- alyst may wish to study the evolution of well-studied static graph properties such as centrality measures, density, conductance, etc., over time. Another approach is through the search and discovery of temporal patterns, where the events that constitute the pattern are spread out over time. Comparative analysis, such as juxtaposition of a statistic over time, or perhaps, computing aggregates such as max or mean over time, possibly gives another style of knowledge discovery into temporal graphs. Most of all, a primitive notion of just being able to access past states of the graphs and performing simple static graph analysis, empowers a data scientist with the capacity to perform analysis in arbitrary and unconventional patterns. Supporting such a diverse set of temporal analytics and querying over large volumes of historical graph data requires addressing several data management challenges. Specifically, there is a want of techniques for storing the historical information in a compact manner, while allowing a user to retrieve graph snapshots as of any time point in the past or the evolution history of a specific node or a specific neighborhood. Further, the data must be stored and queried in a distributed fashion to handle the increasing scale of the data. There is also a need for an expressive, high-level, easy-to-use programming framework that will allow users to specify complex temporal graph analysis tasks, while ensuring those tasks can be executed efficiently in a data-parallel fashion across a cluster. In this paper, we present a graph data management system, called Historical Graph Store (HGS), that provides an ecosystem for managing and analyzing large historical traces of graphs. HGS consists of two key distinct components. First, the Temporal Graph Index (TGI), is an index that compactly stores the entire history of a graph by appropriately partitioning and encoding the differences over time (called deltas). These deltas are organized to optimize the retrieval of several temporal graph primitives such as neighborhood versions, node histories, and graph snapshots. TGI is designed to use a distributed key-value store to store the partitioned deltas, and can thus leverage the scalability afforded by those systems (our im- plementation uses Apache Cassandra1 key-value store). TGI is a tunable index structure, and we investigate the impact of tuning the different parameters through an extensive empirical evaluation. TGI builds upon our prior work on DeltaGraph [21], where the focus was on retrieving individual snapshots efficiently; TGI ex- tends DeltaGraph to support efficient retrieval of subgraphs instead of only full snapshots, retrieval of histories of nodes or subgraphs over past time intervals, and features a highly scalable design over DeltaGraph. The second component of HGS is a Temporal Graph Analy- sis Framework (TAF), which provides an expressive framework to specify a wide range of temporal graph analysis tasks. TAF is based on a novel set of temporal graph operands and operators that en- able parallel execution of the specified tasks at scale in a cluster environment. The execution engine is implemented on Apache Spark [40], a large-scale in-memory cluster computing framework. Outline: The rest of the paper is organized as follows. In Section 2, we survey the related work on graph data stores, temporal indexing, and other topics relevant to the scope of the paper. In Section 3, we provide a sketch of the overall system, including key aspects of the underlying components. We then present TGI and TAF in detail in Sections 4 and 5, respectively. In Section 6, we provide an empirical evaluation, and and conclude with a summary and a list of future directions in Section 7.",Udayan Khurana,IBM TJ Watson Research Center,ukhurana@us.ibm.com,Amol Deshpande,University of Maryland,amol@cs.umd.edu,,,,,,,,,,,,,,,,,,,,,,,,
20200212,1366,Yuya Sasaki,"Graduate School of Information Science and Technology, Osaka University, Osaka, Japan",sasaki@ist.osaka-u.ac.jp,,Sequenced Route Query with Semantic Hierarchy,"Sequenced Route Query with Semantic Hierarchy, Sequenced Route Query with Semantic Hierarchy, Sequenced Route Query with Semantic Hierarchy, Sequenced Route Query with Semantic Hierarchy, Sequenced Route Query with Semantic Hierarchy,  ABSTRACT The trip planning query searches for preferred routes starting from a given point through multiple Point-of-Interests (PoI) that match user requirements. Although previous studies have investigated trip planning queries, they lack flexibility for finding routes because all of them output routes that strictly match user requirements. We study trip planning queries that output multiple routes in a flexible manner. We propose a new type of query called skyline sequenced route (SkySR) query, which searches for all preferred sequenced routes to users by extending the shortest route search with the semantic similarity of PoIs in the route. Flexibility is achieved by the semantic hierarchy of the PoI category. We propose an efficient algorithm for the SkySR query, bulk SkySR algorithm that simultaneously searches for sequenced routes and prunes unnecessary routes effectively. Experimental evaluations show that the proposed approach significantly outperforms the existing approaches in terms of response time (up to four orders of magnitude). Moreover, we develop a prototype service that uses the SkySR query, and conduct a user test to evaluate its usefulness. 1 INTRODUCTION Recently, technological advances in various devices, such as smart phones and automobile navigation systems, have allowed users to obtain real-time location information easily. This has triggered the development of location-based services such as Foursquare, which exploit rich location information to improve service quality. The users of the location-based services often want to find short routes that pass through multiple Points-of-Interest (PoIs); consequently, developing trip planning queries that can find the shortest routes that passes through user-specified categories has attracted considerable attention [4, 10]. If multiple PoI categories, e.g., restaurant and shopping mall, are in an ordered list (i.e., a cat- egory sequence), the trip planning query searches for a sequenced route that passes PoIs that match the user-specified categories in order. Example 1.1. Figure 1 shows a road networkwith the following PoIs: ""Asian restaurant"", ""Italian restaurant"", ""Gift shop"", ""Hobby shop"", and ""Arts&Entertainment (A&E)"". Assume that a user wants to go to an Asian restaurant, an A&E place, and a gift shop in this order from start point vq . The sequenced route query outputs route R1 because it is the shortest route from vq that satisfied the user requirements ?Asian restaurant, A&E, gift shop?. Existing approaches find the shortest route based on the user query. However, such approaches may find an unexpectedly long route because the found PoIs may be distant from the start point. A major problem with the existing approaches is that they only output routes that perfectlymatch the given categories [5, 14, 16]. To overcome this problem, we introduce flexible similarity match- ing based on PoI category classification to find shorter routes in a flexible manner. In the real-world, category classification often forms a semantic hierarchy, which we refer to as a category tree. For example, in Foursquare 1 , the ""Food"" category tree includes ""Asian restaurant,"" ""Italian restaurant,"" and ""Bakery"" as subcat- egories, and the ""Shop &Service"" category includes ""Gift shop,"" ""Hobby shop,"" and ""Clothing store"" as subcategories (Figure 2). We employ this semantic hierarchy to evaluate routes in terms of two aspects, i.e., route length and the semantic similarity between the categories of the PoIs in the route and those specified in the user query. As a result, we can find effective sequenced routes that semantically match the user requirement based on the se- mantic hierarchy. For example, in Figure 1, route R2 satisfies the user requirement because it semantically matches the category sequence because Italian and Asian restaurants are in the same category tree. However, this approach may find a significantly large number of sequenced routes because the number of PoIs that flexibly match the given categories increases significantly. To reduce the number of routes to be output, we employ the skyline concept [2], i.e., we restrict ourselves to searching for the routes that are not worse than any other routes in terms of their scores (i.e., numerical values to evaluate the routes). Based on this concept, we propose the skyline sequenced route (SkySR) query, which applies the skyline concept to the route length and semantic similarity (i.e., we consider route length and semantic similarity as route scores). Given a start point and a sequence of length and semantic similarity. Example 1.2. Table 1 shows real-world examples of sequenced routes in New York city where a user plans to go to a cupcake shop, an art museum, and then a jazz club in this order. The existing approaches output a single route that matches the user's requirement perfectly. The proposed approach can output three additional routes that are shorter than the route found by the existing approach. Note that the additional routes also satisfy the user query semantically. The user can select a preferred route among all the four routes depending on how far he/she does not want to walk or their available time. The SkySR query can provide effective trip plans; however, it incurs significant computational cost because a large num- ber of routes can match the user requirement. Therefore, the SkySR query requires an efficient algorithm. The challenge is to search for SkySRs efficiently by reducing the search space with- out sacrificing the exactness of the result. We propose bulk SkySR algorithm (BSSR for short) that finds exact SkySRs efficiently. Recall that a feature of SkySRs is that their scores are no worse than those of other sequenced routes. BSSR exploits the branch- and-bound algorithm [9], which effectively prunes unnecessary routes based on the upper and lower bounds of route scores. In addition, to improve efficiency more, we employ four techniques to optimize BSSR. (1) First, we initially find sequenced routes to calculate the upper bound. (2) We tighten the upper bound by arranging the priority queue and (3) tighten the lower bound by introducing minimum distances. (4) we keep intermediate results for later processing, which refer to as on-the-fly caching. Our approach significantly outperforms existing approaches in terms of response time (up to four orders of magnitude) with- out increasing memory usage or sacrificing the exactness of the result. The main contributions of this paper are as follows. ? We introduce a semantic hierarchy to the route search query, which allows us to search for routes flexibly. ? We propose the skyline sequenced route (SkySR) query, which finds all preferred routes related to a specified cate- gory sequence with a semantic hierarchy (Section 4). ? We propose an exact and efficient algorithm and its op- timization techniques to process SkySR queries (Section 5). ? We discuss variations and extensions of the SkySR query. The SkySR query can be applied to various user require- ments and environments (Section 6). ? We demonstrate that the proposed approach works well in terms of response time and memory usage by performing extensive experiments. (Section 7). ? We develop a prototype service that employs the SkySR query and conduct a user test to evaluate usefulness of the SkySR query. (Section 8). The remainder of this paper is organized as follows. Section 2 introduces related work. Section 3 describes the problem formu- lation, and Section 4 defines the SkySR query. Section 5 presents the proposed algorithm. In Section 6, we discuss variations and extensions of the SkySR query. Sections 7 and 8 present experi- ment and user test results, respectively, and Section 9 concludes the paper.",Yuya Sasaki,"Graduate School of Information Science and Technology, Osaka University, Osaka, Japan",sasaki@ist.osaka-u.ac.jp,Yoshiharu Ishikawa,"Graduate School of Information Science, Nagoya University, Nagoya, Japan",ishikawa@i.nagoya-u.ac.jp,Yasuhiro Fujiwara,"NTT Software Innovation Center, Tokyo, Japan",fujiwara.yasuhiro@lab.ntt.co.jp,Makoto Onizuka,"Graduate School of Information Science and Technology, Osaka University, Osaka, Japan",onizuka@ist.osaka-u.ac.jp,,,,,,,,,,,,,,,,,,
20200213,1352,Dimitra Papadimitriou,"University Of Trento Trento, Italy",papadimitriou@disi.unitn.it,,Modeling and Exploiting Goal and Action Associations for Recommendations,"Modeling and Exploiting Goal and Action Associations for Recommendations, Modeling and Exploiting Goal and Action Associations for Recommendations, Modeling and Exploiting Goal and Action Associations for Recommendations, Modeling and Exploiting Goal and Action Associations for Recommendations, Modeling and Exploiting Goal and Action Associations for Recommendations, ABSTRACT Recommender systems are used to identify those items in a large collection that are more likely to be of interest to a user. A com- mon principle of most recommenders is that whatever happened in the past is a good indicator of the future. We offer a different perspective. Considering the fact that in real life users do their selections with certain goals in mind, we recommend items (or actions) that help users fulfilling their intended goals using their past only as a way of identifying goals of interest. We introduce a model that connects goals and actions through action sets im- plementing the respective goals. Such a model captures latent associations among goals and actions and allows the ranking of actions considering different user strategies such as to complete at least one goal with the minimum effort (i.e., minimum number of actions), or to open up more paths for fulfillment of more goals in the future. For each strategy we recommend an algorithm that exploits the user action and goal spaces to rank the actions in a different way. We have performed extensive experimental studies to understand how these techniques are related and compare the results against traditional recommendation methods. The experi- ments illustrate that it is not possible to replicate the results of our approach using existing techniques. 1 INTRODUCTION People are daily facing situations in which they have to make choices from large collections of items. Selecting the best answer to a search engine query among those satisfying the query conditions, selecting a movie to watch, an item to purchase, or friend activities to read about in social media, are only some of the most characteristic examples. Recommender systems [3, 7, 12, 16, 20] give advice to users on items that are likely of interest to them. There are two main categories of recommender systems. The first is the collaborative filtering, which is based on the idea that similar users have similar preferences, thus, the analysis of the choices of similar users can result in successful recommendations of items that have not been selected yet. The second category is the content-based which is based on the idea that users would like items that have similar features with items they have liked in the past. The principle behind both approaches is that whatever the past indicated as preference, it is likely to be preferred also in the future. In this work, we approach the problem based on a different principle. There have been studies in psychology and social sciences [4] that have shown that human actions are not random and unrelated events. They may be of course affected by preferences but they are mainly results of rational selections performed with the purpose of achieving some specific goal that a person has set and aims to fulfill [1]. Based on these studies, we advocate that by recognizing the goals for which actions of the past have been performed, it is possible to identify the driving forces of the users ' future actions and make recommendations that better fit these needs. Since the fulfillment of a specific goal may require actions that are highly different in nature, this form of recommendation may recommend actions that are highly different from those of the past, or from those that similar users have done in the past. Note that we may use the term  ""actions "" and not  ""items "" as typically done in rec- ommender systems; with this option we are being more generic since the selection of an item, the purchase of a product, or the watching of a movie are practically all actions. Existing studies in recommender systems have already recog- nized that methods taking into account similarity with what has happened in the past are not always matching user expectations and have tried different techniques that focus on other aspects such as serendipity, novelty and diversity to improve the quality of recommendations [9]. However, these solutions are not principled and are not driven by some specific, user-selected, well-defined target while in many recommendation scenarios there exist targets that users are willing to reach. For instance, in online learning platforms, users may target at specializations or/and degrees. In employment-oriented social networking services such as LinkedIn users are encouraged to take actions that will lead them into their next position. In addition, they can see how some actions can lead to the same target following different career paths. Moreover, users may perform actions that will lead them to the fulfillment of commercial goals such as to get discount coupons, or everyday goals such as to become fit or to cook. Consider, for instance, the case of a customer in a supermar- ket that has placed in the cart a kilo of potatoes and carrots. A content-based recommendation will try to propose products that are close to what is already in the cart, i.e., similar to potatoes and carrots which means it may propose other kinds of vegetables, or even suggest other types of potatoes. On the other hand, a col- laborative filtering system may suggest light beer or red peppers, because these items have been bought in the past by customers with similar preferences. Both methods, through clearly different routes, recommend items based on the customer 's past. Instead, by taking into account that the items in the customer 's cart can be combined with other items to produce one or more food recipes, the system can open up new options to the customer. For instance, considering a recipe to make an olivier (russian) salad that in- cludes: potatoes, carrots and pickles, an item to be recommended would be pickles. Another useful ingredient would be nutmeg that is a spice used for mashed potatoes and pan-fried carrots, two recipes that require products some of which are already in the customer 's cart. Such a recipe-based recommendation of products may not be justified by similarity to products already in the cart, neither by other product combinations found frequently in the carts of other customers. This means that neither association rules nor techniques that detect correlations among items can be em- ployed to make such recommendations since they highly depend on the popularity of these item sets. So, unless we consider the product combinations found in the recipes, these products will not be recommended by other techniques. Furthermore, given the recipes, the recommendations can be optimized for an overall benefit. For example, recommended products may give the ability to the customers to maximize the number of recipes that they can materialize. Considering goals in the recommendation problem is challenging. The challenge comes from the fact that, in real life, there are typically multiple goals that one needs to fulfill at any given time. Each of these goals may require fewer or more actions in order to be fulfilled, and there may exist alternative ways for the fulfillment of a specific goal. Users have to reason on the priorities between the goals they try to achieve and the benefit they will have by the execution of each action towards the fulfillment of these goals. For instance, some users may prefer actions that help them fulfill a goal as soon as possible, while others may prefer actions that help the advancement of as many goals as possible. A goal-oriented recommender will have to leverage the goals by first recognizing the intended user goals, decide the priorities among them, and quantify the benefit of each action in relationship to the intended goals and in conjunction with the other possible actions. We introduce a new family of recommendation strategies, i.e., goal-based recommendations, that deal with the above challenges. The goal-based strategies identify the goals for which exists evi- dence that the user is aiming at achieving. The evidence originates from the previous user activity, i.e., the actions that the user has already performed. Given this goal space, the strategies explore the sets of actions that lead to the fulfillment of these goals and contain actions that the user has already performed to find actions which the user has not performed and may be willing to complete. The sets of actions together with the goals they fulfill constitute the user 's goal implementation space. The likelihood that the users will like an action from the candidate set of actions in this space depends on their approach towards the goals they would like to fulfill. We have identified three different strategies for exploring and exploiting the user 's spaces in order to select the actions to be recommended. The three strategies correspond to three different policies based on which users often make their selections. The first strategy is the Focus that examines each of the action sets in the user 's goal implementation set to find which of them lead to the fulfillment of the goal that is closest to completion, either because most of the required actions have been already performed (Focuscmp ), or because they require only a few more actions (Focuscl ). Then, it forms the recommendation lists from the actions in these action sets. It is the policy preferred by users that need to fulfill at least one goal through the actions in the current recommendation list. The second strategy, Breadth, is not examining each action set in the user 's goal implementation space separately. It considers more than one set of actions at the same time. Specifically, it evaluates and ranks the actions in the user 's action space based on all the sets this action participates and se- lects those actions that belong in as many sets as possible together with as many as possible actions from the user activity. This strat- egy is for users that would like to fulfill as many goals as possible, if possible, through this recommendation list, but in order to max- imize the number of fulfilled goals, they are willing to complete some or all of them in the future, i.e., not only through the actions in the current recommendation list. This way it keeps some  ""paths "" open for the future (i.e., unfulfilled goals) but those paths contain the minimum number of additional actions. We also suggest a third strategy, the Best Match, that similarly to Breadth is not trying to fulfill at least one goal through the current recommendation list. It recommends actions that contribute to the goals of the user 's goal space. However, in contrast to Breadth, Best Match evaluates an action considering the whole goal space, not only the goals to which this specific action contributes. It generates a profile for the user and estimates a similarity between this profile and the actions to be recommended. The action representation shows how much that action contributes to the fulfillment of the various goals and the user profile how many of the user actions contribute to the various goals. It is a policy that may end up in the fulfillment of many goals in the future. However, it is a strategy for users that are interested in actions that are more useful (contribute more) to the goals to which the user has has put more effort in the past (and respectively less to goals to which the user has put less effort). Our contributions can be summarized as follows: ? We introduce and formally define the notion of goal-oriented recommendation, which evaluates every action considering the goals which the current user may be willing to fulfill and how that action contributes to the fulfillment of one or more of these goals together with other actions of the user (Section 3). ? We explain how it differs from existing techniques and why the latter cannot be used to offer this type of recommenda- tion (Section 2). ? We present different strategies for ranking the candidate actions, with each strategy implementing a different policy in prioritizing the goals and selecting the actions to be recommended (Section 5). ? We describe efficient ways of implementing the above strategies and materializing the goal oriented recommenda- tion paradigm (Section 4). ? We study the effectiveness of our methods and compare them to the state-of-the-art recommendation approaches. We show that goal-based approaches can recommend ac- tions that bring the user closer to the fulfillment of goals that are related to her/him, are highly different from each other and at the same time from actions performed by other users in the past (Section 6).",Dimitra Papadimitriou,"University Of Trento Trento, Italy",papadimitriou@disi.unitn.it,Yannis Velegrakis,"University Of Trento Trento, Italy",velgias@disi.unitn.eu,Georgia Koutrika,"Athena Research Center Athens, Greece",georgia@imis.athena-innovation.gr,,,,,,,,,,,,,,,,,,,,,
20200214,1353,Xuebin He,"Worcester Polytechnic Institute, Computer Science Department, MA, USA",xhe2@cs.wpi.edu,,Discovering Correlations in Annotated Databases,"Discovering Correlations in Annotated Databases, Discovering Correlations in Annotated Databases, Discovering Correlations in Annotated Databases, Discovering Correlations in Annotated Databases, Discovering Correlations in Annotated Databases, ABSTRACT Most emerging applications, especially in science domains, main- tain databases that are rich in metadata and annotation information, e.g., auxiliary exchanged comments, related articles and images, provenance information, corrections and versioning information, and even scientists' thoughts and observations. To manage these annotated databases, numerous techniques have been proposed to extend the DBMSs and efficiently integrate the annotations into the data processing cycle, e.g., storage, indexing, extended query languages and semantics, and query optimization. In this paper, we address a new facet of annotation management, which is the dis- covery and exploitation of the hidden corrections that may exist in annotated databases. Such correlations can be either between the data and the annotations (data-to-annotation), or between the anno- tations themselves (annotation-to-annotation). We make the case that the discovery of these annotation-related correlations can be exploited in various ways to enhance the quality of the annotated database, e.g., discovering missing attachments, and recommend- ing annotations to newly inserted data. We leverage the state-of- art in association rule mining in innovative ways to discover the annotation-related correlations. We propose several extensions to the state-of-art in association rule mining to address new challenges and cases specific to annotated databases, i.e., incremental addition of annotations, and hierarchy-based annotations. The proposed al- gorithms are evaluated using real-world applications from the bio- logical domain, and an end-to-end system including an Excel-based GUI is developed for seamless manipulation of the annotations and their correlations. 1. INTRODUCTION Most modern applications annotate and curate their data with various types of metadata information!usually called annotations, e.g., provenance information, versioning timestamps, execution statistics, related comments or articles, corrections and conflictrelated information, and auxiliary exchanged knowledge from dif- ferent users. Interestingly, the number and size of these annotations is growing very fast, e.g., the number of annotations is around 30x, 120x, and 250x larger than the number of data records in Data Bank biological database [3], Hydrologic Earth database [4, 47], and AKN ornithological database [5], respectively. Existing tech- niques in annotation management, e.g., [9, 15, 17, 21, 24], have made it feasible to systematically capture such metadata annotations and efficiently integrate them into the data processing cy- cle. This includes propagating the related annotations along with queries' answers [9, 15, 17, 24, 46], querying the data based on their attached annotations [21, 24], and supporting semantic annotations such as provenance tracking [11, 14, 20, 43], and belief annotations [23]. Such integration is vey beneficial to higher-level applications as it complements the base data with the auxiliary and semanticrich source of annotations. In this paper, we address a new facet of annotation management that did not receive much attention before and has not been ad- dressed by existing techniques. This facet concerns the discovery and exploitation of the hidden correlations that may exist in anno- tated databases. Given the growing scale of annotated databases! both the base data and the annotation sets!important correlations may exist either between the data values and the annotations, i.e., data-to-annotations correlations, or among the annotations them- selves, i.e., annotations-to-annotations correlations. By systematically discovering such correlations, applications can leverage them in various ways as motivated by the following scenarios. Motivation Scenario 1?Discovery of Missing Attachments: Assume the example biological database illustrated in Figure 1. Typi- cally, many biologists may annotate subsets of the data over time! each scientist focuses only on few genes of interest at a time. For example, some of the data records in Figure 1 are annotated with a ^Black Flag' annotation. This annotation may represent a scien- tific article or a comment that is attached to these tuples. By ana- lyzing the data, we observe that most genes having value F1 in the Family column have an attached ^Black Flag' annotation. Such correlation suggests that gene JW0012 is probably missing this annotation, e.g., none of the biologists was working on that gene and thus the article did not get attached to it. However, by discovering the aforementioned correlation, the system can proactively learn and recommend this missing attachment to domain experts for verification. Correlations may also exist among the annotations themselves, e.g., between the ^Black Flag' and the ^Red Flag' an- notations. Without discovering such correlations the database may become ^under annotated' due to these missing attachments. Motivation Scenario 2?Annotation Maintenance under Evolv- ing Data: Data is always evolving and new records are always added to the database. Hence, a key question is: ^For the newly added data records, do any of the existing annotations apply to them?'. Learning the correlations between the data and the an- notations can certainly help in answering such question. For example, the cloud-shaped comment in Figure 1 is attached to all data records having value 101 in the Exp-Id column. Based on this correlation, the system can automatically predict!at inser- tion time!that this annotation also applies to the newly inserted JW0027 tuple. Otherwise, such attachment can be easily missed and important information is lost. Clearly, delegating such task to end-users without providing system-level support!which is the state of existing annotation management engines!is not a practical assumption. Motivation Scenario 3?Annotation-Driven Exploration: The discovered correlations may reveal information about the under- ling data that trigger further investigation or exploration by do- main experts. For example, as highlighted in Figure 1, the ^Red Flag' annotation semantically means invalid or incorrect data. Since these annotations can be added by different biologists and at different times, none of them may observe a pattern in the data. In contrast, by discovering (and reporting) that the ^Red Flag' an- notation has strong correlation with experiment id 105, the domain experts may re-visit the experimental setup of this wet-lab experiment and may revise and re-validate all data generated from it. These scenarios demonstrate the potential gain from capturing the annotation-related correlations. Unfortunately, relying on do- main experts or DB admins to manually define or capture these correlation patterns is evidently an infeasible approach. This is because the correlations may not be known in advance, hard to cap- ture or express, dynamically changing over time, or even not 100% conformed. Moreover, the manual exploration process is error- prone, will not scale to the size of modern annotated databases, and it is a very time- and resource-consuming process. For example, the UniProt biological database has over 150 people working as full- time to maintain and annotate the database [6, 12]. Certainly, such scale of investments may not be viable to many other domains and scientific groups, e.g., it is reported in a recent science survey [49] that 80.3% of the participant research groups do not have sufficient fund for proper data curation. For these reasons, we argue in this paper that the analysis and discovery of the annotation-related as- sociations and correlations should be an integral functionality of the annotation management engine. As a result, the correlations can be timely discovered and maintained up-to-date, and also sys- tematic actions can be taken based on them as highlighted by the motivation scenarios. In this paper, we investigate applying the well-known techniques of association rule mining, e.g., [8, 31, 52], to the domain of anno- tated databases. This is a new and promising domain for association rule mining due to the following reasons: ? Many emerging applications!especially scientific applications!maintain and rely on large-scale annotated databases [3, 5, 6]. It is reported in [1] that the ebird or- nithological database receives more than 1.6 million annotations per month from scientists and bird watchers worldwide. These applications will benefit from the proposed techniques. ? Many annotated databases go under the very expensive and time-consuming process of manual curation, e.g., [2, 6]. The goal from this process is to ensure that correct annotations and curation information are attached to the data, and to enrich the annotations whenever possible. Nevertheless as illustrated in the motivation scenarios, the discovery of the annotation-related correlations can help in enhancing the quality of the annotated database in an auto- mated way. And hence, reducing the effort needed in the manual curation process and freeing the domain scientists for their main task, which is scientific experimentation. ? Interestingly, annotated databases stretch the traditional tech- niques of association rule mining, and present new challenges as discussed in Section 3. For example, the state-of-art techniques in association rule mining fall short in efficiently handling several new cases specific to annotated databases, i.e., they cannot perform incremental maintenance of the discovered rules and they have to re-process the entire database. These cases include: (1) Generalization of Annotations: Annotations can be free-text comments, which may differ in their values but have the same se- mantics. And hence, discovering the correlations based on the values of the raw annotations may miss important patterns. For ex- ample, referring to Figure 1, the correlation pattern involving the Black Flag annotation, i.e., ^Family:F1 = Black Flag' can be detected based on the raw annotation value. This is because all instances of the Black Flag annotation refer to the same scien- tific article. In contrast, for the Red Flag annotation, the actual annotations inserted by scientists have different values, and thus no correlation pattern can detected based on the raw values. How- ever, by generalizing the annotations to a common concept!the Red Flag annotation in our case!we can detect the correlation pat- ten between them and the experiment Id 105. Therefore, building a generalization hierarchy on top of the annotations is an important step. (2) Integration with the Annotation Manager: We propose to build a coherent integration between the association rule mining module and the Annotation Manager component in contrast to the offline mining techniques. As a result, the Annotation Manager can take informed actions based on the discovered rules, e.g., discover potential missing attachments and report them for verification (Mo- tivation Scenario I), and annotate newly inserted data tuples with existing annotations (Motivation Scenario 2). Moreover, since the Annotation Manager cannot guarantee with 100% confidence that the predicted attachments are correct, we propose developing a ver- ification module that enables domain experts to verify the predicted attachments. (3) Incremental Maintenance under Annotation Addition: In an- notated databases, the discovered correlations and association rules need to be incrementally updated under two scenarios, i.e., the ad- dition of new data tuples, and the addition of new annotations. The former case can be handled by existing techniques that ad- dress the incremental update of association rules, e.g., [16]. These techniques assume that the new delta batch changes the size of the database, i.e., the number of data tuples increases. In contrast, in 504 the latter case, the new annotation batches will not change the num- ber of data tuples, instead they change the content of the tuples! assuming the annotations are part of the tuples. Therefore, the ex- isting incremental techniques need to be extended to handle the latter case. In this work, we develop an end-to-end solution that addresses the above challenges in the context of a real-world application and annotation repository, which is a data warehouse for the Caenorhabditis elegans (C. elegans) Worm from biological sci- ences. To facilitate scientists' usage of the developed system, we designed an Excel-based GUI!A tool that most scientists are fa- miliar with!through which all of the proposed functionalities can be performed. The rest of the paper is organized as follows. In Section 2, we present the needed background, preliminaries, and our case study. In Sections 3, and 4, we present the techniques for the discovery and maintenance of the annotation-related correlations, and their exploitation, respectively. Section 5 overviews the related work while Section 6 contains the experimental evaluation. Finally, the conclusion remarks are included in Section 7.",Xuebin He,"Worcester Polytechnic Institute, Computer Science Department, MA, USA",xhe2@cs.wpi.edu,Stephen Donohue,"Worcester Polytechnic Institute, Computer Science Department, MA, USA",donohues@cs.wpi.edu,Mohamed Y. Eltabakh,"Worcester Polytechnic Institute, Computer Science Department, MA, USA",meltabakh@cs.wpi.edu,,,,,,,,,,,,,,,,,,,,,
20200215,1344,Yongming Luo,"Eindhoven University of Technology, The Netherlands",y.luo@tue.nl,,Efficient and scalable trie-based algorithms for computing set containment relations,"Efficient and scalable trie-based algorithms for computing set containment relations, Efficient and scalable trie-based algorithms for computing set containment relations, Efficient and scalable trie-based algorithms for computing set containment relations, Efficient and scalable trie-based algorithms for computing set containment relations, Efficient and scalable trie-based algorithms for computing set containment relations, Abstract-Computing containment relations between massive collections of sets is a fundamental operation in data management, for example in graph analytics and data mining applications. Motivated by recent hardware trends, in this paper we present two novel solutions for computing secontainment joins over massive sets: the Patricia Trie-based Signature Join (PTSJ) and PRETTI+, a Patricia trie enhanced extension of the state-of-the- art PRETTI join. The compact trie structure not only enables efficient use of main-memory, but also significantly boosts the performance of both approaches. By carefully analyzing the algorithms and conducting extensive experiments with various synthetic and real-world datasets, we show that, in many practical cases, our algorithms are an order of magnitude faster than the state-of-the-art. I. INTRODUCTION Sets are ubiquitous in data processing and analytics. A fun- damental operation on massive collections of sets is computing containment relations. Indeed, bulk comparison of sets finds many practical applications in domains ranging from graph analytical tasks (e.g., [1]?[3]) and query optimization [4] to OLAP (e.g., [5], [6]) and data mining systems [7]. As a simple example, consider an online dating website where each user has an associated profile set listing their characteristics such as hobbies, interests, and so forth. User dating preferences are also indicated by a set of such charac- teristics. By executing a set-containment join of the set of user preferences with the set of user profiles, the dating website can determine all potential dating matches for users, pairing each preference set with all users whose profiles contain all desired characteristics. A concrete illustration can be found in Table I. In this paper we consider efficient and scalable solutions to the following formalization of this common problem. Consider two relations R and S, each having a set-valued attribute set. The set containment join of R and S (R 1 S) is defined as R 1 S = {(r, s) | r \forall R   s \forall S   r.set  s.set}. State of the art: Due to its fundamental nature, the theory and engineering of set containment joins have been intensively studied (e.g., [8]?[18]). Existing solutions fall into two general categories: signature-based and information- retrieval-based (IR) methods. Signature-based methods (e.g., [8]?[12]) encode set information into fixed-length bit strings (called signatures), and perform a containment check on the signatures as an initial filter followed by a validation of the TABLE I: Example of set-containment join. If we per- form a set-containment join (1) between user pro- files and user preferences, we retrieve matching pairs {(u1, p1), (u1, p2), (u2, p3)}. (a) user profiles id set signature u1 {b, d, f, g} 0111 u2 {a, c, h} 1011 u3 {a, c, d} 1011 (b) user preferences id set signature p1 {b, d} 0101 p2 {b, f, g} 0110 p3 {a, c, h} 1011 resulting pairs using actual set comparisons. IR-based methods (e.g., [13]?[16]) build inverted indexes upon sets storing tuple IDs in the inverted lists. A merge join between inverted lists will produce tuples that contain all such set elements. Typically auxiliary indexes are created to accelerate inverted index entry look-ups and joins. Most of the focus of the state-of-the-art algorithms has been on disk-based algorithms (e.g., [11]?[13], [15], [16]). Though these algorithms have proven quite effective for joining mas- sive set collections, the performance of these solutions is bounded by their underlying in-memory processing strategies, where less work has been done (see Section II). For example, PSJ [11] and APSJ [12], two advanced disk-based algorithms, share the same in-memory processing strategy with main- memory algorithm SHJ [8], which we'll discuss in detail in Section II-A. To keep up with ever-increasing data volumes and modern hardware trends we need to push the performance of set-containment join to the next level. Therefore, it is essential to revisit (and develop new) in-memory set-contain- ment join algorithms. Such algorithms will serve both as an essential component for main memory databases [19] as well as building blocks and inspiration for external memory and other computation models and platforms. This is challenging because existing work has already investigated many possi- ble optimization techniques, such as bitwise operations [8], caching [13], reusing result set [14] and so on. Contributions: Nonetheless, by carefully analyzing the existing solutions and bringing in new data structures, in this research we propose two novel in-memory set-containment join algorithms that are in many cases an order of magnitude faster than the previous state-of-the-art. In our study, we scale the relations to be joined along three basic dimensions: set cardinality, domain cardinality, and relation size. Here, set cardinality is the size of set values in the relations; domain cardinality is the size of the underlying domain from which set elements are chosen; and relation size is the number of tuples in each relation. The contributions of our study are as follows: ? We propose two novel algorithms for set-containment join. One is for the low set cardinality, high domain cardinality setting (PRETTI+); the other is for the remaining scenarios (PTSJ). Both algorithms make use of the compact Patricia trie data structure. ? Our PTSJ proposal is a signature-based method. Hence, the length of the signature is a critical param- eter for the algorithm's performance. Therefore, we perform a detailed analysis on PTSJ for determining the proper signature length. We also detail how PTSJ can (1) be easily extended to answer other set-oriented queries, such as set-similarity joins, and (2) efficiently be adapted to disk-based environment. ? We present the results of an extensive empirical study of our solutions on a variety of massive real-world and synthetic datasets which demonstrate that our algorithms in many cases perform an order of magni- tude faster than the previous state-of-the-art and scale well with relation size, set cardinality, and domain cardinality. The rest of the paper is organized as follows. In the next section, we introduce the state-of-the-art solutions for set- containment join. In Sections III and IV we propose PTSJ and PRETTI+, our two new algorithms. Section V presents the results of our empirical study of all algorithms. We then conclude in Section VI with a discussion of future directions for research.",Yongming Luo,"Eindhoven University of Technology, The Netherlands",y.luo@tue.nl,George H.L. Fletcher,"Eindhoven University of Technology, The Netherlands",g.h.l.fletcher@tue.nl,Jan Hidders,"Delft University of Technology, The Netherlands",a.j.h.hidders@tudelft.nl,Paul De Bra,"Eindhoven University of Technology, The Netherlands",debra@win.tue.nl,,,,,,,,,,,,,,,,,,
20200216,1354,Andy Diwen Zhu,School of Computer Engineering Nanyang Technological University Singapore,diwen.zhu@gmail.com,,Reachability Queries on Large Dynamic Graphs: A Total Order Approach,"Reachability Queries on Large Dynamic Graphs: A Total Order Approach, Reachability Queries on Large Dynamic Graphs: A Total Order Approach, Reachability Queries on Large Dynamic Graphs: A Total Order Approach, Reachability Queries on Large Dynamic Graphs: A Total Order Approach, Reachability Queries on Large Dynamic Graphs: A Total Order Approach, ABSTRACT Reachability queries are a fundamental type of queries on graphs that find important applications in numerous domains. Although a plethora of techniques have been proposed for reachability queries, most of them require that the input graph is static, i.e., they are inapplicable to the dynamic graphs (e.g., social networks and the Semantic Web) commonly encountered in practice. There exist a few techniques that can handle dynamic graphs, but none of them can scale to sizable graphs without significant loss of efficiency. To address this deficiency, this paper presents a novel study on reachability indices for large dynamic graphs. We first introduce a general indexing framework that summarizes a family of reachability in- dices with the best performance among the existing techniques for static graphs. Then, we propose general and efficient algorithms for handling vertex insertions and deletions under the proposed framework. In addition, we show that our update algorithms can be used to improve the existing reachability techniques on static graphs, and we also propose a new approach for constructing a reachability index from scratch under our framework. We experimentally evaluate our solution on a large set of benchmark datasets, and we demonstrate that our solution not only supports efficient updates on dynamic graphs, but also provides even better query performance than the state-of-the-art techniques for static graphs. Categories and Subject Descriptors G.2.2 [Graph Theory]: Graph Algorithms General Terms Algorithms, Experimentation 1. INTRODUCTION Given a directed graph G and two vertices s and t in G, a reachability query asks whether there exists a path from s to t in G. Reachability queries are a fundamental operation on graphs and have numerous important applications, such as query processing on social networks, the Semantic Web, XML documents, road networks, and program workflows. Devising index structures for reachability queries is non-trivial, as it requires a careful balanc- ing act between pre-computation cost, index size, and query pro- cessing overhead. In particular, if we pre-compute and store the reachability results for all pairs of vertices, then we can process any reachability query in O(1) time but suffer prohibitive costs of pre- processing and space. On the other hand, if we omit indexing and process reachability queries directly on G using depth-first search (DFS) or breadth-first search (BFS), then we minimize space and pre-computation overhead, but fail to ensure query efficiency on large graphs. Previous work [3?14,16,19,22?25,27?32] has proposed numer- ous indexing techniques to efficiently support reachability queries without significant space and pre-computation overheads. Most techniques, however, assume that the input graphG is static, which makes them inapplicable for the dynamic graphs commonly en- countered in practice. For example, the social graph of Twitter is constantly changing, with thousands of new users added per day; the Semantic Web is frequently updated with new concepts and relations; even road networks are subject to changes due to road closures and constructions. There exist a few techniques [4,12,13,16,22,24,32] that are designed for dynamic graphs, but as we discuss in Sections 3 and 8, none of those techniques can scale to sizable graphs without significant loss of efficiency. Specifically, the methods in [4,12,13,16,22,24] incur prohibitive preprocessing costs on graphs with more than one million vertices. Meanwhile, the approach in [32] can handle million-vertex graphs, but it offers a query performance that is generally not much better than a simple BFS approach, as shown in our experiments. In summary, no existing method is able to effectively handle reachability queries on large dynamic graphs. Motivated by this, we present a comprehensive study on scalable reachability indices that support updates. We first introduce a total order labeling (TOL) framework, which summarizes three most advanced meth- ods [8, 17, 30] for reachability queries on static graphs. TOL has two important properties: (i) every reachability index under TOL uniquely corresponds to a total order of vertices in the input graph, and (ii) the total order solely decides the index 's performances in terms of preprocessing, space, and queries. Given these properties, we investigate algorithms that enable us to insert or delete a vertex in a TOL index without changing the order of the other vertices, i.e., without significantly degrading the performance of the index. This results in general algorithms for handling insertions and dele- tions on indices under TOL. In particular, our insertion algorithm is optimal in that it leads to the minimum index size after insertion. Interestingly, we observe that our update algorithms can be utilized to reduce the space consumptions and query costs of a TOL index, by adjusting the total order pertinent to the index. This leads to a general approach for improving any index under TOL, includ- 1323 ing the state-of-the-art techniques [8, 17, 30]. The effectiveness of our adjusting approach shows that the total orders of the techniques in [8, 17, 30] leave much room for enhancement, which motivates us to devise new methods for deriving improved total orders for TOL indices. As a result, we present a new reachability index, But- terfly, which offers reduced preprocessing, space, and query costs than any existing indices under TOL [8,17,30]. We experimentally evaluate TOL using a large variety of benchmark datasets with up to twenty million vertices, and we demonstrate the superiority of TOL against alternative solutions for static and dynamic graphs. In summary, this paper makes the following contributions: ? We propose general and efficient algorithms that enable any index under the TOL framework to support large dynamic graphs (Section 5). ? We develop a technique that can postprocess the state-of-the- art reachability indices [8, 17, 30] to significantly enhance their performances in terms of space overheads and query efficiency (Section 6). ? We devise algorithms to derive improved vertex ordering un- der TOL, based on which we propose Butterfly, a new reach- ability index that dominates the states of the art (Section 7). ? We evaluate our solution on a large set of real and synthetic graphs, and we demonstrate that our solution not only supports efficient updates on large dynamic graphs, but also provides even better query performance than the state-of-the-art techniques for static graphs (Section 8).",Andy Diwen Zhu,School of Computer Engineering Nanyang Technological University Singapore,diwen.zhu@gmail.com,Wenqing Lin,School of Computer Engineering Nanyang Technological University Singapore,keamoulin@gmail.com,Sibo Wang,School of Computer Engineering Nanyang Technological University Singapore,wangsibo.victor@gmail.com,Xiaokui Xiao,School of Computer Engineering Nanyang Technological University Singapore,xkxiao@ntu.edu.sg,,,,,,,,,,,,,,,,,,
20200217,1355, Jeff LeFevre,"University of California, Santa Cruz",jlefevre@ics.uci.edu,,Opportunistic Physical Design for Big Data Analytics,"Opportunistic Physical Design for Big Data Analytics, Opportunistic Physical Design for Big Data Analytics, Opportunistic Physical Design for Big Data Analytics, Opportunistic Physical Design for Big Data Analytics, Opportunistic Physical Design for Big Data Analytics, ABSTRACT Big data analytical systems, such as MapReduce, perform aggressive materialization of intermediate job results in order to support fault tolerance. When jobs correspond to exploratory queries submitted by data analysts, these materializations yield a large set of materialized views that we propose to treat as an opportunistic physical design. We present a semantic model for UDFs that enables effective reuse of views containing UDFs along with a rewrite algorithm that provably finds the minimum-cost rewrite under certain assumptions. An experimental study on real-world datasets using our prototype based on Hive shows that our approach can result in dramatic performance improvements. 1. INTRODUCTION Data analysts have the crucial task of analyzing the ever increasing volume of data that modern organizations collect in order to produce actionable insights. As expected, this type of analysis on big data is highly exploratory in nature and involves an iterative process: the data analyst starts with an initial query over the data, examines the results, then reformulates the query and may even bring in additional data sources, and so on [9]. Typically, these queries involve sophis- ticated, domain-specific operations that are linked to the type of data and the purpose of the analysis, e.g., performing sentiment analysis over tweets or computing network influence. Because a query is often revised multiple times in this scenario, there can be significant overlap between queries. There is an opportunity to speed up these explo- rations by reusing previous query results either from the same analyst or from different analysts performing a related task. MapReduce (MR) has become a de-facto tool for this type of analysis. It offers scalability to large datasets, easy incorporation of new data sources, the ability to query right away without defining a schema up front, and extensibility through user-defined functions (UDFs). An- alyst queries are often written in a declarative query language, e.g., HiveQL or PigLatin, which are automatically translated to a set of MR jobs. Each MR job involves the materialization of intermediate results (the output of mappers, the input of reducers and the output of reducers) for the purpose of failure recovery. A typical Hive or Pig query will spawn a multi-stage job that will involve several such materializations. We refer to these execution artifacts as opportunistic materialized views. We propose to treat these views as an opportunistic physical de- sign and to use them to rewrite queries. The opportunistic nature of our technique has several nice properties: the materialized views are generated as a by-product of query execution, i.e., without additional overhead; the set of views is naturally tailored to the current work- load; and, given that large-scale analysis systems typically execute a large number of queries, it follows that there will be an equally large number of materialized views and hence a good chance of finding a good rewrite for a new query. Our results indicate the savings in query execution time can be dramatic: a rewrite can reduce execution time by up to an order of magnitude. Rewriting a query using views in the context of MR involves a unique combination of technical challenges that distinguish it from the traditional problem of query rewriting. First, the queries and views almost certainly contain UDFs, thus query rewriting requires some semantic understanding of UDFs. These MR UDFs for big data anal- ysis are composed of arbitrary user-code and may involve a sequence of MR jobs. Second, any query rewriting algorithm that can utilize UDFs now has to contend with a potentially large number of operators since any UDF can be included in the rewriting process. Third, there can be a large search space of views to consider for rewriting due to the large number of materialized views in the opportunistic physical design, since they are almost free to retain (storage permitting). Recent methods to reuse MR computations such as ReStore [6] and MRShare [21] lack any semantic understanding of execution artifacts and can only reuse/share cached results when execution plans are syn- tactically identical. We strongly believe that any truly effective so- lution will have to a incorporate a deeper semantic understanding of cached results and  ""look into "" the UDFs as well. Contributions. In this paper we present a novel query-rewrite algorithm that targets the scenario of opportunistic materialized views in an MR system with queries that contain UDFs. We propose a UDF model that has a limited semantic understanding of UDFs, yet enables effec- tive reuse of previous results. Our rewrite algorithm employs tech- niques inspired by spatial databases (specifically, nearest-neighbor searches in metric spaces [12]) in order to provide a cost-based in- cremental enumeration of the huge space of candidate rewrites, gen- erating the optimal rewrite in an efficient manner. Specifically, our contributions can be summarized as follows: ? A gray-box UDF model that is simple but expressive enough to capture a large class of MR UDFs that includes many common analysis tasks. The UDF model further provides a quick way to compute a lower-bound on the cost of a potential rewrite given just the query and view definitions. We provide the model and the types of UDFs it admits in Sections 3?4. ? A rewriting algorithm that uses the lower-bound to (a) gradually explode the space of rewrites as needed, and (b) only attempts a rewrite for those views with good potential to produce a low-cost rewrite. We show that the algorithm produces the optimal rewrite as well as finds this rewrite in a work-efficient manner, under certain assumptions. We describe this further in Sections 6?7. ? An experimental evaluation showing that our methods provide execution time improvements of up to an order of magnitude using real-world data and realistic complex queries containing UDFs. The execution time savings of our method are due to moving much less data and avoiding the high expense of re-reading data from raw logs when possible. We describe this further in Section 8.", Jeff LeFevre,"University of California, Santa Cruz",jlefevre@ics.uci.edu,Jagan Sankaranarayanan,"NEC Labs America, Cupertino, CA",alkis@ics.uci.edu,Hakan Hacg?m? s,"NEC Labs America, Cupertino, CA",jagan@ics.uci.edu,Junichi Tatemura,"NEC Labs America, Cupertino, CA",hakan@ics.uci.edu,Neoklis Polyzotis,"University of California, Santa Cruz",tatemura@ics.uci.edu,Michael J. Carey,"University of California, Irvine",mjcarey@ics.uci.edu,,,,,,,,,,,,
20200218,789,Cristian Consonni,University of Trento,cristian.consonni@unitn.it,,Discovering Order Dependencies through Order Compatibility,"Discovering Order Dependencies through Order Compatibility, Discovering Order Dependencies through Order Compatibility, Discovering Order Dependencies through Order Compatibility, Discovering Order Dependencies through Order Compatibility, Discovering Order Dependencies through Order Compatibility, ABSTRACT A relevant task in the exploration and understanding of large datasets is the discovery of hidden relationships in the data. In particular, functional dependencies have received considerable attention in the past. However, there are other kinds of relation- ships that are significant both for understanding the data and for performing query optimization. Order dependencies belong to this category. An order dependency states that if a table is ordered on a list of attributes, then it is also ordered on another list of attributes. The discovery of order dependencies has been only recently studied. In this paper, we propose a novel approach for discovering order dependencies in a given dataset. Our approach leverages the observation that discovering order dependencies can be guided by the discovery of a more specific form of de- pendencies called order compatibility dependencies. We show that our algorithm outperforms existing approaches on real datasets. Furthermore, our algorithm can be parallelized leading to further improvements when it is executed on multiple threads. We present several experiments that illustrate the effectiveness and efficiency of our proposal and discuss our findings. 1 INTRODUCTION In the big data era, the volume and complexity of available datasets has grown so much that data engineers are having a hard time interpreting the information contained in them. In such a reality, the ability to discover hidden dependencies in some auto- matic way is fundamental. Dependencies across different parts of the data play a significant role in query optimization, since redundant information may be ignored making the query evalu- ation faster. Furthermore, parts of the data may be replaced with others that are easier to manipulate, without affecting the final re- sult. Data profiling may help with data quality since it highlights constraints that may exist in the data but are not fully satisfied and have not been enforced when designing the database. Dependency discovery is not a new challenge. Functional and inclusion dependencies are the most common type of dependencies and have been studied extensively [14]. A functional dependency states that if two different data elements sharing a common structure have the same part A, then some other part B should also have the same value. An inclusion dependency states that the values of the data elements in some part A must be a subset of the values in a subpart B of some other portion of the dataset. An example of functional dependency can be seen in Table 1, that shows a relational table with data regarding yearly incomes, savings and taxes. Assume that the tax system is a progressive one that categorizes the different incomes into brackets, each of them characterized by a tax percentage. Thus, there is a functional dependency from the income amount to the tax brackets, i.e., income  bracket. Since for every income range the percentage is fixed, there are two other functional dependencies from the income to the tax amount and vice-versa, i.e., income   tax and tax   income. Using the transitive property of functional dependencies a new one can be inferred, i.e., tax  bracket. A closer look at Table 1 can illustrate another, stronger, form of dependency: as the income is increasing, bracket and tax amount are increasing as well. In other words, if we were to order the table based on the income column, each one of the bracket and the tax amount columns will also end up being ordered. This form of dependency is known as an order dependency and is typically noted with the ? symbol, i.e., income ? tax, which is read as: income orders tax. The knowledge encoded by order dependencies can be ap- plied to various tasks during the entire data life-cycle [3]: in the design phase, order dependencies can be exploited to assist schema design [21] or for selecting indexes [7]; if data are extracted from unstructured sources, order dependencies can aid knowledge discovery, to find hidden properties of the data; in the context of data profiling [13], data integration and cleansing [5], order dependencies can be used to describe a dataset; for data quality [8], order dependencies can be used as requirements or constraints [1]. The most important application of order dependencies is their use in the optimization of queries; in particular, they can be used to rewrite the ORDER BY clauses in SQL queries in ways similar to that of functional dependencies for the GROUP BY statements [17, 22]. Consider the following query:  Given that the order dependencies income? tax and income? bracket hold, the query optimizer can infer that sorting by income makes the ordering on the other two columns redun- dant, so the ORDER BY clause can be simplified to ORDER BY income. The concept of order dependency in the context of database systems first appeared under the name of point-wise order [9?11]. A point-wise ordering specifies that a set of columns orders an- other set of columns. In the example of Table 1, the point-wise order dependency income,tax ? bracket holds because if both of the tuples (income, tax) and (tax, income) are lexico- graphically ordered, then the column bracket is ordered in the same way. A new definition for order dependency was later in- troduced [21] to represent an order-preserving mapping between lists of attributes. In contrast to point-wise ordering, the new definition was distinguishing tuples with attributes in different order, thus having lists of attributes instead of sets. There are cases where two lists of attributes order each other when taken together. This property is known as order compati- bility and is denoted with the symbol. In Table 1, e.g., it holds that (income, savings)? (savings, income) and (savings, income)? (income, savings) and thus income  savings. Another way to see an order com- patibility dependency between two columns is that their values are bothmonotonically non-decreasing when they are considered pairwise. Dependencies are typically derived from design specifications, from the context of queries or from other known dependencies using inference rules. Discovering dependencies by analyzing the data is a process known as dependency discovery [14]. It conceptually requires to check for all potential dependencies if they hold in the database instance under examination, which may be time consuming. Thus, there is interest in developing strategies that limits the number of combinations to be checked. The task becomes even more challenging in the case of order dependencies, where the order of attributes matters, leading to a search space much larger than that of functional dependencies. In this work we study ways for efficiently discovering order dependencies. We follow a bottom-up approach in which we start by checking short lists of columns and progressively check longer and longer lists. In this process, once an order dependency between two lists of attributes is found not to hold, we prune the search space by ignoring larger lists that include them. In this way, many of the combinations that would have normally been checked are avoided. We advocate that this whole process can be significantly improved by framing the discovery of order dependencies in the context of order compatibility dependencies. This is based on a recently introduced theorem [21] that established that an order dependency holds if and only if a functional and an order com- patibility dependency hold between the two attribute lists of the order dependency. We illustrate in details how the order compat- ibility dependencies can be exploited to find order dependencies and propose a new algorithm for finding them. Recently, two algorithms to automatically detect order depen- dencies in relational data have been proposed: order, proposed by Langer and Naumann [13], and fastod proposed by Szlichta et al. [18]. order explores a lattice of order dependency candidates, in a level-wise fashion reminiscent of the tane algorithm [12]. After building a dependency candidate, order checks its valid- ity against the data and then it applies pruning rules to reduce the search space over the lattice. order has been shown to be incomplete [18], i.e. it does not find the complete set of order dependencies. In particular, this approach is unable to discover dependencies with repeated attributes, for example, the order dependency (income, savings) ? savings of Table 1 cannot be discovered. Dependencies of this form, however, may not be inferred from other dependencies and are useful in the case of queries that involve ordering with multi-column indexes. In the example of Table 1, an index over (income, savings) can be used to simplify the clause ORDER BY savings. fastod [18] is based on a different axiomatization of order dependencies that allows mapping dependencies between lists of attributes to dependen- cies between sets of attributes written in a canonical form. In this way, several order dependencies are mapped to the same set-based canonical form. fastod explores the space of order dependencies of this set-based canonical form, still retaining the ability to find a complete set of dependencies. While we have reproduced the results presented in the original work, we have found that an implementation error of the original work produces wrong results over simple datasets, this vitiates the validity of their results and the comparison with our approach. The approach we present in this paper is able to provide a complete set of dependencies that is based on the idea that the whole process of order dependency discovery could be performed through the search of order compatibility dependencies. While our approach has a higher worst-case complexity than fastod, it outperforms all the state-of-the-art approaches [13, 18] when tested over real datasets. In particular, our contributions are the following:  - we introduce a definition of minimality for a set of order compatibility dependencies that we show being complete in the sense that it can recover all valid order compatibility dependencies that hold over a given instance of relational data;  - we propose a novel algorithm for finding order depen- dencies that is complete and can perform the detection of order dependencies in parallel.  - we perform an exhaustive experimental evaluation that shows the performance of our algorithm in comparison with existing works, including a study of its scalability over big datasets and multiple threads.  - we discuss possible solutions for the discovery of the most important order dependencies in the case of dataset that could not be managed (too many columns) in a reasonable amount of time. The paper is structured as follows: in Section 2 we review the relevant definitions and theorems that formalize the connection between order dependencies and order compatibility dependen- cies. In Section 3 we prove that order dependency discovery can be guided by order compatibility dependencies without losing completeness. Our novel algorithm is presented in Section 4, while Section 5 contains the discussion of our experimental eval- uation. Finally, a thorough review of the related work can be found in Section 6 and we present our conclusions in Section 7.",Cristian Consonni,University of Trento,cristian.consonni@unitn.it,Paolo Sottovia,University of Trento,ps@disi.unitn.it,Alberto Montresor,University of Trento,alberto.montresor@unitn.it,Yannis Velegrakis,Utrecht University and University of Trento,velgias@disi.unitn.eu,,,,,,,,,,,,,,,,,,
20200219,830,Efi Karra Taniskidou,"University of California, Irvine",ekarrata@uci.edu,,Comparative Analysis of Content-based Personalized Microblog Recommendations [Experiments and Analysis],"Comparative Analysis of Content-based Personalized Microblog Recommendations [Experiments and Analysis], Comparative Analysis of Content-based Personalized Microblog Recommendations [Experiments and Analysis], Comparative Analysis of Content-based Personalized Microblog Recommendations [Experiments and Analysis], Comparative Analysis of Content-based Personalized Microblog Recommendations [Experiments and Analysis], Comparative Analysis of Content-based Personalized Microblog Recommendations [Experiments and Analysis], ABSTRACT Microblogging platforms constitute a popular means of real-time communication and information sharing. They involve such a large volume of user-generated content that their users suffer from an information deluge. To address it, numerous recom- mendation methods have been proposed to organize the posts a user receives according to her interests. The content-based methods typically build a text-based model for every individual user to capture her tastes and then rank the posts in her timeline according to their similarity with that model. Even though content-based methods have attracted lots of interest in the data management community, there is no comprehensive evaluation of the main factors that affect their performance. These are: (i) the representation model that converts an unstructured text into a structured representation that elucidates its characteristics, (ii) the source of the microblog posts that compose the user models, and (iii) the type of user 's posting activity. To cover this gap, we systematically examine the performance of 9 state-of-the-art representation models in combination with 13 representation sources and 3 user types over a large, real dataset from Twitter comprising 60 users. We also consider a wide range of 223 plausible configurations for the representation models in order to assess their robustness with respect to their internal parameters. To facilitate the interpretation of our experimental results, we introduce a novel taxonomy of representation models. Our analy- sis provides novel insights into the main factors determining the performance of content-based recommendation in microblogs. 1 INTRODUCTION Microblogging platforms enable the instant communication and interaction between people all over the world. They allow their users to post messages in real-time, often carelessly and ungrammatically, through any electronic device, be it a mobile phone or a personal computer. They also allow for explicit connections between users so as to facilitate the dissemination and consumption of information. These characteristics led to the explosive growth of platforms like Twitter (www.twitter.com), Plurk (www.plurk.com), Sina Weibo (www.weibo.com) and Ten- cent Weibo (http://t.qq.com). Their popularity has led to an information deluge: the number of messages that are transmitted on a daily basis on Twitter alone has jumped from 35 million tweets in 2010 to over 500 million in 2017 [29]. Inevitably, their users are constantly overwhelmed with information. As we also show in our experiments, this sit- uation cannot be ameliorated by presenting the new messages in chronological order; the relatedness with users ' interests is typically more important than the recency of a post. Equally ineffective is the list of trending topics, where the same messages are presented to all users, irrespective of their personal interests. A more principled solution to information deluge is offered by Personalized Microblog Recommendation (PMR). Its goal is to capture users ' preferences so as to direct their attention to the messages that better match their personal interests. A plethora of works actually focuses on Content-based PMR [6, 13, 15, 31, 40, 41, 45], which typically operates as follows: first, it builds a document model for every individual post in the training set by extracting features from its textual content. Then, it constructs a user model by assembling the document models that capture the user 's preferences. Subsequently, it compares the user model to the models of recommendation candidates (documents) with a similarity measure. The resulting similarity scores are used to rank all candidates in descending order, from the highest to the lowest score, thus placing the most relevant ones at the top positions. Finally, the ranked list is presented to the user. Content-based PMR is a popular problem that has attracted a lot of attention in the data management community [1, 15, 29?31, 55]. However, the experimental results presented in the plethora of relevant works are not directly comparable, due to the different configurations that are used for several important, yet overlooked parameters. The core parameter is the representation model that is used for converting a set of unstructured texts into a structured rep- resentation that reveals their characteristics. The available op- tions range from traditional vector space models [41, 65] to topic models [39, 50]. Also crucial is the representation source, i.e., the source of the microblog posts that compose user models. Common choices include the user 's tweets [36] together with their retweets [17, 23, 41, 56] as well as the posts of followers [31, 50, 65] and followees [15, 31, 39]. Another decisive factor is the posting activity of a user, i.e., whether she is an information producer or seeker [5, 35]. Other parameters include the novel challenges posed by the short, noisy, multilingual content of mi- croblogs as well as the external information that enriches their textual content, e.g., concepts extracted from Wikipedia [41] or the content of a Web page, whose URL is mentioned in a post [1]. Despite their significance, little effort has been allocated on assessing the impact of these parameters on Content-based PMR. To cover this gap, we perform a thorough experimental analysis that investigates the following questions:Which representation model is the most effective for recommending short, noisy, multilin- gual microblog posts? Which is the most efficient one? How robust is the performance of each model with respect to its configuration?  Which representation source yields the best performance? How does the behavior of individual users affect the performance of Contentbased MPR? We leave the investigation of external information as a future work, due to the high diversity of proposed approaches, which range from language-specific word embeddings like Glove [49] to self-reported profile information [21]. To investigate the above questions, we focus on Twitter, the most popular microblogging service worldwide, with over 335 million active users per month.1 We begin with a categorization of the representation sources and the users it involves, based on its special social graph: every user u1 is allowed to unilaterally follow another user u2, with u1 being a follower of u2, and u2 a followee foru1; ifu2 follows backu1, the two users are reciprocally connected. Then, we list the novel challenges posed by the short, noisy, user-generated tweets in comparison with the long and curated content of traditional documents. We also introduce a taxonomy of representation models that provides insights into their endogenous characteristics. Based on it, we briefly present nine state-of-the-art representation models and apply them to a dataset of 60 real Twitter users (partitioned into three different categories) in combination with 223 parameter configurations, three user types and 13 representation sources. Finally, we discuss the experimental outcomes in detail, interpreting the impact of every parameter on the performance of Content-based PMR. In short, we make the following contributions: ? We perform the first systematic study for content-based recommendation in microblogging platforms, covering nine rep- resentation models, 13 representation sources and three user types. We have publicly released our code along with guidelines for our datasets2. ?We organize the main representation models according to their functionality in a novel taxonomy with three main cate- gories and two subcategories. In this way, we facilitate the understanding of our experimental results, given that every (sub- )category exhibits different behavior. ?We examine numerous configurations for every representa- tion model, assessing their relative effectiveness, robustness and time efficiency. Our conclusions facilitate their fine-tuning and use in real recommender systems. The rest of the paper is structured as follows: Section 2 provides background knowledge on Twitter and formally defines the recommendation task we are tackling in this work. In Section 3, we present our taxonomy of representation models and describe the state-of-the-art models we consider. We present the setup of our experiments in Section 4 and their results in Section 5. Section 6 discusses relevant works, while Section 7 concludes the paper along with directions for future work.",Efi Karra Taniskidou,"University of California, Irvine",ekarrata@uci.edu,George Papadakis,University of Athens,gpapadis@di.uoa.gr,George Giannakopoulos,NCSR Demokritos,ggianna@iit.demokritos.gr,Manolis Koubarakis,University of Athens,koubarak@di.uoa.gr,,,,,,,,,,,,,,,,,,
20200220,116,Nilesh Dalvi,"Yahoo! Research 4301 Great America Parkway Santa Clara, CA 95054",ndalvi@yahoo-inc.com,,An Analysis of Structured Data on the Web,"An Analysis of Structured Data on the Web, An Analysis of Structured Data on the Web, An Analysis of Structured Data on the Web, An Analysis of Structured Data on the Web, An Analysis of Structured Data on the Web,  ABSTRACT In this paper, we analyze the nature and distribution of structured data on the Web. Web-scale information extraction, or the problem of creating structured tables using ex- traction from the entire web, is gathering lots of research interest. We perform a study to understand and quantify the value of Web-scale extraction, and how structured in- formation is distributed amongst top aggregator websites and tail sites for various interesting domains. We believe this is the first study of its kind, and gives us new insights for information extraction over the Web. General Terms Experimentation, Measurements Keywords Structured Data on the Web, Information Spread, Informa- tion Connectivity 1. INTRODUCTION One of the grand research challenges in the field of in- formation extraction (IE) is to develop effective techniques for Web-scale information extraction. Traditional IE techniques considered in the database community tend to be source-centric, i.e., they can only be deployed to extract from a specific website or data source. However, a range of domain-independent techniques have emerged recently [2, 4, 9, 10, 11, 16, 20] that seek to look at extraction holistically on the entire Web. Our own motivation for this work comes from our research goal of building a Web of concepts [7], where we want to extract, link, and organize entities across a large number of domains. There are some domain-independent efforts, e.g. WebTables [4, 10], that extract all simple tables and lists from the Web and store them as relational data. However, domain-independence makes it difficult to attach semantics to the extracted data. Furthermore, the amount of information that can be extracted is quite limited, since most of the information is outside of simple tables and lists. In contrast, we have advocated a domain-centric approach to the problem, where we want to extract all the entities and their attributes from the entire Web restricting to a specific do- main [7]. For instance, one might be interested in constructing a database of all restaurants, along with their contact information, hours of operation and set of reviews, by ex- tracting information from all the websites on the Web that belong to the restaurant domain. Similarly, one might be interested in constructing databases of all books and their reviews, artists and their discography, or product listings from merchants, and so on. There are several aspects to a complete solution for this end-to-end challenge, which include automatic crawling, clus- tering, extraction, deduplication and linking, all at the scale and diversity of the Web. Some of these problems have been well-studied, e.g. crawling shallow-web [5] and deep- web [18]. Others, like unsupervised site extraction [1, 6, 8, 15, 16, 17, 20], have received lots of attention recently, and are a topic of active research. In this paper, we consider a fundamental problem underlying all these techniques, which is discovering all the sources of structured information on the Web that contain entities in the domain of interest. Understanding the distribution of structured data is crucially important in order to evaluate the feasibility and efficacy of web-scale extraction. Although the problem of web-scale extraction is gathering interest, does the problem exist in reality? What if there exists a definitive source (or a small number of sources) that can be manually wrapped to construct complete databases for the domain of interest? Or, what if the unpopular tail entities that are not mentioned in definitive sources are so invaluable for users that it is not worth extra efforts to dis- cover or extract structured information about them? By using Yahoo! data available to us that includes a crawl of the Web, a database of structured business listings from Yahoo! Local, and Web search logs, we perform a first of a kind large-scale analysis that attempts to quantify the distribution of domain content on the Web for various domains (like books, restaurants, etc.) as well as the value of extracting structured information from the whole Web. We believe our study sheds several new insights to the problem of web-scale information extraction. We summarize our main conclusions here: 1. First, we observe that even for domains with wellestablished aggregator sites (e.g., yelp.com for restau- rants) we need to go to the long tail of websites to build a reasonably complete database of entities in the domain. This provides additional motivation for the problem of domain-centric information extraction. Many of the current efforts on web-scale extraction are domain-independent, in that they try to extract  ""everything from everywhere "", across all domains. In contrast, given our observations, we believe that the problem is challenging even when restricted to a specific domain on the Web. We quantify some of these challenges in this study. 2. Second, there is significant value in extraction for the long tail. While both the demand for entities and the availability of information about the entities decay as we move towards the tail, information availability decays at a much faster rate. This suggests there can be higher value in tail extraction in spite of the lower demand in tail entities. 3. Third, we observe that the aggregate content within a domain is well-connected, and there is a significant amount of content redundancy. While this finding is not very surprising, to the best of our knowledge, this is the first study that quantifies the connectivity and redundancy of structured data on the Web. We believe that structural redundancy within websites, content redundancy across websites, and entity-source connectivity together can be leveraged to develop effective techniques for domain-centric information extraction. Given that the general domain-independent extraction with really high precision and recall seems out of reach of the current state of the research, solving domain- centric extraction might be an important first-step in this direction.",Nilesh Dalvi,"Yahoo! Research 4301 Great America Parkway Santa Clara, CA 95054",ndalvi@yahoo-inc.com,Ashwin Machanavajjhala,"Yahoo! Research 4301 Great America Parkway Santa Clara, CA 95054",mvnak@yahoo-inc.com,Bo Pang,"Yahoo! Research 4301 Great America Parkway Santa Clara, CA 95054",bopang@yahoo-inc.com,,,,,,,,,,,,,,,,,,,,,
20200221,1356,Diego Calvanese,"Free Univ. of Bolzano-Bozen I-39100 Bolzano, Italy",calvanese@inf.unibz.it,,Query Processing under GLAV Mappings for Relational and Graph Databases,"Query Processing under GLAV Mappings for Relational and Graph Databases, Query Processing under GLAV Mappings for Relational and Graph Databases, Query Processing under GLAV Mappings for Relational and Graph Databases, Query Processing under GLAV Mappings for Relational and Graph Databases, Query Processing under GLAV Mappings for Relational and Graph Databases, ABSTRACT Schema mappings establish a correspondence between data stored in two databases, called source and target respec- tively. Query processing under schema mappings has been investigated extensively in the two cases where each target atom is mapped to a query over the source (called GAV, global-as-view), and where each source atom is mapped to a query over the target (called LAV, local-as-view). The general case, called GLAV, in which queries over the source are mapped to queries over the target, has attracted a lot of attention recently, especially for data exchange. However, query processing for GLAV mappings has been con- sidered only for the basic service of query answering, and mainly in the context of conjunctive queries (CQs) in relational databases. In this paper we study query processing for GLAV mappings in a wider sense, considering not only query answering, but also query rewriting, perfectness (the property of a rewriting to compute exactly the certain answers), and query containment relative to a mapping. We deal both with the relational case, and with graph databases, where the basic querying mechanism is that of regular path queries. Query answering in GLAV can be smoothly reduced to a combination of the LAV and GAV cases, and for CQs this reduction can be exploited also for the remaining query processing tasks. In contrast, as we show, GLAV query processing for graph databases is non-trivial and requires new insights and techniques. We obtain upper bounds for an- swering, rewriting, and perfectness, and show decidability of relative containment. 1. INTRODUCTION A schema mapping is a declarative, formal specification of how data structured according to a source schema relate to data conforming to a target schema. Schema mappings are at the basis of several data management scenarios. In data integration [44], they are established between the source database and a so-called mediated schema, i.e., the unified view to be presented to the clients for querying mul- tiple sources in a transparent way. In data exchange [10,43], schema mappings specify how the data at sources should be transformed and packed so as to transfer the desired in- formation from the sources to the target database. Model management [16], and schema and ontology matching [31] are other examples of scenarios where schema mappings play a central role. The interest of database research on schema mappings has been constantly growing in the last years, starting from the pioneering work in the context of data integration (see, e.g., [46, 52]). Such research work can be discussed under three coordinates: (i) the data model in which the schemas are expressed, (ii) the mapping-based tasks that have been studied, and (iii) the type of mechanisms used to express schema mappings. Data models. Most of the early research results on schema mappings refer to the relational model, where the basic classes of queries used to express schema mappings are positive fragments of first-order logic, such as conjunctive queries (CQs) and unions thereof. More recently, schema mappings have been studied also in the context of semistruc- tured data, both XML [10] and graph databases [5, 28]. Methods for extracting information from semistructured data incorporate special querying mechanisms that are not common in traditional database systems. Perhaps, the basic mechanism is that of regular-path queries (RPQs), which retrieve all pairs of nodes in the graph connected by a path conforming to a regular expression [6, 18]. Mapping-based tasks. Different operations and tasks concerning schema mappings have been studied in the literature. For example, in the context of data exchange, one of the basic tasks is the computation of a solution of a schema mapping M with respect to a given source database Ds, i.e., a target database Dt such that the pair (Ds, Dt) satisfies the specification M . In model management, schema map- ping operations such as composition, inversion, and merge have been investigated in detail, especially with the aim of formalizing the typical operations to be carried out in the context of schema evolution. Perhaps, the most interesting class of mapping-based tasks, and the one we concentrate on in this paper, is query processing under schema mapping. Generally speaking, query processing under schema mapping amounts to processing a query expressed over the tar- get schema based on both the data in the source database, and the mapping from the source to the target. There are two basic forms of query processing under schema mappings. The first one, originating with [3], is query-answering, where the goal is to compute the so-called certain answers to a query, i.e., the answers to the query in all target databases satisfying the schema mapping with respect to the source database. The second form, originating with [45], is query- rewriting, which is based on the idea of first using the mapping to reformulate the query in terms of the source alphabet, and then evaluating the resulting query, called the rewriting, over the source database. Note that, in order for the rewritten query to be evaluated at the source, it should be expressed in a specific query language, namely, the one suited for the query evaluation engine we can use for the source database. For this reason, in the rewriting approach, we fix in advance the language (or, the class of queries) used to express the rewriting, and target our rewriting technique toward such language. Obviously, in the case where differ- ent rewritings exist in the chosen class, we should also aim at computing the (or, a)  ""best "" one. Typically, in the litera- ture [41,44]  ""best "" has been intended as  ""maximal "", i.e., the rewriting ensuring the maximal set of answers among those in the class. This is in line with the idea of guaranteeing that the answers returned by the rewriting are themselves certain answers, though not all of them in general. Notice that also  ""minimally containing rewritings "" have been considered in the literature [7, 39], but not much used, especially in data integration and data exchange, since the returned answers are not guaranteed to be certain answers. The relationship between query rewriting and answering is discussed in [27], where it is observed that, apart from seeing query rewriting as a means to perform query answer- ing [45], also query answering can be seen under the lens of query rewriting: indeed, given a schema mapping and a source database, any algorithm for computing the certain answers to a target query based on such a mapping can be considered as a function of the source data, or, in other words, as a rewriting of the target query in terms of the source alphabet. Since such a rewriting computes exaclty the certain answers, it is often called the perfect rewriting. However, we have to take into account that, in general, the answering algorithm is a computable function, and it is ques- tionable whether we can express such a function in terms of a given query language. It is thus of interest, given a maximal rewriting in a class of queries, to be able to check whether such a rewriting is perfect, i.e., it always computes the certain answers [27]. So, perfectness is another relevant task in query processing under schema mappings. The problem of query containment is fundamental to many aspects of database systems, including query opti- mization, determining independence of queries from up- dates, and rewriting queries using views. However, as argued in [50], in data integration, the standard notion of query con- tainment does not suffice. This is why in [50] the authors define relative containment, which formalizes the notion of query containment relative to the sources available to the data-integration system. Relative contanment is indeed a further relevant task concerning schema mappings. Types of mappings. From a syntactic point of view, a schema mapping is constituted by a set of mapping asser- tions, each one specifying a relationship between the source and the target. In its most general form, an assertion specifies a correspondence between a view over the source and a view over the target. In the so-called sound schema map- pings, which are the ones we deal with in this paper, the correspondence is containment: the answers to the view on the source are a subset of the answers to the view on the target. Obviously, the expressive power of this mechanism depends on the expressivity of the query language used in the view definitions. Since the early work on schema map- pings in data integration [44], various restricted forms of such assertions have been considered, in particular GAV and LAV. The most obvious form of mappings are GAV (Global-As-Views) mappings, which associate with each ele- ment of the (global) target schema one view over the source schema, in a way analogous to how views are defined in standard databases. Indeed, processing queries under GAV mappings amounts essentially to substituting (global) view symbols with their definition in terms of the sources. With a surprising stance, [45] reversed this conception, by propos- ing to describe the content of sources in terms of queries over a (virtual) target schema, giving rise to so-called LAV (Local-As-Views) mappings. In this way, precise semantics is assigned to sources. The ability of easily processing LAV mappings for relational data gave new impetus to data integration in the   '90's, on the basis of virtualizing the target schema, while leaving the sources untouched. In the years, it became clear that a pure LAV approach, while compelling from a theoretical point of view, is not practically realizable, because schemas of legacy sources are typically too dirty for allowing a clean description in terms of a query over the target. This is one of the reasons why recent work on data integration and virtually all work on data exchange focuses on GLAV mappings, which generalize both GAV and LAV by mapping queries over the source schema to queries over the target schema. GLAV mappings overcome the limitations of LAV by allowing the use of queries over the sources to cleanse source data to be related to the target schema. The importance of GLAV mappings in modern information systems is also emphasized by the work on ontology- based data access [51], where the domain of interest is formalized by means of an ontology, while the data remain at the sources. In such a setting, the correct relationship be- tween the sources and the concepts in the ontology often can only be expressed by means of a view-to-view mechanism. Contribution. Most of the research on query processing under schema mapping in data integration has concentrated on GAV mappings, and on LAV mappings under the label of view-based query processing (see, for instance the survey in [41]). In data integration, GLAV mappings have been specifically considered in [19, 38, 42], but only for the case of relational databases, and only for query answering and query rewriting. GLAV schema mappings have been mainly investigated in data exchange: in [9,32,33,47] the emphasis is on providing foundations for data exchange systems based on schema mappings, whereas in [11?14, 34?36, 48] the goal is to study operators on schema mappings relevant to model management, notably, composition, merge, and inverse. For XML data, data exchange based on GLAV mappings has been studied in [8, 10]. In this paper we thoroughly investigate query processing under GLAV mappings. We start by reviewing and extend- ing the work on relational databases, where the basic querying mechanism used both in mappings and user queries are CQs. In this case, it is known that query answering can be done by splitting the GLAV mapping into a GAV map- ping followed by a LAV mapping over an intermediate alpha- bet, as shown in [38]. Using this observation, together with the possibility of doing query answering through maximal rewriting, allows us to easily reconstruct a complete picture of query processing under GLAV mappings for CQs in the relational case, including, apart from query answering and query rewriting, also perfectness and relative containment. We then turn to graph databases. Here, the basic querying mechanism is that of regular path queries (RPQs), which allow for a controlled use of recursion necessary to navigate graphs of unbounded size [2,22]. Notice that query processing in the presence of recursion has also been studied in the context of Datalog [15, 20, 29, 30]. However, when we consider these works in the setting of query processing under mappings, the severe restrictions on the use of recursion that are imposed to gain decidability, make them inapplicable to the case of RPQ mappings. Query processing under RPQ LAV and GAV mappings has been studied extensively [25,27,40]. However, while for CQs the results on LAV and GAV can be readily adapted to deal also with GLAV mappings, interestingly, for RPQs this is not the case. In this paper we obtain tight upper bounds for answering, rewriting, and perfectness, and show decid- ability of relative containment, but such results are nontrivial and require new insights and techniques. The paper is organized as follows. In Section 2 we present our framework for query processing under schema mappings, and illustrate the basic techniques developed for the case of LAV mappings. In Section 3, we show that for the relational case these results can be extended easily to GLAV CQ mappings. Then we present our main contributions, which con- sist in novel techniques for the case of graph databases for the various mappingbased tasks, namely query answering and rewriting (Section 4), perfectness (Section 5), and rel- ative containment (Section 6). Finally, Section 7 concludes the paper.",Diego Calvanese,"Free Univ. of Bolzano-Bozen I-39100 Bolzano, Italy",calvanese@inf.unibz.it,Giuseppe De Giacomo,"Maurizio Lenzerini Sapienza Universita` di Roma I-00185 Rome, Italy",lastname@dis.uniroma1.it,Moshe Y. Vardi,"Rice University Houston, TX 77005, U.S.A.",vardi@cs.rice.edu,,,,,,,,,,,,,,,,,,,,,
20200222,1357,Dan Lin,Computer Science Missouri University of Science & Technology,lindan@mst.edu,,A Moving-Object Index for Efficient Query Processing with Peer-Wise Location Privacy,"A Moving-Object Index for Efficient Query Processing with Peer-Wise Location Privacy, A Moving-Object Index for Efficient Query Processing with Peer-Wise Location Privacy, A Moving-Object Index for Efficient Query Processing with Peer-Wise Location Privacy, A Moving-Object Index for Efficient Query Processing with Peer-Wise Location Privacy, A Moving-Object Index for Efficient Query Processing with Peer-Wise Location Privacy, ABSTRACT With the growing use of location-based services, location privacy attracts increasing attention from users, industry, and the research community. While considerable effort has been devoted to invent- ing techniques that prevent service providers from knowing a user 's exact location, relatively little attention has been paid to enabling so-called peer-wise privacy athe protection of a user 's location from unauthorized peer users. This paper identifies an important efficiency problem in existing peer-privacy approaches that simply apply a filtering step to identify users that are located in a query range, but that do not want to disclose their location to the querying peer. To solve this problem, we propose a novel, privacy-policy enabled index called the PEB-tree that seamlessly integrates location prox- imity and policy compatibility. We propose efficient algorithms that use the PEB-tree for processing privacy-aware range and kNN queries. Extensive experiments suggest that the PEB-tree enables efficient query processing. 1. INTRODUCTION We are experiencing an increasing availability of location-based services such as AT&T 's TeleNav GPS Navigator, Sprint 's Family Locator, and Intel 's Thing Finder. A key obstacle to the broad adoption of location-based services is the lack of location privacy protection [2, 20, 30]. Specifically, in a setting where a service provider serves multiple users, a user may have privacy concerns with respect to the service provider as well as the other service users. As an example of the first case, a user may worry that the service provider will disclose the user 's locations (e.g., the user 's daily route) to malicious parties. We use provider-wise privacy for privacy in re- lation to the service provider. As an example of the second case, an employee may not want work colleagues to know his/her location during lunch if he/she is outside the company building. This type of access restriction may also prevent stalking or other personal security threats [24, 34]. We use peer-wise privacy for privacy in relation to peer users. Most research on location privacy thus far has been devoted to provider-wise privacy, and techniques such as spatial cloaking [10, 36], location distortion [18], and encryption [9] have been explored. In relation to peer-wise privacy, only a simple filtering approach has been employed. The setting of the filtering approach is one where users specify their privacy preferences using location privacy policies that capture who is allowed to see the location of who and under what conditions. To answer a peer-wise privacy-aware query, the filtering approach first finds users who satisfy the query 's location requirements in the same way as is done for privacy unaware locationbased queries, i.e., using existing moving object indexing and querying techniques. Only then it filters out users by inspecting their location privacy policies. For example, if a user issues a query for other nearby service users, the service provider not only needs to find nearby users; it also needs to check the privacy policies of the users found to en- sure that they are willing to disclose their locations to the querying user. When potential query results are found solely according to spatial proximity, which is well supported by existing indexing and query processing techniques, very large and unnecessary intermediate results may occur because the policy checking may eliminate most of the results. Section 3 further elaborates on the problem. This paper aims to provide an indexing technique and accompanying query processing algorithms that enable the efficient processing of peer-wise privacy aware queries that serve as the foundation for typical location-based services. Our proposed approach is orthogonal to existing approaches to supporting provider-wise privacy and can be integrated with these to achieve additional privacy. In particular, we propose the so-called Policy-Embedded Bx-tree (PEB-tree), which organizes objects based on both spatial proximity and privacy policy compatibility. The main idea is to generate an indexing key value for each object that encodes location as well as policy information. This way, objects spatially near each other and with compatible privacy policies are assigned similar keys and are placed near each other in the index. The PEB-tree is based on the widely implemented B+-tree, which promises easy integration into existing commercial database systems. Based on the PEB-tree, 37 we provide algorithms for processing privacy-aware range and k nearest neighbor (kNN) queries. The results of extensive empirical studies with the proposals sug- gest that the PEB-tree based algorithms outperform existing tech- niques considerably in terms of I/O cost. The rest of the paper is organized as follows. Section 2 reviews related work. Section 3 gives problem definitions, and Section 4 describes the existing approach used for comparison. Section 5 presents the proposed policy-embedded indexing techniques along with a cost analysis. Then Sections 6 and 7 cover cost modeling and empirical performance studies, respectively. Section 8 concludes and outlines future research directions.",Dan Lin,Computer Science Missouri University of Science & Technology,lindan@mst.edu,Christian S. Jensen,Computer Science Aarhus University,csj@cs.aau.dk,Rui Zhang,Computer Science & Software Engineering The University of Melbourne,rui@csse.unimelb.edu.au,Lu Xiao,Computer Science Missouri University of Science & Technology,lx787@mail.mst.edu,Jiaheng Lu,Key Laboratory of Data Engineering DEKE Renmin University of China,jiahenglu@ruc.edu.cn,,,,,,,,,,,,,,,
20200223,1355,Yubao Wu,"Department of Electrical Engineering and Computer Science, Case Western Reserve University",yubao.wu@case.edu,http://dx.doi.org/10.1145/2588555.2610500.,Fast and Unified Local Search for Random Walk Based K-Nearest-Neighbor Query in Large Graphs,"Fast and Unified Local Search for Random Walk Based K-Nearest-Neighbor Query in Large Graphs, Fast and Unified Local Search for Random Walk Based K-Nearest-Neighbor Query in Large Graphs, Fast and Unified Local Search for Random Walk Based K-Nearest-Neighbor Query in Large Graphs, Fast and Unified Local Search for Random Walk Based K-Nearest-Neighbor Query in Large Graphs, Fast and Unified Local Search for Random Walk Based K-Nearest-Neighbor Query in Large Graphs, ABSTRACT Given a large graph and a query node, finding its k-nearest- neighbor (kNN) is a fundamental problem. Various random walk based measures have been developed to measure the proximity (similarity) between nodes. Existing algorithms for the random walk based top-k proximity search can be categorized as global and local methods based on their search strategies. Global methods usually require an expensive precomputing step. By only searching the nodes near the query node, local methods have the potential to support more ef- ficient query. However, most existing local search methods cannot guarantee the exactness of the solution. Moreover, they are usually designed for specific proximity measures. Can we devise an efficient local search method that applies to different measures and also guarantees result exactness? In this paper, we present FLoS (Fast Local Search), a unified local search method for efficient and exact top-k proximity query in large graphs. FLoS is based on the no local optimum property of proximity measures. We show that many measures have no local optimum. Utilizing this property, we introduce several simple operations on transition probabili- ties, which allow developing lower and upper bounds on the proximity. The bounds monotonically converge to the exact proximity when more nodes are visited. We further show that FLoS can also be applied to measures having local optimum by utilizing relationship among different measures. We perform comprehensive experiments to evaluate the ef- ficiency and applicability of the proposed method. Categories and Subject Descriptors H.2.8 [Database Management]: Database Application- s aData mining ; G.2.2 [Discrete Mathematics]: Graph Theory aGraph algorithms Keywords Local search; nearest neighbors; top-k search; random walk; proximity search  1. INTRODUCTION Given a large graph and a query node, finding its k- nearest-neighbor (kNN) is a primitive operation that has re- cently attracted intensive research interests in the database research community [18, 5, 13, 9]. In general, there are two challenges in this problem. One is to design proximity measures that can effectively capture the similarity between nodes. Another is to devise efficient algorithms to compute the top-k nodes for a given measure. Designing effective proximity (similarity) measures is a difficult task. Random walk based measures have been shown to be effective in many applications. Some examples include discounted/truncated hitting time [17, 18], penalized hitting probability [11, 21], and random walk with restart [1, 20, 3]. Although various proximity measures have been developed, how to efficiently compute them remains a challenging problem. For most random walk based measures, the naive method requires matrix inversion, which is prohibitive for large graphs. Two global approaches have been developed. One applies the power iteration method over the entire graph [16, 9]. Another approach precomputes and stores the inversion of a matrix [20, 8, 10]. The precomputing step is usually expensive and needs to be repeated whenever the graph changes. To improve efficiency, local methods dynamically expand the search range to visit nodes near the query node [17, 19, 18, 21]. Node proximities are estimated based on lo- cal information only. Without using the global information, however, most of the existing local search methods cannot guarantee to find the exact solution. Moreover, they are designed for specific measures and cannot be generalized to other measures. In this paper, we propose FLoS (Fast Local Search), a simple and unified local search method for efficient and exact top-k proximity query in large graphs. FLoS has several desirable properties. ? It guarantees to find the exact top-k nodes. ? It is a general method that can be applied to a variety of random walk based proximity measures. ? It uses a simple local search strategy that needs neither preprocessing nor iterating over the entire graph. The key idea behind FLoS is that we can develop up- per and lower bounds on the proximity of the nodes near the query node. These bounds can be dynamically updated when a larger portion of the graph is explored and will finally converge to the exact proximity value. The top-k nodes can 1139 be identified once the differences between their upper and lower bounds are small enough to distinguish them from the remaining nodes. The theoretical basis of FLoS relies on the no local op- timum property of proximity measures. That is, given a query node q, for any node i (i 6= q) in the graph, i always has a neighbor that is closer to q than i is. We show that many measures have no local optimum. This property en- sures that the proximity of unvisited nodes is bounded by the maximum proximity (or minimum proximity for some measures) in the boundary of the visited nodes. It can be utilized to find the top-k nodes without exploring the entire graph under the assumption that the exact proximity can be computed based on local information. However, for most measures, the exact proximity cannot be computed without searching the entire graph. To tackle this challenge, we in- troduce several simple operations to modify transition prob- abilities, which enable developing upper and lower bounds on the proximity of visited nodes. The developed upper (lower) bounds monotonically decrease (increase) when more nodes are visited. We further study the relationship among different measures and show that FLoS can also be applied to measures having local optimum. Extensive experimen- tal results show that, for a variety of measures, FLoS can dramatically improve the efficiency compared to the state- of-the-art methods.",Yubao Wu,"Department of Electrical Engineering and Computer Science, Case Western Reserve University",yubao.wu@case.edu, Ruoming Jin,"Department of Computer Science, Kent State University",jin@cs.kent.edu,Xiang Zhang,"Department of Electrical Engineering and Computer Science, Case Western Reserve University",xiang.zhang@case.edu,,,,,,,,,,,,,,,,,,,,,
20200224,731,Arijit Khan,"Dept. of Computer Science University of California Santa Barbara, CA 93106",arijitkhan@cs.ucsb.edu,,Neighborhood Based Fast Graph Search in Large Networks,"Neighborhood Based Fast Graph Search in Large Networks, Neighborhood Based Fast Graph Search in Large Networks, Neighborhood Based Fast Graph Search in Large Networks, Neighborhood Based Fast Graph Search in Large Networks, Neighborhood Based Fast Graph Search in Large Networks, ABSTRACT Complex social and information network search becomes impor- tant with a variety of applications. In the core of these applications, lies a common and critical problem: Given a labeled network and a query graph, how to efficiently search the query graph in the target network. The presence of noise and the incomplete knowledge about the structure and content of the target network make it unre- alistic to find an exact match. Rather, it is more appealing to find the top-k approximate matches. In this paper, we propose a neighborhood-based similarity measure that could avoid costly graph isomorphism and edit distance computation. Under this new measure, we prove that subgraph similarity search is NP hard, while graph similarity match is polynomial. By studying the principles behind this measure, we found an information propagation model that is able to convert a large net- work into a set of multidimensional vectors, where sophisticated indexing and similarity search algorithms are available. The proposed method, called Ness (Neighborhood Based Similarity Search), is appropriate for graphs with low automorphism and high noise, which are common in many social and information networks. Ness is not only efficient, but also robust against structural noise and in- formation loss. Empirical results show that it can quickly and accu- rately find high-quality matches in large networks, with negligible cost. Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process; I.2.8 [Problem Solving, Control Methods, and Search]: Graph and tree search strategies General Terms Algorithms, Performance Keywords Graph Query, Graph Search, Graph Alignment, RDF 1. INTRODUCTION Recent advances in social and information science have shown that linked data pervade our society and the natural world around us [36]. Graphs become increasingly important to represent com- plicated structures and schema-less data such as wikipedia, free- base [15] and various social networks. Given an attributed network and a small query graph, how to efficiently search the query graph in the target network is a critical task for many graph applications. It has been extensively studied in chemi-informatics, bioinformatics, XML and Semantic Web. SPARQL [27] is the state-of-the RDF query language for Semantic Web. SPARQL requires accurate knowledge about the graph structure to write a query and also it performs an exact graph pattern matching. However, due to the noise and the incomplete information (structure and content) in many networks, it is not realistic to find exact matches for a given query. It is more appealing to find the top-k approximate matches. Unfortunately, graph similarity measures such as subgraph iso- morphism, maximum common subgraphs, graph edit distance, miss- ing edges that are appropriate for chemical structures and biolog- ical networks, are not suitable for entity-relationship graphs and social networks. There are two challenging issues for these graph theoretic measures. First, entity-relationship graphs and social networks have quite different characteristics from physical networks. They are not governed by physical laws and often full of noise, thus making strict topological similarity examination nearly impossible. How the entities are connected in these networks are not as im- portant as how closely these entities are connected. Second, these graphs are very large and complex with a lot of attributes associ- ated. If accuracy is to be ensured, the algorithms developed for edit distance and missing edges are not scalable. These two is- sues motivate us to invent new graph similarity measures that are less sensitive to structure changes, and have scalable indexing and search solutions. Figure 1(a) shows a graph query to ""Find the athlete who is from ""Romania' and won ""gold' in ""3000m' and ""bronze' in ""1500m' both in ""1984' olympics.1."" Compare this query against a possi- ble match in FreeBase (Olympics) shown in Figure 1(b), it is ob- served that these two graphs are by no means similar under tra- ditional graph similarity definitions. Graph edit distance between these two graphs is 7. The size of their maximum common graph is 3. The number of maximum missing edges for the query graph is 4. However, ""Maricica Purca"" in Figure 1(b) is a good match for the query shown in Figure 1(a), because she has all these attributes quite close to her in Figure 1(b). In practice, it is hard to come up with a query that exactly conforms with the graph structures in the target network due to the lack of schemas in linked data. However, it is easy to write a query like Figure 1(a), where a user connects entities with possible links. As long as the the proximity between these entities is approximately maintained in a query graph, the system shall be able to deliver matches like Figure 1(b). The above approximate query form can serve as a primitive for many advanced graph operators such as RDF query answering, net- work alignment, subgraph similarity search, name disambiguation and database schema matching. For example, based on partial in- formation related to one person, e.g. his friends, one can align his physical social circle with his cyber social network on Facebook. In many cases, nodes in social or information networks have incomplete information or even anonymized information. Nevertheless, the partial neighborhood information available from a query graph will be helpful to identify entities in the target network. Clearly, there is a need to adopt approximate similarity search techniques to solve the above problem. In bioinformatics, approximate graph alignment has been extensively studied, e.g. PathBlast [21], Saga [33]. These studies resort to strict approximation defini- tion such as graph edit distance, whose optimal solution is expen- sive to compute. Since they are targeting a relatively small biolog- ical networks with less than 10k nodes, it is difficult to apply them in social and information networks with thousands or even millions of nodes. As illustrated in NetAlign [23], in order to handle large graphs with 10k nodes, one has to sacrifice accuracy to achieve bet- ter query response time. Recently there have been other studies on approximate matching with large graphs, i.e., TALE [34], SIGMA [24] and G-Ray [35]. However, both TALE and SIGMA consider the number of missing edges as the qualitative measure of approxi- mate matching and hence, the techniques cannot capture the notion of proximity among labels, as shown in Figure 1. G-Ray, on the other hand, tries to maintain the shape of the query by allowing some approximation in the match. Unfortunately, shape is not an important factor in entity-relationship graphs. In this paper, we introduce a novel neighborhood-based similarity measure by vectorizing nodes according to the label distribution of their neighbors. We further extend the similarity notion to graph by finding the embeddings in the target graph that maximize the sum of node matches. This graph matching technique avoids complicated subgraph isomorphism and graph edit distance calculation, which becomes infeasible for large graphs. It is observed that so- cial/information networks usually have more diversified node labels and therefore less auto-isomorphic structure, but may contain more noise. Our objective function can provide better similarity semantics for graphs with various random noise. It simplifies the procedure of graph matching, leading to the development of an efficient graph search framework, called Ness (Neighborhood Based Similarity Search). With the introduction of scalable indices built on vectorized nodes and an intelligent query optimization tech- nique, Ness can quickly and accurately find high-quality matches in large networks, with negligible time cost. Our contributions. We propose a novel similarity search problem in graphs, neighborhood-based similarity search, which com- bines the topological structure and content information together during the search process. The similarity definition proposed in this work is able to avoid expensive isomorphism testing as much as possible. The principles to derive appropriate functions to fit this definition are carefully examined. We found that the information propagation model satisfies these principles, where each node prop- agates a certain fraction of its labels to its neighbors, and thereby we could convert each node into a multidimensional vector, where sophisticated indexing and similarity search algorithms are avail- able. That is, we successfully turn a graph search problem into a high-dimension index problem. We first identify a set of rules to define approximate matches of nodes based on their neighborhood structure and labels. These rules are important since the query may not always have complete information about the exact neighborhood structure in the target graph. The approximate node match concept is further extended to subgraph similarity search, i.e. multiple node alignment for a given query graph. We prove that under this measure, subgraph similarity search is NP hard. However, in comparison with graph isomorphism, which is neither known to be solvable in polynomial time nor NP-hard, graph similarity match is proved to be polyno- mial. We demonstrate that, without performing subgraph isomor- phism testing, it is possible to prune unpromising nodes by itera- tively propagating node information among a shrinking candidate set, which significantly reduces query execution time. We further analyze how to index the vector structure as well as optimize query processing to speed up similarity search. The infor- mation propagation model and the neighborhood vectorization approach keep the index structure much simpler than the graph itself, thus making it easy to be updated dynamically for graph changes arising from node/edge insertion and deletion. In summary, we propose a completely new graph similarity search framework, Ness, to define and determine approximate matches in massive graphs. As tested in real and synthetic networks, Ness is able to find high-quality matches efficiently in large scale networks.",Arijit Khan,"Dept. of Computer Science University of California Santa Barbara, CA 93106",arijitkhan@cs.ucsb.edu,Nan Li,"Dept. of Computer Science University of California Santa Barbara, CA 93106",nanli@cs.ucsb.edu,Xifeng Yan,"Dept. of Computer Science University of California Santa Barbara, CA 93106",xyan@cs.ucsb.edu,Ziyu Guan,"Dept. of Computer Science University of California Santa Barbara, CA 93106",ziyuguan@cs.ucsb.edu,Supriyo Chakraborty,"Dept. of Electrical Engineering University of California Los Angeles, CA 90095",supriyo@ee.ucla.edu,Shu Tao,"IBM T. J. Watson 19 Skyline Drive Hawthorne, NY 10532",shutao@us.ibm.com,,,,,,,,,,,,
20200225,1358,Rishabh Singh,"MIT CSAIL, Cambridge, MA",rishabh@csail.mit.edu,,Learning Semantic String Transformations from Examples,"Learning Semantic String Transformations from Examples, Learning Semantic String Transformations from Examples, Learning Semantic String Transformations from Examples, Learning Semantic String Transformations from Examples, Learning Semantic String Transformations from Examples, ABSTRACT We address the problem of performing semantic transformations on strings, which may represent a variety of data types (or their combination) such as a column in a relational table, time, date, currency, etc. Unlike syntactic transfor- mations, which are based on regular expressions and which interpret a string as a sequence of characters, semantic trans- formations additionally require exploiting the semantics of the data type represented by the string, which may be encoded as a database of relational tables. Manually performing such transformations on a large collection of strings is error prone and cumbersome, while programmatic solutions are beyond the skill-set of end-users. We present a programming by example technology that allows end-users to automate such repetitive tasks. We describe an expressive transformation language for semantic manipulation that combines table lookup operations and syntactic manipulations. We then present a synthesis algorithm that can learn all transformations in the language that are consistent with the user-provided set of input-output examples. We have implemented this technology as an add-in for the Microsoft Excel Spreadsheet system and have evaluated it successfully over several benchmarks picked from various Excel help-forums. 1. INTRODUCTION The IT revolution over the past few decades has resulted in two significant advances: digitization of massive amounts of data and accessibility of computational devices to massive proportions of the population. It is thus not surprising that more than 500 million people worldwide use spreadsheets for storing and manipulating data. These business end-users have myriad diverse backgrounds and include commodity traders, graphic designers, chemists, human resource man- agers, finance pros, marketing managers, underwriters, com- pliance officers, and even mail room clerks!they are not professional programmers, but they need to create small, often one-off, applications to support business functions [6]. Unfortunately, the programming experience since incep- tion has mostly focused on serving the needs of a select class of few million skilled users. In particular, spreadsheet systems like Microsoft Excel allow sophisticated users to write macros using a rich inbuilt library of string and numerical functions, or to write arbitrary scripts using a variety of programming languages like Visual Basic, or .NET. Since end-users are not proficient in programming, they find it too difficult to write desired macros or scripts. The combination of the above-mentioned technical trends and lack of a satisfactory solution has led to a marketplace of hundreds of advertisement-driven help-forums1, some of which contain millions of posts from end-users soliciting help for scripts to manipulate data in their spreadsheets. The experts respond to these requests after some time. From an extensive case study of such spreadsheet help-forums, we observed the following two things. Semantic String Transformations: Several of the requested scripts/macros were for manipulating strings that need to be interpreted as more than a sequence of charac- ters, e.g., as a column entry from some relational table, or as some standard data type such as date, time, or currency. (See 2 for a motivating example.) In this paper, we de- scribe the systematic design of a semantic transformation language for manipulating such strings. Input-Output Examples based Interaction Model: End-users used input-output examples as the most common and natural way of expressing intent to experts on the other side of the help-forums. An expert provides a program/transformation that is consistent with those examples. If the end-user is not happy with the result of the program on any other new input in the spreadsheet, the interaction is repeated with an extended set of input-output examples. This is the natural interface that our tool provides to the end-user. We describe the systematic design of an (induc- tive) synthesis algorithm that can learn desired scripts in our transformation language from very few examples. We observe that most semantic transformations can be expressed as a combination of lookup transformations and syntactic transformations. We use this observation to present a systematic design of the transformation language for performing semantic string transformations. We first present an expressive language for lookup transformations and then extend it by adding syntactic transformations [8]. We also describe a systematic design of the synthesis algorithm for the semantic string transformation language, which can synthesize a set of semantic transformations that are consistent with the given set of input-output examples. We first describe a synthesis algorithm for the lookup trans- formation language Lt, and then extend it to a synthesis algorithm for the extension of Lt with syntactic transfor- mations. Experimental results on our benchmark examples show that our algorithm is scalable and can learn desired transformations from very few examples. This paper makes the following key contributions: ? We describe a lookup transformation language Lt and an inductive synthesis algorithm for it (4). ? We observe that most semantic transformations can be expressed as combination of lookup and syntactic transformations. We extend language Lt with syntactic trans- formations to obtain a very expressive language Lu and describe an inductive synthesis algorithm for it (5). ? We describe an experimental prototype of our system that has an attractive user-interface and is ready to be deployed. We present the evaluation of our prototype on a large collection of benchmarks obtained from help- forums, books, mailing lists and Excel product team (7). This paper is organized as follows. 3 establishes a com- mon framework for describing transformation languages and their inductive synthesis algorithms. 4 and 5 describe novel instantiations of this framework for a lookup trans- formation language and for its extension with a syntactic transformation language. 6 shows applications of the se- mantic transformation language (described in 5) for per- forming transformations on standard data types.",Rishabh Singh,"MIT CSAIL, Cambridge, MA",rishabh@csail.mit.edu,Sumit Gulwani,"Microsoft Research, Redmond, WA",sumitg@microsoft.com,,,,,,,,,,,,,,,,,,,,,,,,
20200226,1207,Shi Gao,"University of California, Los Angeles",gaoshi@cs.ucla.edu,,"RDF-TX: A Fast, User-Friendly System for Querying the History of RDF Knowledge Bases","RDF-TX: A Fast, User-Friendly System for Querying the History of RDF Knowledge Bases, RDF-TX: A Fast, User-Friendly System for Querying the History of RDF Knowledge Bases, RDF-TX: A Fast, User-Friendly System for Querying the History of RDF Knowledge Bases, RDF-TX: A Fast, User-Friendly System for Querying the History of RDF Knowledge Bases, RDF-TX: A Fast, User-Friendly System for Querying the History of RDF Knowledge Bases, ABSTRACT Knowledge bases that summarize web information in RDF triples deliver many benefits, including providing access to encyclopedic knowledge via SPARQL queries and end-user interfaces. As the real world evolves, the knowledge base is updated and the evolu- tion history of entities and their properties becomes of great interest to users. Thus, users need query tools of comparable power and usability to explore such evolution histories or flash-back to the past. An integrated system that supports user-friendly queries and efficient query evaluation on the history of knowledge bases is required. In this paper, we introduce (i) SPARQLT, a temporal ex- tension of SPARQL that expresses powerful structured queries on temporal RDF graphs, (ii) an efficient in-memory query engine that takes advantage of compressed multiversion B+ trees to achieve fast evaluation of SPARQLT queries, and (iii) a query optimizer that improves selectivity estimation of temporal queries and generates efficient join orders using the statistics of temporal RDF graphs. The performance and scalability of our system are validated by extensive experiments on real world datasets, which shows significant performance improvement comparing with other approaches. 1. INTRODUCTION Knowledge bases that summarize valuable information in the RDF format are rapidly growing in terms of scale and significance and playing a crucial role in many applications such as semantic search and question answering. The extraordinary success of crowdsourcing and text mining for knowledge discovery makes it easy to generate and update the information in the knowledge bases. In fact, large knowledge bases undergo frequent changes. Table 1 lists the statistics of Wikipedia Infobox edit history, which shows that updates are quite common in many properties: e.g., on average each value in the population property of the city pages is updated more than 7 times. This is not specific to Wikipedia, but also happens in other knowledge repositories. The management of historical information has emerged as a critical issue for knowledge bases. In fact, timestamping is an impor- tant part of the provenance information that is associated with each RDF triple in the knowledge base. The evolution history of knowledge bases captures and describes the change of real world entities and properties, and thus is of great interest to users. However, the size of the history is very large and the schema of knowledge base is also under evolution, which presents challenges in query language, query processing and indexing. As the RDF model for representing knowledge bases is gain- ing great popularity, the importance of managing and querying the evolution history of knowledge bases is also recognized. Gutier- rez et al. [17] extended the RDF model with time elements and several approaches [16, 29, 30, 32] have been proposed to support the queries on temporal RDF datasets. Most previous works em- ploy relational databases and RDF engines to store temporal RDF triples and rewrite temporal queries into SQL/SPARQL for eval- uation. The languages proposed in these works use an interval- based temporal model which leads to complex expressions for tem- poral queries, e.g., those requiring joins and coalescing [12, 33]. At the physical level, previous approaches exploit indexes such as tGrin [30] to accelerate the processing of simple temporal queries, but they do not explore the use of general temporal indices and query optimization techniques. This limits their scalability and per- formance on large knowledge bases and for complex queries. In this paper, we describe a vertically integrated system RDF- TX (RDF Temporal eXpress) that efficiently supports the data man- agement and query evaluation of large temporal RDF datasets while simplifying the temporal queries for SPARQL programmers and consequently, for end-user interfaces facilitating the expression of the same queries. To support the queries over the evolution his- tory of knowledge bases, we propose efficient storage and index schemes for temporal RDF triples using multiversion B+ tree [7] and implement a query engine which achieves fast query evaluation by taking advantage of comprehensive indices. We also build a query optimizer that generates efficient join orders using a cost- based model and the statistics of temporal RDF graphs. We propose a general and scalable solution for the problem of managing and querying massive temporal RDF data based on three main contributions: ? We propose SPARQLT , a temporal extension of the struc- tured query language SPARQL based on a point-based tem- poral model which simplifies the expression of temporal joins and eliminates the need for temporal coalescing. This ap- proach makes possible end-user interfaces, such as those in [6, 15], where queries are entered via simple by-example condi- tions in the infoboxes of Wikipedia pages. ? We present an efficient main memory system RDF-TX for managing temporal RDF data and evaluating SPARQLTqueries. Our system uses multiversion B+ tree (MVBT) to store and index temporal RDF triples. An effective delta encoding scheme is introduced to reduce the storage overhead of in- dices. The algorithms on MVBT are extended and optimized to exploit the characteristics of the compression scheme and query patterns. Experimental evaluation demonstrates supe- rior performance and scalability of RDF-TX compared with other approaches. ? We implement a query optimizer that generates the efficient join orders of SPARQLT query patterns using the statistics of temporal RDF graphs. To manage temporal statistics, we in- troduce compressed Multi-Version SB Trees (MVSBT) that provide highly accurate estimation of statistics with a small storage overhead. The rest of this paper is organized as follows. Section 2 provides an overview of RDF-TX system and temporal RDF model. Then we present the SPARQLTquery language in Section 3. Section 4 describes our storage model and index compression techniques. The query evaluation techniques are discussed in Section 5. Section 6 introduces a query optimizer for join order optimization. We eval- uate our system on real world datasets in Section 7, and discuss related work in Section 8. Finally, we conclude in Section 9.",Shi Gao,"University of California, Los Angeles",gaoshi@cs.ucla.edu,Jiaqi Gu,,gujiaqi@cs.ucla.edu,Carlo Zaniolo,,zaniolo@cs.ucla.edu,,,,,,,,,,,,,,,,,,,,,
20200227,1359,Jongik Kim,"Chonbuk National University Jeonju, Republic of Korea",jongik@jbnu.ac.kr,,Inves: Incremental Partitioning-Based Verification for Graph Similarity Search,"Inves: Incremental Partitioning-Based Verification for Graph Similarity Search, Inves: Incremental Partitioning-Based Verification for Graph Similarity Search, Inves: Incremental Partitioning-Based Verification for Graph Similarity Search, Inves: Incremental Partitioning-Based Verification for Graph Similarity Search, Inves: Incremental Partitioning-Based Verification for Graph Similarity Search, ABSTRACT We study the problem of graph similarity search with a graph edit distance (GED) constraint. Existing solutions adopt a filtering- and-verification framework, with a focus on the filtering phase where a featurbased index is used to reduce the number of candidate graphs to be verified. These solutions suffer from a computationally expensive verification phase. In this paper, we develop a novel technique called Inves that can significantly reduce the time of verifying a candidate graph. Its main idea is to judiciously and incrementally partition a candidate graph based on the query graph, and use the results to compute a lower bound of their distance. If a full GED computation is needed, Inves utilizes the collected information, and uses novel methods and an A* algorithm to search in the space of possible vertex mappings between the graphs to compute their GED efficiently. A main advantage of Inves is that it can be adopted by a plethora of graph similarity search algorithms. Our extensive experiments on both real and synthetic datasets show that Inves can significantly improve the performance of existing techniques by an order of magnitude. 1 INTRODUCTION Graph data models are widely used in representing complex objects, such as chemical compounds, social networks, and biological structures. Graph search, which finds all occurrences of a query graph in a database of graphs, is a fundamental operation needed in many applications. To tolerate data inconsistency, natural noises, and different data representations in graph search, very often these applications require finding graphs similar to a given query graph. Various similarity measures have been proposed, such as maximum common subgraphs [2, 15], missing edges and features [23, 28], and graph alignment [17]. Among them, one of the commonly used metric is graph edit distance (GED) [5, 6, 22], which can capture the structural difference between graphs, and can be applied to many types of graphs [22, 24]. The GED between two graphs is the minimum number of graph edit operations to transform one to the other, where a graph edit operation is insertion, deletion, or substitution of a single vertex or edge. The problem of graph similarity search is to find graphs in a database whose GED to a query graph is within a given threshold. This problem is challenging because GED computation between two graphs is NP-hard [22]. Generally, a scan-based approach that directly computes the GED between each data graph and the query graph is computationally prohibitive. Many existing solutions adopt a filtering-and-verification framework. An index structure is typically used to generate candidate graphs in the filtering phase, and each candidate is compared with the query graph to find if it is a true match in the verification phase. Existing studies mainly focus on developing a feature-based in- dex to generate candidates in the filtering phase. For example, c-star [22] and k-AT [19] extract tree-structured features from data graphs and build an inverted index on the extracted features. GSimSearch [25, 26] builds an inverted index on path-based fea- tures of graphs. Pars [24] and MLIndex [12] utilize partitions of graphs as features to be indexed. The performance of existing solutions can suffer from too many candidates and an expensive verification phase. Table 1 shows the performance of 100 queries using one of the index- based search algorithms, Pars [24], on an AIDS dataset containing 42,687 graphs (see Section 5 for details). In the table, we use the number of data graphs that have passed a primitive filter named the global label filter (refer to [25] and Section 3.5 for details of the filter). The number of candidates denotes those candidates that require full GED computations. For example, when the threshold \lamda = 5, only 15.5% of data graphs are filtered from the index-based filtering phase. Experiments on other solutions show similar behaviors. To solve this problem, in this paper we develop a novel verifi- cation technique, called Inves1. Given a set of candidate graphs generated from a filtering phase, the proposed technique can effectively reduce the time for verifying if the GED between each candidate graph and the query graph is within a given threshold. Its main idea is to judiciously and incrementally partition the candidate graph based on the query graph, and use the results to try to prune this pair. If a full GED computation is needed, Inves utilizes the collected information, and uses novel methods and an A* algorithm to search in the space of possible vertex mappings between the graphs to compute their GED efficiently. A main advantage of Inves is that it can be adopted by a plethora of graph similarity search algorithms. The following are our contributions: ? We propose Inves as a novel incremental partitioning-based verification technique. Given a candidate graph and a query graph with a GED threshold, Inves incrementally isolates sub- graphs of the candidate graph that cause mismatches with the query graph. If the number of isolated subgraphs is greater than 1 It stands for Incremental partitioning-based verification technique for graph edit similarity search.the threshold, Inves filters out this pair since their GED cannot be within the threshold. In Section 3, we present the details of the incremental partitioning-based verification framework. ? If the pair of graphs cannot be pruned using the generated subgraphs, Inves employs efficient methods based on a well- known A* algorithm for GED computation [14]. It also takes advantage of the partitioning results by first considering those vertices that cause edit errors. In this way, it can significantly reduce the search space of the A* algorithm, thus improve the performance of GED computation (Section 4). ? We conduct extensive experiments to evaluate Inves on both real and synthetic data sets (Section 5). The results show the benefits of the various optimization methods in the technique. In addition, by adopting Inves in existing index-based algorithms, we can significantly reduce the total running time by an order of magnitude. The rest of the paper is organized as follows: Section 2 provides preliminaries and reviews related work. Section 3 presents the proposed verification framework, and Section 4 provides our GED computation methods. Section 5 presents experimental results, and Section 6 concludes the paper.",Jongik Kim,"Chonbuk National University Jeonju, Republic of Korea",jongik@jbnu.ac.kr,Dong-Hoon Choi,"KISTI Daejeon, Republic of Korea",choid@kisti.re.kr,Chen Li,"University of California Irvine, CA",chenli@ics.uci.edu,,,,,,,,,,,,,,,,,,,,,
20200228,1275,Venu Satuluri,Dept. of Computer Science and Engineering The Ohio State University,satuluri@cse.ohio-state.edu,,Local Graph Sparsification for Scalable Clustering,"Local Graph Sparsification for Scalable Clustering, Local Graph Sparsification for Scalable Clustering, Local Graph Sparsification for Scalable Clustering, Local Graph Sparsification for Scalable Clustering, Local Graph Sparsification for Scalable Clustering,  ABSTRACT In this paper we look at how to sparsify a graph i.e. how to reduce the edgeset while keeping the nodes intact, so as to enable faster graph clustering without sacrificing qual- ity. The main idea behind our approach is to preferentially retain the edges that are likely to be part of the same clus- ter. We propose to rank edges using a simple similarity- based heuristic that we efficiently compute by comparing the minhash signatures of the nodes incident to the edge. For each node, we select the top few edges to be retained in the sparsified graph. Extensive empirical results on sev- eral real networks and using four state-of-the-art graph clus- tering and community discovery algorithms reveal that our proposed approach realizes excellent speedups (often in the range 10-50), with little or no deterioration in the quality of the resulting clusters. In fact, for at least two of the four clustering algorithms, our sparsification consistently enables higher clustering accuracies. Categories and Subject Descriptors G.2.2 [Graph Theory]: Graph Algorithms; I.5.3 [Pattern Recognition]: Clustering General Terms Algorithms,Performance Keywords Graph Clustering, Graph Sparsification, Minwise Hashing 1. INTRODUCTION Many real world problems can be modeled as complex interaction networks or graphs. Graph clustering is one of the fundamental primitives for analyzing graphs, with applications as diverse as protein complex detection, com- munity discovery in social networks, optimizations for key- word search [10] and many others. Intuitively, graph clustering seeks to organize the nodes of the graph into clus- ters such that the nodes belonging to the same cluster are more strongly connected to each other, compared to the nodes in the rest of the graph. A number of algorithms for graph clustering have been previously proposed, includ- ing spectral methods [34], weighted kernel k-means [11], lo- cal spectral methods [36], stochastic flow methods [38, 31], max-flow based methods [21], and Kernighan-Lin style ap- proaches [17]. However, extracting such structure in a scal- able fashion remains a major challenge [12]. Large-scale graphs are especially common in the WWW domain, with the rapid growth of many services such as Facebook, Twitter, Orkut and so on, which can be very naturally analyzed using a graph representation [19, 26]. A general purpose solution to scale various data mining algorithms that is often considered in the literature is that of sampling. The basic premise is that operating on a smaller scale version of the problem will realize approximately sim- ilar results to processing the entire dataset, at a fraction of the execution time. This premise has been exploited for var- ious machine learning problems and data mining problems in the past [28]. In a similar spirit, in this paper we attempt to reduce the size of a graph in order to scale up community discovery/graph clustering algorithms. However, we aim to retain all the nodes in the graph and only filter out the edges in the graph - in other words, sparsify the graph. Our goal is to sparsify the graph in such a way that the cluster struc- ture is retained or even enhanced in the sparsified graph. We are not proposing a new graph clustering algorithm; rather we aim to transform the graph in such a way that will help almost any existing graph clustering algorithm, in terms of speed and at little to no cost in accuracy. Our sparsified graphs can also help in visualizing the cluster structure of the original graph. An example illustrating the results that can be obtained using our proposed sparsification algorithm is shown in Fig- ure 1. The original graph (Figure 1(a)) consists of 30 nodes and 214 edges, with the vertices belonging to 3 clusters (in- dicated by the color coding). The result of sparsifying this graph using our proposed sparsification algorithm is shown in Figure 1(b). Note that the sparsified graph only contains 64 of the original 214 edges, and the cluster structure of the graph is also much clearer in this new sparsified graph. The main idea behind our sparsification algorithm is to preferentially retain the edges that are likely to be part of the same cluster. We use a similarity-based sparsification heuristic, according to which edges that connect nodes which have many common neighbors are retained. Using a global similarity threshold for selecting which edges to retain in a graph, however, is problematic since different clusters may have different densities, and using a global threshold may choose far many more edges in the denser clusters, discon- necting the less dense clusters. To overcome this problem, we propose a local sparsification algorithm which chooses a few top edges per node, and thereby avoids this problem by using a locally appropriate threshold for including the edges in the sparsified graph. Local sparsification also ensures that all nodes in the graph are covered, i.e. there is at least one edge incident on each node in the sparsified graph. Specifi- cally, we retain de edges for a node of degree d, where e is a parameter that helps control the overall sparsification ratio of the result. We analyze the characteristics of the resulting sparsified graph in terms of the degree distribution and the final sparsification ratio in terms of the parameter e. Since exact similarity computation between each pair of connected nodes is very expensive, we efficiently approxi- mate the similarity by hashing via minwise independent per- mutations [6] (minwise hashing). Estimating similarity for all pairs of connected nodes in the graph using minhashing is linear in the number of edges in the graph, allowing us to sparsify the graph very quickly. We evaluate the performance of our proposed local sparsification algorithm (named L-Spar) on several real networks such as Wikipedia, Twitter, Orkut, Flickr and also biological networks with ground truth. Our main evaluation method is to compare the quality of clusters obtained from the original graph with the quality of those obtained from the sparsified graph, as well as compare the respective execution times and balance of the resulting cluster sizes. In terms of baselines, we compare against random sampling [16, 2, 1], sampling based on the ForestFire model [22] and the global similarity sparsification (named G-Spar) that we initially propose. We examine the performances of the different sparsifications when coupled with different state-of-the-art clustering algo- rithms (we use Metis [17], Metis+MQI [21], MLR-MCL [31] and Graclus [11]). We summarize our key findings: ? For different networks and different clustering algo- rithms, L-Spar sparsification enables clustering that is, in terms of quality, comparable to the clustering result obtained from the original graph, despite con- taining far fewer edges (typically 10-20%), and signifi- cantly superior to the result obtained by clustering the baseline sparsifications. Indeed, clustering our sparsi- fied graph often improves upon the clustering obtained from the original graph - for example, the F-score of Metis improves by 50% when clustering the sparsified Wiki graph. Similarly, Metis+MQI outputs a giant core cluster containing 85% of the nodes when cluster- ing the original Flickr graph, while the clustering re- sult obtained using the sparsified graph is much more meaningful. ? Clustering the L-Spar sparsified graph as well as the sparsification itself are extremely fast. Metis for in- stance obtains a 52x speedup (including clustering and sparsification times) on theWiki graph.MLR-MCL sim- ilarly obtains speedups in the range of 20x. ? Examples of the kinds of edges that are retained and those that are discarded in the sparsification process confirm that the algorithm is able to discern noisier/weaker connections from the stronger/semantically closer connections. ? We also systematically vary the average degree and  ""mixing""-ness of the clusters of a synthetic graph generator [20] and find that our method becomes increas- ingly effective with higher average degrees as well as with higher mixing parameters.",Venu Satuluri,Dept. of Computer Science and Engineering The Ohio State University,satuluri@cse.ohio-state.edu,Srinivasan Parthasarathy,Dept. of Computer Science and Engineering The Ohio State University,srini@cse.ohio-state.edu,Yiye Ruan,Dept. of Computer Science and Engineering The Ohio State University,ruan@cse.ohio-state.edu,,,,,,,,,,,,,,,,,,,,,
20200301,1360,Michael Benedikt,"Oxford University, UK",michael.benedikt@cs.ox.ac.uk,,Querying Schemas With Access Restrictions,"Querying Schemas With Access Restrictions, Querying Schemas With Access Restrictions, Querying Schemas With Access Restrictions, Querying Schemas With Access Restrictions, Querying Schemas With Access Restrictions, ABSTRACT We study verification of systems whose transitions consist of accesses to a Web-based data-source. An access is a lookup on a relation within a relational database, fixing values for a set of positions in the relation. For example, a transition can represent access to a Web form, where the user is re- stricted to filling in values for a particular set of fields. We look at verifying properties of a schema describing the possi- ble accesses of such a system. We present a language where one can describe the properties of an access path, and also specify additional restrictions on accesses that are enforced by the schema. Our main property language, AccLTL, is based on a first-order extension of linear-time temporal logic, interpreting access paths as sequences of relational structures. We also present a lower-level automaton model, A- automata, which AccLTL specifications can compile into. We show that AccLTL and A-automata can express static analysis problems related to ""querying with limited access patterns"" that have been studied in the database literature in the past, such as whether an access is relevant to an- swering a query, and whether two queries are equivalent in the accessible data they can return. We prove decidability and complexity results for several restrictions and variants of AccLTL, and explain which properties of paths can be expressed in each restriction. 1. INTRODUCTION Many data sources do not expose either a bulk export facility or a query-based interface, enforcing instead many restrictions on the way data is accessed. For example, access to data may only be possible through Web forms, which require bindings for particular fields in the relation [16, 4]. Querying with limited access patterns also arises in other middleware contexts (e.g. federated access to data in Web services) as well as in construction of query interfaces on top of pre-determined indexed accesses [20]. For example, a Web telephone directory might allow several Web forms that serve as access methods to the underlying data. It may have an access method AcM1 accessing a relation Mobile#(name,postcode, street,phoneno), where AcM1 allows one to enter a mobile phone customer's name (the underlined field) and access the corresponding set of tuples containing a postal code, mobile phone number and street name. The same site might have an access method AcM2 on relation Address(street,postcode,name,houseno) allowing the user to enter a street name and postcode, returning all corresponding resident names and housenumbers. Formally an access method consists of a relation and a collection of input positions: for AcM1, position 1 is the sole input position, while for AcM2 the first two positions are input. An access consists of an access method plus a bind- ing for the input positions ? for example putting ""Smith"" into method AcM1 is an access. The response to an access is a collection of tuples for the relation that agree with the binding given in the access. A schema of this sort defines a collection of access paths: sequences consisting of accesses and their responses. The impact of ""limited access patterns"" has thus been the subject of much study in the past decade. It is known that in the presence of limited access patterns, there may be no ac- cess path that completely answers the query, and there may also be many quite distinct paths. For example, the query Address(X,Y, ""Jones"", Z) asking for the address of Jones is not answerable using the access methods AcM1 and AcM2 above. There are certainly many ways to obtain the max- imal answers: one could begin by obtaining all the street names and postcodes associated with Jones in the Mobile# table, entering these into the Address table to see if they match Jones, then taking all the new resident names we have discovered and repeating the process, until a fixedpoint is reached. If, however, Jones does not occur as a name in Mobile#, then this process will not yield Jones' tuple in Address. In general it is known [15] that for any conjunctive query one can construct (in linear time) a Datalog program that produces the maximal answers to a query under access patterns: the program simply tries all possible valid accesses on the database, as in the brute-force algorithm above. In the absence of a complete plan, how can we determine which strategy for making accesses is best? Recent works [4, 3] have proposed optimizing recursive plans, using access pattern analysis to determine that certain kinds of accesses can not extend to a useful path. An example is the work in [3] which proposes limiting the number of accesses to be explored by determining that some accesses are not ""relevant"" . An access is long term relevant if there is an access path that begins with the access and uncovers a new query result, where the removal of the access results in the new result not being discovered. [3] gives the complexity of determining relevance for a number of query languages. Long term relevance is only one property that can be used to measure the value of making a particular access ? for ex- ample we may want to know whether there is an access that reveals several values in the query result. Furthermore, ""lim- ited access patterns"" represent only one possible restriction that limits the possible access paths through a web interface. Many other restrictions may be enforced, e.g: ""Restrictions that follow from integrity constraints on the data: e.g. a mobile phone customer name will not (arguably) overlap with a street name. Thus in an it- erative process for answering the query given above, we should not bother to make accesses to the Mobile# table using street names we have acquired earlier in the process. It is also easy to see that key constraints, and more generally functional dependencies, can play a crucial role in determining whether an access is relevant. - Access order restrictions: e.g. before making any ac- cess to Mobile#, the interface may require a web user to have made at least one access to Address. - Dataflow restrictions; before performing an access to Mobile# on a name, the web user must have received that name as a response to a call to Address. Ideally, a query processor should be able to inspect an ac- cess and determine whether it is a good candidate for use, where the assumptions on the paths as well as the notion of ""good candidate"" could be specified on a per-application ba- sis. In this paper we look for a general solution to specifying and determining which accesses are promising: a language for querying the access paths that can occur in a schema. We show that every schema can be associated with a labelled transition system (LTS), with transitions for each access and nodes for each ""revealed instance"" (information known after a set of accesses). A fragment of the LTS for the schema with access methods AcM1 and AcM2 is given in Figure 1. Paths through the LTS represent possible access/response sequences of the Web-based datasource. There are infinitely many paths ? in fact every access could have many possible responses. But the access restrictions in the schema place limitations on what paths one can find in the LTS. We can then identify a ""query on access paths"" with a query over this transition system. This work will provide a language that allows the user to ask whether a given kind of path through instances of the schema is possible: e.g. is there a path that leads to an instance where a given conjunctive query holds, but where the path never uses access AcM1? Is there a path that satisfies a given set of additional dataflow, access order restrictions, or data integrity constraints? Paths are often queried with temporal logic [13]. We will look at natural variations of First-Order Linear Temporal Logic (FOLTL) for querying access paths. We look at a family of languages denoted AccLTL(L) (""Access LTL""), pa- rameterized by a fragment L of relational calculus. It has a two-tiered structure: at the top level are temporal opera- tors (""eventually"", ""until"") that describe navigation between transitions in a path. The second tier looks at a particular transition, where we have first-order (i.e. relational calcu- lus) queries that can ask whether the transitions satisfy a given property described in L. The relational vocabulary we consider for the ""lower tier"" will allow us to describe transi- tions given by accesses; it allows us to refer to the bindings of the access, the access method used, and the pre- and post-access versions of each schema relation. Consider the following AccLTL sentence: (?ivnivpivsivph Mobile#pre(n, p, s, ph)) U (ivn IsBindAcM1(n) ^ ivsivpivh Addresspre(s, p, n, h)) The relational query prior to the ""until"" symbol U states that there are no entries in Mobile#pre ? the Mobile# ta- ble prior to the access. The query after the until symbol U states that an access was done with method AcM1 and bind- ing n, where value n appeared in the Address table prior to the access. Hence this ""meta query"" returns the set of access paths which have no entries revealed in relation Mobile# un- til an access AC is performed, where AC has method AcM1 and uses a name that already exists in the Address table. In this work we will not be interested in returning all paths satisfying a query (there are generally infinitely many). We will check whether there is a path satisfying a given spec- ification. This is a question of satisfiability for our path query language. We may also want to check that every path through the system is of a certain form; this is the validity problem for the language ? bounds for validity will follow from our results on satisfiability. We denote the logic containing the above sentence by AccLTL(FOiv+Acc), where FOiv+Acc is the collection of positive existential queries over a signature consisting of: the access methods, bindings, and the preand post- access version of each relation used in a transition. AccLTL(FOiv+Acc) can express a wide variety of properties. Unfortunately we show that satisfiability for the logic is undecidable. How- ever, we show that a rich sublanguage of AccLTL(FOiv+Acc), denoted AccLTL+, has a decidable satisfiability problem. In AccLTL+ the formulas involving the bindings only occur positively. We give bounds on the complexity of this fragment, using a novel technique of reduction to contain- ment problems for Datalog. We then look at the exact complexity of smaller language fragments, and show that the complexity can go much lower ? e.g. within the polynomial hierarchy. The main thing we give up in these languages is the ability to express dataflow restrictions. We also study the complexity and expressiveness of extensions of the languages with inequalities and with branching time operators. In summary, our contributions are: - We present the first query language for reasoning about the possible paths of accesses and responses that may appear in a Web form or other limited-access data- source. - We show that combining a natural decidable logic for temporal data (LTL) with conjunctive queries gives an undecidable path query language. - We show that by restricting to queries that are ""bind- ing positive"", we get a decidable path query language. In the process we introduce a new automaton model that corresponds to a process repeatedly querying a Web data source. We show that analysis of these ""access automata"" can be performed via reduction to (decidable) Datalog containment problems. The au- tomaton and logic specification languages are powerful enough to express a rich set of data integrity con- straints, access order restrictions, and data flow restrictions. - We show that the complexity of the logic can be decreased drastically by restricting the ability to express properties of the bindings that occur in accesses. The resulting language can still express important access order and data integrity restrictions, but no dataflow restrictions. - We determine the impact adding inequalities to the re- lational query language, and of adding branching op- erators, both in terms of expressing critical properties of accesses and on complexity of verification. Organization: Section 2 gives the basic definitions related to access patterns, along with our family of languages AccLTL(L). Section 3 gives our results about the full language AccLTL(FOiv+Acc) while Section 4 deals with AccLTL+ and its restrictions. Section 5 discusses extensions of AccLTL+. Section 6 gives conclusions and overviews related work. Most proofs are deferred to the full paper.",Michael Benedikt,"Oxford University, UK",michael.benedikt@cs.ox.ac.uk,Pierre Bourhis,"Oxford University, UK",pierre.bourhis@cs.ox.ac.uk,Clemens Ley,"EPFL, Switzerland",ley.clemens@gmail.com,,,,,,,,,,,,,,,,,,,,,
20200302,1300,Willis Lang,University of Wisconsin,wlang@cs.wisc.edu,,Towards Energy-Efficient Database Cluster Design,"Towards Energy-Efficient Database Cluster Design, Towards Energy-Efficient Database Cluster Design, Towards Energy-Efficient Database Cluster Design, Towards Energy-Efficient Database Cluster Design, Towards Energy-Efficient Database Cluster Design, ABSTRACT Energy is a growing component of the operational cost for many  ""big data "" deployments, and hence has become increasingly im- portant for practitioners of large-scale data analysis who require scale-out clusters or parallel DBMS appliances. Although a number of recent studies have investigated the energy efficiency of DBMSs, none of these studies have looked at the architectural design space of energy-efficient parallel DBMS clusters. There are many challenges to increasing the energy efficiency of a DBMS cluster, including dealing with the inherent scaling inefficiency of parallel data pro- cessing, and choosing the appropriate energy-efficient hardware. In this paper, we experimentally examine and analyze a number of key parameters related to these challenges for designing energy-efficient database clusters. We explore the cluster design space using empirical results and propose a model that considers the key bottlenecks to energy efficiency in a parallel DBMS. This paper represents a key first step in designing energy-efficient database clusters, which is increasingly important given the trend toward parallel database appliances. 1. INTRODUCTION In recent years, energy efficiency has become an important database research topic since the cost of powering clusters is a big component of the total operational cost [13, 15, 19]. As  ""big data "" becomes the norm in various industries, the use of clusters to analyze ever- increasing volumes of data will continue to increase. In turn, this trend will drive up the need for designing energy-efficient data processing clusters. The focus of this paper is on designing such energy-efficient clusters for database analytic query processing. One important problem regarding the energy efficiency of database clusters surrounds the classical problem of non-linear scalability in parallel data processing [12]. Non-linear scalability refers to the inability of a parallel system to proportionally increase performance as the computational resources/nodes are increased. More recently, it was shown that modern parallel DBMSs are still subject to these scalability pitfalls that affect their performance and increase their total operating cost [25, 31].  1.1 Illustrative Experiments First, we present two results that show how varying the cluster size and changing the cluster design can allow us to trade performance for reduced energy consumption for a single query workload. In this paper, we studied the effect of this undesirable scalability phenomenon on the energy efficiency of parallel data processing clusters using a number of parallel DBMSs, including Vertica and HadoopDB. Our first result is shown in Figure 1(a) for TPC-H Q12 (at scale factor 1000) for Vertica. In this figure, we show the relative change, compared to a 16 node cluster, in the energy consumed by the cluster and the query performance1 as we decrease the cluster size two nodes at a time (please see Section 3.1 for more details). Next, we discuss three key insights that can be drawn from Figure 1(a), which also shape the theme of this paper. First, this result shows the classic sub-linear parallel speedup phenomenon; namely, given a fixed problem size, increasing the computing resources by a factor of X provides less than an X times increase in performance [12]. Or, conversely, decreasing the resources to 1/X , results in a relative performance greater than 1/X . We can observe this phenomenon in Figure 1(a) because 1 Here performance is the inverse of the query response time. reducing the cluster size from 16 nodes (16N) to eight nodes (8N), results in a performance ratio greater than 50%. Note that since performance is the inverse of the response time, a performance ratio greater than 50% at 8N means that the response time at 16N is more than half the response time at 8N (i.e., sub-linear speedup). Second, as shown by the solid curve in Figure 1(a), the total energy required to execute the query decreases as we reduce the cluster size from 16N, even though it takes longer to run the query. Recall that energy is a product of the average power drawn by the cluster and the response time of the query. Due to the sub-linear speedup, going from 16N to 8N reduces the performance by only 36%, but the average power drops by roughly half (since we have half the number of nodes). Consequently, the energy consumption ratio (relative to the 16N case) for fewer than 16 nodes is less than 1.0. This is an encouraging energy efficiency gain, albeit at the cost of increased query response time. Lastly, in Figure 1(a), the dotted line shows the line where the Energy Delay Product (EDP) is constant, relative to the 16N case. EDP is defined as energy  X delay (measured in Joules seconds), and is commonly used in the architecture community as a way of studying designs that trade-off energy for performance2. Here  ""energy "" refers to the query energy consumption and  ""delay "" refers to the query response time. A constant EDP means that we have traded x% of performance for an x% drop in energy consumption. Preferably, it would be nice to identify design points that lie below the EDP curve, as such points represent trading proportionally less performance for greater energy savings. In Figure 1(a), all the actual data/design points (on the solid line) are above the EDP curve. In other words, as we reduce the cluster size from 16 nodes to 8 nodes, we are giving up proportionately more performance than we are gaining in energy efficiency. For example, the 10 node configuration (10N) pays a 24% penalty in performance for a 16% decrease in energy consumption over the 16N case. Such trade-offs may or may not be reasonable depending on the tolerance for performance penalties, but the EDP curve makes it clear in which directions the trade-offs are skewed. This observation motivates the key question that is addressed in this paper: What are the key factors to consider when we design an energy-efficient DBMS cluster so that we can favorably trade less performance for more energy savings (i.e., lie below the EDP curve)? To understand the reasons why our observed data points lie above the EDP curve in Figure 1(a), and to carefully study the conditions that can produce design points that lie below the EDP curve, we built and modeled a custom parallel data execution kernel called P-store. The data that we collected from real parallel DBMSs was used to validate the performance and energy model of P-store (see Section 4 for details). Then, using P-store, we systematically explored both the query parameters and the cluster design space parameters to examine their impact on performance and energy efficiency. We also developed an analytical model that allows us to further explore the design space. One of the interesting insights we found using both P-store and our analytical model is that there are query scenarios where certain design points lie below the EDP curve. One such result is shown in Figure 1(b). Here, we use our analytical model to show the energy versus performance trade-off for various eight node cluster designs, when performing a join between the TPC-H LINEITEM and the ORDERS tables (see Section 5 for details). In this figure, similar to Figure 1(a), we plot the relative energy consumed by the system 2 With the growing viewpoint of considering an entire cluster as a single computer [10], EDP is also a useful way of thinking about the interactions between energy consumption and performance when designing data centers [22, 38]. and the response time against a reference point of an eight node Xeon-based ( ""Beefy "") cluster. We then gradually replaced these nodes with mobile Intel i7 based laptops ( ""Wimpy "")3 nodes. The Wimpy nodes do not have enough memory to build in-memory hash tables for the join, and so they only scan and filter the table data before shuffling them off to the Beefy nodes (where the actual join is performed). As opposed to Figure 1(a), which is an experiment done with homogeneous Beefy nodes, Figure 1(b) shows data points below the EDP curve. This result is interesting as it shows that it is possible to achieve a relatively greater energy savings than response time penalty (to lower the EDP) when considering alternative cluster designs. Energy-efficient cluster design with potentially heterogeneous cluster nodes needs to be considered since non-traditional hetero- geneous clusters are now showing up as database appliances, such as Oracle's Exadata Database Machine [1]. Thus, to a certain ex- tent, commercial systems designers have already started down the road to heterogeneous clusters and appliances. Such considerations may also become important if future hardware (e.g., processor and/or memory subsystems) allows systems to dynamically control their power/performance trade-offs. Our paper provides a systematic study of both the software and the hardware design space parameters in an effort to draw conclusions about important design decisions when designing energy-efficient database clusters. 1.2 Contributions The key contributions of this paper are: first,we empirically ex- amine the interactions between the scalability of parallel DBMSs and energy efficiency. Using three DBMSs: Vertica, HadoopDB, and a custom-built parallel data processing kernel, we explore the trade-offs in performance versus energy efficiency when performing speedup experiments on TPC-H queries (e.g., Figure 1(a)). From these results, we identify distinct bottlenecks for performance and energy efficiency that should be avoided for energy-efficient cluster design. Second, non-traditional/wimpy low-power server hardware has been evaluated for its performance/energy-efficiency trade-offs, and we leverage these insights along with our own energy-efficiency micro-benchmarks to explore the design space for parallel DBMS clusters. Using P-store, we provide a model that takes into account these different server configurations and the parallel data processing bottlenecks, and predicts data processing performance and energy efficiency for various node configurations of a database cluster. Third, using our model we study the design space for parallel DBMS clusters, and illustrate interesting cluster design points, under varying query parameters for a typical hash join (e.g., Figure 1(b)). Finally, we organize the insights from this study as (initial) guiding principles for energy-efficient data processing cluster design. The remainder of this paper is organized as follows: Section 2 contains background discussion and related work; Section 3 discusses our findings with Vertica and HadoopDB. In Section 4 we present P-store. In Section 5, we model P-store and use the model to explore interesting cluster design points. Section 6 summarizes our findings as guiding principles for energy-efficient data processing cluster design. We make our concluding remarks in Section 7.",Willis Lang,University of Wisconsin,wlang@cs.wisc.edu,Stavros Harizopoulos,Nou Data,stavros@noudata.com,Jignesh M. Patel,University of Wisconsin,jignesh@cs.wisc.edu,Mehul A. Shah,Nou Data,mehul@noudata.com,Dimitris Tsirogiannis,Microsoft Corp.,dimitsir@microsoft.com,,,,,,,,,,,,,,,
20200303,919,Hyunjung Park,Stanford University,hyunjung@cs.stanford.edu,,Query Optimization over Crowdsourced Data,"Query Optimization over Crowdsourced Data, Query Optimization over Crowdsourced Data, Query Optimization over Crowdsourced Data, Query Optimization over Crowdsourced Data, Query Optimization over Crowdsourced Data, ABSTRACT Deco is a comprehensive system for answering declarative queries posed over stored relational data together with data obtained ondemand from the crowd. In this paper we describe Deco's costbased query optimizer, building on Deco's data model, query language, and query execution engine presented earlier. Deco's objective in query optimization is to find the best query plan to answer a query, in terms of estimated monetary cost. Deco's query semantics and plan execution strategies require several fundamental changes to traditional query optimization. Novel techniques incorporated into Deco's query optimizer include a cost model distinguishing between ""free"" existing data versus paid new data, a cardinality estimation algorithm coping with changes to the database state during query execution, and a plan enumeration algorithm maximizing reuse of common subplans in a setting that makes reuse challeng- ing. We experimentally evaluate Deco's query optimizer, focusing on the accuracy of cost estimation and the efficiency of plan enu- meration. 1. INTRODUCTION Crowdsourcing [8] enables programmers to incorporate human computation into a variety of tasks that are difficult for computer algorithms alone to solve well, e.g., tagging images, categoriz- ing products, and extracting sentiments from Tweets. However, to take advantage of crowdsourcing in practice, programmers have to write custom code using low-level APIs, which leads to some common challenges even for simple applications: improving data quality by resolving inconsistencies in crowdsourced data, integrating crowdsourced data with existing data, and optimizing crowdsourcing workflows for monetary cost and latency. To address these challenges, we are developing Deco (for ""declar- ative crowdsourcing"") [18, 19, 20], a system that answers declar- ative queries posed over stored relational data together with data obtained on-demand from the crowd. In [18], we defined a data model and a query language for Deco: The data model was de- signed to be general, flexible, and principled; the query language extends SQL with simple constructs necessary for crowdsourcing. Based on the data model, we defined a precise semantics for ar- bitrary queries. In [20], we described Deco's query plans, and how the system executes them to minimize monetary cost while re- ducing latency. Deco's query execution engine uses several novel techniques to overcome the limitations of traditional query execu- tion models in the crowdsourcing setting, including a hybrid ex- ecution model, incremental view maintenance inside query plans, two-phase query execution, and dynamic fetch prioritization. In this paper, we describe Deco's cost-based query optimizer. Our goal is to find the best query plan to answer a query, where ""best"" means the least estimated monetary cost across all possi- ble query plans. (Recall in [20] we described how to execute a given plan with least monetary cost. Now we seek to find the best overall plan.) Although Deco's query optimizer has similar overall structure to a traditional query optimizer, there are several funda- mental differences in plan costing and selection that needed to be addressed in our setting, to reflect Deco's query semantics and plan execution strategies. Distinguishing between existing vs. new data: To estimate mon- etary cost properly, Deco's cost model must distinguish between existing data obtained by past queries (or otherwise present in the database), versus new data to be obtained on-demand from the crowd. Existing data is ""free"", so all of the monetary cost is as- sociated with new data. Deco's cost model must take into account the existing data that might contribute to the query result, in order to estimate the cardinality of new data required to produce the re- sult. In our setting, the estimated cardinality of new data directly translates to the monetary cost to answer the query. Estimating cardinality and database state simultaneously: As Deco executes a query, it also changes the state of database by storing new data obtained from the crowd. Cardinality estimation obvi- ously must be based on some final database state, which needs to be estimated as well. Deco's cardinality estimation algorithm simulta- neously estimates cardinality and the end-state of the database, using a top-down recursive process: Starting from the root operator, each operator passes a requirement for the end-state to its subplans; as the recursion unwinds, the subplans return their estimated cardi- nality based on the expected end-state. Note that Deco's cardinality estimation is holistic: the cardinality of a subplan depends on the entire plan, not just the subplan. Exploiting limited subplan reuse opportunities: In Deco, dif- ferent physical plans corresponding to the same logical plan may produce different query results. (We will see in our query semantics that many possible results are valid in a crowdsourcing set- ting.) Thus, unlike in traditional optimization, estimated cardinality is a property of physical plans rather than logical plans. As a result, in comparison with traditional plan enumeration, there are far fewer opportunities to avoid redundant computation across alternative plans or prune inferior subplans early. Combined with holistic cardinality estimation, Deco does need to explore a large number of plans, but we reuse common subplans to the extend possible. Our experiments show that Deco's query optimizer succeeds in choosing inexpensive query plans for a wide variety of settings, within reasonable optimization time. The rest of the paper proceeds as follows: ? We review Deco's data model, query language, and query exe- cution engine (Section 2), summarizing material from [18, 20]. ? We describe how Deco estimates the monetary cost of execut- ing a given query plan (Section 3). ? We describe the search space of alternative Deco query plans (Section 4). ? We present Deco's plan enumeration algorithm, which explores the search space and applies the cost estimation algorithm to choose the best plan (Section 5). ? We experimentally evaluate Deco's query optimizer in terms of the accuracy of cost estimation and the efficiency of plan enumeration (Section 6). Related work is covered in Section 7, and we conclude with future directions in Section 8.",Hyunjung Park,Stanford University,hyunjung@cs.stanford.edu,Jennifer Widom,Stanford University,widom@cs.stanford.edu,,,,,,,,,,,,,,,,,,,,,,,,
20200304,1300,Willis Lang,"Microsoft Gray Systems Lab, *University of Wisconsin",wilang@cs.wisc.edu,,Partial Results in Database Systems,"Partial Results in Database Systems, Partial Results in Database Systems, Partial Results in Database Systems, Partial Results in Database Systems, Partial Results in Database Systems, ABSTRACT As the size and complexity of analytic data processing systems continue to grow, the effort required to mitigate faults and performance skew has also risen. However, in some environments we have encountered, users prefer to continue query execution even in the presence of failures (e.g., the unavailability of certain data sources), and receive a ""partial"" answer to their query. We explore ways to characterize and classify these partial results, and describe an analytical framework that allows the system to perform coarse to fine-grained analysis to determine the semantics of a partial result. We propose that if the system is equipped with such a framework, in some cases it is better to return and explain partial results than to attempt to avoid them. 1. INTRODUCTION In this paper we consider evaluating relational queries over multi- ple information sources, some of which might return incomplete tuple sets. This situation could arise in a wide variety of scenarios. For example, it could arise with queries spanning a collection of loosely coupled cloud databases, if one or more of them is temporarily down or unusable (say due to network congestion or misconfigurations); we may also see it with queries in a traditional parallel RDBMS, if a node fails during query evaluation and its data becomes unavailable; or it could even appear with queries in a single node system, if some base tables or views are known to be incomplete. Consider a specific example the authors have encountered: today, with public clouds (e.g., AzureDB), users can sign up for multiple independent instances of relational databases. A significant num- ber of these users choose to ""self-shard"" (or horizontally partition) their tables across hundreds to thousands of these databases. A critical point is that in such a scenario, each of the sharded rela- tional database systems is an independent entity, and there is no unifying system collectively managing the collection of relational systems. These customers often wish to query over the totality of these systems, but unfortunately, poor latency, connection failures, misconfigurations, or system crashes are all quite possible in any of the loosely coupled databases. At this point the law of large numbers becomes fatal - even with 99.9% uptime, a query over a 1000-shard table will likely have at least one inaccessible shard, and if executing the distributed query requires all of the 1000 systems to be accessible during execution, the query may literally never complete. In every instance of an incomplete input, our traditional database instincts tell us that the solution is to fix the problem: we should either replicate the data sources comprising the distributed system or make them more reliable; we should add replication and failover to the nodes of our parallel DBMS; or we should embark on data clean- ing and repairing efforts to fix the incomplete tables and views in the single node system [4,5,7,10,14,16,20,30,33?36]. However, these solutions can be either financially costly, performance hindering, or both. Furthermore, in certain cases, such as querying over loosely coupled cloud sources, an error external to the database or miscon- figurations may be impossible to fix. Finally, consistent querying techniques that rely on functional dependencies and integrity con- straints currently become inapplicable in this environment. That is why, in this paper we consider a different approach: letting the query run to ""completion"" despite the incomplete input(s). In some cases, of course, this is a very bad idea. When reporting numbers to the SEC, or billing a customer, or the like, incomplete answers are not acceptable. However, there are use cases in which the user may be willing to accept an answer computed with incom- plete inputs. For example, the user may be doing exploratory work to gain some insight, or may be interested in answering a query like ""find me 1000 customers satisfying the following condition..."" In such cases, it may be preferable to return imperfect answers rather than to have the query fail, or to incur a delay, or incur the cost and effort of ensuring that such failures happen very rarely. The astute reader may be wondering ""haven't I seen something like this approximate result stuff before?"" Undoubtedly the answer is ""yes,"" but here we are working in a very different setting. Rather than viewing query processing as an incremental process in which the query processor systematically explores more and more of the input to yield successively closer approximations to the true result [23,28, 31], we are interested in query processing in which, due to forces out of the control of the query processor, part of the input is simply not available and will not become available during the query's lifetime. Of course, merely returning such an answer to an unsuspecting user would be very poor form; the system needs to tell the user that this result is computed based upon incomplete data. Furthermore, the more the system can guarantee about the partial result, or explain to the user about the result, the better. This raises the following intriguing question: is there anything we can say about a computed answer based upon incomplete data other than ""be careful, this is based on incomplete data?"" For example, can we make any guarantees (about what the user may or may not receive)? Can we classify the types of anomalies that might result and develop sound mechanisms for determining when they can and cannot occur? This brings us to the heart of this work: we present a broad clas- sification of what can ""go wrong"" when evaluating queries over incomplete data, and show how to detect the various anomalies that arise by analyzing how they are created and propagated through the operators in a query plan. This classification can be used either proactively, where the user specifies ""I will only tolerate the following kind of anomalies;"" or after the fact, where the system returns ""here is what I can tell you about the anomalies that might exist in this result."" We present our initial approach to interactively displaying such information to the user in Section 6. Note that although the root cause of incomplete information may be straightforward, its impact on a query plan is less obvious. The nature of a query result over incomplete inputs is not simply that we receive fewer tuples. Figure 1 shows a simple query that concisely illustrates ""all that can go wrong"" when a system produces a partial result. In the figure, we consider the result of a simple aggregation query over table R. Suppose that the scan of R is incomplete (perhaps R itself is incomplete, or perhaps R is partitioned and some partition of R resides on a currently inaccessible node.) In our example, the result for group ""C' is correct, but every other row is problematic. The three main differences between the true result and the result over incomplete data are: (1) the average value calculated for group ""A' is incorrect; (2) a tuple for group ""D' is produced even though it is not found in the true result; and (3) the partial result does not have a tuple for either group ""B' or group ""E'. Each of these anomalies occurred because the scan of table R was not complete, but these anomalies surface at different times and for different reasons during query execution. For anomaly (1), if we are missing any tuples from R that contribute to the group for ""A', it is not hard to see that the average calculation may be wrong. For anomaly (3), the result tuple for group ""B' is perhaps missing, because all of the tuples in R that contribute to group ""B' are missing, while the result tuple for group ""E' may be absent because tuples missing from the scan ofR caused the computation of an incorrect average for group ""E', which in turn failed the HAVING clause. Finally, anomaly (2) arose because an incorrect value for the aggregate for group ""D' caused the group to mistakenly satisfy the HAVING clause. Anomaly (2) is unique because it demonstrates that results over incorrect inputs may have ""extra"" tuples in their output (not in the True result); we call such tuples ""phantom"" tuples. This example provides the intuition behind our classification of the errors that can arise when evaluating relational queries over incomplete inputs. Our classification can be viewed as expanding the space of answers produced by database systems as shown in Figure 2. The status quo, of course, is to return only complete and correct tuple sets. But in addition to these complete and correct results, Figure 2 characterizes additional kinds of results with two orthogonal axes: cardinality and correctness. For cardinality we consider four possible categories: complete, missing tuples but no phantom tuples, phantom tuples but no missing tuples, or indetermi- nate (both missing tuples and phantom tuples). Correctness refers to the values in the tuples that are returned, and is simpler: either the values are credible (correct) or not credible (possibly incorrect). Figure 3 displays this same taxonomy in a different format, representing the options in the cardinality dimension as a partial order. The challenge, of course, is to determine, for a particular query with a given incomplete input, where in this classification the result set lies. This is the key to returning only the result sets that satisfy user requirements if they impose them. For example, if the user specifies that she wants no phantom tuples, we should analyze the query execution and only return result tuples if we can guarantee phantom tuples are not present, or refuse to return any answer at all if we cannot. It is also the key to providing a user with after the fact information about the result if that is their desire. For example, telling the user that both phantoms and missing tuples could have occurred and letting them decide whether to accept the answer or not. Due to the dynamic nature of failure detection, the classification of a partial result is plan dependent. The precision of our classification depends on the depth to which we are willing to analyze all the information about the missing data and the query. Finally, the last challenge is to communicate our partial result semantics back to the user in a meaningful yet intuitive way. In this work, we present partial result production as a first-class ap- proach to deal with data access failures in database systems. Existing work that is similar to ours focuses on the basic tenet of producing the true result, albeit through the analysis of iterations of partial an- swers and through certain operating assumptions [9, 37]. Our work is complementary to existing research in the areas such as online ag- gregation [23,28,31], data provenance and lineage [8,13,19,26,39], and approximate query processing [2, 11, 15, 18, 32] in that we are providing certainties for an uncertain result produced due to failures. The contributions of this paper are as follows: ? We identify the tuple set properties that can be used to classify partial results. (Section 2) ? We present a partial result analysis framework with four mod- els that determine the degree of our partial result classification precision. (Section 3) ? We describe how relational operators propagate partial result semantics in our framework. (Section 4) ? We describe the relationship between query optimization, failure models, and our partial result classification. (Section 5) ? We implemented a prototype system and discuss how a user may use and tune a partial result-aware system. (Section 6) Related work follows in Section 7 along with our conclusions and future work in Section 8.",Willis Lang,"Microsoft Gray Systems Lab, *University of Wisconsin",wilang@cs.wisc.edu,Rimma V. Nehme,"Microsoft Gray Systems Lab, *University of Wisconsin",rimman@cs.wisc.edu,Eric Robinson,"Microsoft Gray Systems Lab, *University of Wisconsin",errobins@cs.wisc.edu,Jeffrey F. Naughton,"Microsoft Gray Systems Lab, *University of Wisconsin",naughton@cs.wisc.edu,,,,,,,,,,,,,,,,,,
20200305,1361,Rimma Nehme,"Microsoft Jim Gray Systems Lab Madison, WI 53703",rimman@microsoft.com,,Automated Partitioning Design in Parallel Database Systems,"Automated Partitioning Design in Parallel Database Systems, Automated Partitioning Design in Parallel Database Systems, Automated Partitioning Design in Parallel Database Systems, Automated Partitioning Design in Parallel Database Systems, Automated Partitioning Design in Parallel Database Systems, ABSTRACT In recent years, Massively Parallel Processors (MPPs) have gained ground enabling vast amounts of data processing. In such environ- ments, data is partitioned across multiple compute nodes, which results in dramatic performance improvements during parallel query execution. To evaluate certain relational operators in a query correctly, data sometimes needs to be re-partitioned (i.e., moved) across compute nodes. Since data movement operations are much more expensive than relational operations, it is crucial to design a suitable data partitioning strategy that minimizes the cost of such expensive data transfers. A good partitioning strategy strongly de- pends on how the parallel system would be used. In this paper we present a partitioning advisor that recommends the best partitioning design for an expected workload. Our tool recommends which ta- bles should be replicated (i.e., copied into every compute node) and which ones should be distributed according to specific column(s) so that the cost of evaluating similar workloads is minimized. In contrast to previous work, our techniques are deeply integrated with the underlying parallel query optimizer, which results in more accurate recommendations in a shorter amount of time. Our experimental evaluation using a real MPP system, Microsoft SQL Server 2008 Parallel Data Warehouse, with both real and synthetic workloads shows the effectiveness of the proposed techniques and the importance of deep integration of the partitioning advisor with the underlying query optimizer. Categories and Subject Descriptors H.2 [Database Management]: Query Processing ; H.2.2 [Physical Design]: Access Methods General Terms Algorithms Keywords distributed databases 1. INTRODUCTION High-performance computing has undergone many changes in recent years. One of the major trends has been the wide adoption of massively parallel processing (MPP) systems. An MPP system is a distributed computer system which consists of many individual nodes, each of which is essentially an independent computer in itself. Each node, in turn, consists of at least one processor, its own memory, and a link to the network that connects all nodes together. Queries executed in such environments tend to be complex, involv- ing many joins, nested sub-queries and aggregation and are usually long-running and resource-intensive (see Figure 1). During query execution, data often needs to be transferred across nodes, which is a relatively expensive operation compared to relational operators. Excessive data transfers can significantly slow down query execution, deteriorate the performance of the overall system, and negatively impact the user experience. If data is originally partitioned in an adequate way, such expensive data transfer operations can be minimized. Clearly, the selection of the best way to partition the data in a distributed environment is a critical physical database design problem. Previous studies on automated partitioning design can be roughly classified into two categories: optimizer-independent (which inter- pret and model the optimization information outside of the database to perform the tuning [29, 31]), and  ""shallowly-integrated "" with query optimizer (which largely use the optimizer as a black-box to perform the what-if optimization calls [25]). The problem with such loosely-integrated approaches is two-fold: first, the quality of the resulting partitioning recommendations is likely to suffer when the tuning tools are not completely in-sync with optimizer 's de- cisions, and second, the performance of a tuning tool is likely to diminish due to narrow APIs between the tool and the DBMS. We propose to address the problem of partition design tuning for parallel databases by introducing an advisor that is deeply-integrated with parallel query optimizer. Specifically, our partitioning advisor 1137 exploits the optimizer 's cost model as well as its internal data struc- ture, called the MEMO, to efficiently find the best possible partition- ing configuration. Furthermore, we leverage the MEMO structure to infer lower bounds on partial partitioning configurations ? the prop- erty that enables a very efficient branch and bound search strategy through the combinatorial search space of all feasible partition con- figurations. In addition to using the partitioning advisor when loading a brand- new database into a distributed environment, a new partitioning strategy may also be recommended whenever: - data is migrated to a new system. - the workload on a database changes substantially. - the database has been heavily updated (e.g., tables are added or removed, or statistical information changes). - performance has significantly degraded. We 've implemented our partitioning advisor in Microsoft SQL Server 2008 Parallel Data Warehouse [4], a real MPP system1, which is a scalable distributed engine that delivers performance at low cost through massive parallel processing. In summary, the pri- mary contributions of our work include: - We exploit the concept of a  ""shell appliance "" to simulate a physically distributed database with various partitioning con- figurations stored on a single machine as if it were a regular database (but with no actual data). - We leverage the MPP optimizer 's concise optimization search space (the MEMO) to infer lower bounds on partial configura- tions and exploit this property for a better traversal of the partitioning configuration search space. - We evaluate our partitioning advisor against the shallow in- tegration approaches like genetic- and rank-based techniques from [25] to enumerate the search space. - We implemented our partitioning advisor in a real MPP sys- tem and present our experimental evaluation using the TPC-H, TPC-DS benchmarks and several real-life datasets. The rest of the paper is structured as follows. Section 2 provides the background necessary to better understand the technical aspects of this work. Section 3 gives the problem definition and states our main assumptions. Section 4 reviews shallowly-integrated ap- proaches for finding the best partitioning configuration. Section 5 describes the details of our proposed deeply-integrated approach to partition design tuning. Section 6 reports our experimental evalu- ation, and Section 7 discusses possible extensions to the advisor. Section 8 reviews the related work.",Rimma Nehme,"Microsoft Jim Gray Systems Lab Madison, WI 53703",rimman@microsoft.com,Nicolas Bruno,"Microsoft Redmond, WA 98052 USA",nicolasb@microsoft.com,,,,,,,,,,,,,,,,,,,,,,,,
20200306,740,Arvind Arasu,"Microsoft Research Redmond, WA 98052, USA",arvinda@microsoft.com,,On Active Learning of Record Matching Packages,"On Active Learning of Record Matching Packages, On Active Learning of Record Matching Packages, On Active Learning of Record Matching Packages, On Active Learning of Record Matching Packages, On Active Learning of Record Matching Packages, ABSTRACT We consider the problem of learning a record matching package (classifier) in an active learning setting. In active learning, the learning algorithm picks the set of examples to be labeled, unlike more traditional passive learning setting where a user selects the labeled examples. Active learning is important for record matching since manually identifying a suitable set of labeled examples is difficult. Previous algorithms that use active learning for record matching have serious limitations: The packages that they learn lack quality guarantees and the algorithms do not scale to large input sizes. We present new algorithms for this problem that overcome these limitations. Our algorithms are fundamentally differ- ent from traditional active learning approaches, and are designed ground up to exploit problem characteristics specific to record matching. We include a detailed experimental evaluation on real- world data demonstrating the effectiveness of our algorithms. Categories and Subject Descriptors H.2 [Database Management]: Systems General Terms Algorithms, Performance Keywords Data Cleaning, Record Matching, Active Learning 1. INTRODUCTION Record Matching is the problem of identifying matching or du- plicate records, records that correspond to the same real-world en- tity. An example record matching task is to identify bibliographic records in Citeseer [13] and DBLP [18] that correspond to the same publication. Figure 1 shows a toy example with two tables R (records R1-R3) and S (records S1-S3) containing organization names and addresses, and the goal is to find pairs of records that represent the same organization; these are likely to be the pairs (R1, S3) and (R2, S2). Record matching is a well-studied problem and has applications in information integration [5, 22], data warehous- ing [1], census data [42] and health-care records management [35]. The standard approach to record matching is to use textual similarity between the records to determine whether or not two records are matches [20]. Informally, in Figure 1, the matching pair (R1, S3) is textually similar, while the non-matching pair (R1, S1) is not. Current approaches typically compute a variety of similarity scores for a candidate pair of records and these scores are combined using some logic to determine if the pair is a match or not. A similar- ity score quantifies textual similarity between the two records on some subset of attributes, and is computed using a string similarity function such as edit distance, jaccard, and cosine similarity [20]. Since manually coming up with a logic for combining similarity scores is difficult, the current state-of-art uses a learning based approach: In this approach, record matching is viewed as a classification problem where each pair has to be classified as a match or a nomatch, and a suitable classifier is learned using labeled exam- ples of matching and non-matching pairs. The pair-wise similarity scores serve as features for the classification. Prior work has con- sidered a variety of classifiers such as SVMs [7], decision trees [11, 37], and naive Bayes [42]. However, there is a crucial difference between record matching and standard classification problems: In a typical record match- ing task, the number of non-matches far exceeds the number of matches. For concreteness, consider a record matching task involv- ing two tables with a million records each, and assume that each record matches with 10 records of the other table on average. The number of matches for this task is ? 107, while the number of non-matches is ? 1012. This imbalance makes it very difficult to identify a suitable set of labeled examples so that a learned clas- sifier has high quality [37]. Standard techniques such as picking 783 pairs of records at random or using some other distribution do not work very well. Prior work [8, 11] has proposed using a filter based on textual similarity to eliminate a large number of non-matches followed by sampling to pick examples. As we show using our experiments, while this approach mitigates the problem described above, it does not eliminate it. It introduces the additional problem of picking a good filter. Active Learning: Motivated by these considerations, we explore the use of active learning for record matching. In active learning, the learning algorithm itself picks the examples to be labeled. The hope is that the algorithm can exploit this additional flexibility to pick examples that are most informative for the learning task. This eliminates the user 's burden of picking suitable examples or a good filter. While active learning has been previously studied for record matching [37, 40], the proposed algorithms have two limitations: First, these algorithms do not provide a principled interface using which a user can control the quality of a learned classifier. (Informally, in record matching, the quality of a classifier is measured using its precision and recall; the recall of a classifier is the num- ber of pairs that it classifies as a match and the precision is the fraction of these pairs that are true matches.) For example, we do not know of a systematic way of using the algorithms of [37, 40] to ensure that the learned classifier has precision above some threshold. Further, we observed in our experiments that the behav- ior of these algorithms can be unpredictable and precision and/or recall of the learned classifier can decrease when more labeled ex- amples are provided. This unpredictability makes it difficult to use these algorithms in record matching settings with specific quality requirements. A second limitation of these algorithms is that they do not scale to large inputs. For each requested label, these algorithms iterate over all record pairs, and the number of such pairs is quadratic in the input size. However, we can address this limitation using blocking techniques as we discuss subsequently. We propose new algorithms for active learning of record matching classifiers. Our algorithms cover decision trees and linear clas- sifiers (which include SVMs), and do not have the limitations of previous algorithms: In particular, our algorithms allow a user to specify a precision threshold as input. The learned classifier is guaranteed to have a precision greater than this threshold and (under certain reasonable assumptions) recall close to the best possible given the precision constraint. Our algorithms also differ from pre- vious algorithms in that they are designed from scratch for record matching, and do not just invoke a known learning algorithm as a black-box. This enables our algorithms to exploit problem features that are specific to record matching. Our active learning algorithms can also be easily adapted to yield new learning algorithms in the traditional passive learning setting. Efficiency considerations: For large record matching instances, it is inefficient to consider all pairs of candidate records, classify them as a match or non-match, and output those classified as a match. The traditional approach to handling this problem is to use blocking [6, 32]. Blocking is a heuristic filtering step that selects a subset of candidate pairs of records, and only the selected pairs are considered for subsequent classification. A good blocking scheme has an efficient implementation and eliminates few true matches. An example blocking scheme for the instances like Figure 1 is to select pairs of records that agree on the first letter of the Name column for subsequent classification. A more sophisticated blocking scheme is to select pairs that have jaccard similarity at least 0.8 on the Name column; such string-similarity joins can be efficiently evaluated using techniques proposed in [2, 25, 38]. We develop a simple integration of blocking into the learning problem that enables our algorithms to handle large inputs. Given a blocking scheme, our algorithms learn a classifier that when used in conjunction with the blocking scheme, has maximum recall and precision above a specified threshold. This integration of block- ing and active learning reduces the number of labeling requests. For example, assume that the first letter blocking scheme described above is used for record matching. An active learning algorithm without a knowledge of this blocking scheme might request a la- bel for a pair that does not agree on the first letter, and the labeling effort on that pair is wasted since such pairs would never be con- sidered for classification. We argue that previous algorithms [37, 40] can also be modified to similarly exploit blocking functions. In our experiments, we use these algorithms with this modification for comparison purposes (and demonstrate that our algorithms perform better). Roadmap: In Section 2, we introduce definitions and notation and formalize the active learning problem. In Section 3, we introduce and discuss an interesting monotonicity property in record match- ing, which is exploited in the design of our algorithms. We present our algorithms in Sections 4 and 5, and evaluate their performance empirically in Section 6. We cover related work in Section 7.",Arvind Arasu,"Microsoft Research Redmond, WA 98052, USA",arvinda@microsoft.com,Michaela G,"tz Cornell University Ithaca, NY 14853, USA",goetz@cs.cornell.edu,Raghav Kaushik,"Microsoft Research Redmond, WA 98052, USA",skaushi@microsoft.com,,,,,,,,,,,,,,,,,,,,,
20200307,1295,Wenfei Fan,University of Edinburgh,wenfei@inf.ed.ac.uk,,Performance Guarantees for Distributed Reachability Queries,"Performance Guarantees for Distributed Reachability Queries, Performance Guarantees for Distributed Reachability Queries, Performance Guarantees for Distributed Reachability Queries, Performance Guarantees for Distributed Reachability Queries, Performance Guarantees for Distributed Reachability Queries, ABSTRACT In the real world a graph is often fragmented and distributed across different sites. This highlights the need for evaluating queries on distributed graphs. This paper proposes ditributed evaluation algorithms for three classes of queries: reachability for determining whether one node can reach another, bounded reachability for deciding whether there exists a path of a bounded length between a pair of nodes, and regular reachability for checking whether there exists a path connecting two nodes such that the node labels on the path form a string in a given regular expression. We develop these algorithms based on partial evaluation, to explore parallel computation. When evaluating a query Q on a distributed graph G, we show that these algorithms possess the follow- ing performance guarantees, no matter how G is fragmented and distributed: (1) each site is visited only once; (2) the total network traffic is determined by the size of Q and the fragmentation of G, independent of the size of G; and (3) the response time is decided by the largest fragment of G rather than the entire G. In addition, we show that these algorithms can be readily implemented in the MapReduce framework. Using synthetic and real-life data, we experi- mentally verify that these algorithms are scalable on large graphs, regardless of how the graphs are distributed. 1. INTRODUCTION Large real-life graphs are often fragmented and stored dis- tributively in different sites, e.g., social networks [27], Web services networks [23] and rdf graphs [16,26]. For instance, a graph representing a social network may be distributed across different servers and data centers for performance, management or data privacy reasons [12, 23, 25, 27] (e.g., social graphs of Twitter and Facebook are geo-distributed to different data centers [12, 25]). Moreover, various data of people (e.g., friends, products, companies) are typically found in different social networks [27], and have to be taken together when one needs to find the complete information about a person. With this comes the need for effective tech- Figure 1: Querying a distributed social network niques to query distributed graphs, for e.g., computing rec- ommendations [17] and social network aggregations [27]. There have been a number of algorithms and distributed graph database systems for evaluating queries on distributed graphs (e.g., [3, 6, 11, 29, 30]). However, few of these algo- rithms and systems provide performance guarantees, on the number of visits to each site, network traffic (data shipment) or computational cost (response time). The need for developing efficient distributed evaluation algorithms with per- formance guarantees is particularly evident for reachability queries, which are most commonly used in practice. This paper advocates to evaluate queries on distributed graphs based on partial evaluation. Partial evaluation (a.k.a. program specialization) has been proved useful in a variety of areas including compiler generation, code optimization and dataflow evaluation (see [18] for a survey). Intuitively, given a function f(s, d) and part of its input s, partial evalu- ation is to specialize f(s, d) with respect to the known input s. That is, it conducts the part of f  's computation that depends only on s, and generates a partial answer, i.e., a residual function f  ' that depends on the as yet unavailable input d. This idea can be naturally applied to distributed query evaluation. Indeed, consider a query posed on a graph G that is partitioned into fragments (F1, . . . , Fn), where Fi is stored in site Si. To compute Q(G), each site Si can find the partial answer to Q in fragment Fi in parallel, by taking Fi as the known input s while treating the fragments in the other sites as yet unavailable input d. These partial answers are collected and combined by a coordinator site, to derive the answer to query Q in the entire G. Example 1: Figure 1 depicts a fractionG of a recommenda- tion network, where each node denotes a person with name and job titles (e.g., database researcher (DB), human re- source (HR)), and each directed edge indicates a recommen- dation. The graph G is geo-distributed to three data centers DC1, DC2 and DC3, each storing a fragment of G. Consider a query Q given in Fig. 1, posed at DC1. It is to find whether there exists a chain of recommendations from a CTO Ann to her finance analyst (FA) Mark, through either a list of DB people or a list of HR people. Observe that such a path exists: (Ann, CTO)   (Walt, HR)   (Mat, HR)   (Fred, HR)   (Emmy, HR)   (Ross, HR)   (Mark, FA). However, it is nontrivial to verify this in the distributed setting. A naive method is to first ship data from DC1, DC2 and DC3 to a single site, and then evaluate the query using an al- gorithm developed for centralized data (i.e., graphs stored in a single site). This is infeasible because its data ship- ment may be prohibitively expensive and worse still, may not even be allowed for data privacy. Another way is to use a distributed graph traversal algorithm, by sending messages between different sites. This, however, requires messages to be sent along DC1   DC2   DC1   DC2   DC3   DC1, incurring unbounded number of visits to each site, excessive communication cost, and unnecessary delay in response. We can do better by using partial evaluation. We send the query Q to DC1, DC2 and DC3, as is. We compute the partial answers to (sub-queries of) Q at each site, in parallel, by taking the fragment residing in the site as known input and introducing Boolean variables to indicate unknown in- put (i.e., fragments in the other sites). The partial answers are vectors of Boolean formulas, one associated with each node that has an edge from a fragment stored at another site. These Boolean formulas indicate (1) at DC1, from Ann there exist an HR path to Walt and a DB path to Bill, and from Fred there is an HR path to Emmy; (2) at DC2, there exist an HR path from Emmy to Ross, an HR path from Mat to Fred; and (3) at DC3, there exists an HR path from Ross to Mark. These partial answers are collected by a coordinator site (DC1), which solves a system of equations formed by these Boolean formulas that are recursively defined, to find the truth values of those Boolean variables. It yields answer true to Q, i.e., there exists an HR path from Ann to Mark. We will show that this method guarantees the following: (1) each site is visited only once; (2) besides the query Q, only 2 messages are sent, all to the coordinator, and each message is independent of the size of G, and (3) partial eval- uation is conducted in parallel at each site, without waiting for the outcome or messages from any other site. ? While there has been work on query answering via par- tial evaluation [2, 3, 6, 11], the previous work has focused on either trees [2, 3, 6] or non-recursive queries expressed in first-order logic (FO) [11]. We are not aware of any pre- vious algorithms based on partial evaluation for answering reachability queries, which are beyond FO, on possibly cyclic graphs that are arbitrarily fragmented and distributed. Contributions. We provide distributed evaluation algo- rithms for three classes of reachability queries commonly used in practice, via partial evaluation. We show that these algorithms posses several salient performance guarantees. (1) Our first algorithm is developed for reachability queries (Section 3), to decide whether two given nodes are con- nected by a path [31]. We show that when evaluating such a query on a distributed graph G, the algorithm (a) visits each site only once, (b) is in O(|Vf ||Fm|) time, and (c) its total amount of data shipped is bounded by O(|Vf | 2), where |Vf | is the number of nodes that have edges across different sites, and |Fm| is the size of the largest fragment in G. (2) Our second algorithm is for evaluating bounded reacha- bility queries (Section 4), for determining whether two given nodes are connected by a path of a bounded length [31]. We show that this algorithm has the same performance guaran- tees as its counterpart for reachability queries. (3) Our third algorithm is to evaluate regular reachability queries (Section 5), to decide whether there exists a path be- tween a pair (u, v) of nodes such that the node labels on the path satisfy a regular expression R. When evaluating such a query on a distributed graph G, the algorithm (a) visits each site only once, (b) is in O(|Fm||R| 2 + |R|2|Vf | 2) time, and (c) has network traffic bounded by (|R|2|Vf | 2), where |Fm| and |Vf | are as above, and |R| is the size of regular expression R, which is much smaller than |Vf | and |Fm|. (4) We also develop a MapReduce [7] algorithm for evaluat- ing regular reachability queries (Section 6). This shows that partial evaluation can be readily implemented in the widely used MapReduce framework. The algorithm can be easily adapted to evaluate (bounded) reachability queries, which are special cases of regular reachability queries. (5) We experimentally evaluate the efficiency and scalability of our algorithms(Section 7). We find that our algorithms scale well with both the size of graphs and the number of fragments. For instance, it takes 16 seconds to answer a regular reachability query on graphs with 1.5M (million) nodes and 2.1M edges, partitioned into 10 fragments. We also find that the communication cost of our algorithms is low. Indeed, the amount of data shipped by our algorithms is no more than 11% of the graphs in average. For reacha- bility queries on real-life graphs, our algorithms take only 6% of running time of the algorithms based on message passing [21], and visit each site only once as opposed to 625 visits in average by its counterpart [21]. In addition, our MapReduce algorithm is efficient. We contend that partial evaluation yields a promising approach to evaluating queries on distributed graphs. It guarantees that (1) the number of visits to each site is min- imum; (2) the total network traffic is independent of the size of the entire graph; (3) the evaluation is conducted in parallel, and its cost depends on the largest fragment of a partitioned graph and the number of nodes with edges to different sites, rather than the entire graph; and (4) it imposes no constraints on how the graph is fragmented and distributed. Moreover, it can be readily implemented in the MapReduce model, as verified in our experimental study. Related Work. We categorize related work as follows. Distributed databases. A variety of distributed database systems have been developed. (1) Distributed relational databases (see [24]) can store graphs in distributed rela- tional tables, but do not support efficient graph query eval- uation [8, 9]. (2) Non-relational distributed data storage manage distributed data via various data structures, e.g., sorted map [4], key/value pairs [8]. These systems are built forprimary-key only operations [8,9], or simple graph queries (e.g., degree, neighborhood)1, but do not efficiently sup- port distributed reachability queries. (3) Distributed graph databases. Neo4j1 is a graph database optimized for graph traversal. Trinity2 and HyperGraphDB3 are distributed sys- tems based on hypergraphs. Unfortunately, they do not sup- port efficient distributed (regular) reachability queries. Closer to our work is Pregel [21], a distributed graph querying system based on message passing It partitions a graph into clusters, and selects a master machine to assign each part to a slave machine. A graph algorithm allows (a) the nodes in each slave machine to send messages to each other, and (b) the master machine to communicate with slave machines. Several algorithms (distance, etc.) sup- ported by Pregel are addressed in [21]. Similar message- sending approaches are also developed in [13]. These algo- rithms differ from ours as follows. (a) In contrast to our algorithms, the message passing model in Pregel may seri- alize operations that can be conducted in parallel, and have no bound on the number of visits to each site, as shown by our experimental study (Section 7). (b) How to support reg- ular reachability query is not studied in [21]. On the other hand, the techniques of Pregel can be combined with partial evaluation to support local processing of reachability queries at each site (see Section 3). Distributed graph query evaluation. Several algorithms have been developed for evaluating queries on distributed graphs (see [19] for a survey). (1) Querying distributed trees [2,3,6]. Partial evaluation is used to evaluate XPath queries on dis- tributed XML data modeled as trees [3, 6], as well as for evaluating regular path queries [2]. It is nontrivial, however, to extend these algorithms to deal with (possibly cyclic) graphs. Indeed, the network traffic of [3,6] is bounded by the number of fragments and the size of the query, in contrast to the number of nodes with edges to different fragments in our setting. Moreover, we study (regular) reachability queries, which are quite different from XPath. Finally, our algo- rithms only visit each site once, while in [2] each site may be visited multiple times. (2) Querying distributed semi- structured data [13,28?30]. Techniques for evaluating regu- lar path queries on distributed, edge-labeled, rooted graphs are studied in [30] and extended in [29], based on message passing. It is guaranteed that the total network traffic is bounded by n2, where n is the number of edges across dif- ferent sites. A distributed BFS algorithm is given in [28], which takes nearly cubic time in graph size, and a table of exponential size to achieve a linear time complexity, and is impractical for large graphs. These differ from our algorithms as follows. (a) Our algorithms guarantee that each site is visited only once, as opposed to twice [30]. (b) As remarked earlier, message passing may unnecessarily serialize operations, while our algorithms explore parallelism via partial evaluation. While an analysis of computational cost is not given in [29, 30], We show experimentally that our algorithms outperform theirs (Section 7). There has also been recent work on evaluating SPARQL queries on distributed RDF graphs [11], which is not appli- caple to our setting due to (a) no performance guarantees or complexity bounds are provided in [11], and (b) the queries considered in [11] are expressible in FO, while we study (reg- ular) reachability queries beyond FO.",Wenfei Fan,University of Edinburgh,wenfei@inf.ed.ac.uk,Xin Wang,1University of Edinburgh, x.wang-36@sms.ed.ac.uk,Yinghui Wu,1University of Edinburgh,y.wu-18@sms.ed.ac.uk,,,,,,,,,,,,,,,,,,,,,
20200308,1362,Yongrui Qin ,"School of Computer Science The University of Adelaide Adelaide, SA 5005, Australia",yongrui.qin@adelaide. edu.au,,SIEF: Efficiently Answering Distance Queries for Failure Prone Graphs,"SIEF: Efficiently Answering Distance Queries for Failure Prone Graphs, SIEF: Efficiently Answering Distance Queries for Failure Prone Graphs, SIEF: Efficiently Answering Distance Queries for Failure Prone Graphs, SIEF: Efficiently Answering Distance Queries for Failure Prone Graphs, SIEF: Efficiently Answering Distance Queries for Failure Prone Graphs, ABSTRACT Shortest path computation is one of the most fundamental operations for managing and analyzing graphs. A number of methods have been proposed to answer shortest path dis- tance queries on static graphs. Unfortunately, there is lit- tle work on answering distance queries on dynamic graphs, particularly graphs with edge failures. Today 's real-world graphs, such as the social network graphs and web graphs, are evolving all the time and link failures occur due to var- ious factors, such as people stopping following others on Twitter or web links becoming invalid. Therefore, it is of great importance to handle distance queries on these failure- prone graphs. This is not only a problem far more difficult than that of static graphs but also important for processing distance queries on evolving or unstable networks. In this paper, we focus on the problem of computing the shortest path distance on graphs subject to edge failures. We pro- pose SIEF, a Supplemental Index for Edge Failures on a graph, which is based on distance labeling. Together with the original index created for the original graph, SIEF can support distance queries with edge failures efficiently. By exploiting properties of distance labeling on static graphs, we are able to compute very compact distance labeling for all singe-edge failure cases on dynamic graphs. We exten- sively evaluate our algorithms using six real-world graphs and confirm the effectiveness and efficiency of our approach. Categories and Subject Descriptors E.1 [Data]: Data Structures aGraphs and networks; H.2.8 [Database management]: Database Applications aGraph Indexing and Querying General Terms Algorithms, Experimentation, Performance Keywords Shortest paths, distance query, 2-hop labeling, edge failure 1. INTRODUCTION Recent years have witnessed the fast emergence of massive graph data in many application domains, such as the World Wide Web, linked data technology, online social networks, and Web of Things [21, 19, 22, 25]. In a graph, one of the most fundamental challenges centers on the efficient compu- tation of the shortest path or distance between any given pair of vertices. For instance, distances or the numbers of links between web pages on a web graph can be considered a robust measure of web page relevancy, especially in rele- vance feedback analysis in web search [21]. In RDF graphs of linked data, the shortest path distance from one entity to another is important for ranking entity relationships and keyword querying [19, 14]. For online social networks, the shortest path distance can be used to measure the closeness centrality between users [22]. A large body of indexing techniques have been recently proposed to process exact shortest path distance queries on graphs [10, 23, 9, 8, 2, 26, 15]. Among them, a signifi- cant portion of indexes are based on 2-hop distance label- ing, which is originally proposed by Cohen et al. [12]. The 2-hop distance labeling techniques pre-compute a label for each vertex so that the shortest path distance between any two vertices can be computed by giving their labels only. These labeling indexes, such as [10, 8, 2, 15], have been proved to be efficient, i.e., being able to answer a distance query within microseconds. Motivation. The above mentioned approaches generally make the assumption that graphs are static. However, in reality, many graphs are subject to edge failures. In this pa- per, we refer to graphs that are not subject to edge failures as stable graphs, i.e., static graphs. Similarly, we refer to graphs that are subject to edge failures as unstable graphs. For example, the emerging social Web of Things calls for graph data management with edge failures because smart things are normally moving and their connectivity could be intermittent, leading to frequent and unpredictable changes in the corresponding graph models [11, 25]. Another exam- ple is web graphs. It is not uncommon that some web links become invalid as the web evolves. All these are examples of unstable graphs, which are common in the real-world, calling for efficient graph computations by considering link failures. We believe that it is imperative to design novel algorithms that can compute shortest path indexes for fast response on distance queries avoiding any failed edge. Some real-world applications/scenarios that require the computation of shortest path distance avoiding a failed edge are de- scribed in the following. Scenario 1. The most vital arc problem [17, 6] aims to identify the edge on a given shortest path and the removal of this edge results in the longest replacement path. Here, a replacement path means a shortest path from a source vertex to a destination vertex in a graph that avoids a specified edge. To find the most vital arc in a graph, we need to compute the shortest path distances efficiently when we are given an arc (i.e., an edge) to avoid. Scenario 2. In the sensitivity analysis and in many analytical applications of transportation networks, govern- ment agencies need to evaluate different road segments (i.e., to find how much a road segment is worth) through Vickrey pricing [16], such that maintenance budget can be allocated accordingly, or the amount of tolls can be adjusted reason- ably [24]. For example, if tolls are not charged appropriately and avoiding an expensive toll point causes only a small de- tour, then it is more likely that most drivers would take the detour, rather than pay for the toll. Scenario 3. In order to develop game-theoretic and price-based mechanisms to share bandwidth and other network resources, a natural economic question is [16]: how much is an edge in a network worth to a user who wants to send data between two nodes along a shortest path? Or in other words, what is the penalty of avoiding an edge in the given network? These application scenarios reveal an urge for handling shortest path computations in a graph with single-edge fail- ures. Here, single-edge failure refers to graph failures with only one failed edge at a time [5]. Note that, other types of edge failure, such as dual-failure in [13], may allow multiple failed edges at a time. But they are considered much harder than single-failure [13]. To shed light on these challenging issues, we focus on single-edge failures in this paper. Contributions. Since 2-hop labeling has shown its power to support instant responses to shortest path distance queries on stable graphs, our work aims at extending this technique to support unstable graphs. Existing shortest path index- ing techniques based on 2-hop labeling can be used to pre- compute the whole shortest path index for a graph. The resulted indexes can normally answer distance queries fast using moderate storage space [2, 15]. However, applying in- dexing techniques designed for static/stable graphs directly to evolving/unstable graphs may lead to inefficiency. When considering every single-edge failure case and constructing a corresponding index for each case, the size of all these indexes will become too big to manage. For instance, a snapshot of the Gnutella peer-to-peer (P2P) file sharing network in August 2002 contains more than 6,000 vertices1 and 20,000 edges. Using state-of-the-art method, Pruned Land- mark Labeling (PLL) [2], the index size is slightly more than 5 MB. However, suppose we want to construct such index for each single-edge failure case, the total index size would be more than 5 X 20, 000 = 105 MB. To address the deficiency of existing shortest path indexing techniques, this paper proposes a generic framework named SIEF, a Supplemental Index for Edge Failures on a graph, to construct compact shortest path indexes efficiently for unstable graphs where single-edge failures may exist. As an initial attempt on this challenging issue, we focus on un- weighted, undirected graphs. Similar to other distance labeling based indexing methods [2, 15], our method can be extended to weighted and/or directed graphs. We highlight our main contributions in the following. ? We present the concept of well-ordering 2-hop distance labeling and identify its important properties that can be utilized to design algorithms for shortest path in- dexes on graphs with edge failures. ? We analyze shortest path index constructions on graphs with edge failures theoretically. We develop the corre- sponding theorems as well as novel algorithms to en- able constructions of compact indexes for all the single- edge failure cases of the entire graph. By applying our approach to the aforementioned Gnutella P2P dataset, the size of the generated SIEF index together with the original index created for the original graph is merely 14 MB, which is much more compact than 105 MB by directly using PLL method [2] to construct indexes for each single-edge failure case. ? We conduct extensive experiments on six real-world graphs to verify the efficiency and effectiveness of our method. The results show that our method can efficiently answer shortest path distance queries avoiding a failed edge with very compact labeling indexes. The rest of this paper is organized as follows. In Section 2, we review the related work. In Section 3, we present some preliminaries on 2-hop distance labeling. We then present the framework and the details of our approach in Section 4. In Section 5, we report the results of an extensive experi- mental study using six graphs from real-world. Finally, we present some concluding remarks in Section 6.",Yongrui Qin ,"School of Computer Science The University of Adelaide Adelaide, SA 5005, Australia",yongrui.qin@adelaide. edu.au,Quan Z. Sheng,"School of Computer Science The University of Adelaide Adelaide, SA 5005, Australia",michael.sheng@adelaide. edu.au,Wei Emma Zhang ,"School of Computer Science The University of Adelaide Adelaide, SA 5005, Australia",wei.zhang01@adelaide. edu.au,,,,,,,,,,,,,,,,,,,,,
20200309,779,Christina Pavlopoulou,"University of California, Riverside",cpavl001@ucr.edu,,A Parallel and Scalable Processor for JSON Data,"A Parallel and Scalable Processor for JSON Data, A Parallel and Scalable Processor for JSON Data, A Parallel and Scalable Processor for JSON Data, A Parallel and Scalable Processor for JSON Data, A Parallel and Scalable Processor for JSON Data, ABSTRACT Increasing interest in JSON data has created a need for its efficient processing. Although JSON is a simple data exchange format, its querying is not always effective, especially in the case of large repositories of data. This work aims to integrate the JSONiq ex- tension to the XQuery language specification into an existing query processor (Apache VXQuery) to enable it to query JSON data in parallel. VXQuery is built on top of Hyracks (a framework that generates parallel jobs) and Algebricks (a language-agnostic query algebra toolbox) and can process data on the fly, in con- trast to other well-known systems which need to load data first. Thus, the extra cost of data loading is eliminated. In this paper, we implement three categories of rewrite rules which exploit the features of the above platforms to efficiently handle path expressions along with introducing intra-query parallelism. We evaluate our implementation using a large (803GB) dataset of sensor readings. Our results show that the proposed rewrite rules lead to efficient and scalable parallel processing of JSON data. 1 INTRODUCTION The Internet of Things (IoT) has enabled physical devices, build- ings, vehicles, smart phones and other items to communicate and exchange information in an unprecedented way. Sophisticated data interchange formats have made this possible by leveraging their simple designs to enable low overhead communication be- tween different platforms. Initially developed to support efficient data exchange for web-based services, JSON has become one of the most widely used formats evolving beyond its original specification. It has emerged as an alternative to the XML format due to its simplicity and better performance [28]. It has been used frequently for data gathering [22], motion monitoring [20], and in data mining applications [24]. When it comes time to query a large repository of JSON data, it is imperative to have a scalable system to access and process the data in parallel. In the past there has been some work on building JSONiq add-on processors to enhance relational database systems, e.g. Zorba [2]. However, those systems are optimized for single-node processing. More recently, parallel approaches to support JSON data have appeared in systems like MongoDB [10] and Spark [7]. Nev- ertheless, these systems prefer to first load the JSON data and transform them to their internal data model formats. On the other hand systems like Sinew [29] and Dremel [27] cannot query raw JSON data. They need a pre-processing phase to convert the input file into a readable binary for them (typically Parquet [3]). They can then load the data, transform it to their internal data model and proceed with its further processing. The above efforts are ex- amples of systems that can process JSON data by converting it to their data format, either automatically, during the loading phase, or manually, following the pre-processing phase. In contrast, our JSONiq processor can immediately process its JSON input data without any loading or pre-processing phases. Loading large data files is a significant burden for the overall system 's execution time as our results will show in the experimental section. Although, for some data, the loading phase takes place only in the beginning of the whole processing, in most real-time applications, it can be a repetitive action; data files to be queried may not always been known in advance or they may be updated continuously. Instead of building a JSONiq parallel query processor from scratch, given the similarities between JSON and XQuery, we decided to take advantage of Apache VXQuery [4, 17], an ex- isting processor that was built for parallel and scalable XQuery processing. We chose to support the JSONiq extension to XQuery language [8] to provide the ability to process JSON data. XQuery and JSONiq have certain syntax conflicts that need to be resolved for a processor to support both of them, so we enhanced VX- Query with the JSONiq extension to the XQuery language, an alteration of the initial JSONiq language designed to resolve the aforementioned conflicts [9]. In extending Apache VXQuery, we introduce three categories of JSONiq rewrite rules (path expression, pipelining, and group-by rules) to enable parallelism via pipelining and to minimize the required memory footprint. A useful by-product of this work is that the proposed group-by rules turn out to apply to both XML and JSON data querying. Through experimentation, we show that the VXQuery proces- sor augmented with our JSoniq rewrite rules can indeed query JSON data without adding the overhead of the loading phase used by most of the state-of-the art systems. The rest of the paper is organized as follows: Section 2 presents the existing work on JSON query processing, while Section 3 out- lines the architecture of Apache VXQuery. Section 4 introduces the specific optimizations applied to JSON queries and how they have been integrated into the current version of VXQuery. The experimental evaluation appears in Section 5. Section 6 concludes the paper and presents directions for future research.",Christina Pavlopoulou,"University of California, Riverside",cpavl001@ucr.edu,"E. Preston Carman, Jr","University of California, Riverside",ecarm002@ucr.edu,Till Westmann,Couchbase,tillw@apache.org,Michael J. Carey,"University of California, Irvine",mjcarey@ics.uci.edu,Vassilis J. Tsotras,"University of California, Riverside",tsotras@cs.ucr.edu,,,,,,,,,,,,,,,
20200310,1250,Thanh T. L. Tran,"Department of Computer Science University of Massachusetts, Amherst",ttran@cs.umass.edu,,PODS: A New Model and Processing Algorithms for Uncertain Data Streams,"PODS: A New Model and Processing Algorithms for Uncertain Data Streams, PODS: A New Model and Processing Algorithms for Uncertain Data Streams, PODS: A New Model and Processing Algorithms for Uncertain Data Streams, PODS: A New Model and Processing Algorithms for Uncertain Data Streams, PODS: A New Model and Processing Algorithms for Uncertain Data Streams, ABSTRACT Uncertain data streams, where data is incomplete, imprecise, and even misleading, have been observed in many environments. Feed- ing such data streams to existing stream systems produces results of unknown quality, which is of paramount concern to monitoring applications. In this paper, we present the PODS system that sup- ports stream processing for uncertain data naturally captured using continuous random variables. PODS employs a unique data model that is flexible and allows efficient computation. Built on this model, we develop evaluation techniques for complex relational operators, i.e., aggregates and joins, by exploring advanced statistical theory and approximation. Evaluation results show that our techniques can achieve high performance while satisfying accuracy requirements, and significantly outperform a state-of-the-art sampling method. A case study further shows that our techniques can enable a tornado detection system (for the first time) to produce detection results at stream speed and with much improved quality. Categories and Subject Descriptors H.2 [Database Management]: Systems General Terms Algorithms, Design, Performance, Theory 1. INTRODUCTION Uncertain data streams, where data is incomplete, imprecise, and even misleading, have been observed in a variety of environments, such as sensor networks measuring temperature and light [9, 14], ra- dio frequency identification (RFID) networks [17, 29], GPS systems [18], and weather radar networks [20]. As these data streams are col- lected by monitoring applications, they often undergo sophisticated query processing to derive useful high-level information. However, feeding uncertain data streams directly to existing stream systems can produce results of unknown quality. This issue is of paramount concern to monitoring applications that trigger actions based on the derived information. Our work is particularly motivated by two emerging applications. The first is object tracking and monitoring using RFID readers [29]. RFID data streams are highly noisy due to the sensitivity of sensing to the orientation of reading and environmental factors such as metal objects and interference. When such streams are used to detect, for instance, safety violations regarding flammable objects, the quality of the alerts raised is a critical issue to the end application. The second application is tornado detection [20], where meteorological data streams are collected from a radar network and processed in a real-time stream system. Data uncertainty can arise from environmental noise, device noise, and inaccuracies of vari- ous radar components. Such uncertainty can propagate through the entire stream system, making tornado detection results error-prone. Given the potential social impact of such a system, it is absolutely vital that the system capture the quality of its detection results. In this paper, we address uncertain data stream processing for data that is naturally modeled using continuous random variables, such as many types of sensor data and financial data. Given such data, our work supports relational query processing under uncertainty. For each relational operator, we aim to fully characterize the distribution of each tuple produced from uncertain data. Such distributions, called result tuple distributions, allow the visualization of data uncertainty in any step of the processing. They also allow the stream system to feed the tuples output from one operator as input to another and characterize the results of further processing a evidently, having only statistics such as mean and variance for the tuples output from the previous operator is not enough to do so. Challenges. Uncertain data stream processing as described above raises two challenges: First, it is computationally difficult to obtain result distributions when input tuples are modeled using continuous random variables. Such computation often involves multivariate integrals or requires new algorithms to be designed if an integral- based solution does not exist. Second, such computation must be performed for high-volume data streams. While approximation is a common approach to improving efficiency, the technique must be able to achieve a small bounded error while meeting stringent performance requirements. Despite a flurry of recent work on uncertain data management, the two challenges stated above have not been adequately addressed. Most probabilistic databases [2, 3, 6, 24, 30] and stream systems [7, 16, 19] model tuples using discrete random variables and evaluate queries using the possible worlds semantics. The continuous nature of our data, however, precludes the use of these techniques as the possible values of a continuous random variable are infinite and cannot be enumerated. The state-of-the-art techniques for continuous random variables 159 employ either multivariate integrals or Monte Carlo simulation. The integral-based approach to aggregation [6] performs n-1 integrals to compute the sum of n tuples. While the result is exact, its com- putation is too slow for stream processing, as we shall show later in this paper. The Monte Carlo approach [12, 15, 25] samples from the input tuple distributions and computes the result tuple distribu- tion from the samples. For real-world data, however, it is difficult, sometimes impossible, to know right the number of samples needed to guarantee both accuracy and efficiency for complex relational operations, as we also show in our performance study. Our work presented in this paper originates from our belief that for a significant fragment of relational algebra, faster and more accurate algorithms are possible. Then these algorithms can be used to improve existing Monte Carlo systems when queries involve the set of common relational operations that these algorithms support. Scope and contributions. In this paper, we present a PrObabilistic Data Stream system, which we call PODS, that supports relational processing of uncertain data streams modeled using continuous random variables. The architectural design of PODS, as described in [11], is to extend the box-arrow paradigm for stream processing [4] such that tuples carry distributions to describe uncertainty and relational operators transform these distributions while processing tuples. This paper, in particular, focuses on the data model and processing algorithms for two complex operators, aggregates and joins. 1 These operators are crucial to our target applications but have not been sufficiently addressed. Further in the streaming context, our goal is to perform such complex operations at high speed, e.g., thousands of tuples per second or higher. More specifically, our contributions include: Data model. The foundation of PODS is a unique data model based on Gaussian Mixture distributions. This model is highly flexi- ble as it subsumes Gaussian distributions and can model arbitrary real-world distributions [21]. It further allows efficient computation by exploiting Gaussian properties and powerful statistical methods for continuous variables. Most importantly, this model allows a closed-form solution for a range of relational operations, among which this paper focuses on aggregates and joins, and the result distributions of these operations still obey Gaussian Mixture distri- butions. Our model stands in contrast to those based on histograms [12] and weighted particles [18], which indicate the use of samples in computation. Aggregates. Our data model empowers us to design novel al- gorithms for aggregates, such as sum and avg, that are grounded in statistical theory. Our first algorithm obtains exact result dis- tributions of aggregates while completely eliminating the use of integrals (in contrast to using multiple integrals in [6]). However, the formulas for result distributions produced by the exact algorithm grow exponentially in the number of aggregated tuples. Hence, we provide two approximation methods to simplify the formulas for result distributions. These techniques, when combined into a hybrid solution, can satisfy arbitrary application accuracy requirements while achieving high speed in stream processing. Joins. Our data model also enables efficient, accurate evalua- tion techniques for joins. We propose two types of joins to suit different application semantics. The first type models equi-joins on continuous-valued uncertain attributes as a join of an input stream and a probabilistic view. PODS supports such joins with efficient regression techniques to construct the view and given the view, a closed-form solution in Gaussian Mixture Models to represent result distributions. The second (traditional) type of join pairs tuples from two inputs for inequality comparison and is modeled by a cross- product followed by a selection. PODS supports such joins with exact result distributions and methods for pruning tuples with low existence probabilities. Evaluation. We perform a thorough evaluation of our techniques for joins and aggregates, and compare them with sampling-based methods ([12] for aggregates and a home-grown method for joins). Results of this study demonstrate our advantages in both accuracy and speed over the sampling-based methods, due to the use of our data model and techniques for continuous random variables. We further perform a case study of tornado detection by feeding a trace collected from a real tornadic event into the PODS system. Our results show that fitting the data to the PODS model and using its processing techniques to characterize uncertainty enables the tornado detection algorithm (for the first time) to produce detection results at stream speed with much improved quality. ",Thanh T. L. Tran,"Department of Computer Science University of Massachusetts, Amherst",ttran@cs.umass.edu,Liping Peng,"Department of Computer Science University of Massachusetts, Amherst",lppeng@cs.umass.edu,Boduo Li,"Department of Computer Science University of Massachusetts, Amherst",boduo@cs.umass.edu,Yanlei Diao,"Department of Computer Science University of Massachusetts, Amherst",yanlei@cs.umass.edu,Anna Liu,"Department of Mathematics and Statistics University of Massachusetts, Amherst",anna@math.umass.edu,,,,,,,,,,,,,,,
20200311,1314,Xin Cao,"School of Computer Engineering, Nanyang Technological University, Singapore",xcao1@e.ntu.edu.sg,,Keyword-aware Optimal Route Search,"Keyword-aware Optimal Route Search, Keyword-aware Optimal Route Search, Keyword-aware Optimal Route Search, Keyword-aware Optimal Route Search, Keyword-aware Optimal Route Search, ABSTRACT Identifying a preferable route is an important problem that finds ap- plications in map services. When a user plans a trip within a city, the user may want to find ""a most popular route such that it passes by shopping mall, restaurant, and pub, and the travel time to and from his hotel is within 4 hours."" However, none of the algorithms in the existing work on route planning can be used to answer such queries. Motivated by this, we define the problem of keyword- aware optimal route query, denoted by KOR, which is to find an optimal route such that it covers a set of user-specified keywords, a specified budget constraint is satisfied, and an objective score of the route is optimal. The problem of answering KOR queries is NP-hard. We devise an approximation algorithm OSScaling with provable approximation bounds. Based on this algorithm, another more efficient approximation algorithm BucketBound is proposed. We also design a greedy approximation algorithm. Results of empirical studies show that all the proposed algorithms are capable of answering KOR queries efficiently, while the BucketBound and Greedy algorithms run faster. The empirical studies also offer in- sight into the accuracy of the proposed algorithms. 1. INTRODUCTION Identifying a preferable route in a road network is an important problem that finds applications in map services. For example, map applications like Baidu Lvyou 1 and Yahoo Travel 2 offer tools for trip planning. However, the routes that they provide are collected from users and are thus pre-defined. This is a significant deficiency since there may not exist any pre-defined route that meets the user needs. The existing solutions (e.g., [16, 17, 22]) for trip planning or route search are often insufficient in offering the flexibility for users to specify their requirements on the route. Consider a user who wants to spend a day exploring a city. She is not familiar with the city and she might pose such a query: ""Find the most popular route to and from my hotel such that it passes by shopping mall, restaurant, and pub, and the time spent on the road in total is within 4 hours.The example query above has two hard constraints: 1) the points of interests preferred by the user, as expressed by a set of keywords that should be covered in the route (e.g., ""shopping mall"", ""restaurant"" and ""pub""); 2) a budget constraint (e.g., travel time) that should be satisfied by the route. The query aims to identify the optimal route under the two hard constraints, such that an objective score is optimized (e.g., route popularity [4]). Note that route popularity can be estimated by the number of users traveling a route, obtained from the user traveling histories recorded in sources such as GPS trajectories or Flickr photos [4]. In general, the budget constraint and the objective score can be of various different types, such as travel duration, distance, popularity, travel budget, etc. We consider two different attributes for budget constraint and objective score because users often need to balance the trade-off of two as- pects when planning their trips. For example, a popular route may be quite expensive, or a route with the shortest length is of little in- terests. In the example query, it is likely that the most popular route requires traveling time more than 4 hours. Hence, a route search- ing system should be able to balance such trade-offs according to users' different preferences. We refer to the aforementioned type of queries as keyword-aware optimal route query, denoted as KOR. Formally, a KOR query is defined over a graph G, and the input to the query consists of five parameters, vs, vt, , ?, and f , where vs is the source location of the route in G, vt is the target location,  is a set of keywords, ? is a budget limit, and f is a function that calculates the objective score of a route. The query returns a path R in G starting at vs and ending at vt, such thatRminimizes f(R) under the constraints thatR satisfies the budget limit ? and passes through locations that cover the query keywords in . To the best of our knowledge, none of the existing work on trip planning or route search (e.g., [16, 17, 22]) is applicable for KOR queries. Furthermore, the problem of solving KOR queries can be shown to be NP-hard by a reduction from the weighted constrained shortest path problem [8]. It can also be viewed as a generalized traveling salesman problem [11] with constraints. This leads to an interesting question: is it possible to derive efficient solutions to answering KOR queries? Due to the hardness of answering KOR queries, in this paper, we answer the aforementioned question affirmatively with three ap- proximation algorithms. The first approximation algorithm has a performance bound and is denoted by OSScaling. In OSScaling, we first scale the objective value of every edge to an integer by a parameter ? to obtain a scaled graph denoted by GS . Specifically, in the scaled graph GS , each partial route is represented by a ""label"", which records the query keywords already covered by the partial route, the scaled objective score, the original objective score, and the budget score of the route. At each node, we maintain a list of ""useful"" labels corresponding to the routes that go to that node. Starting from the source node, we keep creating new partial routes by extending the current ""best"" partial route to generate new la- bels, until all the potentially useful labels on the target node are generated. Finally, the route represented by the label with the best objective score at the target node is returned. We prove that the algorithm returns routes with objective scores no worse than 1 1?? times of that of the optimal route. The worst case complexity of OSScaling is polynomial with 1 ? , the budget constraint ?, the number of edges and nodes in G, and it is expo- nential in the number of query keywords, which is usually small in our targeted applications, as it is well known that search engine queries are short, and an analysis on a large Map query log [25] shows that nearly all queries contain fewer than 5 words. Our second algorithm improves on the algorithm OSScaling, which is referred to as BucketBound. It also returns approximate solutions to KOR queries with performance guarantees. However, it is more efficient than OSScaling. The algorithm can always return a route whose objective score is at most  (  > 1 is a parameter) times of the one found by OSScaling. The algorithm divides the traversed partial routes into different ""buckets"" according to the best possible objective scores they can achieve. This enables us to develop a novel way to detect if a feasible route (covering all query keywords and satisfying the budget constraint) is in the same bucket with the one found by OSScaling. When we find a feasible route that falls in the same bucket as the route found by OSScaling, we return it as the result. Finally, we also present a greedy approach for the problem. From the starting location, we keep selecting the next location greedily, taking into account all the three constraints in the KOR query. This is repeated until we reach the target location. This algorithm is efficient, although it may generate a route that violates the two hard constraints of KOR: covering all query keywords and satisfying the budget constraint. In summary, our contributions are threefold. First, we propose the keyword-aware optimal route (KOR) query, and we show that the problem of solving KOR queries is NP-hard. Second, we present two novel approximation algorithms both with provable performance bounds for the KOR problem. We also provide a greedy approach. Third, we study the properties of the paper's proposals empirically on a graph extracted from a large collection of Flickr photos. The results demonstrate that the proposed solutions offer scalability and excellent performance. The rest of the paper is organized as follows: Section 2 formally defines the problem and establishes the computational complexities of the problem. Section 3 presents the proposed algorithms. We report on the empirical studies in Section 4. Finally, we cover the related work in Section 5 and offer conclusions in Section 6.",Xin Cao,"School of Computer Engineering, Nanyang Technological University, Singapore",xcao1@e.ntu.edu.sg,Lisi Chen,"School of Computer Engineering, Nanyang Technological University, Singapore",lchen012@e.ntu.edu.sg,Gao Cong,"School of Computer Engineering, Nanyang Technological University, Singapore",gaocong@ntu.edu.sg,Xiaokui Xiao,"School of Computer Engineering, Nanyang Technological University, Singapore",xkxiao@ntu.edu.sg,,,,,,,,,,,,,,,,,,
20200312,1118,Nodira Khoussainova,"Department of Computer Science and Engineering, University of Washington, Seattle, WA, USA",nodira@cs.washington.edu,,PerfXplain: Debugging MapReduce Job Performance,"PerfXplain: Debugging MapReduce Job Performance, PerfXplain: Debugging MapReduce Job Performance, PerfXplain: Debugging MapReduce Job Performance, PerfXplain: Debugging MapReduce Job Performance, PerfXplain: Debugging MapReduce Job Performance,  ABSTRACT While users today have access to many tools that assist in performing large scale data analysis tasks, understanding the performance characteristics of their parallel computations, such as MapReduce jobs, remains difficult. We present PerfXplain, a system that en- ables users to ask questions about the relative performances (i.e., runtimes) of pairs of MapReduce jobs. PerfXplain provides a new query language for articulating performance queries and an algo- rithm for generating explanations from a log of past MapReduce job executions. We formally define the notion of an explanation together with three metrics, relevance, precision, and generality, that measure explanation quality. We present the explanation-generation algorithm based on techniques related to decision-tree building. We evaluate the approach on a log of past executions on Amazon EC2, and show that our approach can generate quality explanations, out- performing two na-?ve explanation-generation methods. 1. INTRODUCTION The volume of data collected by businesses today is rapidly in- creasing. This data includes web crawls, search logs, click streams, network monitoring logs, and others. At the same time, tools for analyzing that data are becoming increasingly powerful and easy to use. Examples include parallel database management systems [27, 39, 47], MapReduce-based systems [14, 19, 28, 31, 40, 41], and others [9, 13, 33, 46, 52]. Large-scale clusters for carrying out this analysis are also becoming common-place. As a result, vast vol- umes of data are analyzed every day by a large variety of users. Increasingly, users who write the MapReduce programs [19, 28], Pig Latin scripts [40], or declarative queries (e.g., HiveQL [31] or SQL) to analyze the data are not experts in parallel data processing, but are experts in some other domain. They need to ask a variety of questions on their data and these questions keep changing. For these users to be successful, they need to be self-sufficient in their data analysis endeavours. They cannot rely on administrators or distributed systems experts to help them debug and tune their anal- ysis workloads, because there simply are not enough experts. While most users already have tools to test and debug the correct- ness of their SQL queries or MapReduce programs before running them at massive scale, there are limited tools to help understand, diagnose, and debug any performance problems. The performance of parallel programs can be challenging to understand. As an example, when a user runs a MapReduce job and the job seems to take an abnormally long time, the user has no easy way of knowing if the problem is coming from the cluster (e.g., high load or machine failures), from some configuration parameters, from the job itself, or from the input data. In this paper, we present PerfXplain, a system that assists users in debugging the performance of MapReduce applications in a shared-nothing cluster. PerfXplain lets users formulate perfor- mance queries in its own language called the PerfXplain Query Language (PXQL). A PXQL query identifies two MapReduce jobs or tasks. Given the pair of jobs (tasks), the query can inquire about their relative performances: e.g., Why did two MapReduce jobs take the same amount of time even though the second one processed half the data? Why was the last task in a MapReduce job faster than any of the other tasks in that job? Given a query in PXQL, PerfXplain automatically generates an explanation for this query. Informally, an explanation consists of two predicates that hold true about the pair of identified execu- tions. The first predicate, which we refer to as the despite clause, maximizes the probability of seeing the expected behavior. Meanwhile, the second predicate, called the because clause, maximizes the probability of the observed behavior. For example, if a user asks ""why was the last task in this MapReduce job faster than any of the other tasks"", an explanation might be: ""even though the last task processed the same amount of data as the other tasks (despite clause), it was faster most likely because the overall memory utilization on the machine was lower (because clause) when it exe- cuted"". When the predicate in the despite clause is true, a pair of tasks typically has the same runtime. Within that context, the be- cause clause then explains why the user observed a performance different than anticipated. The despite clause thus helps ensure that the explanation given by the because clause is relevant to the identified pair of tasks, rather than just producing a generally-valid argument. Hence, unlike prior work, which focused on predicting relational query performance [25, 26], predicting MapReduce job perfor- mance [24, 37, 38], automatically tuning MapReduce jobs [10, 20, 29, 30, 34] or relational queries [6, 8, 15, 16], and automatically di- agnosing failures [21], the goal of PerfXplain is to explain the per- formance similarity or difference between pairs of MapReduce job or task executions. In this paper, we focus on explaining runtimes, but our approach can directly be applied to other performance met- rics. Additionally, while our implementation and evaluation focus on MapReduce jobs, PerfXplain represents the execution of a sin- gle job or task as a vector of features, where each configuration parameter and runtime metric is a feature. As such, the approach is more broadly applicable. PerfXplain uses machine learning to generate explanations. All performance queries in PerfXplain take the following form: The user specifies what behavior he or she expected (e.g., ""I expected the last task to take the same amount of time as the others""), option- ally why the user expected that behavior (e.g., ""all tasks executed the same join algorithm""), and what behavior the user observed (e.g., ""the last task was faster than the others""). To produce its explanations, PerfXplain utilizes a log of past MapReduce job ex- ecutions along with their detailed configuration and performance metrics. Given a PXQL query, PerfXplain, identifies positive examples (pairs of jobs/tasks that performed as the user expected), and negative examples (pairs of jobs/tasks that performed as the user observed). From these examples, PerfXplain learns both the most likely reason why the pair should have performed as expected and, within that context, the most likely cause why the pair per- formed as observed. PerfXplain generates explanations from these two models. The key challenge for generating these explanations is to ensure that every explanation is highly precise, and at the same time as general as possible so that the user can apply this newly acquired knowledge to other scenarios. Overall, we make the fol- lowing contributions: 1. We propose a simple language, PXQL, for articulating queries about the performance of a pair of MapReduce jobs or tasks (Sections 3.1 and 3.2). 2. We formally define the notion of a performance explanation and three metrics relevance, precision, and generality to as- sess the quality of an explanation (Section 3.3). 3. We develop an approach for efficiently extracting performance explanations that have high relevance, high precision, and good generality from a log of past MapReduce job exe- cutions (Section 4). 4. We evaluate the approach using a log of MapReduce jobs executed on Amazon EC2 [1]. We show that PerfXplain is able to generate explanations with higher precision than two na-?ve explanation-generation techniques, and offer a better trade-off between precision and generality (Section 6).",Nodira Khoussainova,"Department of Computer Science and Engineering, University of Washington, Seattle, WA, USA",nodira@cs.washington.edu,Magdalena Balazinska,"Department of Computer Science and Engineering, University of Washington, Seattle, WA, USA",magda@cs.washington.edu,Dan Suciu,"Department of Computer Science and Engineering, University of Washington, Seattle, WA, USA",suciu@cs.washington.edu,,,,,,,,,,,,,,,,,,,,,
20200313,1363,Nikos Tsikoudis,Brandeis University,tsikudis@cs.brandeis.edu,,RQL: Retrospective Computations over Snapshot Sets,"RQL: Retrospective Computations over Snapshot Sets, RQL: Retrospective Computations over Snapshot Sets, RQL: Retrospective Computations over Snapshot Sets, RQL: Retrospective Computations over Snapshot Sets, RQL: Retrospective Computations over Snapshot Sets, ABSTRACT Applications need to analyze the past state of their data to provide auditing and other forms of fact checking. Retrospective snapshot systems that support computations over data store snapshots, allow applications using simple data stores like Berkeley DB or SQLite, to provide past state analysis in a convenient way. Cur- rent snapshot systems however, offer no satisfactory support for computations that analyze multiple snapshots. We have devel- oped a Retrospective Query Language (RQL), a simple declarative extension to SQL that allows to specify and run multi-snapshot computations conveniently in a snapshot system, using a small number of simple mechanisms defined in terms of relational con- structs familiar to programmers. We describe RQL mechanisms, explain how they translate into SQL computations in a snapshot system, and show how to express a number of common analysis patterns with illustrative examples. We also describe how we implemented RQL in a simple way utilizing SQLite UDF framework in a Berkeley DB data store using Retro page-level incremen- tal snapshot system. Multi-snapshot computations running over page-level incremental snapshots bring up interesting performance issues that have not been studied before. We present the first study defining a performance envelope for multi-snapshot computations over page-level incremental snapshots. 1 INTRODUCTION To provide auditing and other forms of claim checking more and more applications need to answer questions, often formulated after the fact, about the past states of their data. To free applications from the burden of managing past states on their own, data management systems need to run ad-hoc computations over past states of the objects they store. Computations over past states have been long supported by temporal databases, used by applications in specialized domains but not used by general applications because of cost and perfor- mance penalty for in-production operation. More recently, cheap storage and interest in using past state analytics for in-production operation led to development of systems that integrate past state analytics in a database [7, 11, 13], and all major vendors today offer products providing OLTP and OLAP processing in a single system [16]. These products however are not a good match for In- ternet applications that store their data in simple key value stores such as Berkeley DB (BDB) [15] or SQLite [8] and need past state analysis for on-line historical claim checking or auditing. Today however, even applications using key value stores can support past state analysis using snapshot systems that support retro- spection, the ability of a data store to run queries over consistent snapshots of application past state as if they were the current state [22]. Retrospection makes it easy for programmers to provide expressive past state analysis since it allows to implement ad-hoc queries as general programs in the application language using the application code base and then run these programs on the snapshots of interest. For example, Retro [21], a snapshot system of BDB, allows to analyze the state of BDB SQLite appli- cations at a particular point in time simply by running SQLite queries over the corresponding BDB snapshot. As convenient as it is to analyze a single point in time using a snapshot system, many analyses concern multiple data points. Retro and other snapshot systems come short when it comes to analyzing multiple snapshots. A programmer needs to write a C script that manually identifies snapshots of interest, queries each snapshot separately, manually collects the results, and then processes the results. This approach is cumbersome, error prone and onerous for a SQL programmer who needs to learn a new language. The programmer would much prefer to specify the computation in a declarative manner using the language of the application. To help with programming the desired computation logic for multiple snapshot analysis, we have developed a Retrospective Query Language (RQL), a simple declarative extension to SQL that allows to specify and run multi-snapshot computations with- out the need to use a low-level script. RQL mechanisms combine in a modular way high-level relational constructs to express gen- eral SQL computations over arbitrary sets of past BDB SQLite application snapshots. The constructs specify in SQL the set of snapshots that identify the past states of interest, the computa- tions over each snapshot, and the computations that process the results. We describe RQL mechanisms and explain how each high- level mechanism translates into a SQL computation over multiple snapshots in the Retro snapshot system. We also show how to express a number of common analysis patterns with illustrative examples.We then describe howwe implemented RQL in a simple way using SQLite UDF. RQL programs bring to light important performance considera- tions that arise when programs compute over multiple snapshots. For one, RQL mechanisms allow to specify computations over arbitrary size sets of snapshots. The number of snapshots stored by a snapshot system such as Retro, only limited by available storage, can be very high given today 's low storage costs. Each snapshot includes the entire state of the database. RQL program therefore can compute over potentially very large amounts of data. A programmer needs to know how much CPU, memory and I/O resources his program requires, especially in today 's utility computing environments. Furthermore, an important per- formance consideration in the design of snapshot systems like Retro is to avoid interfering with the data store performance so that snapshots can be created at required frequency without blocking or disrupting in-production application performance. Retro snapshot system achieves this by using a low-cost copy-on- write technique that creates an incremental page-level snapshot representation with a compact snapshot index [22, 23]. Such rep- resentation is known to be slower to compute with but the slow- down is considered to be an acceptable trade-off to preserve in- production performance. The reason a computation runs slower over a snapshot and incurs higher resource costs compared to Industrial and Applications Paper the current database state is that the snapshot state needs to be assembled on-the-fly from the incremental representation. Inherently however, with a snapshot representation created using copy-on-write, consecutive snapshots share large parts of their state. An RQL program that iterates computations over multiple consecutive snapshots can therefore reduce its costs and improve performance by assembling such a shared state only once. The performance of programs computing over multiple page-level copy-on-write snapshots however has not been studied before. To this end, we implemented RQL in SQLite BDB using Retro snapshot system and conducted an experimental study that characterizes the performance envelope of RQL programs. The goal of the study is to explain the performance in a way that is easy to understand by the programmer. Since the programmer specifies RQL mechanisms as a modular composition of relational constructs in SQLite, we relate the performance of RQL program to the performance of its SQL components, a cost that should be familiar to the programmer. Our experiments use workloads derived from the standard TPC-H benchmark to evaluate RQL performance so that even though our system is different from other past state systems, its performance is explained using a standard workload. In summary, the paper makes the following contributions: ? RQL, a SQLite extension that allows to express compu- tations over multiple snapshots in a convenient way in the language of the application. Our focus here is SQL extension but we believe a similar approach can be used for BDB applications in other languages since the BDB/Retro system is language-independent. ? An implementation of RQL system using SQLite UDF. ? A performance study of RQL programs including the first analysis explaining the performance of computations running over multiple page-level copy-on-write snapshots. While the performance results reported in our study are specific to our system, our performance analysis is more general and applies to other page-level copy-on-write snapshot systems. The rest of the paper is organized as follows. Section 2 describes the RQL mechanisms, Section 3 outlines the salient points of RQL implementation using SQLite UDF, Section 4 briefly outlines the basic structure of the copy-on-write snapshot system Retro, providing the background for our performance analysis, Section 5 describes the experimental study, Section 6 considers the related work, Section 7 concludes.",Nikos Tsikoudis,Brandeis University,tsikudis@cs.brandeis.edu,Liuba Shrira,Brandeis University,liuba@cs.brandeis.edu,Sara Cohen,"Hebrew University, Jerusalem",sara@cs.huji.ac.il,,,,,,,,,,,,,,,,,,,,,
20200314,1364,Mohamed A. Soliman,"Greenplum San Mateo, USA",mohamed.soliman@emc.com,,Ranking with Uncertain Scoring Functions: Semantics and Sensitivity Measures,"Ranking with Uncertain Scoring Functions: Semantics and Sensitivity Measures, Ranking with Uncertain Scoring Functions: Semantics and Sensitivity Measures, Ranking with Uncertain Scoring Functions: Semantics and Sensitivity Measures, Ranking with Uncertain Scoring Functions: Semantics and Sensitivity Measures, Ranking with Uncertain Scoring Functions: Semantics and Sensitivity Measures, ABSTRACT Ranking queries report the top-K results according to a user-defined scoring function. A widely used scoring func- tion is the weighted summation of multiple scores. Often times, users cannot precisely specify the weights in such functions in order to produce the preferred order of results. Adopting uncertain/incomplete scoring functions (e.g., us- ing weight ranges and partially-specified weight preferences) can better capture user's preferences in this scenario. In this paper, we study two aspects in uncertain scor- ing functions. The first aspect is the semantics of ranking queries, and the second aspect is the sensitivity of computed results to refinements made by the user. We formalize and solve multiple problems under both aspects, and present novel techniques that compute query results efficiently to comply with the interactive nature of these problems. Categories and Subject Descriptors H.2.4 [Database Management]: Systems General Terms Algorithms, Design, Experimentation, Performance Keywords Uncertainty, Scoring, Top-k, Ranking, Aggregation 1. INTRODUCTION Scoring (ranking) functions are among the most common forms of preference specification. A prominent application scenario is joining multiple data sources and ranking join results according to some score aggregation function. The class of queries captured by this scenario is usually referred to as rank join [8], where the objective is to compute the top-K join results based on a given scoring function. The order of rank join results depends on the chosen score aggregation function. In the simplest but very common case, a linear aggregation function is adopted, which is specified as a weighted sum of scores. For example, Figure 1 shows a rank join query, where Restaurant-Hotel join results are ranked based on a weighted sum of the rating and the number of stars, while reporting only the top 5 join results. 1.1 Motivation and Challenges Often times users cannot precisely specify the weights of the scoring function (e.g., wR and wH in Figure 1) in order to produce the preferred order of results. This problem is usually handled either by the user in an interactive trial- and-error manner, or by the machine through learning from user's feedback (e.g., learning weights from user's preference judgment on object pairs [16]). Both approaches have serious limitations. Trial-and-error is a tedious and a timeconsuming process that can be very challenging especially to novice users. On the other hand, weight learning requires a sufficiently large number of user-provided training examples, which can be too demanding. In many scenarios, adopting an uncertain/incomplete preference specification language can better capture user's preferences. For example, it is much easier for users to pro- vide approximate weights (e.g., ranges) and other hints (e.g., relative importance of the weights) than specifying exact weights. This does not change the goal, which is obtaining a representative ordering of results satisfying user's preferences, but rather provides a flexible and more natural means of preference specification. Even when a user provides what are believed to be the right weights, it is crucial to analyze the sensitivity of the computed ordering with respect to changes in the given weights. This can provide significant help in data analy- sis in interactive and exploratory scenarios. For example, when fine-tuning the scoring function, a user may be inter- ested in quantifying the largest change in the weights that does not introduce perturbations in the computed ordering. We discuss the previous observations using Figure 2, which shows 4 join results for the query in Figure 1. To illustrate the consequences of uncertain weights, as- sume that the non-negative weights wR and wH are uncer- tain and normalized to add up to 1. We adopt the convention that results are sorted in descending order of scores. There are 5 possible orderings \lamda1, . . . ,\lamda5 corresponding to all the possible weight vectors (we only show wR as wH = 1?wR). While wR is continuous in [0,1], only a finite number of pos- sible orderings exists. These orderings depend on the ranges of wR and wH as well as the scores of the join results. Each ordering has properties related to how robust the ordering is wrt. the weight space, and its relationship to other orderings (we give more details in Section 2.2). To illustrate the importance of quantifying the sensitivity of a specific ordering or weight vector, assume that a user specifies (wR, wH) as (0.16, 0.84). The corresponding top-2 results are thus \lamda3, \lamda1. By changing the weights slightly to (0.17, 0.83) (i.e., wR increased by only 6%), the top-2 results become \lamda3, \lamda2. Such sensitivity of the computed ordering to small weight changes may be important to quantify in interactive data analysis. We advocate the need to model and manage uncertainty in scoring functions. We envision uncertain scoring functions as a means to bridge the gap between imprecise specifica- tions, which are more natural in describing user's prefer- ences, and score-based ranking models, which are based on precise formulations of scoring functions. We identify two important problems pertinent to these settings: ? Finding a Representative Ordering. When weights are uncertain, a set of possible orderings is induced. The problem is finding a representative ordering \lamda? with plausible properties that can be given to the user as an answer. Moreover, when relative preferences among weights are specified (e.g., in Figure 2, the influence of rating can be further emphasized by adding the con- straint wR > wH), finding a representative ordering satisfying such preferences is imperative. ? Quantifying Sensitivity. When weights are given as input, a corresponding ordering \lamda can be computed. Two sensitivity analysis problems arise. The first prob- lem is quantifying the largest weights change, in the neighborhood of the input weight vector, that does not introduce perturbations in \lamda. The second problem is quantifying the likelihood of obtaining an ordering identical to \lamda, given a random weight vector. 1.2 Contributions and Paper Organization Our key contributions are summarized as follows: ? We formulate four novel problems in the context of un- certain scoring functions. We propose multiple query semantics to allow users to reason about the reported answers (Section 2). ? We propose two approaches to compactly represent the set of possible orderings induced by an uncertain scor- ing function. We give a bound on the number of possi- ble orderings, and present several efficient techniques to compute representative orderings under different se- mantics (Section 3). ? We introduce a generalization of our methods to han- dle partially-specified preferences on the scoring func- tion weights (Section 4). ? We present efficient techniques for quantifying the sen- sitivity of computed results with respect to changes in the input weights (Section 5). We also conduct an extensive experimental study (Sec- tion 6) on real and synthetic data to evaluate the effective- ness and scalability of our solution.",Mohamed A. Soliman,"Greenplum San Mateo, USA",mohamed.soliman@emc.com,Ihab F. Ilyas,"University of Waterloo Waterloo, Canada",ilyas@uwaterloo.ca,Davide Martinenghi,"Politecnico di Milano Milano, Italy",davide.martinenghi@polimi.it,Marco Tagliasacchi,"Politecnico di Milano Milano, Italy",marco.tagliasacchi@polimi.it,,,,,,,,,,,,,,,,,,
20200315,1129,Panagiotis Bouros,"Department of Computer Science
University of Hong Kong
Pokfulam Road, Hong Kong",pbouros@cs.hku.hk,,SpatioTextual Similarity Joins,"Spatio-Textual Similarity Joins, Spatio-Textual Similarity Joins, Spatio-Textual Similarity Joins,Spatio-Textual Similarity Joins, Spatio-Textual Similarity Joins, ABSTRACT Given a collection of objects that carry both spatial and textual information, a spatio-textual similarity join retrieves the pairs of objects that are spatially close and textually similar. As an example, consider a social network with spatially and textually tagged per- sons (i.e., their locations and profiles). A useful task (for friendship recommendation) would be to find pairs of persons that are spatially close and their profiles have a large overlap (i.e., they have common interests). Another application is data de-duplication (e.g., finding photographs which are spatially close to each other and high overlap in their descriptive tags). Despite the importance of this operation, there is very little previous work that studies its efficient eval- uation and in fact under a different definition; only the best match for each object is identified. In this paper, we combine ideas from state-of-the-art spatial distance join and set similarity join methods and propose efficient algorithms that take into account both spatial and textual constraints. Besides, we propose a batch processing technique which boosts the performance of our approaches. An experimental evaluation using real and synthetic datasets shows that our optimized techniques are orders of magnitude faster than baseline solutions. 1. INTRODUCTION Databases are becoming increasingly complex over the years, as entities can be easily  'tagged ' with different types of auxiliary information, such as keywords and spatial locations. For example, webpages contain keywords and they may also be associated to lo- cations; photographs in photo-sharing services, such as Flickr, are assigned descriptive tags and spatial locations; persons in social networks and customer databases have profile entries (keywords) and addresses. The enrichment of objects with multi-source descriptive information allows for more complex queries and anal- ysis tasks over the data. For example, Flickr offers an API, via which users can search for photos by specifying keywords and a spatial search range. Recently, there has been a growing interest by research and industry to use space as another dimension for organizing and searching text and set-valued data. ?Work supported by grant HKU 714212E from Hong Kong RGC. line with this trend, we investigate the evaluation of spatiotextual similarity join (ST-SJOIN) queries; given two collections of objects R and S that carry both spatial and textual information, the ST-SJOIN retrieves the subset J of R X S, such that for every (r, s)   J , r is spatially close to s, based on a distance threshold (i.e., distl(r, s)   , where distl denotes distance between loca- tions), and the set similarity between r and s also exceeds a threshold  (i.e., simt(r, s)   , where simt denotes textual similarity). Figure 1 illustrates how the join can be used for social recommen- dations; four men (r1 to r4) and four women (s1 and s4) are joined based on their locations and interests (shown as keyword sets next to the points). Assuming qualifying pairs should have Euclidean distance distl at most = 0.3 and Jaccard similarity simt at least  = 0.5, the result of the join is {(r2, s2), (r2, s3)}. Applications. ST-SJOIN finds application in a wide range of do- mains, where spatial and textual information is available for a set of entities. Below, we discuss some examples. Personal databases. As illustrated in the above example, social networking applications can use the join to identify pairs of peo- ple who have similar profiles and they are in nearby locations. The result can be used for social recommendations. The join can also serve as a module for customer segmentation on a database of customers; the objective is to find groups of people who live nearby and have similar profiles, for directed marketing. Redundant or dirty data. Data deduplication and cleaning is a classic application of set-similarity joins [10, 16]; originally, in this process, only the textual similarity of data is considered. The spatio-textual similarity join can improve the effectiveness of de- tecting near-identical entities. For example, consider a database of spatially and textually tagged images (e.g., Flickr). Finding similar image pairs based solely on their tag similarity may not be suffi- cient, if the tags are not location dependent. Thus, an image tagged as  'bridge ' is textually similar to other bridge photos around the world, but can only be actually similar to photographs of the same bridge (taken from nearby locations). A spatio-textual (self) join can be used to identify pairs of images showing the same subject. Databases with POIs. Applying an ST-SJOIN on a database with points of interest (POIs) can also serve various applications. Pairs of businesses with common themes (e.g., Chinese restaurants) lo- cated near each other could collaborate in various ways (e.g., joint advertisement and promotion, location-based market analysis, shar- ing business processes or inventories). As another example, con- sider a touristic application, which finds pairs of thematically sim- ilar POIs (e.g., archaeological museums), closely located on a map and jointly includes them in targeted tour recommendations. Contribution. Recently, there has been a lot of work on spatio- textual similarity queries [7, 12, 15, 20, 24]. The input of such a query is a spatial location l and a set of terms K, and the ob- jective is to find objects from a collection R, which are spatially close to l and textually similar to K. In addition, spatial joins [5, 6, 9] and set similarity joins [2, 4, 10, 25, 27, 28] are well-studied problems. However, to our knowledge, there is only one work ([3]) on spatio-textual similarity joins, where in fact the problem is de- fined differently; given two datasets R and S, the best match in S for each object in R is retrieved. This paper attempts to fill this gap by studying efficient solutions for this interesting query operation. Like previous work on set similarity joins, we mainly focus on the self-join (i.e., R = S), which is the case for the most representa- tive applications of this query operation. Still, our solutions easily generalize to the general case, where R 6= S. We explore techniques which consider both join thresholds si- multaneously during search. In a nutshell, our methods exploit spatial indexing and pruning techniques to reduce the space where the (more expensive) textual similarity predicate needs to be verified; for the latter, they adapt the state-of-the-art algorithm for set- similarity joins [28]. We investigate alternative approaches for spa- tial pruning, based on a dynamic grid partitioning or a preexisting spatial index. Besides, we propose a batch processing technique which dynamically partitons the objects into groups based on their spatial locations and textual content and then performs the join at the groups level. This technique greatly improves the performance of all our methods; as we demonstrate, it is orthogonal to the spatial join predicate, since it drastically reduces the cost of the state-of- the-art set-similarity join algorithm [28]. We perform experiments with large-scale real and synthetic datasets, showing that our pro- posed techniques offer orders of magnitude performance improvement compared to baseline solutions. Outline. The rest of the paper is organized as follows. Section 2 reviews related work. Section 3 formally defines the ST-SJOIN oper- ation. Section 4 describes in detail the state-of-the-art set-similarity join algorithm. Our methods and their group-level evaluation are presented in Sections 5 and 6, respectively. Section 7 includes an experimental evaluation and Section 8 concludes the paper.",Panagiotis Bouros,"Department of Computer Science
University of Hong Kong
Pokfulam Road, Hong Kong",pbouros@cs.hku.hk,Shen Ge,"Department of Computer Science
University of Hong Kong
Pokfulam Road, Hong Kong",sge@cs.hku.hk,Nikos Mamoulis,"Department of Computer Science
University of Hong Kong
Pokfulam Road, Hong Kong",nikosg@cs.hku.hk,,,,,,,,,,,,,,,,,,,,,
20200316,1029,Lu Chen,"College of Computer Science, Zhejiang University, Hangzhou, China", luchen@cs.zju.edu.cn,,Efficient Metric Indexing for Similarity Search ,"Efficient Metric Indexing for Similarity Search, Efficient Metric Indexing for Similarity Search , Efficient Metric Indexing for Similarity Search , Efficient Metric Indexing for Similarity Search, Efficient Metric Indexing for Similarity Search,  Abstract aThe goal in similarity search is to find objects  similar to a specified query object given a certain similarity  criterion. Although useful in many areas, such as multimedia  retrieval, pattern recognition, and computational biology, to  name but a few, similarity search is not yet supported well by  commercial DBMS. This may be due to the complex data types  involved and the needs for flexible similarity criteria seen in real  applications. We propose an efficient disk-based metric access  method, the Space-filling curve and Pivot-based B+-tree (SPB-tree),  to support a wide range of data types and similarity metrics. The  SPB-tree uses a small set of so-called pivots to reduce  significantly the number of distance computations, uses a space- filling curve to cluster the data into compact regions, thus  improving storage efficiency, and utilizes a B+-tree with  minimum bounding box information as the underlying index.  The SPB-tree also employs a separate random access file to  efficiently manage a large and complex data. By design, it is easy  to integrate the SPB-tree into an existing DBMS. We present  efficient similarity search algorithms and corresponding cost  models based on the SPB-tree. Extensive experiments using real  and synthetic data show that the SPB-tree has much lower  construction cost, smaller storage size, and can support more  efficient similarity queries with high accuracy cost models than is  the case for competing techniques. Moreover, the SPB-tree scales  sublinearly with growing dataset size.  I. INTRODUCTION  The objective of similarity search is to find objects similar  to a given query object under a certain similarity criterion.  This kind of functionality has been used in many areas of  computer science as well as in many application areas. For  instance, in pattern recognition, similarity queries can be used  to classify a new object according to the labels of already  classified nearest neighbors; in multimedia retrieval, similarity  queries can be utilized to identify images similar to a specified  image; and in recommender systems, similarity queries can be  employed to generate personalized recommendations for users  based on their own preferences.  Considering the wide range of data types in the above  application scenarios, e.g., images, strings, and protein  sequences, a generic model is desirable that is capable of  accommodating not just a single type, but a wide spectrum. In  addition, the distance metrics for comparing the similarity of  objects, such as cosine similarity used for vectors, and edit  distance used for strings, are not restricted to the Euclidean  distance (i.e., the L2-norm). To accommodate a wide range of  similarity notions, we consider similarity queries in generic  metric spaces, where no detailed representations of objects are  required and where any similarity notion that satisfies the  triangle inequality can be accommodated.  A number of metric access methods exist that are designed  to accelerate similarity search in generic metric spaces. They  can be generally classified into two categories, namely,  compact partitioning methods [12], [15], [17], [21] and pivot- based methods [9], [20], [27], [42]. Compact partitioning  methods divide the space into compact regions and try to  discard unqualified regions during search, while pivot-based  methods store pre-computed distances from each object in the  database to a set of pivots. Given two objects q and o, the  distance d(q, o) cannot be smaller than |d(q, p) ? d(o, p)| for  any pivot p, due to the triangle-inequality. Hence, it may be  possible to prune an object o as a match for q using the lower  bound value |d(q, p) ? d(o, p)| instead of calculating d(q, o).  This capability makes pivot-based approaches outperform  compact partitioning methods in terms of the number of  distance computations, one of the key performance criteria in  metric spaces. Nonetheless, pivot-based approaches need large  space to store pre-computed distances, and their I/O costs are  often high because the data needed to process a similarity  query is not well clustered. Due to the above, we propose a  hybrid method that integrates the compact partitioning into a  pivot-based approach.  To design an efficient metric access method (MAM), three  challenging issues have to be addressed. The first is how to  support efficient similarity retrieval in terms of the number of  distance computations (i.e., CPU cost) and the number of page  accesses (i.e., I/O cost). We tackle this by identifying and using  a small set of effective pivots for reducing significantly the  number of distance computations during search, and we utilize  a space-filling curve (SFC) to cluster objects into compact  regions, to further boost performance. The second challenge is  to achieve low-cost index storage, construction, and  manipulation. To reduce the storage cost, we store multi- dimensional pre-computed distances as one-dimensional  integers using the SFC, and we employ a B+-tree to support  efficient index construction and manipulation. The third  challenge is how to efficiently manage a large set of complex  objects (e.g., DNA, images). Towards this, we develop a disk- based MAM that maintains the index and the data separately,  to ensure the efficiency of the index. The resulting proposal is  called the Space-filling curve and Pivot-based B+-tree (SPB- tree). It keeps complex objects in a separate random access  file (RAF) and uses a B+-tree with additional minimum  bounding box (MBB) to index objects after a two-stage pivot  and SFC mapping. The SPB-tree is generic, as it does not rely  on the detailed representations of objects, and it can support  any distance notion that satisfies the triangle inequality. To  sum up, the key contributions are as follows:   We develop the SPB-tree, which integrates the compact  partitioning with a pivot-based approach. The tree  utilizes a space-filling curve and a B+-tree to efficiently  index pre-computed distances and to cluster objects into  compact regions.  ? We propose an efficient pivot selection algorithm for  identifying a small set of effective pivots in order to  reduce significantly the number of distance computations  during the search.  ? We present efficient similarity search algorithms,  including range and k nearest neighbor (kNN) queries,  and we provide corresponding cost models.  ? We conduct extensive experiments using both real and  synthetic data sets to compare the SPB-tree against other  MAMs, finding that the SPB-tree has much lower  construction and storage cost, and supports more efficient  similarity queries with high accuracy cost models. Also,  the SPB-tree scales well with the data size.  The rest of this paper is organized as follows. Section II  reviews related work. Section III presents the problem  statement. Section IV describes the SPB-tree and the pivot  selection algorithm. Section V details the similarity query  algorithms and their corresponding cost models. Considerable  experimental results and our findings are reported in Section  VI. Finally, Section VII concludes the paper with some  directions for future work.",Lu Chen,"College of Computer Science, Zhejiang University, Hangzhou, China", luchen@cs.zju.edu.cn,Yunjun Gao,"College of Computer Science, Zhejiang University, Hangzhou, China",gaoyj@cs.zju.edu.cn,Xinhan Li,"College of Computer Science, Zhejiang University, Hangzhou, China",lixh@cs.zju.edu.cn,Christian S. Jensen,"Department of Computer Science, Aalborg University, Denmark",cg@cs.zju.edu.cn,Gang Chen,"College of Computer Science, Zhejiang University, Hangzhou, China",csj@cs.aau.dk,,,,,,,,,,,,,,,
20200317,959,Jinbao Wang,"School of Computer Science and Technology, Harbin Institute of Technology Harbin, China",wangjinbao@comp.nus.sg,,Indexing Multi-dimensional Data in a Cloud System,"Indexing Multi-dimensional Data in a Cloud System, Indexing Multi-dimensional Data in a Cloud System, Indexing Multi-dimensional Data in a Cloud System, Indexing Multi-dimensional Data in a Cloud System, Indexing Multi-dimensional Data in a Cloud System,  ABSTRACT Providing scalable database services is an essential require- ment for extending many existing applications of the Cloud platform. Due to the diversity of applications, database ser- vices on the Cloud must support large-scale data analytical jobs and high concurrent OLTP queries. Most existing work focuses on some specific type of applications. To provide an integrated framework, we are designing a new system, epiC, as our solution to next-generation database systems. In epiC, indexes play an important role in improving overall performance. Different types of indexes are built to provide efficient query processing for different applications. In this paper, we propose RT-CAN, a multi-dimensional indexing scheme in epiC. RT-CAN integrates CAN [23]- based routing protocol and the R-tree based indexing scheme to support efficient multi-dimensional query processing in a Cloud system. RT-CAN organizes storage and compute nodes into an overlay structure based on an extended CAN protocol. In our proposal, we make a simple assumption that each compute node uses an R-tree like indexing struc- ture to index the data that are locally stored. We propose a query-conscious cost model that selects beneficial local R- tree nodes for publishing. By keeping the number of per- sistently connected nodes small and maintaining a global multi-dimensional search index, we can locate the compute nodes that may contain the answer with a few hops, making the scheme scalable in terms of data volume and number of compute nodes. Experiments on Amazon's EC2 show that our proposed routing protocol and indexing scheme are ro- bust, efficient and scalable. Categories and Subject Descriptors H.2.4 [Systems]: Distributed databases; C.2.4 [Distributed System]: Distributed databases General Terms Algorithm, Design, Experimentation, Performance Keywords Cloud, Index, Query Processing 1. INTRODUCTION Deploying database services on the Cloud poses new chal- lenges to the community. Lacking scalability and reliability, conventional database designs and principles cannot be di- rectly applied to the new platform. Therefore, a new archi- tecture that is specially tailored for the Cloud is essential. Analytical jobs and online transactions are two basic data- base workload types. Most recent work on the Cloud fo- cuses on large-scale data analytical jobs [22, 5, 12] while some work such as [10] attempts to support high concurrent OLTP queries. epiC (elastic power-aware data-instensive Cloud) [2], is a cloud-based data management system that is being implemented jointly by the National University of Singapore and Harbin Institute of Technology as an inte- grated solution for both analytical jobs and OLTP transac- tions. epiC is designed to provide primitive operators based on the principle of filter and refine to serve as basic building blocks for more complex operators so as to achieve the nec- essary parallelism and fault tolerance. An SQL query will then be translated into a DAG (Directed Acrylic Graph) of primitive operators, and a master server schedules and mon- itors the processing of such operators. As in conventional DBMS, these operators may apply index-based processing or scan-based processing. Indexes can indeed improve query performance by orders of magnitude for both large-scale an- alytical jobs [16] and OLTP queries [10]. In epiC, indexing schemes adopt a two-layer approach, which is light weight and query-pattern conscious, and ride on local indexes. The first application that we are implementing is a com- munity travelog system designed to help travelers plan their travel itinerary and share their experiences and photos. In such a system, we need to support photo search via key- words/tags and geo-tags. Specifically, each photo object is expressed as {g0, g1, ..., gm, k0, k1, ..., kn}, where gi repre- sents its geographic information (e.g., latitude, longitude, location name, etc.) and ki describes the features of the photo (date, topic, photo type, color histogram, textures, etc.). Typical queries include searching photos with spe- cific tags in a given location and finding the geo informa- tion about a specific photo. To efficiently support the above queries, a multi-dimensional indexing strategy, called RT- CAN, is designed for epiC. In this paper, we present our implementation of RT-CAN. We target internet-scale applications, where hundreds of servers are used to support tera-byte data and millions of 591 users; the index must therefore be able to scale up when more servers are added or more data are inserted. One pop- ular solution is to apply a master server to maintain the global index. All queries are sent to the master server to search the global index and then forwarded to correspond- ing servers. Due to the fact that the size of the global index is proportional to the size of the data and the number of concurrent requests is huge, the master server risks being a bottleneck. Therefore, we propose distributing the global index across servers. Each server only maintains a portion of the global index. The distributed approach improves scal- ability and fault tolerance. To distribute the global index, servers are organized into an overlay network. Specifically, CAN (Content Addressable Network) is adopted as it is suitable for multi-dimensional search. Our global index is built on top of the local indexes. To search the local data efficiently, an R-tree like multi- dimensional index should be built for the local database. In the global index, instead of indexing every tuple, we publish local R-tree nodes into the global index. Consequently, the global index in our system plays the role of an ^overview' in- dex, composed of R-tree nodes from different servers. There- fore, we name our global index RT-CAN (the R-Tree based index in CAN). Indexing R-tree nodes reduces the size of the global index and hence lowers maintenance cost. Once we have located an R-tree node in the global index, we can continue search in the local R-tree, starting from the node registered in the global index. The main challenge to the design of an efficient RT-CAN scheme is the distribution of the global index onto compute servers. In RT-CAN, the multi-dimensional search space is partitioned into zones based on CAN's protocols, which is similar in concept to that of the grid-file and the kd-tree [9]. Each server maintains a zone, and the partitions are adjusted adaptively. A ^hot' zone can balance its load with its neighbors by shrinking its zone. For the fact that an R-tree node represents a multi-dimensional hyper-cube, we build a mapping function between R-tree nodes and CAN servers. If an R-tree node is inserted into the global index, it will be published to mapped servers. To process a query, we search the global index and route the query to servers whose zones overlap with the query. Upon receiving the query, the servers will process the query in parallel. We apply CAN's routing protocols to forward R-tree nodes and queries from their owners to the mapped servers. In d-dimensional CAN, the average routing cost is d 4 N 1 d , where N is the number of servers. To reduce routing cost, we apply a variant of CAN [11] by maintaining Chord-like routing neighbors along each dimension on the same grid interval, where the routing cost is reduced to log N 4 . Moreover, a routing buffer is maintained in each server to further reduce routing cost. It is not practical to publish every R-tree node to the global index as that incurs high maintenance overhead. There- fore, it suffices to select a portion of R-tree nodes that are useful for routing purposes and publish them in the global in- dex. Since different selections incur different costs (routing cost, maintenance cost and query cost), the challenge lies in choosing an index strategy which incurs the least total cost. We present a scheme to estimate the cost of a possi- ble indexing strategy. Based on the statistics about query distribution and updates, our scheme can dynamically tune the indexing strategy to minimize cost. This paper makes the following contributions: 1. A distributed global index, the RT-CAN index, is pro- posed to support efficient multi-dimensional search. The RT-CAN index is built on top of local R-trees and published to cluster servers. 2. We present a method to map a selected R-tree node to a CAN node. Query processing algorithms are de- signed to support point, range and KNN queries for the RT-CAN index. 3. A cost model is proposed to estimate the cost of dif- ferent indexing strategies, based on which a dynamic tuning algorithm is proposed to selectively publish lo- cal R-tree nodes onto the global index. The tuning algorithm optimizes maintenance cost and query cost. 4. Experiments on Amazon's EC2 verify the effectiveness and efficiency of the RT-CAN index. The rest of this paper is organized as follows: Section 2 presents related works while Section 3 gives an overview of our indexing scheme. Section 4 presents our query process- ing algorithms, including the point, range and KNN queries. Section 5 presents index selection and tuning methods, and describes the selection of R-tree nodes and the method to tune them according to the current workload. Section 6 presents the performance evaluation, and we conclude in Section 7.",Jinbao Wang,"School of Computer Science and Technology, Harbin Institute of Technology Harbin, China",wangjinbao@comp.nus.sg,Sai Wu,"School of Computing, National University of Singapore, Singapore",,Hong Gao,"School of Computer Science and Technology, Harbin Institute of Technology Harbin, China",honggao@comp.nus.sg,Jianzhong Li,"School of Computer Science and Technology, Harbin Institute of Technology Harbin, China",lijzh@comp.nus.sg,Beng Chin Ooi,"School of Computing, National University of Singapore, Singapore",,,,,,,,,,,,,,,,
20200318,1365,Kaiji Chen,University of Southern Denmark,chen@imada.sdu.dk,,Online Data Partitioning in Distributed Database Systems,"Online Data Partitioning in Distributed Database Systems, Online Data Partitioning in Distributed Database Systems, Online Data Partitioning in Distributed Database Systems, Online Data Partitioning in Distributed Database Systems, Online Data Partitioning in Distributed Database Systems, ABSTRACT Most of previous studies on automatic database partitioning focus on deriving a (near-)optimal (re)partition scheme ac- cording to a specific pair of database and query workload and oversees the problem about how to efficiently deploy the de- rived partition scheme into the underlying database system. In fact, (re)partition scheme deployment is often non-trivial and challenging, especially in a distributed OLTP system where the repartitioning is expected to take place online without interrupting and disrupting the processing of nor- mal transactions. In this paper, we propose SOAP, a system framework for scheduling online database repartitioning for OLTP workloads. SOAP aims to minimize the time frame of executing the repartition operations while guaranteeing the correctness and performance of the concurrent processing of normal transactions. SOAP packages the repartition oper- ations into repartition transactions, and then mixes them with the normal transactions for holistic scheduling optimization. SOAP utilizes a cost-based approach to rank the repartition transactions ' scheduling priorities, and leverages a feedback model in control theory to determine in which order and at which frequency the repartition transactions should be scheduled for execution. When the system is under heavy workload or resource shortage, SOAP takes a further step by allowing repartition operations to piggy- back onto the normal transactions so as to mitigate the resource contention. We have built a prototype on top of PostgreSQL and conducted a comprehensive experimental study on Amazon EC2 to validate SOAP 's significant performance advantages. Categories and Subject Descriptors H.2.4 [Database Management]: Systems - Distributed Databases and Transaction Processing General Terms Algorithms, Design, Performance, Experimentation Keywords Distributed Database Systems, Online Data Partitioning, Transaction Scheduling 1. INTRODUCTION The difficulty of scaling front-end applications is well known for DBMSs executing highly concurrent workloads. One ap- proach to this problem employed by many Web-based companies is to partition the data and workload across a large number of commodity, shared-nothing servers using a costeffective, distributed DBMS. The scalability of online trans- action processing (OLTP) applications on these DBMSs depends on the existence of an optimal database design, which defines how an application 's data and workload is parti- tioned across the nodes in a cluster, and how queries and transactions are routed to the nodes. This in turn deter- mines the number of transactions that access data stored on each node and how skewed the load is across the cluster. Optimizing these two factors is critical to scaling complex systems: a growing fraction of distributed transactions and load skew can degrade performance by a factor of over ten. Hence, without a proper design, a DBMS will perform no better than a single-node system due to the overhead caused by blocking, inter-node communication, and load balancing issues. Automatic database partitioning has been extensively re- searched in the past. As a consequence, nowadays, most DBMSs offer database partitioning design advisory tools. The idea of these tools analyze the workload at a given time and suggest a (near-)optimal repartition scheme in a cost- based or policy-based manner, with the expectation that the system performance can thereby always maintain a consistently high level. It is then the DBA 's responsibility to deploy the derived repartition scheme into the underlying database system, which however often posts great challenges to the DBA. On the one hand, the repartition operations should be executed fast enough so that the new partition scheme can start to take effect as soon as possible. How- ever, granting high execution priorities to the repartition- ing operations will inevitably slow down or even stall the normal transaction processing on the database system. On the other hand, the repartitioning procedure should be as transparent to the users as possible. In other words, the normal user transactions ' correctness must not be violated and the processing performance should not be significantly influenced. Obviously, even skilled DBAs may not be able to easily figure out the best ways of deploying repartition schemes, especially when the workload changes over time and has bursts and peaks. As a result, automatic partition scheme deployment satisfying the above requirements is highly desirable. Surprisingly, few previous studies have been devoted to this important research problem. In this paper, we focus on the problem about how to optimally execute a database repartition plan consisting of a set of repartition operations in a distributed OLTP sys- tem, where the repartitioning is expected to take place on- line without interrupting and disrupting the normal trans- actions ' processing. We propose SOAP, a system frame- work for scheduling online database repartitioning for OLTP workloads. SOAP aims to minimize the time frame of ex- ecuting the repartition operations while guaranteeing the correctness and performance of the concurrent normal trans- action processing. SOAP models and groups the repartition operations into repartition transactions, and then mixes them with the nor- mal transactions for holistic scheduling optimization. There are two basic strategies for SOAP to schedule the reparti- tion transactions, which are similar to the techniques used in state-of-the-art database systems ' online repartitioning solutions. The first strategy is to maximize the speed of applying the repartition plan and submit all the reparti- tion transactions to the waiting queue with a priority higher than the normal transactions. The second strategy schedules repartition transactions only when the system is idle. Both basic strategies lack the flexibility to find a good trade-off between the two contradicting objectives: maximizing the speed of executing repartition transactions and minimizing the interferences to the processing of normal transactions. As a result, SOAP interleaves the repartition transactions with normal transactions, and leverages feedback models in control theory to determine in which order and at which fre- quency the repartition transactions should be scheduled for execution. In the feedback-based method the repartition transactions have the same priority as the normal transactions, hence they will contend with normal transactions for the locks of database objects and significantly increase the system 's workload, especially when the system is under heavy loads or resource shortage. To mitigate this issue, SOAP utilizes a piggyback-based approach, which injects repartition opera- tions into the normal transactions. The overhead of acquir- ing and releasing locks as well as performing the distributed commit protocols incurred by a repartition transaction can be saved if the normal transaction that it piggybacks on will access the same set of database objects. While the piggyback-based approach consumes less re- sources, it fails to take use of the available system resources to speed up the repartitioning process. This may leave some resources unused when the system workload is low and there are few transactions to piggyback on. Therefore, SOAP adopts a hybrid approach that is composed by a piggyback module and the feedback module. When the system work- load does not use up all the system 's resources, we can make use of the available resources to repartition the data be- fore the actual arrival of transactions that will access them, meanwhile the piggyback-based approach will attempt to let the repartition transactions piggyback on the normal trans- actions when they arrive. To summarize, we make the following contributions with this work: ? To the best of our knowledge, we are among the first to specifically study the problem of online deploying database partition schemes into OLTP systems. ? We propose a feedback model that realizes dynamic scheduling of the repartition operations. ? We also propose a piggyback approach to execute se- lected repartition operations within normal transac- tions to further mitigate the repartitioning overhead. ? We have built a SOAP prototype on top of PostgreSQL, and conducted a comprehensive experimental study on Amazon EC2 that validates SOAP 's significant perfor- mance advantages. The rest of this paper is organized as follows. In Section 2, we describe the generic SOAP system architecture, as well as how SOAP realizes online repartitioning for OLTP work- loads. In Section 3, we elaborate SOAP 's feedback-based, piggyback-based and hybrid approaches of online schedul- ing repartition operations. Section 4 presents the experi- ment set-up and experimental results of a SOAP prototype on an Amazon EC2 cluster. We discuss the related works in Section 5 and then conclude in Section 6.",Kaiji Chen,University of Southern Denmark,chen@imada.sdu.dk,Yongluan Zhou,University of Southern Denmark,zhou@imada.sdu.dk,Yu Cao,EMC Labs China EMC Corporation,yu.cao@emc.com,,,,,,,,,,,,,,,,,,,,,
20200319,1046,Manoj K Agarwal,"IBM Research-India, New Delhi",manojkag@in.ibm.com,,Real Time Discovery of Dense Clusters in Highly Dynamic Graphs:  Identifying Real World Events in Highly Dynamic Environments,"Real Time Discovery of Dense Clusters in Highly Dynamic Graphs:  Identifying Real World Events in Highly Dynamic Environments, Real Time Discovery of Dense Clusters in Highly Dynamic Graphs:  Identifying Real World Events in Highly Dynamic Environments, Real Time Discovery of Dense Clusters in Highly Dynamic Graphs:  Identifying Real World Events in Highly Dynamic Environments, Real Time Discovery of Dense Clusters in Highly Dynamic Graphs:  Identifying Real World Events in Highly Dynamic Environments, Real Time Discovery of Dense Clusters in Highly Dynamic Graphs:  Identifying Real World Events in Highly Dynamic Environments, ABSTRACT  Due to their real time nature, microblog streams are a rich source  of dynamic information, for example, about emerging events.  Existing techniques for discovering such events from a microblog  stream in real time (such as Twitter trending topics), have several  lacunae when used for discovering emerging events; extant graph  based event detection techniques are not practical in microblog  settings due to their complexity; and conventional techniques,  which have been developed for blogs, web-pages, etc., involving  the use of keyword search, are only useful for finding information  about known events. Hence, in this paper, we present techniques  to discover events that are unraveling in microblog message  streams in real time so that such events can be reported as soon as  they occur. We model the problem as discovering dense clusters  in highly dynamic graphs. Despite many recent advances in graph  analysis, ours is the first technique to identify dense clusters in  massive and highly dynamic graphs in real time. Given the  characteristics of microblog streams, in order to find clusters  without missing any events, we propose and exploit a novel graph  property which we call short-cycle property. Our algorithms find  these clusters efficiently in spite of rapid changes to the microblog  streams. Further we present a novel ranking function to identify  the important events. Besides proving the correctness of our  algorithms we show their practical utility by evaluating them  using real world microblog data. These demonstrate our  technique's ability to discover, with high precision and recall,  emerging events in high intensity data streams in real time. Many  recent web applications create data which can be represented as  massive dynamic graphs. Our technique can be easily extended to  discover, in real time, interesting patterns in such graphs.  1. INTRODUCTION and MOTIVATION  Microblogging sites such as have become a rich source  of information about any ""event"", ranging from breaking news  stories to earthquakes or information about local concerts.  Empirical studies [10] [11] show that (i) Twitter is often the first  medium to break important events such as earthquakes, often in a  matter of seconds after they occur and more importantly (ii) they  highlight the need to discover all such events (and not just events  related to earthquakes [10]) in real time from microblog streams.  Note that by ""real time' we mean that events need to be  discovered as early as possible after they start unraveling in the  microblog stream. Such information about emerging events can be  immensely valuable if it is discovered timely and made available.  This work is done as part of the PhD at IIT-Bombay, India.  One obvious way to find information on microblogging sites is to  use keyword search. There are many microblog search engines  which allow users to find real-time microblogs relevant to a  keyword query (e.g., twitter search). These search engines allow  users to register their (continuous) keyword queries and return a  stream of events, trends or news items relevant to the query.  However, these search techniques do not help the user to  ""discover"" the event but can be used to gather follow up  information about the event. One could argue that the event could  have been discovered by a continuous query with a keyword, say,  ""earthquake"". However, note that a user would have to register a  large number of such keyword queries to discover all possible  types of events, something that is clearly not feasible.  The major challenge in achieving the goal of building a real time  event discovery and tracking system lies in correlating the right  microblog messages, among the hundreds of thousands of  messages that are continuously being generated. The problem is  exacerbated by the fact that the keywords used to describe the  event might vary from one user to another and could also change  over time due to the evolving nature of real time events. Hence  classifier [10] or keyword search techniques may not be practical.  This paper addresses these problems and presents a technique for  discovering events in a microblog stream in real time.  Whenever an event happens, there will be a few keywords which  will show burstiness (display a sudden jump in frequency). Hence  a simple and obvious way to discover events is to keep track of  the most popular words, something that is already done by twitter,  and displayed as trending topics. A keyword (or a pair of  consecutively occurring words) is recognized as a trending topic  by Twitter if it is popular over a period of time. However, as  reported in, several thousand tweets over a  relatively short period of time are needed to identify an event as  trending topic. Therefore, (1) using keywords appearing in  Twitter trending topics' does not serve the purpose of discovering events  in Twitter real time' (as by then the event would no longer be an  emerging event) and (2) it is not necessary that all important  events do become trending topics. Further, once a set of keywords  becomes popular, they would remain so for a long time thereby  overshadowing any new emerging events. Moreover, rather than  reporting individual keywords or a pair of consecutive keywords  it might be more meaningful and insightful to identify a set of  correlated keywords (not necessarily occurring consecutively).  In order to identify an emerging topic, we need to identify a set of  keywords which are temporally correlated, i.e., they show  burstiness at the same time and are spatially correlated, i.e., they  co-occur in temporally correlated messages from the same user. In  order to capture these characteristics we use a dynamic graph  model which uses the moving window paradigm and is  constructed using the most recent messages present in the message  stream. An edge between two nodes -- representing two keywords  -- indicates that messages from a user within the recent sliding  window involve the respective keywords. We use these properties  to formulate our problem as that of cluster discovery in a dynamic  graph. Figure 1 shows a partial graph induced by 6 real twitter  messages (comprising 12 keywords). 6 of these keywords show  burstiness (e.g., at least 2 occurrences). Keywords co-occurring together in messages from a user (within 6 messages) share an  edge. We discover the cluster ""earthquake struck eastern Turkey""  in the graph, denoting an event. Two other keywords (""massive""  and ""moderate"") were also bursty within the graph but they are  not part of cluster (due to weak spatial correlation). When the  window is moved at a fixed rate (oldest 2 messages expire and 2  most recent messages are added), a new keyword (""5.9"") gets  added to the cluster (denoting the intensity of earthquake).    Figure 1: Event cluster embedded in a graph drawn on Twitter data  The example above highlights several issues central to our  problem, specifically a) the cluster definition should be able to  capture the imperfect correlation among keywords belonging to an  event. Not all keywords are used by all the users (there is no edge  between ""eastern' and ""struck'); b) we should be able to capture  the evolving nature of events in a highly dynamic environment  (""5.9' joined the cluster later); and, c) the identified events should  truly be categorizable as emerging real-world events by filtering  out spurious and unimportant events.  The graph is highly dynamic and complex, i.e., keywords (and  associated edges) present in the graph get added and deleted at a  fast pace; (Twitter reports more than 2300 tweets/sec [12] with  potentially multiple simultaneous events present); the technique to  identify the emerging events needs to be highly scalable.  Specifically, it should be able to identify and maintain the events  in a massive and highly dynamic graph.  1.1 Problem Formulation, Solution Ingredients  Let Si represent a set of keywords (potentially spread over  multiple messages) from a unique user i in a time window that  spans from time (t - .w) to current time t, where represents unit  time called quantum and .w is the length of the time window. Let  Sw t={S1 Sm} be a set of keywords sent by m unique users in the  microblog stream in a given time window. As we are interested in  discovering emerging events, Sw t contains the messages from a  sliding window (of size .w) over the message stream. Time unit  denotes the fixed rate at which the window is moved.  Correlated Keyword Graph (CKG) captures the properties of  microblog contents. We represent all the keywords, after  removing stop words, appearing in the messages in the current  window as nodes in an undirected graph, CKG (we use the terms  node and keyword interchangeably in this paper). CKG is a  dynamic graph whose state at time t, is Gt = (Vt, Et) where Vt is  the subset of keywords appearing in message set Sw t. Thus, two  keywords are said to be temporally correlated iff they appear in Vt  and are said to be spatially correlated if they have an edge  between them in Et. An edge links two keywords iff they both  appear in a keyword set Si belonging to a user i.  Thus using the sliding window paradigm, a keyword is present in  CKG if the keyword appears in at least one message in the current  window. Since the window moves forward with time, CKG is  highly dynamic where nodes and edges appear and disappear in  real time. Further, a node in CKG can be either in a ""high"" state  or a ""low"" state. A node moves into high state if there is a sudden  increase in the frequency of its occurrence in the message stream.  Each edge in CKG is associated with a weight which signifies the  probability of the words associated with the edge appearing in  temporally correlated messages from a set of users. One of the  challenges in working with highly dynamic microblog data is the  size of the generated CKG. We overcome this challenge by  constructing a much smaller Active CKG (AKG) from the original  CKG such that (1) the clusters discovered in AKG are no different  from those discovered in the CKG and (2) it is orders of  magnitude smaller than the original CKG.  Emerging events are identified by discovering clusters in CKG.  Given CKG, our problem of discovering emerging events can be  mapped to identifying significant properties of the graph. For  example, the burstiness of the keywords is captured by the state  associated with a node. Temporal correlation can be captured by  the moving window and spatial correlation can be identified by  the weight associated with the edges. Using these properties, at a  high level, our problem of event identification is similar to  discovering a ""cluster"" within CKG. The cluster would consist of  a set of keywords (e.g., ""earthquake"", ""struck"", ""Turkey"") where  each keyword would be bursty and would exhibit temporal and  spatial correlation with the other words in the cluster.   CKG is an undirected graph, i.e., is a tree of its biconnected  components. A graph is said to be biconnected if for any pair of  nodes in the graph there are at least two independent paths  between them. Two paths are independent if they do not have any  nodes in common except the end points. In a connected graph,  two biconnected clusters can be connected with each other with  just one path (had there been more paths between two clusters,  they will merge into one cluster). We assume that nodes within  biconnected components are more likely to be associated with the  same event compared to nodes across components.   Biconnected components are the most encompassing forms of  clusters in an undirected graph, next only to a connected graph  itself being considered to be a cluster. However, if we choose to  consider all biconnected components as our clusters, we may end  up discovering massive and more often meaningless clusters in a  large and dynamic graph. The other option is to consider only  complete cliques, wherein each node is connected with all the  other nodes in the clique, as clusters of interest to us. Complete  cliques are more likely to represent interesting real world events  but considering only complete cliques as clusters does not suit our  scenario because a) different users may use different sets of  keywords to describe the same event and b) keywords associated  with an event change rapidly in the microblogging stream due to  the evolving nature of real time events.  Considering 1/2-quasi cliques (MQCs) as clusters of interest  contributes to good precision and recall of discovered events. As  noted above, identifying events from the biconnected components  in a CKG is likely to result in high recall (i.e., identify more real  world events) but low precision (i.e., identify many non events as  real-world events); the opposite is likely to be true for complete  cliques. Therefore instead of finding either complete cliques or  just biconnected components, we focus on 1/2-quasi cliques as our  clusters of interest. A cluster is a -quasi clique if each node in the  cluster is adjacent to at least .(N-1) nodes in the cluster where  is a number between 0 and 1 and N is the cluster size. When is  1, the cluster is a complete clique. A biconnected component has   =2/N-1. For a connected graph, the minimum value of can be  1/N-1. As explained above, none of the two extreme values of is  suitable in our environment. Therefore a natural choice is to set  to their mean in order to balance precision and recall. Hence, in  order to discover meaningful clusters in a dynamic environment,  we identify those components of a graph as clusters that have >  moderate  massive  eastern  struck  Turkey 5.9  earthquake  981 (1/2+1/N-1) or 1/2. We call these cliques majority quasi  cliques (MQCs) since each node of the cluster is connected with a  majority of the remaining nodes in the cluster.  Exploiting short cycle property (SCP) of MQCs makes event  discovery a tractable and local problem. It has been shown [14]  that discovering 1/2-quasi cliques is an NP-complete problem even  for static graphs. Fortunately, we are able to show that î-quasi  cliques possess a unique property which we call short cycle  property (SCP): any edge in the cluster has at least one cycle of  length at most 4 within the cluster. (In Section 4, we define the  short-cycle property formally and show that (1) SCP is a  necessary but not sufficient condition for MQC, (2) SCP is a  sufficient but not necessary condition for bi-connected  components, and, (3) SCP can be exploited to identify events by  discovering clusters which possess the short cycle property (called  approximate MQCs (aMQCs)).  The key advantage of using SCP for defining clusters is that we  can discover dense clusters (aMQCs) efficiently and locally  without using any global state information. For a dynamic graph,  a cluster is said to be locally processable if for each incoming or  departing node (or edge) to the graph, the cluster can be  discovered by processing only its adjacent edges and nodes. Since  these computations are local in nature, they are efficient, a pre- condition for discovering clusters in a highly dynamic graph.  Further, multiple independent additions and deletions are allowed  simultaneously on the graph. On the other hand, any processing  which needs the graph to be stable (i.e., no addition or deletion is  allowed in the graph during the course of computation) is called  global processing. We believe that ours is the very first attempt to  develop a technique to discover dense clusters in a highly  dynamic graph. We propose efficient algorithms for discovering  and maintaining the clusters in a dynamic graph as nodes and  edges get added and deleted due to the moving window. We prove  the correctness of our algorithms and experimentally show that  our use of aMQC to define clusters helps us to discover emerging  events correctly and efficiently.  Globally consistent ranking of events can be achieved by  exploiting local properties of clusters. In order to consume  events, a ranking function is needed such that important events are  ranked higher compared to spurious or less important news.  However, due to the highly dynamic environment and real time  considerations, no ranking function which needs any global  information can be used. We present a novel and highly efficient  ranking function that ranks events by just exploiting the local  cluster properties corresponding to each event, yet delivers a  globally consistent ranking in a best effort manner.  Suppose two clusters discovered by us pertain to the same event  but they could not get merged into a single event because (1)  users used synonymous keywords to describe the event; (2) users  indeed used different keywords, providing different perspectives  about the same event; (3) the messages are posted in different  languages. All these cases can be addressed by pre-processing the  messages or post-processing the discovered clusters. For instance,  one can use dictionary/thesaurus to address issues (1) and (3). For  (2), clusters pointing to the same event should show temporal  correlation. Therefore, one can post-process the discovered  clusters (within a given time window) to correlate such clusters.  Further, suppose there is an ongoing discussion among tweeters  about a controversial topic (resulting in many messages) but it is  not a real world event. Typically, such ""events"" are ranked low  compared to real world events due to their slow rate of spread. We  may want to report even those events if they are ranked  sufficiently high, but often one may want to ignore such events by  post-processing the discovered clusters to identify such events.  Post/pre-processing of keywords and discovered clusters and  event categorization are orthogonal to the technique presented in  this paper. It can be used to further enhance our technique and is  part of our future work.  1.2 Research Contributions  ? We present a new technique to discover and maintain dense  clusters in massive and highly dynamic graphs in real time.  In contrast, other clustering techniques, such as those based  on data mining, are not only inherently slow in such  environments they are also not suitable (details in Section 2).  ? In Section 3 we present our strategy to construct a much  smaller Active CKG (AKG) from the original CKG to help  us efficiently discover and maintain the clusters, which is  imperative in a dynamic environment.  ? We model the problem of discovering the emerging events in  real time in microblog streams as discovering approximate  1/2-quasi cliques, which possess the short-cycle property. This  property is especially useful in highly dynamic microblog  environments as it helps us maintain the clusters locally  without using any global state information. We also prove  the correctness of the algorithms (Section 4).  ? We propose efficient algorithms for maintaining the clusters  locally even under numerous additions and deletions of  nodes and edges (Section 5).  ? In Section 6, we present our ranking function such that more  important events are highly likely to be ranked higher by just  using local cluster properties.  ? Through an experimental study of our technique using real  twitter data, we demonstrate its ability to (1) discover the  emerging events in real time -- with high precision and  recall; (2) process at almost double the rate of current Twitter  intensity on a machine of moderate configuration; (3)  discover emerging events around the same time or much  before it is seen on Google headlines; (4) discover additional  events which do not appear in Google headlines (Section 7).  Discovering dense clusters in highly dynamic graphs efficiently  and in real time has many applications in social networks, IP  networks, telecommunication networks and for real time business  analytics. Extant algorithms to discover dense clusters in dynamic  graphs work on snapshot based techniques [2] and have severe  limitations with regard to real time analytics. Our technique to  discover clusters in massive and highly dynamic graphs in real  time improves upon the state-of-the-art and can be easily extended  for many such applications.",Manoj K Agarwal,"IBM Research-India, New Delhi",manojkag@in.ibm.com,Krithi Ramamritham,"IIT-Bombay, Mumbai, India",krithi@cse.iitb.ac.in,Manish Bhide,"IBM India Software Labs, Hyderabad",abmanish@in.ibm.com,,,,,,,,,,,,,,,,,,,,,
20200320,731,Arijit Khan,"Dept. of Computer Science University of California at Santa Barbara, CA 93106-5110, USA",arijitkhan@cs.ucsb.edu,,Towards Proximity Pattern Mining in Large Graphs,"Towards Proximity Pattern Mining in Large Graphs, Towards Proximity Pattern Mining in Large Graphs, Towards Proximity Pattern Mining in Large Graphs, Towards Proximity Pattern Mining in Large Graphs, Towards Proximity Pattern Mining in Large Graphs, ABSTRACT Mining graph patterns in large information networks is crit- ical to a variety of applications such as malware detection and biological module discovery. However, frequent subgraphs are often ineffective to capture association existing in these applications, due to the complexity of isomorphism testing and the inelastic pattern definition. In this paper, we introduce proximity pattern which is a significant departure from the traditional concept of frequent subgraphs. Defined as a set of labels that co-occur in neighborhoods, proximity pattern blurs the boundary between itemset and structure. It relaxes the rigid structure constraint of frequent subgraphs, while introducing connectivity to frequent itemsets. Therefore, it can benefit from both: efficient mining in itemsets and structure proximity from graphs. We developed two models to define proximity patterns. The second one, called Normalized Probabilistic Association (NmPA), is able to transform a complex graph mining problem to a simplified probabilistic itemset mining problem, which can be solved efficiently by a modified FP-tree algorithm, called pFP. NmPA and pFP are evalu- ated on real-life social and intrusion networks. Empirical results show that it not only finds interesting patterns that are ignored by the existing approaches, but also achieves high performance for finding proximity patterns in large- scale graphs. Categories and Subject Descriptors H.2.8 [Database Applications]: data mining; I.5.1 [Pattern Recognition]: Models astatistical General Terms Algorithms, Performance Keywords Graph, Association, Pattern, Mining. 1. INTRODUCTION Graph patterns are building blocks for several key graph applications, including graph indexing, graph search, graph classification and clustering [37, 13, 12, 40]. Existing graph pattern mining algorithms, like those developed in [19, 22, 23, 32, 9, 20, 28], achieved great success using strategies that efficiently traverse the pattern space. However, the defini- tion of frequent subgraphs might not be appropriate for new application scenarios present in social and information net- works. First, the definition is not elastic enough to capture fuzzy patterns existing in massive attributed graphs. Figure 1 shows one example, where each node is attached with a set of labels. These labels can be movies recommended by a user, functions carried by a gene, or intrusions initiated by a computer. As illustrated in Figure 1, a, b, c often occur together and formulate an association pattern, while d, c are not associated together. However, {a, b, c} is neither a frequent subgraph, nor a frequent itemset if we treat each node as a transaction. Pattern {a, b, c} has three characteristics: (1) Proximity, these three labels are tightly connected; (2) Frequency, they appear many times; (3) Flexibility, they are not always connected in the same way. Due to these char- acteristics, we can not apply the traditional frequent graph mining algorithms such as FSG [23] and gSpan [36] to find them. On the other hand, frequent itemset mining [4, 16] can not be used either, since {a, b, c} do not appear in the same set of nodes. ab e a a ab c bc d d b c f Figure 1: Proximity Pattern {a, b, c} Secondly, for small graphs such as chemical structures, isomorphism checking is never a problem as demonstrated by the existing frequent graph mining algorithms. However, for large graphs like intrusion networks and social networks, there can be a huge set of isomorphic embeddings existing for frequent subgraphs. It becomes costly to generate all kinds of frequent subgraphs. To overcome the above two issues, we propose a new graph pattern concept, called Proximity Pattern. A proximity pattern is a subset of labels that re- peatedly appear in multiple tightly connected subgraphs in 867 G. {a, b, c} in Figure 1 is an example. Proximity pattern is an itemset. However, it has a connectivity requirement: the labels must be associated tightly and frequently in the graph. For example, in a social network, it can be a set of movies that are watched by multiple groups of users. That is, in order to find proximity patterns among movies, one should not only consider the collection of movies watched by each person (in this case, it is a traditional itemset min- ing problem); instead, one should also consider the movies watched by his or her friends and friends of friends. In this case, labels associated with two different nodes are related due to the connection between these two nodes. The same mining problem also exists in finding associations of intrusions on the Internet, where each node corresponds to an IP address and there is a directed edge between two IP addresses if an intrusion attack takes place between them. It is interesting to find the association of different attack types, which can be used to analyze intrusions. In this paper, we first introduce an intuitive neighbor association model to define and allocate proximity patterns by identifying the embeddings of these patterns in a graph and then finding a weighted maximum independent set among these embeddings. Although this approach is intuitive, it is inefficient to find patterns in large graphs due to the com- plexity of embedding enumeration and maximum indepen- dent set finding. Therefore, we redefine proximity patterns from an influence point of view, using a probabilistic information propagation model. Based on this model, we pro- pose novel techniques for finding proximity pattern within a large graph, which consider conditional probabilistic association of the labels at each vertex. In the end, a statistical test is developed to measure the significance of discovered proximity patterns. Our Contributions. To the best of our knowledge, this is the first paper introducing the concept of proximity patterns in large graphs. We model the problem of determining the proximity among labels in two distinct approaches, neighbor association and information propagation. While the neighbor association model is a direct approach of finding the association among labels based on their distance across the edges of the graph, we have shown that this method is not efficient for large scale graphs. In the information propagation model, we develop novel probabilistic techniques to determine the proximity among labels in a graph database, based on the Markov model [29]. We justify that they will be efficient as well as consistent under interpretations of  ""relation between trans- actions "" and the  ""association of labels "". The propagation model is able to transform a complex graph mining problem to a simplified probabilistic itemset mining problem, which can be solved efficiently by a modified FP-tree algorithm, called pFP(probabilistic FP-growth). Furthermore, for the discovered patterns, we define an objective function that will measure their interestingness using randomized test. In summary, we propose a complete pipeline to define and mine proximity patterns in massive graphs in a scalable manner. As tested in real-life social networks and intrusion networks, proximity patterns turn to be interesting and are able to capture patterns missed by frequent itemsets and frequent subgraphs. The rest of the paper is organized as follows. In Section 2, we define the abstract problem formulation and prelimi- naries. Our models are introduced in Sections 3 and 4. The probabilistic frequent itemset mining algorithm is described in Section 5. We analyze the experimental results in Sec- tion 6, and discuss related work in Section 7, followed by conclusions in Section 8.",Arijit Khan,"Dept. of Computer Science University of California at Santa Barbara, CA 93106-5110, USA",arijitkhan@cs.ucsb.edu,Xifeng Yan,"Dept. of Computer Science University of California at Santa Barbara, CA 93106-5110, USA",xyan@cs.ucsb.edu,Kun-Lung Wu,"IBM T. J. Watson Research Center 19 Skyline Drive, Hawthorne NY 10532, USA",klwu@us.ibm.com,,,,,,,,,,,,,,,,,,,,,
20200321,1350,Yu Sun,"Department of Computing and Information Systems, University of Melbourne, Victoria, Australia",sun.y@unimelb.edu.au,,K-Nearest Neighbor Temporal Aggregate Queries,"K-Nearest Neighbor Temporal Aggregate Queries, K-Nearest Neighbor Temporal Aggregate Queries, K-Nearest Neighbor Temporal Aggregate Queries, K-Nearest Neighbor Temporal Aggregate Queries, K-Nearest Neighbor Temporal Aggregate Queries, ABSTRACT We study a new type of queries called the k-nearest neigh- bor temporal aggregate (kNNTA) query. Given a query point and a time interval, it returns the top-k locations that have the smallest weighted sums of (i) the spatial distance to the query point and (ii) a temporal aggregate on a cer- tain attribute over the time interval. For example, find a nearby club that has the largest number of people visiting in the last hour. This type of queries has emerging applica- tions in location-based social networks, location-based mo- bile advertising and social event recommendation. It is a great challenge to efficiently answer the query due to the highly dynamic nature and the large volume of the data and queries. To address this challenge, we propose an index named TAR-tree, which organizes locations by integrating the spatial and temporal aggregate information. We per- form a detailed analysis on the cost of processing kNNTA queries using the TAR-tree. The analysis shows that the TAR-tree results in much fewer node accesses than alterna- tives. Furthermore, we propose two enhancements for the kNNTA query: (i) an algorithm suggesting the least amount of weights to be adjusted to explore different query results and (ii) a collective processing scheme to share index traversal among a batch of queries. We conduct extensive exper- iments using real-world data sets. The results validate the accuracy of the cost analysis and show that the TAR-tree outperforms alternatives by up to ten times in node accesses. The results also show that the weight adjustment algorithm and collective processing scheme outperform their baselines by significant margins. 1. INTRODUCTION Location-based services (LBSs) have a large market and this market is growing rapidly. A well-known global market research company MarketsandMarkets forecasts in a recent report that the LBSs market will grow from $8.12 billion in 2014 to $39.87 billion in 2019. Location-based social net- works (LBSNs) [31] have been a driving force for the growth of LBSs. Many emerging applications enable users to ex- plore their neighborhood with rich social information in a highly customized fashion. For example, using the function- ality Places Nearby (e.g., in Facebook or Foursquare), users may want to find nearby attractions that have the most vis- its recently or find a nearby club that is gathering the most people in the last hour; using the functionality Explore (e.g., in Flickr or Instagram), users may want to browse photos taken nearby and have the most likes lately. These applications require ranking locations (or geotagged media contents) based on two criteria: (i) the spatial dis- tance and (ii) a temporal aggregate on a certain attribute (e.g., the visits or likes). The spatial distance indicates the degree of closeness while the temporal aggregate reflects the social opinion in a certain period. These applications ex- hibit three key characteristics, which create a highly dy- namic environment: (i) The visits or likes happen contin- uously, making the aggregate data grow rapidly. For in- stance, there were 3 million check-ins per day in Foursquare by May 2014. The number of the aforementioned requests is also very large. (ii) The time interval a user interested in is highly customized, which may vary from hours (e.g., for retrieving current events) to years (e.g., for long term anal- yses). (iii) The users may adjust their weighting on the two criteria widely to explore results of different preferences. The skyline operator [6] can support multi-criteria deci- sion problems. However, the skyline operator is computa- tionally expensive even for static data and queries. The highly dynamic environment and the large volume of re- quests and objects generated in LBSNs make it prohibitive to use the skyline operator. Moreover, users are not given the flexibility in determining their preference over the two criteria. Following existing studies [9][15][22], we rank the locations using a weighted sum of the spatial distance and the temporal aggregate. We formulate the problem as the k-nearest neighbor temporal aggregate (kNNTA) query (for- mally defined in Section 3). Apart from the above applica- tions in LBSNs, kNNTA queries are useful in many other applications in urban computing [32] where the spatial dis- tance and a temporal aggregate are considered simultane- ously, such as location-based mobile advertising and social event recommendation. The kNNTA query requires quick response since users usu- ally use the query to browse locations or geotagged media contents in the neighborhood. Due to the dynamic nature and the huge volume of the data and queries, having an effi- cient solution to this type of queries is challenging. Existing indexing structures cannot manage the locations effectively     493 10.5441/002/edbt.2015.43 based on both spatial closeness and temporal aggregate in- formation simultaneously (detailed discussion in the related work, Section 2). To efficiently process the kNNTA query, we propose a novel index named the TAR-tree, in which the locations are organized by integrating the spatial and tem- poral aggregate information. We perform a detailed analy- sis on the cost of query processing using the TAR-tree. The analysis shows that the TAR-tree results in much fewer node accesses than alternatives that organize the locations based on only the spatial or the temporal aggregate information. The analysis can also be used as a cost model for query optimization. Furthermore, we propose two enhancements for the kNNTA query: (i) To help users explore results of different preferences, we propose an efficient algorithm sug- gesting the least amount of weights to be adjusted between the two criteria so that the query results will change. (ii) To handle large number of queries, we propose a collective processing scheme to share index traversal among a batch of queries. In summary, the main contributions of this paper are as follows. ? We propose a query called the k-nearest neighbor tem- poral aggregate (kNNTA) query to address emerging applications that requires ranking locations on both (i) the spatial distance and (ii) a temporal aggregate on a certain attribute. ? We propose a novel index named the TAR-tree to ef- ficiently process the kNNTA query. We perform a de- tailed analysis on the cost of query processing using the TAR-tree, which shows that the TAR-tree results in much fewer node accesses than alternatives. ? We propose two enhancements for the kNNTA query: (i) an algorithm suggesting the least amount of weights to be adjusted to explore different query results and (ii) a collective processing scheme to share index traversal among a batch of queries. ? We conduct extensive experiments using real-world data sets. The results validate the accuracy of the cost anal- ysis, and show that the TAR-tree outperforms alterna- tives by up to ten times in node accesses. The results also show that the weight adjustment algorithm and collective processing scheme outperform their baselines by significant margins. The rest of the paper is organized as follows. Section 2 reviews related work. Section 3 formalizes the kNNTA query. Section 4 presents the TAR-tree. Section 5 discusses group- ing strategies. Section 6 provides the analysis. Section 7 gives two enhancements. Section 8 reports the experiment results and Section 9 concludes the paper.",Yu Sun,"Department of Computing and Information Systems, University of Melbourne, Victoria, Australia",sun.y@unimelb.edu.au, Jianzhong Qi,"Department of Computing and Information Systems, University of Melbourne, Victoria, Australia",jianzhong.qi@unimelb.edu.au,Yu Zheng,"Microsoft Research, Beijing, P.R.China",yuzheng@microsoft.com,Rui Zhang,"Department of Computing and Information Systems, University of Melbourne, Victoria, Australia",rui.zhang@unimelb.edu.au,,,,,,,,,,,,,,,,,,
20200322,1366,Oktie Hassanzadeh ,IBM T.J. Watson Research Center,hassanzadeh@us.ibm.com,,Discovering Linkage Points over Web Data,"Discovering Linkage Points over Web Data, Discovering Linkage Points over Web Data, Discovering Linkage Points over Web Data, Discovering Linkage Points over Web Data, Discovering Linkage Points over Web Data, ABSTRACT A basic step in integration is the identification of linkage points, i.e., finding attributes that are shared (or related) between data sources, and that can be used to match records or entities across sources. This is usually performed using a match operator, that associates attributes of one database to another. However, the mas- sive growth in the amount and variety of unstructured and semi- structured data on the Web has created new challenges for this task. Such data sources often do not have a fixed pre-defined schema and contain large numbers of diverse attributes. Furthermore, the end goal is not schema alignment as these schemas may be too hetero- geneous (and dynamic) to meaningfully align. Rather, the goal is to align any overlapping data shared by these sources. We will show that even attributes with different meanings (that would not qualify as schema matches) can sometimes be useful in aligning data. The solution we propose in this paper replaces the basic schemamatching step with a more complex instance-based schema analysis and linkage discovery. We present a framework consisting of a library of efficient lexical analyzers and similarity functions, and a set of search algorithms for effective and efficient identification of linkage points over Web data. We experimentally evaluate the effectiveness of our proposed algorithms in real-world integration scenarios in several domains. Categories and Subject Descriptors H.2.4 [Database Management]: Systems; H.2.5 [Database Man- agement]: Heterogeneous Databases Keywords Record Linkage, Entity Resolution, Link Discovery, Schema Matching, Data Integration 1. INTRODUCTION Many increasingly important data management and mining tasks require integration and reconciliation (or fusion) of data that re- side in large and heterogeneous data sources. Data integration is generally defined as combining data to provide users with a uni- fied view of the data [15] whereas in data fusion, duplicates are merged and conflicting attributes values are identified and possibly repaired in order to provide a single consistent value for each data attribute [3]. Data fusion therefore involves duplicate detec- tion, also known as Entity Resolution or record linkage, where the goal is to identify data records that refer to the same entity. The first step in a data integration or fusion system is identification of linkage points between the data sources, i.e., finding correspon- dences between attributes in the data sources. Traditionally, this is performed by schema matching, where the goal is to identify the schema elements of the various data sources that are semantically related. However, the massive growth in the amount of unstruc- tured and semi-structured data in data warehouses and on the Web has created new challenges for this task. In what follows, we first describe an example real-world integration scenario and then de- scribe the unique challenges not addressed in previous work. Consider a scenario where data about public companies is gath- ered from different sources on the Web. We have collected three data sets from online sources: Freebase [28], DBpedia [23], and the U.S. Securities and Exchange Commission (SEC) [29]. The data sets are respectively extracted using Freebase 's Metaweb Query Language, DBpedia 's SPARQL endpoint, and IBM SystemT [4] applied on the online SEC forms. Each of the three data sets is converted into a collection of JSON encoded records. Each JSON record is a tree representing various facts about a public company (an entity). Table 1 shows some statistics over these data sets. One can see that the three data sets are very different in structure. JSON trees in DBpedia have over 1700 different paths (or attributes), and describe 1.9 million facts (or entity-attribute-value triples), while JSON trees in SEC have only 72 distinct paths, but describe 4.5 mil- lion facts. This shows that DBpedia contains very heterogeneous records, while records in the SEC data set have a more consistent structure. Another observation is that Freebase has over 74,000 JSON records describing 1.9 million facts, while SEC has only ap- proximately 2,000 records, but describes 4.5 million facts. The SEC data contains much more elaborate records than Freebase. The three data sets should have significant overlap due to the common topic (public companies). Despite the fact that the SEC data has the greatest structural regularity, this data does not sub- sume the other data sets, so alignment and use of these data sets together can provide more information than any single source. Toward data integration, we first need to identify linkage points, i.e., paths (attributes) in the JSON trees that are shared or related among the data sets and that are useful in identifying entities that can be linked (a formal definition is given in the next section). One possible approach is to apply schema matching algorithms [18] based on the schema information of the data sets. A purely schema-based matching algorithm would fail in many cases. For instance, DBpedia contains the labels dbpedia:stockSymbol, dbpedia:stockTicker and dbpedia:tickerSymbol for stock symbols.1 Further investigation of the instances reveals that each of these three attributes in DBpedia actually contain only a single value, perhaps because the DBpedia extraction algorithm has been unable to extract the stock symbols from Wikipedia. So, the label name in this case does not reflect the data, and is not use- ful for matching. Moreover, matching Freebase and SEC based on the ticker symbol and stockSymbol attributes results in ambiguous links (one company matched with more than one com- pany on the other side). This happens because some (subsidiary) companies in Freebase share stock symbols with their parent com- panies. This shows that these stock symbol attributes are not as strong linkage points as one would expect. Even if the schema labels are meant to be representative and can be used for matching schema elements, there could be differences in data representation and style that make match- ing the records difficult. In fact, our experience in this and other similar scenarios (as described in Section 5) show that for the most interesting and useful linkage points, schema la- bels and values do not match using simple string comparison. For example, there are different attribute labels used for URLs (e.g., url in Freebase, foaf:page and foaf:homepage in DBpedia) and there are different ways of writing URLs (e.g., http://ibm.com vs. http://www.ibm.com/). Another example is different representations of identifiers, e.g., the unique identifiers in SEC, called CIKs, are fixed-length numbers such as #0000012345 stored in an attribute labeled cik but Free- base represents them as /business/cik/12345 stored in the identifier attribute id. There are also cases where only a part of the values can be used to link the records. For example, URI http://dbpedia.org/resource/Citigroup in DBpe- dia matches with ID /en/topic/citigroup in Freebase, and URL http://www.audi.com/ matches with name Audi. Since Citigroup and Audi are relatively rare names, (URI,ID) and (URL,name) attribute pairs can effectively be used to link these records. However, such linkage points can easily be missed un- less the user has a thorough knowledge of both data sets. Gaining such knowledge could be challenging as it may require examining a large and representative portion of the data to understand when an attribute could be useful in linking a portion of the data. In traditional data integration systems, identification of link- age points is performed either manually (possibly using a user- interface designed for matching schema elements) or by an auto- matic or semi-automatic schema matching algorithm. However, the size and heterogeneity of the schema, along with schema errors present in many sources that use automated information extraction, make many existing schema-based approaches inaccurate in align- ing may data sets. In addition, the size and heterogeneity of Web data makes existing instance-based approaches [14, 22] ineffective and inefficient in aligning data. 1The data we have used was retrieved in January 2010. DBpedia data and schema have considerably changed since then, so the statistics and examples here may no longer be current, but can be verified using the data dumps available on our project page [30]. In this paper, we present a framework for identification of link- age points for multi-source Web data integration. Our framework includes a novel class of search algorithms to identify strong link- age points (that is, attributes that can be used to link entities across data sets) even when such attributes are weak schema matches. Im- portantly, we are specifically looking for attributes that help in iden- tification of entities that can be linked. So unlike in schema match- ing, we are not interested in finding all corresponding attributes (for example, matching color and colour). As a result, our search can be much more focused. Our algorithms take advantage of 1) a library of lexical analyzers, 2) fast record-level and token- level inverted indices, 3) a library of similarity functions, and 4) a set of filtering strategies to filter false-positive results of the search. We have implemented and experimentally evaluated the framework in several real world Web data integration scenarios such as the one described above. We show the effectiveness of different com- ponents of the framework in discovering linkage points in these scenarios, and how the discovered linkage points can enhance the record linkage process. Next, we present our problem definition. Section 3 presents our proposed framework, and Section 4 presents the details of the search algorithms for attribute selection and identification of link- age points. We present a thorough experimental evaluation of the search algorithms in Section 5. Sections 6, 7, and 8 conclude the paper with a summary of the results, brief overview of the related work, and a few interesting directions for future work.",Oktie Hassanzadeh ,IBM T.J. Watson Research Center,hassanzadeh@us.ibm.com,Ken Q. Pu,UOIT,ken.pu@uoit.ca,Soheil Hassas Yeganeh,University of Toronto,soheil@cs.toronto.edu,Renee J. Miller,University of Toronto,miller@cs.toronto.edu,Lucian Popa,IBM Research ?? Almaden,lpopa@us.ibm.com,Mauricio A. Hernandez,IBM Research ?? Almaden,mahernan@us.ibm.com,Howard Ho,IBM Research ?? Almaden,ctho@us.ibm.com,,,,,,,,,
20200323,1367,Manasi Vartak,Massachusetts Institute of Technology,mvartak@mit.edu@csail.mit.edu,,Refinement Driven Processing of Aggregation Constrained Queries,"Refinement Driven Processing of Aggregation Constrained Queries, Refinement Driven Processing of Aggregation Constrained Queries, Refinement Driven Processing of Aggregation Constrained Queries, Refinement Driven Processing of Aggregation Constrained Queries, Refinement Driven Processing of Aggregation Constrained Queries, ABSTRACT Although existing database systems provide users an efficient means to select tuples based on attribute criteria, they however provide lit- tle means to select tuples based on whether they meet aggregate requirements. For instance, a requirement may be that the cardi- nality of the query result must be 1000 or the sum of a particular attribute must be < $5000. In this work, we term such queries as ""Aggregation Constrained Queries"" (ACQs). Aggregation con- strained queries are crucial in many decision support applications to maintain a product's competitive edge in this fast moving field of data processing. The challenge in processing ACQs is the unfamil- iarity of the underlying data that results in queries being either too strict or too broad. Due to the lack of support of ACQs, users have to resort to a frustrating trial-and-error query refinement process. In this paper, we introduce and define the semantics of ACQs. We pro- pose a refinement-based approach, called ACQUIRE, to efficiently process a range of ACQs. Lastly, in our experimental analysis we demonstrate the superiority of our technique over extensions of ex- isting algorithms. More specifically, ACQUIRE runs up to 2 orders of magnitude faster than compared techniques while producing a 2X reduction in the amount of refinement made to the input queries. 1. INTRODUCTION Databases provide a number of ways to efficiently select tuples of interest to the user by constraining attributes of individual tuples, for instance, return tuples that meet the criteria price < $50, join results between tuples in table A and table B that match on attribute ""id,"" etc. However, little effort has been focused on a means of se- lecting tuples based on whether they satisfy aggregate constraints. For instance, select tuples with average price < $10, number of tuples = 1000, etc. The ability to apply aggregate constraints along with constraints on tuples' individual attribute values is important in many applications as illustrated below. ? In advertising campaigns (such as Example 1), the budget restricts the number of users that can be reached [4]; as a re- sult, the campaign manager must select users based not only on demographics but also whether the total number of users (i.e. the COUNT) is within the budget limit. In a supply chain application, a requirement on the total num- ber of parts to be ordered from suppliers translates to a constraint on the sum of the number of parts available with each supplier (Example 2). As a result, queries must place constraints not only on part specifications but also the SUM of the parts available. ? When analyzing large data sets through aggregates [15], users often want to identify what input tuples produced outliers in aggregate values (e.g. select patients who had extremely high average cost). In this case, the user would like to place con- straints on the AVG aggregate. Example 1. HighStyle Designers would like to run a Facebook 1 ad campaign to get more users to ""like"" their page. The cam- paign budget of $10, 000 will allow HighStyle to reach 1 million customers. Therefore, when the campaign manager, Alice, selects target users, she must not only constrain her search based on cus- tomer demographics but also based on the total number of cus- tomers who must be reached. This situation thus calls for an ""Aggregation Constrained Query"" (ACQ).  Figure 1 shows the Facebook's Advertising Interface2 that allows campaign manager Alice to select target users for her ad. In terms of SQL, Alice has to run the following query: Q1: SELECT * FROM Users WHERE location in ('Boston', 'New York', 'Seattle', 'Miami', 'Austin') AND (gender = 'Women') AND (25 <= age <= 35) AND (education = 'CollegeGrad') AND (relationshipStatus = 'Single') AND (interests IN {'Retail', 'Shopping'}) Need for Query Refinement. For Alice's above query, Face- book estimates the reach to be 393,980 users, i.e. only 40% of the required 1 million users. While the results of query Q1 precisely satisfy Alice's selection predicates, they are far from meeting her aggregate constraints. In fact, selection and aggregation constraints are orthogonal in most cases. As a result, we need to refine various query predicates in order to meet the aggregate constraints. Current Approach. In existing systems Alice has to manually alter her criteria to encompass more users while ensuring that the semantics of her query are not altered. While some selection cri- teria (e.g. gender and shopping interest) may be fixed, Alice can try potentially infinite refinements of her predicates such as target consumers in additional cities; alter age range; relax relationship status; or any combination of the above. Repeatedly altering the original query and having its size estimated is not only inefficient for the backend, but the process is tedious and frustrating for Alice. Desired User Experience. A much better user experience can be provided if Alice was allowed to specify her (1) demographic criteria (query), and (2) aggregate constraints, and the database en- gine can then execute variations of the input query such that the aggregate constraints are met. The output of such a search would be a set of refined queries that change Q1 as little as possible while meeting the aggregate constraints (in our case the audience size). Alice would then simply pick the query that best meets her selec- tion criteria. In this paper, we encode ACQs by introducing two SQL key- words CONSTRAINT and NOREFINE, where CONSTRAINT cap- tures the aggregate constraint and NOREFINE specifies whether the predicate should not be refined. The encoded Query Q1 is: Q1': SELECT * FROM Users CONSTRAINT COUNT(*)=1M WHERE location in ('Boston', 'New York', 'Seattle', 'Miami', 'Austin') AND (gender = 'Women') NOREFINE AND (25 <=age<=35) AND (education = 'CollegeGrad') AND (relationshipStatus = 'Single') AND (interests IN {'Retail', 'Shopping'}) NOREFINE; Running Q1' will automatically generate alternate queries that produce 1M customers and alter Q1 as little as possible. Example 2. HybridCars Co. would like to place an order for 100,000 units of a burnished steel part having specific size, whole- sale price less than $1000, and from suppliers who have a low ac- count balance. On the TPC-H benchmark, HybridCars runs query Q2 to find the suppliers with whom to place the order. Q2: SELECT * FROM supplier, part, partsupp WHERE (s_suppkey = ps_suppkey) AND (p_partkey = ps_partkey) AND (s_acctbal < 2000) AND (p_retailprice < 1000) AND (p_size = 10) AND (p_type = 'SMALL BURNISHED STEEL') As in Example 1, this situation calls for an ACQ as we would like to constrain the total number of available parts, i.e. sum of the number of parts available per supplier (i.e. SUM(ps_availqty)) in addition the select predicates. We can encode the ACQ as Q2' to produce alternate refined queries. As before, the NOREFINE key- word associated with p_type and p_size indicate that these predi- cates cannot be altered. Q2': SELECT * FROM supplier, part, partsupp CONSTRAINT SUM(ps_availqty) >= 0.1M WHERE (s_suppkey = ps_suppkey) NOREFINE AND (p_partkey = ps_partkey) NOREFINE AND (p_retailprice < 1000) AND (s_acctbal < 2000) AND (p_size = 10) NOREFINE AND (p_type = 'SMALL BURNISHED STEEL') NOREFINE Building a system to execute ACQs is challenging because the number of possible refined queries is exponential in the number of predicates. Hence an exhaustive search of all possible queries is prohibitively expensive. Moreover even for aggregates such as COUNT, finding a query that meets its constraint is an NP-Hard problem [1]. In this paper, we limit ourselves to ACQs with nu- merical select and join predicates, and aggregates that satisfy the optimal substructure property (Section 2). Additionally, we focus on the problem of expanding predicates to meet constraints, rather than the inverse problem of shrinking queries returning too many tuples. Contributions. We propose a technique to efficiently execute ACQs and our contributions are summarized as follows: ? We introduce and define semantics of a new class of queries called an Aggregation Constrained Query (ACQ). These special purpose queries are of value in real-world applica- tions and are amenable to clever execution techniques. ? We propose a technique called ACQUIRE to execute ACQs via query refinement. ACQUIRE auto-generates alternative refined queries that minimize changes to the original query while meeting aggregate constraints. ? We combine the building blocks of breadth-first-search and dynamic programming in a novel way to elegantly and effi- ciently re-use query results. We call this Incremental Aggre- gate Computation (Section 5). ? We propose sensible default query refinement scoring and aggregate error functions. The design principle of ACQUIRE is general and therefore we allow user defined predicate re- finement scoring and aggregate error functions. The func- tions used in this work are merely sensible defaults. ? Our experimental analysis on TPC-H dataset demonstrates that ACQUIRE consistently out-performs extensions to cur- rent techniques by up to 2 orders of magnitude. Moreover, queries recommended by ACQUIRE are on average closer to the original query by a factor of 2X more than the compared techniques (Section 8). ",Manasi Vartak,Massachusetts Institute of Technology,mvartak@mit.edu@csail.mit.edu,Venkatesh Raghavan,Pivotal Inc.,vraghavan@pivotal.io@csail.mit.edu,Elke Rundensteiner,Worcester Polytechnic Institute,3rundenst@csail.mit.edu,Samuel Madden,Massachusetts Institute of Technology,madden@csail.mit.edu,,,,,,,,,,,,,,,,,,
20200324,1368,Kyriaki Dimitriadou,"Brandeis University, Waltham, MA, USA",kiki@cs.umass.edu,http://dx.doi.org/10.1145/2588555.2610523.,Explore-by-Example: An Automatic Query Steering Framework for Interactive Data Exploration,"Explore-by-Example: An Automatic Query Steering Framework for Interactive Data Exploration, Explore-by-Example: An Automatic Query Steering Framework for Interactive Data Exploration, Explore-by-Example: An Automatic Query Steering Framework for Interactive Data Exploration, Explore-by-Example: An Automatic Query Steering Framework for Interactive Data Exploration, Explore-by-Example: An Automatic Query Steering Framework for Interactive Data Exploration, ABSTRACT Interactive Data Exploration (IDE) is a key ingredient of a diverse set of discovery-oriented applications, including ones from scien- tific computing and evidence-based medicine. In these applica- tions, data discovery is a highly ad hoc interactive process where users execute numerous exploration queries using varying predi- cates aiming to balance the trade-off between collecting all relevant information and reducing the size of returned data. Therefore, there is a strong need to support these human-in-the-loop applications by assisting their navigation in the data to find interesting objects. In this paper, we introduce AIDE, an Automatic Interactive Data Exploration framework, that iteratively steers the user towards in- teresting data areas and ""predicts"" a query that retrieves his objects of interest. Our approach leverages relevance feedback on database samples to model user interests and strategically collects more sam- ples to refine the model while minimizing the user effort. AIDE in- tegrates machine learning and data management techniques to pro- vide effective data exploration results (matching the user's interests with high accuracy) as well as high interactive performance. It delivers highly accurate query predictions for very common conjunctive queries with very small user effort while, given a reasonable number of samples, it can predict with high accuracy complex conjunctive queries. Furthermore, it provides interactive performance by limiting the user wait time per iteration to less than a few sec- onds in average. Our user study indicates that AIDE is a practical exploration framework as it significantly reduces the user effort and the total exploration time compared with the current state-of-the-art approach of manual exploration. Categories and Subject Descriptors H.2.4 [Systems]: Relational Databases Keywords data exploration; database sampling; query formulation. 1. INTRODUCTION Traditional DBMSs are designed for applications in which the user queries to be asked are already well understood. There is, however, a class of interactive data exploration (IDE) applications, in which users are trying to make sense of the underlying data space by experimenting with queries, backtracking on the basis of query results and rewriting their queries aiming to discover interesting data objects. IDE often incorporates ""human-in-the-loop' and it is fundamentally a long-running, multi-step process with the user data interests often specified in imprecise terms. One example of IDE can be found in the domain of evidencebased medicine (EBM). Such applications often involve the gener- ation of systematic reviews, a comprehensive assessment of the to- tality of evidence that addresses a well-defined question, such as the effect on mortality of giving versus not giving drug A within three hours of a symptom B. While a content expert can judge whether a given clinical trial is of interest or not (e.g., by reviewing pa- rameter values such as disease, patient age, etc.), he often does not have a priori knowledge of the exact attributes that should be used to formulate a query to collect all relevant clinical trials. Therefore the user relies on an ad hoc process that includes three steps: 1) processing numerous selection queries with iteratively varying selection predicates, 2) reviewing returned objects (i.e., trials) and classifying them to relevant and irrelevant, and 3) adjusting accord- ingly the selection query for the next iteration. The goal here is to discover a query that balances the trade-off between collecting all relevant clinical trials and reducing the size of returned trial docu- ments. These ""manual"""" explorations are typically labor-intensive: they may take days to weeks to complete since users need to examine thousands of objects. Scientific applications, such as ones analyzing astrophysical sur- veys (e.g., [2, 4]), also suffer from similar situations: scientists may not be able to express their data interests precisely. Instead, they may want to navigate through a subspace of the data set (e.g., a region of the sky) to find objects of interest, or may want to see a few samples, provide yes/no feedback, and expect the system to find more similar objects. To address the needs of IDE applications, we propose an auto- matic interactive data exploration (AIDE) framework that automat- ically ""steers"" the user towards data areas relevant to his interest. Our approach integrates the three IDE steps-query formulation, query processing, and result reviewing-into a single automatic process, significantly reducing the user effort and the overall ex- ploration time. In AIDE, the user engages in a ""conversation"" with the system indicating his interests, while in the background the system automatically formulates and processes queries that collect data matching the user interest. 517 In AIDE, the user is prompted to label a set of strategically col- lected sample objects (e.g., clinical trials) as relevant or irrelevant to his IDE task. Based on his feedback, AIDE generates the user exploration profile which is used to collect a new set of sample objects. These new samples are presented to the user and his relevance feedback is incorporated into his profile. In the background, AIDE leverages the user profile to automatically generate data ex- traction queries that retrieve more objects relevant to the user's IDE task while minimizing the retrieval of irrelevant ones. The design of AIDE raises new challenges. First, AIDE operates on the unlabeled space of the whole data space that the user aims to explore. To offer effective data exploration results (i.e., accu- rately predict the user's interests) it has to decide and retrieve in an online fashion the example objects to be extracted and labeled by the user. Second, to achieve desirable interactive experience for the user, AIDE needs not only to provide accurate results, but also to minimize the number of samples presented to the user (which determines the amount of user effort). These challenges cannot be addressed by existing machine learn- ing techniques. Classification algorithms (e.g., [9]) can be lever- aged to build the user model and the information retrieval com- munity offers solutions on incrementally incorporating relevance feedback in these models (e.g., [31]). However, these approaches operate under the assumption that the sample set shown to the user is either known a priori or, in the case of online classification, it is provided incrementally by a different party. Therefore, classi- fication algorithms do not deal with which data samples to show to the user. Furthermore, the active learning community has pro- posed solutions that maximize the accuracy of the model while minimizing the number of samples shown to the user. However, these techniques are domain specific (e.g., document ranking [25], image retrieval [23], etc.) and they exhaustively examine all ob- jects in the data set in order to identify the best samples to show to the user [24]. Therefore, they implicitly assume negligible sample acquisition costs and hence cannot offer interactive performance on big data sets as expected by IDE applications. In either case, model learning and sample acquisition are decoupled, with the active learning algorithms not addressing the challenge of how to minimize the cost of sample acquisition. To address the above challenges, AIDE closely integrates clas- sification model learning (from existing labeled samples) and ef- fective data exploration and sample acquisition (deciding new data areas to sample). Our techniques leverage the classification proper- ties of decision tree learning to discover promising data exploration areas from which new training samples are extracted, as well as to minimize the number of samples required. These techniques aim to predict linear patterns of user interests, i.e., range selection queries. The specific contributions of this work are the following: 1. We introduce AIDE, a novel, automatic data exploration framework, that navigates the user throughout the data space he wishes to explore. AIDE relies on the user's feedback on example objects to formulate queries that retrieve data rele- vant to the user. It employs a unique combination of machine learning, data exploration, and sample acquisition techniques to deliver highly accurate predictions of linear patterns of user interests with interactive performance. 2. We propose data exploration techniques that leverage the properties of classification models to identify single objects of interest, expand them to more accurate areas of interests, and progressively refine the characterization of these areas. 3. We introduce optimizations that reduce the number of sam- ples required by each of the proposed exploration techniques, the number of sample extraction queries, and the user wait time. Our optimizations are designed to address the trade-off between quality of results (i.e., accuracy) and efficiency (i.e., the total exploration time which includes the total sample re- viewing time and wait time by the user). 4. We evaluated our implementation of AIDE using the SDSS database [4] and a user study. Our results indicate that AIDE can predict common conjunctive queries with a small num- ber of samples, while given an acceptable number of labeled samples it predicts highly complex disjunctive queries with high accuracy. AIDE also offers interactive performance as the user wait time per iteration is less than a few seconds in average. Our user study revealed that AIDE can reduce the user's labeling effort by up 87%, with an average of 66% re- duction. When also including the sample reviewing time, it reduced the total exploration time by 47% in average. The rest of the paper is organized as follows. Section 2 outlines the AIDE framework and the phases of our data exploration approach. Section 3 discusses the object discovery phase, and Sec- tions 4 and 5 describe the misclassified and boundary exploitation phase, respectively. Section 6 presents our experimental results. Section 7 discusses the related work and we conclude in Section 8.",Kyriaki Dimitriadou,"Brandeis University, Waltham, MA, USA",kiki@cs.umass.edu,Olga Papaemmanouil,"Brandeis University, Waltham, MA, USA",olga@cs.umass.edu,Yanlei Dia,"University of Massachusetts, Amherst, MA, USA",yanlei@cs.umass.edu,,,,,,,,,,,,,,,,,,,,,
20200325,866,Foteini Katsarou,"School of Computing Science University of Glasgow, UK",f.katsarou.1@research.gla.ac.uk,,Subgraph Querying with Parallel Use of Query Rewritings and Alternative Algorithms,"Subgraph Querying with Parallel Use of Query Rewritings and Alternative Algorithms, Subgraph Querying with Parallel Use of Query Rewritings and Alternative Algorithms, Subgraph Querying with Parallel Use of Query Rewritings and Alternative Algorithms, Subgraph Querying with Parallel Use of Query Rewritings and Alternative Algorithms, Subgraph Querying with Parallel Use of Query Rewritings and Alternative Algorithms, ABSTRACT Subgraph queries are central to graph analytics and graph DBs. We analyze this problem and present key novel discoveries and observations on the nature of the problem which hold across query sizes, datasets, and top-performing algorithms. Firstly, we show that algorithms (for both the de- cision and matching versions of the problem) suffer from straggler queries, which dominate query workload times. As related research caps query times not reporting results for queries exceeding the cap, this can lead to erroneous con- clusions of the methods ' relative performance. Secondly, we study and show the dramatic effect that isomorphic graph queries can have on query times. Thirdly, we show that for each query, isomorphic queries based on proposed query rewritings can introduce large performance benefits. Fourthly, that straggler queries are largely algorithm-specific: many challenging queries to one algorithm can be executed effi- ciently by another. Finally, the above discoveries naturally lead to the derivation of a novel framework for subgraph query processing. The central idea is to employ parallelism in a novel way, whereby parallel matching/decision attempts are initiated, each using a query rewriting and/or an alternate algorithm. The framework is shown to be highly ben- eficial across algorithms and datasets. CCS Concepts ?Information systems  Database query processing; ?Mathematics of computing   Graph algorithms; Keywords Graph databases, graph query processing, subgraph isomorphism 1. INTRODUCTION Graphs are ideal for representing complex entities and their relationships/interactions and subgraph querying is es- sential to graph analytics. In subgraph querying, given a pattern graph (query) and a graph DB, we want to know whether it is contained in each DB graph (the decision prob- lem) and/or find all its occurrences within it (the match- ing problem). Subgraph querying entails the subgraph iso- morphism problem (abbreviated as sub-iso), which is NP- complete. Subgraph querying has received a lot of atten- tion. Related work is categorized in two major categories: the filter-then-verify (FTV) and the no-filter, verify (NFV) methods. Numerous methods have been proposed for the problem and three recent experimental analysis papers ([7, 9, 12]) compare and stress-test proposed methods. In this work, we conduct a comprehensive analysis of this problem. Our analysis aims to (i) lead to interesting novel findings about the nature of the problem and existing solu- tions, (ii) analyse and quantify said discoveries and their ef- fect on well-established existing solutions, and (iii) show that the findings can be used to develop a framework that can offer large performance gains. Specifically, we first recognize the existence of ""straggler ""queries; i.e., queries whose execu- tion time is dramatically higher than the rest. This holds for all query workloads and all datasets examined and across all tested FTV and NFV algorithms. Subsequently, we reveal and quantify the interesting fact that isomorphic instances of queries can have a wild variation in querying times. Then we generate isomorphic instances of the original query using statistics on vertex-label frequencies and/or vertex degrees and we investigate their performance. Moreover, for NFV methods in particular, we additionally show that challeng- ing queries are algorithm-specific, with a straggler query for one algorithm possibly being easy for others. Finally, we incorporate these findings in a novel framework, coined the -framework, that exploits parallelism for both FTV and NFV methods, achieving large performance gains. Specifi- cally, instead of trying to come up with new algorithms for sub-iso testing, we utilize isomorphic query rewritings and existing alternative algorithms in parallel. Extensive experi- mentation shows that our framework can be highly beneficial across datasets and workloads, and for both FTV and NFV methods.",Foteini Katsarou,"School of Computing Science University of Glasgow, UK",f.katsarou.1@research.gla.ac.uk,Nikos Ntarmos,"School of Computing Science University of Glasgow, UK",nikos.ntarmos@glasgow.ac.uk,Peter Triantafillou,"School of Computing Science University of Glasgow, UK",peter.triantafillou@glasgow.ac.uk,,,,,,,,,,,,,,,,,,,,,
20200326,1369,Yinan Li,University of Wisconsin?Madison,yinan@cs.wisc.edu,,WHAM: A High-throughput Sequence Alignment Method,"WHAM: A High-throughput Sequence Alignment Method, WHAM: A High-throughput Sequence Alignment Method, WHAM: A High-throughput Sequence Alignment Method, WHAM: A High-throughput Sequence Alignment Method, WHAM: A High-throughput Sequence Alignment Method, ABSTRACT Over the last decade the cost of producing genomic sequences has dropped dramatically due to the current so called ""next-gen"" sequencing methods. However, these next-gen sequencing methods are critically dependent on fast and sophisticated data processing methods for aligning a set of query sequences to a reference genome using rich string matching models. The focus of this work is on the design, development and evaluation of a data processing system for this crucial ""short read alignment"" problem. Our system, called WHAM, employs novel hash-based indexing methods and bitwise operations for sequence alignments. It allows richer match models than existing methods and it is significantly faster than the existing state-of-the-art method. In addition, its relative speedup over the existing method is poised to increase in the future in which read sequence lengths will increase. The WHAM code is available at http://www.cs.wisc.edu/wham/. Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information search and retrieval-search process; H.2.4 [Database Management]: Systems-textual databases General Terms Algorithms, performance Keywords Sequence alignment, approximate string matching, bit-parallelism 1. INTRODUCTION Last summer marked the 10th anniversary of the sequencing of the first human genome [1, 21]. This key scientific discovery has been a turning point for modern life sciences and has dramatically changed the way in which researchers approach nearly every as- pect of biomedical sciences, ranging from deciphering basic cel- lular mechanisms to drug discovery and drug design for personal- ized medicine. A crucial part of this first human genome assembly was using advanced data processing methods to assemble the entire genome from vast sets of data items, each of which described only a small portion (called a ""read"") of the genome. The human genome project delivered its first draft ahead of schedule, primarily because of the use of advanced data processing methods. Existing sequencing technology has come a long way from the technology of 10 years ago, both in terms of speed (with which they can read parts of the genome) and the cost of reading each of the 3 billion ""bases"" that make up the whole human genome. As a consequence, while the first human genome took a few billion dol- lars to assemble, today with the help of next-generation sequencing machines an entire genome can be read for a few thousand dollars. Interestingly, data processing techniques are an even more cru- cial aspect of assembling genomes today. The pressing problem now with the next-generation of sequencing is the so called ""nextgeneration gap"" ? namely, the data processing cost associated with genomic analysis is now the dominating cost of producing a genome sequence [15]. A key component of this data processing task is the alignment of short sequence reads [15]. The focus of this paper is on the design and evaluation of a novel data processing system for speeding up this alignment task by an order of magnitude and more, while accommodating flexible match models. Our system is called WHAM ? an acronym for Wisconsin's High-throughput Alignment Method. WHAM employs novel database-style index- ing, optimization and query processing techniques. At its heart, the short sequence read alignment problem is similar to the common substring matching problem in data processing sys- tems. At a high-level, the next-gen sequencing works as follows: First the (DNA) sample to be sequenced is broken (by treating with restriction enzymes or using mechanical force) into a number of short pieces. These pieces are then cut into equal-length fragments called ""reads"". The bases/characters in each fragment are read by the sequencing machine. From the computational perspective the set of reads/fragments can be modeled as a set of strings/sequences. The subsequent data processing task involves aligning each read se- quence against some scaffolding/reference genome sequence. For example, when assembling the genome for a specific indi- vidual to look for variations that cause specific diseases, the refer- ence genome is often the publicly available human genome. Thus, from the string matching perspective, one can view the computation task as matching a set of equal-length short strings (reads) against a large reference string (the entire genome), using some string match- ing model, as described below. When a read is compared to the reference genome, it is either deemed as a valid or an invalid alignment. The validity of an align- ment is measured in terms of mismatches and gaps. A mismatch occurs when a base on the reference genome and a base on the read are aligned, but aren't the same base. A gap occurs when a base is aligned with an empty space. Either or both of these alignments may be the ""right answer"" when aligning a read if it is advanta- 445 geous to do so, such as when it allows many other base pairs in the set of reads to be aligned with the reference genome sequence. Often, aligners use two mismatches and no gaps as the definition of a valid alignment, but these models are likely to get more sophisti- cated in the future [15]. Previous methods for sequencing reads include Maq [12], SOAP [13], and Bowtie [6]. These techniques employ a compact in- dex that works well for aligning so called ""short reads"", namely reads that are typically around 30-50 characters long. In particu- lar, Bowtie ? the leading state-of-the-art method ? has focused on utilizing the properties of Burrows-Wheeler Transforms (BWT) [7] to index the reference sequence. A BWT-based index has a small memory footprint, making Bowtie feasible on computers with only 2GB of memory. However, memory is quickly becoming cheaper (while the genome size is constant), making it unnecessary to place such tight memory restrictions on the alignment software. In ad- dition, BWT-based methods use a prefix matching technique that requires only a few iterations to produce a valid alignment. While this technique works well with short reads, its performance de- grades rapidly as the read length increases. Unfortunately, im- pending technological advances with the next-generation sequenc- ing machines are expected to push up the read lengths to produce what are called ""long reads"", and loosely refer to read strings that are many factors longer than the ""short reads"". This trend implies that the existing BWT-based aligners will likely become computa- tionally infeasible in the near future. In addition, sequence aligners of the future will also need to accomodate more errors in the match models (for long reads) [9, 15], and the performance of existing methods rapidly deteoriates when using these richer match models. Our WHAM method addresses the short-comings of the existing methods. It uses a novel hash-based index built on the reference genome that accommodates complex string matching models that are natural for read alignments. This hash index is optimized for space and speed, and to work efficiently with long reads. The hash index quickly finds potential hits for each read. This list of poten- tial hits may contain some false positives that have to be checked using a precise test. This test for an actual hit is essentially a com- plex string matching function, which is computationally expensive. Another novel aspect of WHAM is the use of bitwise operations to perform this string matching test efficiently. Finally, WHAM uses a novel architecture that incorporates an optimizer (to opti- mize the indices that are built) and a compressor to reduce the size of the input data, providing a complete data processing and man- agement module that fits into existing computational pipelines for high-throughput genomic sequencing. We have compared WHAM using a number of real datasets against the leading and commonly used read alignment method ? Bowtie. Our results show that WHAM is often orders of magnitude faster than Bowtie, and allows for richer match models. The relative speedup of WHAM over Bowtie increases as the read length as well as the number of errors increases, which bodes well for the fu- ture in which we expect to see longer reads that need to be matched with more relaxed/richer match models. The remainder of this paper is organized as follows: The WHAM method is described in Section 2. Results from an extensive em- pirical evaluation are presented in Section 3. Section 4 discusses related work, and Section 5 contains our concluding remarks.",Yinan Li,University of Wisconsin?Madison,yinan@cs.wisc.edu,Allison Terrell,University of Wisconsin?Madison,aterrell@cs.wisc.edu,Jignesh M. Patel,University of Wisconsin?Madison,jignesh@cs.wisc.edu,,,,,,,,,,,,,,,,,,,,,
20200327,718,Angela Bonifati,"University of Lille & INRIA, France",angela.bonifati@inria.fr,,Learning Path Queries on Graph Databases,"Learning Path Queries on Graph Databases, Learning Path Queries on Graph Databases, Learning Path Queries on Graph Databases, Learning Path Queries on Graph Databases, Learning Path Queries on Graph Databases,  ABSTRACT We investigate the problem of learning graph queries by exploiting user examples. The input consists of a graph database in which the user has labeled a few nodes as posi- tive or negative examples, depending on whether or not she would like the nodes as part of the query result. Our goal is to handle such examples to find a query whose output is what the user expects. This kind of scenario is pivotal in several application settings where unfamiliar users need to be assisted to specify their queries. In this paper, we focus on path queries defined by regular expressions, we identify fundamental difficulties of our problem setting, we formal- ize what it means to be learnable, and we prove that the class of queries under study enjoys this property. We ad- ditionally investigate an interactive scenario where we start with an empty set of examples and we identify the informa- tive nodes i.e., those that contribute to the learning process. Then, we ask the user to label these nodes and iterate the learning process until she is satisfied with the learned query. Finally, we present an experimental study on both real and synthetic datasets devoted to gauging the effectiveness of our learning algorithm and the improvement of the interactive approach. 1. INTRODUCTION Graph databases [41] are becoming pervasive in several application scenarios such as the Semantic Web [5], social [37] and biological [36] networks, and geographical databases [2], to name a few. A graph database is essentially a directed, edge-labeled graph. As an example, consider in Figure 1 a graph representing a geographical database having as nodes the neighborhoods of a city area (N1 to N6), along with cinemas (C1 and C2), and restaurants (R1 and R2) in such neighborhoods. The edges represent public transportation facilities from a neighborhood to another (using labels tram and bus), along with other kind of facilities (using labels cinema and restaurant). For instance, the graph indicates that one can travel by bus between the neighborhoods N2 and N3, that in the neighborhood N4 exists a cinema C1, and so on. Many mechanisms have been proposed to query a graph database, the majority of them being based on regular ex- pressions [7, 41]. By continuing on our running example, imagine that a user wants to know from which neighbor- hoods in the city represented in Figure 1 she can reach cine- mas via public transportation. These neighborhoods can be retrieved using a path query defined by the following regular expression: q "" ptram` busqx' cinema The query q selects the nodes N1, N2, N4, and N6 as they are entailed by the following paths in the graph: N1 tram??? N4 cinema????? C1, N2 bus?? N1 tram??? N4 cinema????? C1, N4 cinema????? C1, N6 cinema????? C2. Although very expressive, graph query languages are diffi- cult to understand by non-expert users who are unable to specify their queries with a formal syntax. The problem of assisting non-expert users to specify their queries has been recently raised by Jagadish et al. [24, 34]. More concretely, they have observed that ""constructing a database query is often challenging for the user, commonly takes longer than the execution of the query itself, and does not use any in- sights from the database"". While they have mentioned these problems in the context of relational databases, we argue that they become even more difficult to tackle for graph databases. Indeed, graph databases usually do not carry proper metadata as they lack schemas and/or do not exhibit a clear distinction between instances and schemas. The absence of metadata along with the difficulty of visualizing possibly large graphs make unfeasible traditional query specification paradigms for non-expert users, such as query by example [43]. Our work follows the recent trend of specifying graph queries by example [33, 25]. Precisely, we focus on graph queries using regular expressions, which are fun- damental building blocks of graph query languages [7, 41], while both [33, 25] consider simple graph patterns. While the problem of executing path queries defined by regular expressions on graphs has been extensively studied recently [6, 32, 27], no research has been done on how to ac- tually specify such queries. Our work focuses on the problem of assisting non-expert users to specify such path queries, by exploiting elementary user input. By continuing on our running example, we assume that the user is not familiar with any formal syntax of query languages, while she still wants to specify the above query q on the graph database in Figure 1 by providing examples of the query result. In particular, she would positively or negatively label some graph nodes according to whether or not they would be selected by the targeted query. Thus, let us imagine that the user labels the nodes N2 and N6 as positive examples because she wants these nodes as part of the result. Indeed, one can reach cinemas from N2 and N6, respectively, through the following paths: N2 bus?? N1 tram??? N4 cinema????? C1, N6 cinema????? C2. Similarly, the user labels the node N5 as a negative exam- ple since she would not like it as part of the query result. Indeed, there is no path starting in N5 through which the user can reach a cinema. We also observe that the query q above is consistent with the user's examples because q selects all positive examples and none of the negative ones. Unfortunately, there may exist an infinite number of queries consistent with the given examples. Therefore, we are interested to find either the ""exact"" query that the user has in mind or, alternatively, an equivalent query, which is close enough to the user's expectations. Apart from assisting unfamiliar users to specify queries, our research has other crucial applications, such as min- ing scientific workflows. Regular expressions have already been used in the literature as a well-suited mechanism for inter-workflow coordination [21]. The path queries on graph databases that we study in this paper can be applied to assist scientists in identifying interrelated workflows that are of interest for them. For instance, assume that a biologist is interested in retrieving all interrelated workflows having a pattern that starts with protein purification, continues with an arbitrary number of protein separation steps, and ends with mass spectrometry. This corresponds to the following regular expression: ProteinPurification'ProteinSeparationx'MassSpectrometry. Instead of specifying such a pattern in a formal language, the biologist may be willing to label some sequences of modules from a set of available workflows as positive or negative examples, as illustrated in Figure 2. Our algorithms can be thus applied to infer the workflow pattern that the biolo- gist has in mind. Typically in graphs representing work- flows the labels are attached to the nodes (e.g., as in Fig- ure 2) instead of the edges. In this paper, we have opted for Figure 2: A set of scientific workflows examples. edge-labeled graphs rather than node-labeled graphs, but our algorithms and learning techniques for the considered class of queries are applicable to the latter class of graphs in a seamless fashion. The problem of mining scientific workflows has been considered in recent work [8], which leverages data instances as representatives of the input and output of a workflow module. In our approach, we rely on simpler user feedback, namely Boolean labeling of sequences of modules across interrelated workflows. Since our goal is to infer the user queries while mini- mizing the amount of user feedback, our research is also applicable to crowdsourcing scenarios [19], in which such minimization typically entails lower financial costs. Indeed, we can imagine that crowdworkers provide the set of posi- tive and negative examples mentioned above for path query learning. Moreover, our work can be used in assisting non- expert users in other fairly complex tasks, such as speci- fying schema mappings [42] i.e., logical assertions between two path queries, one on a source schema and another on a target schema. To the best of our knowledge, our work is the first to study the problem of learning path queries defined by regular ex- pressions on graphs via user examples. More precisely, we make the following main contributions: ? We investigate a learning framework inspired by com- putational learning theory [26], in particular by gram- matical inference [18] and we identify fundamental dif- ficulties of such a framework. We consequently propose a definition of learnability adapted to our setting. ? We propose a learning algorithm and we precisely char- acterize the conditions that a graph must satisfy to guarantee that every user's goal query can be learned. Essentially, the main theoretical result of the paper states that for every query q there exists a polynomial set of examples that given as input to our learning al- gorithm guarantees the learnability of q. Additionally, our learning algorithm is guaranteed to run in poly- nomial time, whether or not the aforementioned set of examples is given as input. ? We investigate an interactive scenario, which boot- straps with an empty set of examples and builds it along the way. Indeed, the learning algorithm finely 110 interacts with the user by proposing nodes that can be labeled and repeats the interactions until the goal query is learned. More precisely, we analyze what it means for a node to be informative for the learning process, we show the intractability of deciding whether a node is informative or not, and we propose efficient strategies to present examples to the user. ? To evaluate our approach, we have run experiments on both real-world and synthetic datasets. Our study shows the effectiveness of the learning algorithm and the advantage of using an interactive strategy, which significantly reduces the number of examples needed to learn the goal query. Finally, we would like to spend a few words on the class of queries that we investigate in this paper. As already men- tioned, we focus on regular expressions, which are funda- mental for graph query languages [7, 41] and lately used in the definition of SPARQL property paths1. Graph queries defined by regular expressions have been known as regular path queries. Intuitively, such queries retrieve pairs of nodes in the graph s.t. one can navigate between them with a path in the language of a given regular expression [7, 41]. Al- though the usual semantics of regular path queries is binary (i.e., selects pairs of nodes), in this paper we consider a gen- eralization of this semantics that we call monadic, as it out- puts only the originated nodes of the paths. The motivation behind using a monadic semantics is essentially threefold. First, it entails a larger space of potential solutions than a binary semantics. Indeed, with the latter semantics the end node of a path is fixed, which basically corresponds to have a smaller number of candidate paths that start at the orig- inated node and that can be possibly labeled by the user. Second, in our learning framework, the amount of user effort should be kept as minimal as possible (which led to design an interactive scenario) and thus we let the user focus solely on the originated nodes of the paths rather than on pairs of nodes. Third, the development of the learning algorithm for monadic queries is extensible to binary queries and n-ary queries in a straightforward fashion, as shown in the paper. Organization. In Section 2, we introduce some basic no- tions. In Section 3, we define our framework for learning from a set of examples, we present our learning algorithm, and we prove our learnability results. In Section 4, we propose an interactive algorithm and characterize the quantity of information of a node. In Section 5, we experimentally evaluate the performance of our algorithms. Finally, we conclude our paper and outline future directions in Section 6. Due to space restrictions, in this paper we omit the proofs of several results and we refer to the appendix of our technical report [11] for detailed proofs. Related work Learning queries from examples is a popular and interesting topic in databases. Very recently, algorithms for learning relational queries (e.g., quantifiers [1], joins [14, 15]) or XML queries (e.g., tree patterns [38]) have been proposed. Besides learning queries, researchers have investigated the learnabil- ity of relational schema mappings [40], as well as schemas [9] and transformations [30] for XML. A fairly close problem to 1http://www.w3.org/TR/sparql11-query/ learning is definability [4]. In this paragraph, we discuss the positioning of our own work w.r.t. these and other papers. A wealth of research on using computational learning the- ory [26] has been recently conducted in databases [1, 9, 14, 30, 38, 40]. In this paper, we use grammatical inference [18] i.e., the branch of machine learning that aims at constructing a formal grammar by generalizing a set of examples. In par- ticular, all the above papers on learning tree patterns [38], schemas [9, 16], and transformations [30] are based on it. Our definition of learnability is inspired by the well-known framework of language identification in the limit [20], which requires a learning algorithm to be polynomial in the size of the input, sound (i.e., always return a concept consistent with the examples given by the user or a special null value if such concept does not exist) and complete (i.e., able to produce every concept with a sufficiently rich set of exam- ples). In our case, we show that checking the consistency of a given set of examples is intractable, which implies that there is no algorithm able to answer null in polynomial time when the sample is inconsistent. This leads us to the conclusion that path queries are not learnable in the classical framework. Consequently, we slightly modify the framework and require the algorithm to learn the goal query in polynomial time if a polynomially-sized characteristic set of examples is provided. This learning framework has been recently employed for learning XML transformations [30] and is referred to as learning with abstain since the algorithm can abstain from answering when the characteristic set of examples is not provided. The classical algorithm for learning a regular language from positive and negative word examples is RPNI [35], which basically works as follows: (i) construct a DFA (usu- ally called prefix tree acceptor or PTA [18]) that selects all positive examples; (ii) generalize it by state merges while no negative example is covered. Unfortunately, RPNI cannot be directly adapted to our setting since the input positive and negative examples are not words in our case. Instead, we have a set of graph nodes starting from which we have to select (from a potentially infinite set) the paths that led the user to label them as examples. After selecting such paths, we generalize by state merges, similarly to RPNI. A problem closely related to learning is definability, re- cently studied for graph databases [4]. Learning and definability have in common the fact that they look for a query consistent with a set of examples. The difference is that learning allows the query to select or not the nodes that are not explicitly labeled as positive examples while definability requires the query to select nothing else than the set of pos- itive examples (i.e., all other nodes are implicitly negative). Nonetheless, some of the intractability proofs for definability can be adapted to our learning framework to show the in- tractability of consistency checking (cf. Section 3). To date, no polynomial algorithms have been yet proposed to con- struct path queries from a consistent set of examples.",Angela Bonifati,"University of Lille & INRIA, France",angela.bonifati@inria.fr,Radu Ciucanu,"University of Lille & INRIA, France",radu.ciucanu@inria.fr,Aurelien Lemay,"University of Lille & INRIA, France",aurelien.lemay@inria.fr,,,,,,,,,,,,,,,,,,,,,
20200328,1370,Kaibo Wang,"Department of Computer Science and Engineering, The Ohio State University",wangka@cse.ohio-state.edu,,Accelerating Pathology Image Data Cross-Comparison on CPU-GPU Hybrid Systems,"Accelerating Pathology Image Data Cross-Comparison on CPU-GPU Hybrid Systems, Accelerating Pathology Image Data Cross-Comparison on CPU-GPU Hybrid Systems, Accelerating Pathology Image Data Cross-Comparison on CPU-GPU Hybrid Systems, Accelerating Pathology Image Data Cross-Comparison on CPU-GPU Hybrid Systems, Accelerating Pathology Image Data Cross-Comparison on CPU-GPU Hybrid Systems, ABSTRACT As an important application of spatial databases in pathology imaging analysis, cross-comparing the spatial boundaries of a huge amount of segmented micro-anatomic objects demands extremely data- and compute-intensive operations, requiring high throughput at an affordable cost. However, the performance of spatial database systems has not been satisfactory since their implementations of spatial operations cannot fully utilize the power of modern parallel hardware. In this paper, we provide a customized software solution that exploits GPUs and multi-core CPUs to acceler- ate spatial cross-comparison in a cost-effective way. Our so- lution consists of an efficient GPU algorithm and a pipelined system framework with task migration support. Extensive experiments with real-world data sets demonstrate the ef- fectiveness of our solution, which improves the performance of spatial cross-comparison by over 18 times compared with a parallelized spatial database approach. 1. INTRODUCTION Digitized pathology images generated by high resolution scanners enable the microscopic examination of tissue spec- imens to support clinical diagnosis and biomedical research [10]. With the emerging pathology imaging technology, it is essential to develop and evaluate high quality image anal- ysis algorithms, with iterative efforts on algorithm validation, consolidation, and parameter sensitivity studies. One essential task to support such work is to provide efficient tools for cross-comparing millions of spatial boundaries of segmented micro-anatomic objects. A commonly adopted cross-comparing metric is Jaccard similarity [35], which com- putes the ratio of the total area of the intersection divided by the total area of the union between two polygon sets. Building high-performance cross-comparing tools is chal- lenging, due to data explosion in pathology imaging anal- ysis, as in other scientific domains [22, 27]. Whole-slide images made by scanning microscope slides at diagnostic resolution are very large: a typical image may contain over 100,000x100,000 pixels, and millions of objects such as cells or nuclei. A study may involve hundreds of images obtained from a large cohort of subjects. For a large-scale interrelated analysis, there may be dozens of algorithms  a with vary- ing parameters  a generating many different result sets to be compared and consolidated. Thus, derived data from images of a single study is often in the scale of tens of terabytes, and will be increasingly larger in future clinical environments. Pathologists mainly rely on spatial database management systems (SDBMS) to execute spatial cross-comparison [36]. However, cross-comparing a huge amount of polygons is time-consuming using SDBMSs, which cannot fully utilize the rich parallel resources of modern hardware. In the era of high-throughput computing, unprecedentedly rich and low-cost parallel computing resources, including GPUs and multi-core CPUs, have been available. In order to use these resources for maximizing execution performance, applica- tions must fully exploit both thread-level and data-level par- allelisms and well utilize SIMD (Single Instruction Multiple Data) vector units to parallelize workloads. However, supporting spatial cross-comparison on a CPU- GPU hybrid platform imposes two major challenges. First, parallelizing spatial operations, such as computing areas of polygon intersection and union, on GPUs requires efficient algorithms. Existing CPU algorithms, e.g., those used in SDBMSs, are branch intensive with irregular data access patterns, which makes them very hard, if not impossible, to parallelize on GPUs. Efficient GPU algorithms, if exist, must successfully exploit massive data parallelisms in the cross-comparing workload and execute them in an SIMD fashion. Second, a GPU-friendly system framework is re- quired to drive the whole spatial cross-comparing workload. The special characteristics of the GPU device require data batching to mitigate communication overhead, and coordi- nated device sharing to control resource contention. Fur- thermore, due to the diversity of hardware configurations and workloads, task executions have to be balanced between GPUs and CPUs to maximize resource utilization. In this paper, we present a customized solution, SCCG (Spatial Cross-comparison on CPUs and GPUs), to address the challenges. Through detailed profiling, we identify that the bottleneck of cross-comparing query execution mainly comes from computing the areas of polygon intersection and union. This explains the low performance of SDBMSs and motivates us to design an efficient GPU algorithm, called PixelBox, to accelerate the spatial operations. Both the design and the implementation of the algorithm are opti- mized thoroughly to ensure its high performance on GPUs. Moreover, we develop a pipelined system framework for the whole workload, and design a dynamic task migration com- ponent to solve the load balancing problem. The pipelined framework has advantages for its natural support of data batching and GPU sharing. The task migration component further improves system throughput by balancing workloads between GPUs and CPUs. The main contributions of this paper are as follows: 1) PixelBox, an efficient GPU algorithm and its optimized im- plementation for computing Jaccard similarity of polygon sets; 2) a pipelined framework with task migration support for spatial cross-comparison on a CPU-GPU hybrid plat- form; and 3) a demonstration of our solution 's performance (18x speedup over a parallelized SDBMS) with extensive and intensive experiments using real-world pathology data sets. The rest of the paper is organized as follows. Section 2 introduces the background and identifies the problem with SDBMSs in processing spatial cross-comparing queries. Our GPU algorithm, PixelBox, is presented in Section 3 to accel- erate the bottleneck spatial operations. Section 4 introduces the pipelined framework and the design of a task migration facility for workload balancing. Comprehensive experiments and performance evaluation are presented in Section 5, fol- lowed by related works in Section 6 and conclusions in Section 7.",Kaibo Wang,"Department of Computer Science and Engineering, The Ohio State University",wangka@cse.ohio-state.edu,Yin Huai,"Department of Computer Science and Engineering, The Ohio State University",huai@cse.ohio-state.edu,Rubao Lee,"Department of Computer Science and Engineering, The Ohio State University",liru@cse.ohio-state.edu,Fusheng Wang,"Center for Comprehensive Informatics, Emory University",fusheng.wang@emory.edu,Xiaodong Zhang,"Department of Computer Science and Engineering, The Ohio State University",zhang@cse.ohio-state.edu,Joel H. Saltz,"Center for Comprehensive Informatics, Emory University",jhsaltz@emory.edu,,,,,,,,,,,,
20200329,1258,Thorsten Papenbrock,"Hasso Plattner Institute (HPI) 14482 Potsdam, Germany",thorsten.papenbrock@hpi.de,,Data-driven Schema Normalization,"Data-driven Schema Normalization, Data-driven Schema Normalization, Data-driven Schema Normalization, Data-driven Schema Normalization, Data-driven Schema Normalization, ABSTRACT Ensuring Boyce-Codd Normal Form (BCNF) is the most popular way to remove redundancy and anomalies from datasets. Normalization to BCNF forces functional depen- dencies (FDs) into keys and foreign keys, which eliminates duplicate values and makes data constraints explicit. De- spite being well researched in theory, converting the schema of an existing dataset into BCNF is still a complex, manual task, especially because the number of functional dependen- cies is huge and deriving keys and foreign keys is NP-hard. In this paper, we present a novel normalization algorithm called Normalize, which uses discovered functional depen- dencies to normalize relational datasets into BCNF. Nor- malize runs entirely data-driven, which means that redun- dancy is removed only where it can be observed, and it is (semi-)automatic, which means that a user may or may not interfere with the normalization process. The algorithm introduces an efficient method for calculating the closure over sets of functional dependencies and novel features for choosing appropriate constraints. Our evaluation shows that Normalize can process millions of FDs within a few min- utes and that the constraint selection techniques support the construction of meaningful relations during normalization. 1. FUNCTIONAL DEPENDENCIES A functional dependency (FD) is a statement of the form X ""C A with X being a set of attributes and A being a single attribute from the same relation R. We say that the left- hand-side (Lhs) X functionally determines the right-hand- side (Rhs) A. This means that whenever two records in an instance r of R agree on all their X values, they must also agree on their A value [7]. More formally, an FD X ""C A holds in r, iff iiit1, t2 \forall r : t1[X] = t2[X]i t1[A] = t2[A]. In the following, we consider only non-trivial FDs, which are FDs with A /\forall X. Table 1 depicts an example address dataset for which the two functional dependencies Postcode""CCity and Postcode""CMayor hold. Because both FDs have the same Lhs, we can aggregate them to the notation Postcode""CCity,Mayor. The presence of this FD introduces anomalies in the data- set, because the values Potsdam, Frankfurt, Jakobs, and Feldmann are stored redundantly and updating these values might cause inconsistencies. So if, for instance, some Mr. Schmidt was elected as the new mayor of Potsdam, we must correctly change all three occurrences of Jakobs to Schmidt. Such anomalies can be avoided by normalizing relations into the Boyce-Codd Normal Form (BCNF). A relational schema R is in BCNF, iff for all FDs X ""C A in R the Lhs X is either a key or superkey [7]. Because Postcode is neither a key nor a superkey in the example dataset, this relation does not meet the BCNF condition. To bring all relations of a schema into BCNF, one has to perform six steps, which are explained in more detail later: (1) discover all FDs, (2) ex- tend the FDs, (3) derive all necessary keys from the extended FDs, (4) identify the BCNF-violating FDs, (5) select a vio- lating FD for decomposition (6) split the relation according to the chosen violating FD. The steps (3) to (5) repeat un- til step (4) finds no more violating FDs and the resulting schema is BCNF-conform. We find several FD discovery algorithms, such as Tane [14] and HyFD [19], that serve step (1), but there are, thus far, no algorithms available to efficiently and automatically solve the steps (2) to (6). For the example dataset, an FD discovery algorithm would find twelve valid FDs in step (1). These FDs must be ag- gregated and transitively extended in step (2) so that we find, inter alia, First,Last""CPostcode,City,Mayor and Post- code""CCity,Mayor. In step (3), the former FD lets us derive the key {First, Last}, because these two attributes function- ally determine all other attributes of the relation. Step (4), then, determines that the second FD violates the BCNF condition, because its Lhs Postcode is neither a key nor su- perkey. If we assume that step (5) is able to automatically select the second FD for decomposition, step (6) decom- poses the example relation into R1(First, Last,Postcode) and R2(Postcode,City,Mayor) with {First, Last} and {Postcode} being primary keys and R1.Postcode""CR2.Postcode a foreign key constraint. Table 2 shows this result. When again checking for violating FDs, we do not find any and stop the normalization process with a BCNFconform result. Note that the redundancy in City and Mayor has been removed and the total size of the dataset was reduced from 36 to 27 values. Because memory became a lot cheeper in the last years, there is a trend of not normalizing datasets for performance reasons. Normalization is, hence, today often claimed to be obsolete. This claim is false and ignoring normalization is dangerous for the following reasons [8]: 1. Normalization removes redundancy and, in this way, decreases error susceptibility and memory consumption. While memory might be relatively cheep, data errors can have serious and expensive consequences and should be avoided at all costs. 2. Normalization does not necessarily decrease query performance; in fact, it can even increase the performance. Some queries might need some additional joins after normalization, but others can read the smaller relations much faster. Also, more focused locks can be set, increasing parallel ac- cess to the data, if the data has to be changed. So the performance impact of normalization is not determined by the normalized dataset but by the application that uses it. 3. Normalization increases the understanding of the schema and of queries against this schema: Relations become smaller and closer to the entities they describe; their com- plexity decreases making them easier to maintain and ex- tend. Furthermore, queries against the relations become eas- ier to formulate and many mistakes are easier to avoid. For instance, aggregations over columns with redundant values are hard to formulate correctly. In summary, normalization should be the default and denormalization a conscious decision, i.e., ""we should de- normalize only at a last resort [and] back off from a fully normalized design only if all other strategies for improving performance have failed, somehow, to meet requiremnts"", C. J. Date, p. 88 [8]. The objective of this work is to normalize a given relational instance into Boyce-Codd Normal Form. Note that we do not aim to recover a certain schema nor do we aim to design a new schema using business logic. To solve the nor- malization task, we propose a data-driven, (semi-)automatic normalization algorithm that removes all FD-related redun- dancy while still providing full information recoverability. Being data-driven means that all FDs used in the normal- ization process are extracted directly from the data and that all decomposition proposals are based solely on data- characteristics. In other words, we consider only redundancy that can actually be observed in a given relational instance. The advantage of a data-driven normalization approach over state-of-the-art schema-driven approaches is that it can use the data to expose all syntactically valid normalization options, i.e., functional dependencies with evidence in the data, so that the algorithm (or the user) must only decide for a normalization path and not find one. The number of FDs can, indeed, become large, but we show that an algorithm can effectively propose the semantically most appropriate options. Furthermore, knowing all FDs allows for a more efficient normalization algorithm as opposed to having only a subset of FDs. Research challenges. In contrast to the vast amount of research on normalization in the past decades, we do not assume that the FDs are given, because this is almost never the case in practice. We also do not assume that a human data expert is able to manually identify them, because the search is difficult by nature and the actual FDs are often not obvious. The FD Postcode""CCity from our example, for instance, is commonly believed to be true although it is usu- ally violated by exceptions where two cities share the same postcode; the FD Atmosphere""CRings, on the other hand, is difficult to discover for a human but in fact holds on various datasets about planets. For this reason, we automatically discover all (minimal) FDs. This introduces a new challenge, because we now deal with much larger, often spurious, but complete sets of FDs. Using all FDs of a particular relational instance in the normalization process further introduces the challenge of selecting appropriate keys and foreign keys from the FDs (see Step (5)), because most of the FDs are coincidental, i.e., they are syntactically true but semantically false. This means that when the data changes these semantically invalid FDs could be violated and, hence, no longer work as a con- straint. So we introduce features to automatically identify (and choose) reliable constraints from the set of FDs, which is usually too large for a human to manually examine. Even if all FDs are semantically correct, selecting ap- propriate keys and foreign keys is still difficult. The decisions made here define which decompositions are executed, because decomposition options are often mutually exclu- sive: If, for instance, two violating FDs overlap, one split can make the other split infeasible. This happens, because BCNF normalization is not dependency preserving [12]. In all these constellations, however, some violating FDs are semantically better choices than others, which is why violating FDs must not only be filtered but also ranked by such qual- ity features. Another challenge, besides guiding the normalization pro- cess in the right direction, is the computational complexity of the normalization. Beeri and Bernstein have proven that the question ""Given a set of FDs and a relational schema that embodies it, does the schema violate BCNF?"" is NP- complete in the number of attributes [3]. To test this, we need to check that the Lhs of each of these FDs is a key or a super key, i.e., if each Lhs determines all other attributes. This is trivial if all FDs are transitively fully extended, i.e., they are transitively closed. For this reason, the complex- ity lies in calculating these closures (see Step (2)). Because no current algorithm is able to solve the closure calculation efficiently, we propose novel techniques for this sub-task of schema normalization. Overall, the number of functional dependencies in datasets is typically much greater than a human expert can manually cope with [18]. A normalization algorithm must, therefore, be able to handle such very large inputs automatically. 343 Contributions. We propose a novel, instance-based schema normalization algorithm called Normalize that can perform the normalization of a relational dataset automati- cally or supervised by an expert. Being able to put a human in the loop enables the algorithm to combine its analytical strengths with the domain knowledge of an expert. With Normalize and this paper, we make the following contri- butions: a) Schema normalization. We show how the entire schema normalization process can be implemented as one algorithm, which no previous work has done before. We discuss each component of this algorithm in detail. The main contribu- tion of our (semi-)automatic approach is to incrementally weed out semantically false FDs by focusing on those FDs that are most likely true. b) Closure calculation. We present two efficient closure algorithms, one for general FD result sets and one for complete result sets. Their core innovations include a more focused ex- tension procedure, the use of efficient index-structures, and parallelization. These algorithms are not only useful in the normalization context, but also for many other FD-related tasks, such as query optimization, data cleansing, or schema reverse-engineering. c) Violation detection. We propose a compact data struc- ture, i.e., a prefix tree, to efficiently detect FDs that violate BCNF. This is the first approach to algorithmically improve this step. We also discuss how this step can be changed to discover violating FDs for normal forms other than BCNF. d) Constraint selection. We contribute several features to rate the probability of key and foreign key candidates for actually being constraints. With the results, the candidates can be ranked, filtered, and selected as constraints during the normalization process. The selection can be done by either an expert or by the algorithm itself. Because all pre- vious works on schema normalization assumed all input FDs to be correct, this is the first solution for a problem that has been ignored until now. e) Evaluation. We evaluate our algorithms on several datasets demonstrating the efficiency of the closure calcu- lation on complete, real-world FD result sets and the feasi- bility of (semi-)automatic schema normalization. The remainder of this paper is structured as follows: First, we discuss related work in Section 2. Then, we introduce the schema normalization algorithm Normalize in Section 3. The following sections go into more detail explaining the closure calculation in Section 4, the key derivation in Sec- tion 5, and the violation detection in Section 6. Section 7, then, introduces assessment techniques for key and foreign key candidates. The normalization algorithm is finally eval- uated in Section 8 and we conclude in Section 9.",Thorsten Papenbrock,"Hasso Plattner Institute (HPI) 14482 Potsdam, Germany",thorsten.papenbrock@hpi.de,Felix Naumann,"Hasso Plattner Institute (HPI) 14482 Potsdam, Germany",felix.naumann@hpi.de,,,,,,,,,,,,,,,,,,,,,,,,
20200330,1371,Pengcheng Xiong,"NEC Laboratories America Cupertino, CA, USA",pxiong@nec-labs.com,,A Software-Defined Networking based Approach for Performance Management of Analytical Queries on Distributed Data Stores,"A Software-Defined Networking based Approach for Performance Management of Analytical Queries on Distributed Data Stores, A Software-Defined Networking based Approach for Performance Management of Analytical Queries on Distributed Data Stores, A Software-Defined Networking based Approach for Performance Management of Analytical Queries on Distributed Data Stores, A Software-Defined Networking based Approach for Performance Management of Analytical Queries on Distributed Data Stores, A Software-Defined Networking based Approach for Performance Management of Analytical Queries on Distributed Data Stores, ABSTRACT Nowadays data analytics applications are accessing more and more data from distributed data stores, creating a large amount of data traffic on the network. Therefore, distributed analytic queries are prone to suffer from poor performance when they encounter network contention, which can be quite common in a shared network. Typical distributed query op- timizers do not have a way to solve this problem because they treat the network as a black-box: they are unable to monitor it, let alone control it. With the new era of softwaredefined networking (SDN), we show how SDN can be effectively exploited for performance management for analytical queries in distributed data store environments. More specifically, we present a group of methods to leverage SDN 's visi- bility into and control of the network 's state that enable dis- tributed query processors to achieve performance improve- ments and differentiation for analytical queries. We demon- strate the effectiveness of the methods through detailed experimental studies on a system running on a software-defined network with commercial switches. To the best of our knowledge, this is the first work to analyze and show the opportunities of SDN for distributed query optimization. It is our hope that this will open up a rich area of research and technology development in distributed data intensive com- puting. 1. INTRODUCTION To become more efficient, effective, and competitive, en- terprises are expecting ever increasing benefits from data analytics. To meet this demand, data analytics platforms are including more data sources, which may be both internally and externally available. These data sources are often stored in distributed data stores. Data analytics applications or data scientists query the data from these distributed stores and merge and join the data to generate coherent analysis reports [14, 17]. With continuously increasing data sizes, querying and joining data from distributed sources can generate a significant amount of data traffic on the network, an issue that is exacerbated if the network is shared with other applications as well. Therefore, optimizing queries that ac- cess the distributed data stores, and specifically optimizing their network utilization, is likely to be an important prob- lem to address in order to deliver improved query performance and query service differentiation. In this context, we focus on the problem of how a dis- tributed query optimizer can exploit the capabilities offered by software-defined networking (SDN) [13, 15] to improve query performance and provide query service differentiation. For concreteness and simplicity of presentation and implementation, we consider an instance of the problem in which all data stores are relational databases. Although there are significant differences, there are some striking similarities be- tween the problems encountered in modern multi-source dis- tributed data analysis and the traditional distributed database query optimization [12]. In addition to CPU and I/O costs, distributed query optimizers also consider the cost of com- munication over the network when choosing the best query plan [12]. However, traditionally, the interaction between the query optimizer and the network has been limited in that the op- timizer has no direct way to (1) inquire about the current status and performance of the network, or (2) control the network with directives, e.g., making bandwidth reserva- tions, which would improve query performance and query service differentiation. where the join tables are hosted in two separate data store sites. (We will discuss the setup details later in 955 Section 6.) We can see that, as the status of the network between the sites changes (vertical dotted lines) during the experimental period (x-axis), query execution time is significantly affected. If a distributed query optimizer treats the network as a fixed cost, black box resource and sticks to a fixed plan, which is the case for most distributed database query optimizers, it may not be able to choose the plan with the shortest execution time. Although some distributed query optimizers and execution systems try to react in some fashion to unexpected network conditions [3, 19], the decisions they make are either heuristic-driven or potentially inaccurate due to an inability to predict the true costs of remote data access. It is perhaps reasonable to expect that with greater visi- bility into the network 's state, a distributed query optimizer could make more accurate cost estimates for different query plans and make better informed decisions. Moreover, it is also intuitively clear that if the optimizer could have some control of the network 's future state, a distributed query optimizer could request and reserve the network bandwidth for a specific query plan and thereby improve query perfor- mance and query service differentiation. The problem is that historically the networks that distributed data management systems rely upon have made both these tasks difficult or impossible. However, we are currently entering a new era of softwaredefined networking (SDN) [13, 15]. Networks with SDN en- able the distributed query optimizer to achieve such visibil- ity into and control of the network 's state. By decoupling the system that makes decisions about where traffic is sent (the control plane) from the underlying systems that forward traffic to the selected destination (the data plane), network services can be managed through an abstraction of lower level functionality. Thus, SDN raises the possibility that it is for the first time feasible and practical for distributed query optimizers to carefully monitor and even control the network. Our goal in this paper is to begin the exploration of this capability, and to try to gain insight into whether it really is a promising new development for distributed query optimization. Our contributions: In this paper, we give a preliminary answer to the affirmative: SDN can indeed be effectively exploited for the performance management of analytical queries in distributed data store environments. To the best of our knowledge, this is the first work to analyze and show the opportunities SDN provides for distributed query optimization. It is our hope that this will open up a rich area of research and technology development in distributed data intensive computing. Our specific contributions are the following: (1) We present a method that adaptively selects the op- timal query plan based on the information provided by the network before the query execution. This method observes the status of the network and reacts by adapting the query execution plan to one that yields better performance. (2) We present two methods that allow a distributed query processor to deliver differentiated query service to the users with different priorities. The first method allows for net- work traffic prioritization and the second method provides the capability of reserving a certain amount of bandwidth for specific queries and making use of that guaranteed band- width during query optimization. These methods achieve run-time query service differentiation in shared and highly utilized networks, which was not possible before. (3) We propose a method to model dynamic communication costs. We integrate the model into a distributed query optimizer along with an existing computational cost model and show its effectiveness. (4) We report on our implementation of the techniques in a prototype distributed data store environment that is built using multiple instances of open source databases running on a  ""real"" SDN network with commercial OpenFlow enabled switches. Our experimental results confirm our expectations and clearly show the benefits of the SDN technologies.",Pengcheng Xiong,"NEC Laboratories America Cupertino, CA, USA",pxiong@nec-labs.com,Hakan Hacg?m? s,"NEC Laboratories America Cupertino, CA, USA",hakan@nec-labs.com,Jeffrey F. Naughton,"University of Wisconsin Madison, WI, USA",naughton@cs.wisc.edu,,,,,,,,,,,,,,,,,,,,,
20200331,921,Iman Elghandour,University of Waterloo,ielghand@cs.uwaterloo.ca,,ReStore: Reusing Results of MapReduce Jobs,"ReStore: Reusing Results of MapReduce Jobs, ReStore: Reusing Results of MapReduce Jobs, ReStore: Reusing Results of MapReduce Jobs, ReStore: Reusing Results of MapReduce Jobs, ReStore: Reusing Results of MapReduce Jobs, ABSTRACT Analyzing large scale data has emerged as an important ac- tivity for many organizations in the past few years. This large scale data analysis is facilitated by the MapReduce programming and execution model and its implementations, most notably Hadoop. Users of MapReduce often have anal- ysis tasks that are too complex to express as individual MapReduce jobs. Instead, they use high-level query languages such as Pig, Hive, or Jaql to express their complex tasks. The compilers of these languages translate queries into workflows of MapReduce jobs. Each job in these workflows reads its input from the distributed file system used by the MapReduce system and produces output that is stored in this distributed file system and read as input by the next job in the workflow. The current practice is to delete these intermediate results from the distributed file system at the end of executing the workflow. One way to improve the per- formance of workflows of MapReduce jobs is to keep these intermediate results and reuse them for future workflows submitted to the system. In this paper, we present ReStore, a system that manages the storage and reuse of such intermediate results. ReStore can reuse the output of whole MapReduce jobs that are part of a workflow, and it can also create additional reuse opportunities by materializing and storing the output of query execution operators that are executed within a MapReduce job. We have implemented ReStore as an extension to the Pig dataflow system on top of Hadoop, and we experimentally demonstrate significant speedups on queries from the PigMix benchmark. 1. INTRODUCTION Massive scale data analysis has become a main activity for many enterprises and research groups. Companies such as Facebook, Yahoo, and Google now own petabyte-scale data warehouses that are accessed on a regular basis using ad hoc queries and periodic batch jobs [7, 16], and terabyte-scale data warehouses are now common in many smaller companies. This large scale data analysis is currently supported by the MapReduce programming and execution model [9] and its implementations such as Hadoop [1], which is now one of the major platforms for data analysis. Users of MapReduce often have analysis tasks that are too complex to express as one MapReduce job. Instead, they often use high-level query languages such as Pig Latin [11, 14], Hive [15], or Jaql [8] to express their complex analy- sis tasks. The compilers of these query languages translate queries into workflows of MapReduce jobs, such as the one shown in Figure 1. Optimizing the performance of such workflows is important given the popularity of MapReduce and these query languages on top of it. Each job in a workflow of MapReduce jobs produces out- put that is stored in the distributed file system used by the MapReduce system (e.g., HDFS [3] in the case of Hadoop). These intermediate results are used as input by subsequent jobs in the workflow. For example, Job3 in Figure 1 produces output that is used as input by Job4 and Job5. The current practice is to delete these intermediate outputs after finishing the execution of the workflow. In this paper, we present ReStore, a system that improves the performance of workflows of MapReduce jobs generated from high-level query languages by storing the intermediate results of executed workflows and reusing them for future workflows submitted to the system. We expect reusing the output of MapReduce jobs to be beneficial since it is common for enterprises to have large data sets on which many data analysis queries are executed (e.g., the usage log data in internet companies such as Face- book). Queries on these data sets typically perform the fol- lowing steps: (1) load the data set, (2) perform some simple processing to filter out unnecessary data, and (3) perform extra processing on the small fraction of the loaded data that passes the filter. Steps 1 and 2 of one workflow are likely to be repeated in other workflows, and even parts of Step 3. These steps are repeated in many queries, and the MapReduce jobs that execute them can be replaced by reading the stored outputs of similar jobs that were executed in past workflows, and whose output we have stored using ReStore. Moreover, even if full jobs cannot be reused, parts of a job (which we call a sub-job) can be useful for future workflows, and ReStore can materialize and store the outputs of such sub-jobs. Finding sharing opportunities among queries that are sub- mitted in the same batch to a MapReduce cluster has been studied in [5] and [13], but these works focus on sharing between queries that are executed concurrently and are lim- ited to sharing one operator between multiple queries. In this paper, we enable queries submitted at different times to share results and we can share large portions of the executed workflows. The importance of sharing is illustrated by the fact that Facebook stores the result of any query in its MapReduce cluster for seven days so that it can be shared among users [16]. ReStore can be built on top of dataflow language processors such as Pig, Hive, or Jaql. These language processors translate queries into workflows of MapReduce jobs. Each of these MapReduce jobs has a physical query execution plan that contains one or more physical operators that are exe- cuted by this job. Each language has a fixed set of physical operators such as Filter, Select, and Join. The workflows of MapReduce jobs are submitted to ReStore, which per- forms the following: (1) it rewrites the MapReduce jobs in a submitted workflow to reuse job outputs previously stored in the system, (2) it stores the outputs of executed jobs for future reuse, (3) it creates more reuse opportunities by storing the outputs of sub-jobs in addition to whole MapReduce jobs, and (4) it selects the outputs of jobs to keep in the distributed file system and those to delete. After ReStore rewrites a MapReduce job, it submits it to the MapReduce system to be executed. These steps can be viewed as analo- gous to the steps of building and using materialized views for relational databases [12]. In this paper, we focus on describ- ing steps 1?3 and present a brief discussion of techniques for performing step 4. In the rest of the paper we present the following contri- butions: ? A framework for creating reuse opportunities between workflows of MapReduce jobs and taking advantage of these opportunities (Section 2). ? A technique to rewrite input workflows of MapReduce jobs to reuse the results of previously executed jobs stored in the system (Section 3). ? A technique to increase reuse opportunities by materi- alizing the outputs of sub-jobs of the executed MapRe- duce jobs (Section 4). ? A proposal of a primitive set of heuristic rules for de- ciding which of the candidate MapReduce job outputs to keep and which to discard (Section 5). ? An implementation of ReStore on top of the Pig sys- tem [11, 14] (Section 6), and an experimental study using this implementation (Section 7). ",Iman Elghandour,University of Waterloo,ielghand@cs.uwaterloo.ca,Ashraf Aboulnaga,University of Waterloo,ashraf@cs.uwaterloo.ca,,,,,,,,,,,,,,,,,,,,,,,,
20200401,938,Jennie Duggan,Brown University,jennie@cs.brown.edu,,Contender: A Resource Modeling Approach for Concurrent Query Performance Prediction,"Contender: A Resource Modeling Approach for Concurrent Query Performance Prediction, Contender: A Resource Modeling Approach for Concurrent Query Performance Prediction, Contender: A Resource Modeling Approach for Concurrent Query Performance Prediction, Contender: A Resource Modeling Approach for Concurrent Query Performance Prediction, Contender: A Resource Modeling Approach for Concurrent Query Performance Prediction, ABSTRACT Predicting query performance under concurrency is a difficult task that has many applications in capacity planning, cloud computing, and batch scheduling. We introduce Contender, a new resource- modeling approach for predicting the concurrent query perfor- mance of analytical workloads. Contender 's unique feature is that it can generate effective predictions for both static as well as ad- hoc or dynamic workloads with low training requirements. These characteristics make Contender a practical solution for real-world deployment. Contender relies on models of hardware resource contention to predict concurrent query performance. It introduces two key met- rics, Concurrent Query Intensity (CQI) and Query Sensitivity (QS), to characterize the impact of resource contention on query interac- tions. CQI models how aggressively concurrent queries will use the shared resources. QS defines how a query 's performance changes as a function of the scarcity of resources. Contender integrates these two metrics to effectively estimate a query 's concurrent exe- cution latency using only linear time sampling of the query mixes. Contender learns from sample query executions (based on known query templates) and uses query plan characteristics to gen- erate latency estimates for previously unseen templates. Our experimental results, obtained from PostgreSQL/TPC-DS, show that Contender 's predictions have an error of 19% for known templates and 25% for new templates, which is competitive with the state-of- the-art while requiring considerably less training time. 1. INTRODUCTION Concurrent query execution offers numerous benefits for database applications. It can decrease the time required to execute analytical workloads [1, 2] and lead to better use of hardware re- sources by exploiting parallelism. At the same time, concurrent execution raises numerous challenges, including reasoning about how interleaving queries will affect one another 's rate of progress. As multiple queries compete for hardware resources, their interactions may be positive, neutral, or negative [3]. For example, a positive interaction may occur if two queries share a table scan; one may prefetch data for the other and they both enjoy a modest speedup. In contrast, if two queries access disjoint data and are I/O-bound, they will slow each other down. Accurate concurrent query performance prediction (CQPP) stands to benefit a variety of applications. This knowledge would allow system administrators to make better scheduling decisions for large query batches, reducing the completion time of individual queries and that of the entire batch [4]. With CQPP, cloud-based database applications would be able to make more informed re- source provisioning and query-to-server assignment plans [5, 6]. High quality predictions would also pave the way for more re- fined query progress indicators by analyzing in real time how re- source availability affects a query 's estimated completion time. Moreover, accurate CQPP could enable query optimizers to create concurrency-aware execution plans. Because of such important applications, there has been much re- cent work on CQPP for both transactional [7] and analytical work- loads [1, 8]. We focus primarily on CQPP for analytical queries (OLAP), for which existing techniques [1, 8] suffer from two main limitations. First, they require that performance samples of differ- ent query mixes are collected before predictions may be produced. Hence, their prediction models are valid for only known query tem- plates. Second, the sampling requirements for existing approaches grow polynomially in proportion to the complexity of their work- loads, limiting their viability in real-world deployments. In this paper, we propose a general CQPP framework, called Contender, that is practical to use for static as well as dynamic and ad-hoc workloads. Dynamic workloads are present in many exploration-oriented database applications including science, engi- neering, and business. Contender relies on models of resource con- tention for analytical, concurrently executing queries. It leverages both resource usage statistics and semantic information from query execution plans to model a query 's performance as it changes due to concurrent queries that are competing for the same resources. Specifically, our query performance predictions are based on mod- eling the query 's I/O bandwidth and how it is affected by memory availability. Scarcity of I/O bandwidth and memory are the dom- inant sources of slowdown for analytical queries executing under concurrency; these queries access very large data sets while filling the available memory with intermediate results, further limiting the available resources [9]. Contender first models how query performance varies under dif- ferent resource availability scenarios. Specifically, for each query for which we want to predict its performance (the primary query), we create its performance range, which we call the continuum. We define the lower bound of this continuum as the query 's execution time in isolation, which gives its minimum execution latency. The upper bound of this range is provided by limiting the availability of I/O bandwidth and memory to simulate the worst-case scenario for a query executing under concurrency. In the next steps, Contender quantifies the resource availabil- ity during the primary query 's execution to predict where in the performance range (continuum) the primary 's latency will reside. The framework leverages query plan information of the concur-     109 10.5441/002/edbt.2014.11 rent queries - those running simultaneously with a primary - to estimate the conditions under which the primary query will be ex- ecuted. Specifically, we propose the Concurrent Query Intensity (CQI) metric to quantify the I/O usage of concurrent queries and use this metric to estimate the availability of I/O bandwidth for the primary query. A unique feature of the CQI metric is that it mod- els the percentage of time concurrent queries compete directly with the primary for shared resources. We show that CQI is highly cor- related with concurrent query latency. Given the CQI value, Con- tender builds a performance prediction model (Query Sensitivity (QS)) to predict the primary query 's latency. A unique feature of Contender is that it can make performance predictions for new templates without requiring a priori models. In- stead, our framework first assembles a set of QS reference models for the templates it has seen already. Next it learns the predictive models for new templates based on these reference models. An interesting discovery is that new templates with similar behavior in isolation (e.g., latency, I/O-to-CPU ratios), have similar perfor- mance prediction models (e.g., QS models). This approach elimi- nates the need to collect samples on how new queries interact with the existing workload, which is the main bottleneck of previous work [1, 8]. Therefore, Contender dramatically simplifies the pro- cess of supporting unpredictable or evolving query workloads. Lastly, Contender reduces training requirements as it eliminates the need to collect samples of query performance under varying resource availability conditions. We show that query plan char- acteristics, paired with resource profiling on a template 's isolated performance, are effective in predicting how a query reacts to dif- fering levels of hardware availability by comparing it to other, simi- lar queries. This permits us to estimate the worst-case performance scenario with reduced sampling of new templates. Therefore, Con- tender significantly lowers training time for new queries compared with existing work [1, 8]. We show that our sampling requirements can be reduced from polynomial to linear, and with further restric- tions, to even constant time. For completeness, our work also includes a study of CQPP using well-known machine learning techniques. These algorithms effec- tively predict the performance of queries executed in isolation (with a prediction error of around 25%) [10, 11]. Our results demon- strate that these models are poorly fitted to the complex case of concurrent query executions and motivate the need for more ad- vanced techniques. Our main contributions can be summarized as follows: ? We evaluate established machine learning approaches for CQPP and establish the need for more advanced techniques. ? We introduce novel resource-contention metrics to predict how analytical queries behave under concurrency: Concur- rent Query Intensity quantifies the resources to which a query accesses when executing with others, and Query Sensitivity models how a query 's performance varies as a function of resource availability. ? We leverage these metrics to predict latency of unseen tem- plates using linear-time sampling of query executions. ? We further generalize our approach by predicting worst-case scenario performance of templates, reducing our sampling overhead to constant time. Our paper is organized as follows. In Section 2, we briefly in- troduce the characteristics of the analytical workload used for eval- uating our performance predictions. In Section 3, we evaluate so- phisticated machine learning techniques to predict new template performance. Section 4 examines the relationship between I/O con- tention and query performance. Next, we describe our approach to building models for CQPP. We present our experimental results in Section 6, survey related work in Section 7, and conclude.",Jennie Duggan,Brown University,jennie@cs.brown.edu,Olga Papaemmanouil,Brandeis University,olga@cs.brandeis.edu,Ugur Cetintemel,Brown University,ugur@cs.brown.edu,Eli Upfal,Brown University,eli@cs.brown.edu,,,,,,,,,,,,,,,,,,
20200402,770,Chen Luo,"University of California, Irvine",cluo8@uci.edu,,Umzi: Unified Multi-Zone Indexing for Large-Scale HTAP,"Umzi: Unified Multi-Zone Indexing for Large-Scale HTAP, Umzi: Unified Multi-Zone Indexing for Large-Scale HTAP, Umzi: Unified Multi-Zone Indexing for Large-Scale HTAP, Umzi: Unified Multi-Zone Indexing for Large-Scale HTAP, Umzi: Unified Multi-Zone Indexing for Large-Scale HTAP, ABSTRACT The rising demands of real-time analytics have emphasized the need for Hybrid Transactional and Analytical Processing (HTAP) systems, which can handle both fast transactions and analyt- ics concurrently. Wildfire is such a large-scale HTAP system prototyped at IBM Research - Almaden, with many techniques developed in this project incorporated into the IBM 's HTAP product offering. To support both workloads efficiently, Wild- fire organizes data differently across multiple zones, with more recent data in a more transaction-friendly zone and older data in a more analytics-friendly zone. Data evolve from one zone to another, as they age. In fact, many other HTAP systems have also employed the multi-zone design, including SAP HANA, Mem- SQL, and SnappyData. Providing a unified index on the large volumes of data across multiple zones is crucial to enable fast point queries and range queries, for both transaction processing and real-time analytics. However, due to the scale and evolving nature of the data, this is a highly challenging task. In this pa- per, we present Umzi, the multi-version and multi-zone LSM-like indexing method in the Wildfire HTAP system. To the best of our knowledge, Umzi is the first indexing method to support evolving data across multiple zones in an HTAP system, provid- ing a consistent and unified indexing view on the data, despite the constantly on-going changes underneath. Umzi employs a flexible index structure that combines hash and sort techniques together to support both equality and range queries. Moreover, it fully exploits the storage hierarchy in a distributed cluster envi- ronment (memory, SSD, and distributed shared storage) for index efficiency. Finally, all index maintenance operations in Umzi are designed to be non-blocking and lock-free for queries to achieve maximum concurrency, while only minimum locking overhead is incurred for concurrent index modifications. 1 INTRODUCTION The popularity of real-time analytics, e.g., risk analysis, online recommendations, and fraud detection etc., demands data man- agement systems to handle both fast concurrent transactions (OLTP) and large-scale analytical queries (OLAP) over fresh data. These applications ingest data at high-speed, persist them into disks or shared storage, and run analytical queries simultaneously over newly ingested data to derive insights promptly. The necessity of real-time analytics prompts the emergence of Hybrid Transactional and Analytical Processing (HTAP) sys- tems, e.g., MemSQL [7], SnappyData [28], SAP HANA [21], and among others. HTAP systems support both OLTP and OLAP queries in a single system, thus allowing real-time analytics over freshly ingested data. Wildfire [15] is a large-scale HTAP system, prototyped at IBM research - Almaden. Many of the techniques developed in this research project have been incorporated into the IBMDb2 Event Store offering [4].Wildfire leverages the Spark ecosystem [10] to enable large-scale data processing with differ- ent types of complex analytical requests (SQL, machine learning, graph analysis, etc), and compensates Spark with an underlying engine that supports fast transactions with snapshot isolation and accelerated analytics queries. Furthermore, it stores data in open format (Parquet [8]) on shared storage, so that other big data systems can access consistent snapshots of data in Wildfire. The back-end shared storage that Wildfire supports includes dis- tributed file systems like Hadoop Distributed File System (HDFS) and GlusterFS [2], as well as object-based storage on cloud like Amazon S3 and IBM Cloud Object Storage. To support efficient point lookups and range queries for high- speed transactional processing and real-time analytics, fine-grained indexing is mandatory in a large-scale HTAP system likeWildfire. However, indexing large volumes of data in an HTAP system is highly non-trivial due to the following challenges. Challenges due to shared storage. First of all, for large- scale HTAP, memory-only solutions are not enough. As a result, most HTAP systems, including Wildfire, persist data in highly- available fault-tolerant shared storage, like HDFS and Amazon S3, etc. However, most of these shared storage options are not good at random access and in-place update. For example, HDFS only supports append-only operations and optimizes for block-level transfers, and object storage on cloud allows neither random access inside an object nor update to an object. To accommodate the unique characteristics of shared storage, index operations, e.g., insert, update and delete, have to leverage sequential I/Os without in-place updates. Naturally, LSM-like index structures are more appealing. Furthermore, a typical shared storage prefer a small number of large files to a large number of small files. This is not only because of the overhead in metadata management, e.g., the maximum number of files supported by an HDFS cluster is determined by how much memory is available in the namenode, but more importantly because of the reduced seek time overhead when accessing larger files. Finally, accessing remote shared storage through networks for index lookups is costly. Thus, indexing methods on HTAP must fully exploit the storage hierarchy in a distributed cluster environment for efficiency. Particularly, nowadays, we can take advantage of large memories and SSDs in modern hardware. Due to the large scale of data inHTAP systems, however, only themost frequently accessed portions of indexes can be cached locally, while leaving cold entries in shared storage. Effective caching mechanisms must be developed to facilitate index lookup. Challenge due to evolving nature of data. Since HTAP systems have to support both transactional and analytical workloads efficiently, many of them [7, 14, 21, 23, 28] store data in different organizations, typically one organization good for trans- actions on the more recent data and one organization good for analytics on the older data. We call the different data organizations zones. As data age in the system, they evolve from the transaction-friendly zone to the analytics-friendly zone. In Wild- fire, transactions first append writes into a transaction log, which is then groomed into columnar data blocks. The groomed data is further periodically post-groomed to a more analytics-friendly organization that is optimal for queries by creating data versions, data partitioning, and larger data blocks. SAP HANA organizes data into a read-optimized main store and a write-optimized delta store. Writes are first buffered into the row-major delta store, which is further transformed into the columnar main store to facilitate analytical queries. Some loosely-coupled HTAP so- lutions employ NoSQL stores, like HBase [3] or Cassandra [1], for operational workloads, and periodically copy data from the NoSQL stores into files in columnar format like Parquet or ORC- File on the shared storage, so that SQL-on-Hadoop engines, like Hive [35] or SparkSQL [13], can efficiently query them. The data evolution across different zones in these HTAP systems/solutions is constantly on-going, posing a significant challenge to building and maintaining indexes. Existing indexing solutions on multi-zone HTAP systems ei- ther support index on the transaction-friendly zone only, like in SnappyData [28] and the loosely coupled HTAP solutions, or support separate indexes on different zones, like in MemSQL [7]. First of all, being able to efficiently query historical data is very important for real-time analytics, especially for analytical queries that are part of a transaction in the true HTAP scenario. As a re- sult, the index needs to cover both recent data and historical data. Secondly, having separate indexes on different zones exposes a divided view of data. This requires queries to perform extra work to combine index query results that span multiple zones. In particular, with the constant evolving nature of HTAP data, it is non-trivial for queries to make sure that there is no duplicate or missing data in the final results. Therefore, it is highly desirable to have a consistent and unified index across the different zones in an HTAP system. Contributions.To tackle the challenges of indexing in a large- scale HTAP system,we present Umzi, themulti-version andmulti- zone LSM-like index in the context of Wildfire. Umzi provides a consistent and unified indexing view across the groomed and post-groomed zones in Wildfire. To the best of our knowledge, Umzi is the first unified multi-zone indexing method for largescale HTAP systems. Umzi employs an LSM-like structure with lists of sorted index runs to avoid in-place updates. A novel index-run format that combines hash and sort techniques is introduced to flexibly an- swer equality/range queries as well as the combination of both using the index. Runs are organized into multiple levels as in today 's NoSQL systems, e.g., LevelDB [6] and RocksDB [9]. A new run is added into the lowest level, i.e., level 0, and runs are periodically merged into higher levels within a zone. However, when data evolve from one zone to another, an index evolve op- eration is introduced to build new index runs in the new zone and garbage-collect obsolete index runs from the old zone in a coordinated way, so that the entire index is always in a consistent state. To fully exploit the storage hierarchy, lower index levels can be made non-persistent to speed up frequent merges, and we dynamically adjust cached index runs from memory and SSD to speed-up index lookups and transactional processing. In Umzi, all operations are carefully designed to be non-blocking such that readers, i.e., index queries, are always lock-free while only negligible locking overhead is incurred for index maintenance. Paper organization. Section 2 provides the background on Wildfire. Section 3 describes an overview of the Umzi index. Section 4 presents the internal structure of Umzi components. Section 5 describes index maintenance operations in Umzi. Sec- tion 6 discusses some design decisions of Umzi to exploit the storage hierarchy. Section 7 introduces methods for processing index queries, i.e., range scans and point lookups. Section 8 re- ports the experimental evaluation of Umzi. Section 9 surveys related work. Finally, Section 10 concludes this paper.",Chen Luo,"University of California, Irvine",cluo8@uci.edu,Pinar Tozun,IT University of Copenhagen,pito@itu.dk,Yuanyuan Tian,IBM Research - Almaden,ytian@us.ibm.com,Ronald Barber,IBM Research - Almaden,rjbarber@us.ibm.com,Vijayshankar Raman,IBM Research - Almaden,ravijay@us.ibm.com,Richard Sidle,IBM,ricsidle@ca.ibm.com,,,,,,,,,,,,
20200403,1372,Haoran Li,"Math and Computer Science Department, Emory University Atlanta, GA",hli57@emory.edu,,Differentially Private Synthesization of Multi-Dimensional Data using Copula Functions,"Differentially Private Synthesization of Multi-Dimensional Data using Copula Functions, Differentially Private Synthesization of Multi-Dimensional Data using Copula Functions, Differentially Private Synthesization of Multi-Dimensional Data using Copula Functions, Differentially Private Synthesization of Multi-Dimensional Data using Copula Functions, Differentially Private Synthesization of Multi-Dimensional Data using Copula Functions, ABSTRACT Differential privacy has recently emerged in private statistical data release as one of the strongest privacy guarantees. Most of the existing techniques that generate differentially private histograms or synthetic data only work well for single dimensional or low-dimensional histograms. They become problematic for high dimensional and large domain data due to increased perturbation error and computation complexity. In this paper, we propose DPCopula, a differentially private data synthesization technique using Copula functions for multi-dimensional data. The core of our method is to compute a differentially private copula function from which we can sample synthetic data. Copula functions are used to describe the dependence between multivariate random vec- tors and allow us to build the multivariate joint distribution using one-dimensional marginal distributions. We present two methods for estimating the parameters of the copula functions with differential privacy: maximum likelihood estimation and Kendall's \lamda estimation. We present formal proofs for the privacy guarantee as well as the conver- gence property of our methods. Extensive experiments using both real datasets and synthetic datasets demonstrate that DPCopula generates highly accurate synthetic multi- dimensional data with significantly better utility than state- of-the-art techniques. 1. INTRODUCTION Privacy preserving data analysis and publishing [14, 15, 3] has received considerable attention in recent years as a promising approach for sharing information while preserving data privacy. Differential privacy [14, 15, 22] has recently emerged as one of the strongest privacy guarantees for statis- tical data release. A statistical aggregation or computation is DP1 if the outcome is formally indistinguishable when run with and without any particular record in the dataset. The level of indistinguishability is quantified by a privacy budget  A common mechanism to achieve differential privacy is the Laplace mechanism [16] which injects calibrated noise to a statistical measure determined by the privacy budget  , and the sensitivity of the statistical measure influenced by the inclusion and exclusion of a record in the dataset. A lower privacy parameter requires larger noise to be added and provides a higher level of privacy. Many mechanisms (e.g. [14, 18, 29]) have been proposed for achieving differential privacy for a single computation or a given analytical task and programming platforms have been implemented for supporting interactive differentially private queries or data analysis [28]. Due to the composibility of differential privacy [28], given an overall privacy budget constraint, it has to be allocated to subroutines in the computation or each query in a query sequence to ensure the overall privacy. After the budget is exhausted, the database can not be used for further queries or computations. This is especially challenging in the scenario where multiple users need to pose a large number of queries for exploratory analysis. Several works started addressing effective query answering in the interactive setting with differential privacy given a query workload or batch queries by considering the correlations between queries or query history [38, 8, 43, 23, 42].  release with differential privacy (e.g. [5, 27, 39, 19, 12, 41, 9, 10]). Given an original dataset, the goal is to publish a DP statistical summary such as marginal or multi-dimensional histograms that can be used to answer predicate queries or to generate DP synthetic data that mimic the original data. For example, Figure 1 shows an example dataset and a one-dimensional marginal histogram for the attribute age. The main approaches of existing work can be illustrated by Figure 2(a) and classified into two categories: 1) parametric methods that fit the original data to a multivariate distribution and makes inferences about the parameters of the distribution (e.g. [27]). 2) nonparametric methods that learn empirical distributions from the data through histograms (e.g. [19, 41, 9, 10]). Most of these work well for single dimensional or low-order data, but become problematic for data with high dimensions and large attribute domains. This is due to the facts that: 1) The underlying distribution of the data may be unknown in many cases or different from the assumed distribution, especially for data with arbitrary margins and high dimen- sions, leading the synthetic data generated by the parametric methods not useful; 2) The high dimensions and large attribute domains result in a large number of histogram bins that may have skewed distributions or extremely low counts, leading to significant perturbation or estimation errors in the non-parametric histogram methods; 3) The large domain space \pym i=1 |Ai| 2 (i.e. the number of histogram bins) incurs a high computation complexity both in time and space. For DP histogram methods that use the original histogram as inputs, it is infeasible to read all histogram bins into memory simultaneously due to memory constraints, and external algorithms need to be considered. Our contributions. In this paper, we present DPCopula, a novel differentially private data synthesization method for high dimensional and large domain data using copula functions. Copula functions are a family of distribution functions representing the dependence structure implicit in a multivariate random vector. Intuitively, any high- dimensional data can be modeled as two parts: 1) marginal distributions of each individual dimension, and 2) the depen- dence among the dimensions. Copula functions have been shown to be effective for modeling high-dimensional joint distributions based on continuous marginal distributions [31, 34, 4, 24]. They are particularly attractive due to several reasons. First, when we have more margins' (Marginal distribution is shortened as margin in the paper) information than the joint distribution of all dimensions, they can be used to generate any joint distributions based on known margins and correlations among all dimensions. Second, they can be used to model non-parametric dependence for 2We define \pym i=1 |Ai| as the domain space of all dimensions, where |Ai| is the domain size of the ith attribute and m is the number of attributes random variables. Further, we observe that existing DP histogram methods are efficient and effective for generating single dimensional marginal histograms, but not for high- dimensional data; and that the marginal distributions for discrete data in a large domain can be considered approx- imately continuous. Motivated by the above facts, the key idea of our proposed solution is to generate synthetic data from DP copula functions based on DP margins. We summarize our contributions below. 1) We propose a DPCopula framework to generate high dimensional and large domain DP synthetic data. It com- putes a DP copula function and samples synthetic data from the function that effectively captures the dependence implicit in the high-dimensional datasets. With the copula functions, we can separately consider the margins and the joint dependence structure of the original data instead of modeling the joint distribution of all dimensions as shown in Figure 2(b). The DPCopula framework allows direct sampling for the synthetic data from the margins and the copula function. Although existing histogram techniques can be used to generate DP synthetic data, post-processing is required to enforce non-negative histogram counts or con- sistencies between counts which results in either degraded accuracy or high computation complexity. 2) We present two methods, DPCopula-MLE (we short- en maximum likelihood estimation as MLE in the paper) and DPCopula-Kendall, for estimating parameters of the Gaussian copula function, a commonly used elliptical class of copula functions modeling the Gaussian dependence. We focus on semi-parametric Gaussian copula as most real- world high-dimensional data has been shown to follow the Gaussian dependence structure [31]. It can be used not only to model data with Gaussian joint distributions, but also data with arbitrary marginal distributions or joint distributions as long as they follow Gaussian dependence. DPCopula-MLE computes correlation among dimensions using DP MLE while DPCopula-Kendall computes DP correlation among dimensions using Kendall's \lamda correlation which is a general nonlinear rank-based correlation. 3) We present formal analysis of differential privacy guarantees and computation complexity for the two DPCopula estimation methods. We also provide analysis of their con- vergence properties. Extensive experiments using both real datasets and synthetic datasets demonstrate that DPCopula generates highly accurate synthetic multi-dimensional data and significantly outperforms state-of-the-art techniques for range count queries.",Haoran Li,"Math and Computer Science Department, Emory University Atlanta, GA",hli57@emory.edu,Li Xiong,"Math and Computer Science Department, Emory University Atlanta, GA",lxiong@mathcs.emory.edu,Xiaoqian Jiang,"Biomedical Informatics Division, UC San Diego La Jolla, CA",xiaoqian.jiang@gmail.com,,,,,,,,,,,,,,,,,,,,,
20200404,1373,Manish Patil,Louisiana State University USA,mpatil@csc.lsu.edu,,Similarity Joins for Uncertain Strings,"Similarity Joins for Uncertain Strings, Similarity Joins for Uncertain Strings, Similarity Joins for Uncertain Strings, Similarity Joins for Uncertain Strings, Similarity Joins for Uncertain Strings, ABSTRACT A string similarity join finds all similar string pairs between two input string collections. It is an essential operation in many applications, such as data integration and cleaning, and has been extensively studied for deterministic strings. Increasingly, many applications have to deal with impre- cise strings or strings with fuzzy information in them. This work presents the first solution for answering similarity join queries over uncertain strings that implements possible-world semantics, using the edit distance as the measure of similar- ity. Given two collections of uncertain strings R, S, and input (k,\lamda ), our task is to find string pairs (R,S) between collections such that Pr(ed(R,S)  k) > \lamda i.e., the probability of the edit distance between R and S being at most k is more than probability threshold \lamda . We can address the join problem by obtaining all strings in S that are similar to each string R in R. However, existing solutions for answering such similarity search queries on uncertain string databases only support a deterministic string as input. Exploiting these solutions would require exponentially many possible worlds of R to be considered, which is not only inef- fective but also prohibitively expensive. We propose various filtering techniques that give upper and (or) lower bound on Pr(ed(R,S)  k) without instantiating possible worlds for either of the strings. We then incorporate these techniques into an indexing scheme and significantly reduce the filtering overhead. Further, we alleviate the verification cost of a string pair that survives pruning by using a trie structure which allows us to overlap the verification cost of exponentially many possible instances of the candidate string pair. Finally, we evaluate the effectiveness of the proposed approach by thorough practical experimentation. Categories and Subject Descriptors H.2 [DATABASE MANAGEMENT]: Systems!Query processin Keywords Uncertain strings; string joins; edit distance 1. INTRODUCTION Strings form a fundamental data type in computer systems and string searching has been extensively studied since the inception of computer science. String similarity search takes a set of strings and a query string as input, and outputs all the strings in the set that are similar to the query string. A join extends the notion of similarity search further and re- quire all similar string pairs between two input string sets to be reported. Both similarity search and similarity join are central to many applications such as data integration and cleaning. Edit distance is the most commonly used simi- larity measure for strings. The edit distance between two strings r and s, denoted by ed(r, s), is the minimum number of single-character edit operations (insertion, deletion, and substitution) needed to transform r to s. Edit distance based string similarity search and join has been extensively studied in the literature for deterministic strings [7, 3, 2, 13, 18, 5]. However, due to the large number of applications where uncertainty or imprecision in values is either inherent or desirable, recent years have witnessed increasing attention devoted to managing uncertain data. Several probabilistic database management systems (PDBMS), which can repre- sent and manage data with explicit probabilistic models of uncertainty, have been proposed to date [17, 16]. Imprecision in data introduces many challenges for similarity search and join in databases with probabilistic string attributes, which is the focus of this paper. Uncertainty model: Analogous to the models of uncer- tain databases, two models - string-level and character-level - have been proposed recently by Jeffrey Jestes et al. [10] for uncertain strings. In the string-level uncertanity model all possible instances for the uncertain string are explicitly listed to form a probability distribution function (pdf). In contrast, the character-level model describes distributions over all characters in the alphabet for each uncertain posi- tion in the string. We focus on the character-level model as it is realistic and concise in representing the string uncertainty. Let \sigma= {c1, c2, ..., c\sigma} be the alphabet. A character-level uncertain string is S = S[1]S[2]...S[l], where S[i] (1  i  l) is a random variable with discrete distribution over \sigmai.e., S[i] is a set of pairs (cj , pi(cj)), where cj # \sigma and pi(cj) is the probability of having symbol cj at position i. Formally S[i] = {(cj , pi(cj))|cj ?= cm for j ?= m, and ! j pi(cj) = 1}. When the context of a string is unclear we represent pi(cj) 1471 for string S by Pr(S[i] = cj). Throughout we use a lower case character to represent a deterministic string (s) against the uncertain string denoted by a upper case character (S). Let |S| (|s|) be the length of string S (s). Then the possible worlds of S is a set of all possible instances s of S with prob- ability p(s), ! p(s) = 1. S being a character-level uncertain string, |S| = |s| for any of its possible instances. Query semantics: In addition to capturing uncertainty in the data, one must define the semantics of queries over the data. In this regard, a powerful model of possible-world semantics has been the backbone of analyzing the correctness of database operations on uncertain data. For uncertain string attributes, Jestes et al. [10] made the first attempt to extend the notion of similarity. They used ex- pected edit distance (eed) over all possible worlds of two uncertain strings. Given strings R and S, eed(R,S) =! ri,sj p(ri)p(sj)ed(ri, sj), where sj (ri) is an instance of S (R) with probability p(sj) (p(ri)). Though eed seems like a natural extension of edit distance as a measure of sim- ilarity, it has been shown that it does not implement the possible-world semantics completely at the query level [6]. Consider a similarity search query on a collection of deter- ministic strings with input string r. Then, string s is an output only if ed(r, s)  k. For such a query R over an uncertain string collection, possible world semantics dictate that we apply the same predicate ed(r, s)  k for each pos- sible instance r of R, s of S and aggregate this over all worlds. Thus, a possible world with instances r, s can con- tribute in deciding whether S is similar to R only if s is within the desired edit distance of r. However, for the eed measure, all possible worlds (irrespective but weighted by edit distance) contribute towards the overall score that de- termines the similarity of S with R. To overcome this prob- lem, in [6] the authors have proposed a (k,\lamda )-matching se- mantic scheme. Using this semantic, given a edit distance threshold k and probability threshold \lamda , R is similar to S if Pr(ed(R,S)  k) > \lamda . We use this similarity definition in this paper for answering join queries. Problem definition: Given two sets of uncertain strings R and S, an edit-distance threshold k and a probability thresh- old \lamda , similarity join finds all similar string pairs (R,S) # R / S such that Pr(ed(R,S)  k) > \lamda . Without loss of generality, we focus on self join in this paper i.e. R = S. Related work: Uncertain/Probabilistic strings have been the subject of study for the past several years. Efficient al- gorithms and data structures are known for the problem of string searching in uncertain text [8, 1, 9, 19]. In [6] authors have studied the approximate substring matching problem, where the goal is to report the positions of all substrings of uncertain text that are similar to the query string. Re- cently, the problem of similarity search on a collection of uncertain strings has been addressed in [4]. However, most of these works support only deterministic strings as query in- put. Utilizing these techniques for uncertain string as input would invariably need all its possible worlds to be enumerated, which may not be feasible to do taking into account the resultant exponential blowup in query cost. Though the problem of similarity join on uncertain strings has been studied in [10], it makes use of expected edit distance as a measure of similarity. We make an attempt to address some of the challenges involved in uncertain string processing by investigating similarity joins on them in this paper. Our Contributions: In this paper, we present a comprehensive investigation on the problem of similarity joins for uncertain strings using (k,\lamda )-matching [6] as the similarity definition and make the following contributions: ? We propose a filtering scheme that integrates q-gram fil- tering with probabilistic pruning, and we present an in- dexing scheme to facilitate such filtering. ? We extend the frequency distance filtering introduced in [4] for an uncertain string pair and improve its performance while maintaining the same space requirement. ? We propose the use of a trie data structure to efficiently compute the exact similarity probability (as given by (k,\lamda )- matching) for a candidate pair (R,S) without explicitly comparing all possible string pairs. ? We conduct comprehensive experiments which demonstrate the effectiveness of all proposed techniques in answering similarity join queries.",Manish Patil,Louisiana State University USA,mpatil@csc.lsu.edu,Rahul Shah,Louisiana State University USA,rahul@csc.lsu.edu,,,,,,,,,,,,,,,,,,,,,,,,
20200405,769,Chen Jason Zhang,"Hong Kong University of Science and Technology, Hong Kong, China",czhangad@cse.ust.hk,,Reducing Uncertainty of Schema Matching via Crowdsourcing,"Reducing Uncertainty of Schema Matching via Crowdsourcing, Reducing Uncertainty of Schema Matching via Crowdsourcing, Reducing Uncertainty of Schema Matching via Crowdsourcing, Reducing Uncertainty of Schema Matching via Crowdsourcing, Reducing Uncertainty of Schema Matching via Crowdsourcing, ABSTRACT Schema matching is a central challenge for data integration sys- tems. Automated tools are often uncertain about schema matchings they suggest, and this uncertainty is inherent since it arises from the inability of the schema to fully capture the semantics of the repre- sented data. Human common sense can often help. Inspired by the popularity and the success of easily accessible crowdsourcing platforms, we explore the use of crowdsourcing to reduce the un- certainty of schema matching. Since it is typical to ask simple questions on crowdsourcing platforms, we assume that each question, namely Correspondence Correctness Question (CCQ), is to ask the crowd to decide whether a given correspondence should exist in the correct matching. We propose frameworks and efficient algorithms to dynamically manage the CCQs, in order to maximize the uncertainty reduction within a limited budget of questions. We develop two novel approaches, namely ^Single CCQ' and ^Multiple CCQ', which adaptively select, publish and manage the questions. We verified the value of our solutions with simulation and real implementation. 1. INTRODUCTION Schema matching refers to finding correspondences between el- ements of the two given schemata, which is a critical issue for many database applications such as data integration, data warehousing, and electronic commerce [20]. Figure 1 illustrates a running example of the schema matching problem: given two relational schemataA andB describing faculty information, we aim to determine the correspondences (indicated by dotted lines), which identify attributes representing the same concepts in the two. There has been significant work in developing automated algorithms for schema matching (please refer to [20] for a comprehensive sur- vey). The majority combines linguistic, structural and instancebased information. In general, it is still very difficult to tackle schema matching completely with an algorithmic approach: some ambiguity remains. This ambiguity is unlikely to be removed be- cause it is believed that typically ^the syntactic representation of schemata and data do not completely convey the semantics of dif- ferent databases' [14]. Given this inherent ambiguity, many schema matching tools will produce not just one matching, but rather a whole set of possible matchings. In fact, there is even a stream of work dealing with models of possible matchings, beginning with [3]. The matching tool can produce a result similar to the upper part of Table 1, with one matching per row, associated with a probability that it is the correct matching. Given a set of possible matchings, one can create an integrated database that has uncertain data, and work with this using any of several systems that support probabilistic query processing over uncertain data, such as [9][1]. However, preserving the uncertainty complicates the query processing and increases storage cost. So we would prefer to make choices earlier, if possible, and eliminate (or reduce) the uncertainty to be propagated. It has been suggested [18] that human insights are extremely conducive for reducing the uncertainty of schema matching, so the correct matching can be manually chosen by the user from among the possible matchings offered by the system. In a traditional back-end database environ- ment, where the human `user' is a DBA, setting up a new integrated database, such a system can work well. However, in today's world, with end-users performing increas- ingly sophisticated data accesses, we have to support users who are interested, say, in combining data from two different web sources, and hence require an `ad hoc' schema matching. Such users may not be experts, and will typically have little knowledge of either source schema. They are also likely to have little patience with a system that asks them to make difficult choices, rather than just giv- ing them the desired answer. In other words, users may not them- selves be a suitable source of human insight to resolve uncertainty in schema matching. Fortunately, we have crowdsourcing technology as a promising option today. There exist platforms, such as Amazon Mechani- cal Turk, which can serve as sources of human perceptions. The data concerning an explicit problem can be queried via publishing questions, named Human Intelligent Tasks (a.k.a HITs). The work- flow of publishing HITs can be automated with available APIs (e.g. REST APIs) [4]. To the extent that our end-user is not an expert, the opinion of a crowd of other non-experts is likely to be better than that of our end-user. It is well-known that crowdsourcing works best when tasks can be broken down into very simple pieces. An entire schema match- ing may be too large a grain for a crowd ? each individual may have small quibbles with a proposed matching, so that a simple bi- nary question on the correctness of matchings may get mostly neg- ative answers, with each user declaring it less than perfect. On the other hand, asking open-ended questions is not recommended for a crowd, because it may be difficult to pull together a schema match- ing from multiple suggestions. We address this challenge by pos- ing to the crowd questions regarding individual correspondences for pairs of attributes, one from each schema being matched. This much simpler question, in most circumstances, can be answered with a simple yes or no. Of course, this requires that we build the machinery to translate between individual attribute correspon- dences and possible matchings. Fortunately, this has been done before, in [3], and is quite simple: since schema match options are all mutually exclusive, we can determine the probability of each correspondence by simply adding up the probabilities of matchings in which the correspondence holds. For example, in Table 1, the correspondence c1 holds in m1 and m2, but not m3. Therefore, its probability is obtained as 0.45 + 0.3 = 0.75. Our problem then is to choose wisely the correspondences to ask the crowd to obtain the highest certainty of correct schema match- ing at the lowest cost. For schema matching certainty, we choose entropy as our measure ? we are building our system on top of a basic schema-matching tool, which is estimating probabilities for schema matches it produces. When the tool obtains a good match, it can associate a high probability. When there is ambiguity or con- fusion, this translates into multiple lower probability matches, with associated uncertainty and hence higher entropy. Our first algorithm, called Single CCQ, determines the single most valuable correspondence query to ask the crowd, given a set of possible schema matchings and associated correspondences, all with probabilities. Intuitively, one may try a simple greedy approach, choosing the query that reduces entropy the most. However, there are three is- sues to consider. First, the correspondences are not all independent, since they are related through candidate matchings. So it is not ob- vious that a greedy solution is optimal. Second, even finding the query that decreases entropy the most can be computationally ex- pensive. Third, we cannot assume that every person in the crowd answers every question correctly ? we have to allow for wrong an- swers too. We address all three challenges below. Usually, we are willing to ask the crowd about more than one correspondence, even if not all of them. We could simply run Sin- gle CCQ multiple times, each time greedily resolving uncertainty in the most valuable correspondence. However, we can do better. For this purpose, we develop Multiple CCQ, an extension of Sin- gle CCQ, that maintains kmost contributive questions in the crowd, and dynamically updates questions according to newly received an- swers. To summarize, we have made the following contributions, 1. In Section 3.1 and Section 4.1, we propose an entropy-based model to formulate the uncertainty reduction caused by a single CCQ and multiple CCQs, respectively. 2. In Section 4.3, we prove that the uncertainty reduction is equivalent to the joint entropy of CCQs, by considering each CCQ as a binary variable. We further show that greedy is indeed optimal. 3. For the Single CCQ approach, we propose an explicit frame- work, and derive an efficient algorithm for the ^Single CCQ' in Section 3. We introduce an index structure and pruning technique for efficiently finding the Single CCQ. We also develop the math- ematical machinery to deal with possibly erroneous answers from the crowd. 4. For the Multiple CCQ approach, we prove the NP-hardness of Multiple CCQ Problem in Section 4, and propose an efficient (1+ ) approximation algorithm, with effective pruning techniques. In addition, Section 5 reports and discusses the experimental study on both simulation and real implementation. We review and compare our solutions with related work in Section 6. In Section 7, we conclude the paper, and discuss several future works.",Chen Jason Zhang,"Hong Kong University of Science and Technology, Hong Kong, China",czhangad@cse.ust.hk,Lei Chen,"Hong Kong University of Science and Technology, Hong Kong, China",leichen@cse.ust.hk,H. V. Jagadish,"University of Michigan, Ann Arbor, MI, USA",jag@umich.edu@cse.ust.hk,Chen Caleb Cao,"Hong Kong University of Science and Technology, Hong Kong, China",caochen@cse.ust.hk,,,,,,,,,,,,,,,,,,
20200406,867,Francois Goasdoue,"Leo team, INRIA Saclay and LRI, Universite Paris-Sud 11",Francois.Goasdoue@inria.fr,,View Selection in Semantic Web Databases,"View Selection in Semantic Web Databases, View Selection in Semantic Web Databases, View Selection in Semantic Web Databases, View Selection in Semantic Web Databases, View Selection in Semantic Web Databases, ABSTRACT We consider the setting of a Semantic Web database, containing both explicit data encoded in RDF triples, and implicit data, im- plied by the RDF semantics. Based on a query workload, we address the problem of selecting a set of views to be materialized in the database, minimizing a combination of query processing, view storage, and view maintenance costs. Starting from an existing rela- tional view selection method, we devise new algorithms for recom- mending view sets, and show that they scale significantly beyond the existing relational ones when adapted to the RDF context. To account for implicit triples in query answers, we propose a novel RDF query reformulation algorithm and an innovative way of incorporating it into view selection in order to avoid a combinatorial explosion in the complexity of the selection process. The interest of our techniques is demonstrated through a set of experiments. 1. INTRODUCTION A key ingredient for the Semantic Web vision [4] is a data format for describing items from the real and digital world in a machine- exploitable way. The W3C 's resource description framework (RDF, in short [26]) is a leading candidate for this role. At a first look, querying RDF resembles querying relational data. Indeed, at the core of the W3C 's SPARQL query language for RDF [27] lies conjunctive relational-style querying. There are, however, several important differences in the data model. First, an RDF data set is a single large set of triples, in contrast with the typical relational database featuring many relations with varying numbers of attributes. Second, RDF triples may feature blank nodes, standing for unknown constants or URIs; an RDF database may, for instance, state that the author of X is Jane while the date of X is 4/1/2011, for a given, unknown resource X . This contrasts with standard relational databases where all attribute values are either constants or null. Finally, in typical relational databases, all data is explicit, whereas the semantics of RDF entails a set of implicit triples which must be reflected in query answers. One important source of implicit triples follows from the use of an (optional) RDF. Schema (or RDFS, in short [26]), to enhance the descriptive power of an RDF data set. For instance, assume the RDF database con- tains the fact that the driverLicenseNo of John is 12345, whereas an RDF Schema states that only a person can have a driverLicenseNo. Then, the fact that John is a person is implicitly present in the database, and a query asking for all person instances in the database must return John. The complex, graph-structured RDF model is suitable for describing heterogeneous, irregular data. However, it is clearly not a good model for storing the data. Existing RDF platforms therefore assume a simple (application-independent) storage model, comple- mented by indexes and efficient query evaluation techniques [1, 15, 16, 17, 20, 23], or by RDF materialized views [6, 9]. While indexes or views speed up the evaluation of the fragments of queries match- ing them, the query processor may still need to access the main RDF database to evaluate the remaining fragments of the queries. We consider the problem of choosing a (relational) storage model for an RDF application. Based on the application workload, we seek a set of views to materialize over the RDF database, such that all workload queries can be answered based solely on the recom- mended views, with no need to access the database. Our goal is to enable three-tier deployment of RDF applications, where clients do not connect directly to the database, but to an application server, which could store only the relevant views; alternatively, if the views are stored at the client, no connection is needed and the application can run off-line, independently from the database server. RDF datasets can be very different: data may be more or less structured, schemas may be complex, simple, or absent, updates may be rare or frequent. Moreover, RDF applications may differ in the shape, size and similarity of queries, costs of propagating updates to the views etc. To capture this variety, we characterize candidate view sets by a cost function, which combines (i) query evaluation costs, (ii) view maintenance costs and (iii) view storage space. Our contributions are the following: 1. This is the first study of RDF materialized view selection sup- porting the rewriting of all workload queries. We show how to model this as a search problem in a space of states, inspired from a previous work in relational data warehousing [21]. 2. Implicit triples entailed by the RDF semantics [26] must be re- flected in the recommended materialized views, since they may par- ticipate to query results. Two methods are currently used to include implicit tuples in query results. Database saturation adds them to the database, while query reformulation leaves the database intact and modifies queries in order to also capture implicit triples. Our approach requires no special adaptation if applied on a saturated database. For the reformulation scenario, we propose a novel RDF query reformulation algorithm. This algorithm extends the state of the art in query processing in the presence of RDF Schemas [3, 97 5], and is a contribution applying beyond the context of this work. Moreover, we propose an innovative method of using reformulation (called post-reformulation) which enables us to efficiently take into account implicit triples in our view selection approach. 3. We consider heuristic search strategies, since the complexity of complete search is extremely high. Existing strategies for rela- tional view selection [21] grow out of memory and fail to produce a solution when the number of atoms in the query workload grows. Since RDF atoms are short (just three attributes), RDF queries are syntactically more complex (they have more atoms) than relational queries retrieving the same information, making this scale problem particularly acute for RDF. We propose a set of new strategies and heuristics which greatly improve the scalability of the search. 4. We study the efficiency and effectiveness of the above algo- rithms, and their improvement over existing similar approaches, through a set of experiments. This paper is organized as follows. Section 2 formalizes the problem we consider. Section 3 presents the view selection prob- lem as a search problem in a space of candidate states, whereas Section 4 discusses the inclusion of implicit RDF triples in our approach. Section 5 describes the search strategies and heuristics used to navigate in the search space. Section 6 presents our experimental evaluation. Section 7 discusses related works, then we conclude.",Francois Goasdoue,"Leo team, INRIA Saclay and LRI, Universite Paris-Sud 11",Francois.Goasdoue@inria.fr,Konstantinos Karanasos,"Leo team, INRIA Saclay and LRI, Universite Paris-Sud 11",Konstantinos.Karanasos@inria.fr,Julien Leblay,"Leo team, INRIA Saclay and LRI, Universite Paris-Sud 11",Julien.Leblay@inria.fr,Ioana Manolescu,"Leo team, INRIA Saclay and LRI, Universite Paris-Sud 11",Ioana.Manolescu@inria.fr,,,,,,,,,,,,,,,,,,
20200407,1374,Ziyu Guan,"Dept. of Computer Science University of California Santa Barbara, CA, 93106, USA ",ziyuguan@cs.ucsb.edu,,Assessing and Ranking Structural Correlations in Graphs,"Assessing and Ranking Structural Correlations in Graphs, Assessing and Ranking Structural Correlations in Graphs, Assessing and Ranking Structural Correlations in Graphs, Assessing and Ranking Structural Correlations in Graphs, Assessing and Ranking Structural Correlations in Graphs, ABSTRACT Real-life graphs not only have nodes and edges, but also have events taking place, e.g., product sales in social networks and virus infection in communication networks. Among different events, some exhibit strong correlation with the network structure, while others do not. Such structural correlation will shed light on viral influence existing in the corresponding network. Unfortunately, the traditional association min- ing concept is not applicable in graphs since it only works on homogeneous datasets like transactions and baskets. We propose a novel measure for assessing such structural correlations in heterogeneous graph datasets with events. The measure applies hitting time to aggregate the proximity among nodes that have the same event. In order to calcu- late the correlation scores for many events in a large network, we develop a scalable framework, called gScore, using sampling and approximation. By comparing to the situation where events are randomly distributed in the same network, our method is able to discover events that are highly corre- lated with the graph structure. gScore is scalable and was successfully applied to the co-author DBLP network and so- cial networks extracted from TaoBao.com, the largest online shopping network in China, with many interesting discoveries. Categories and Subject Descriptors H.2.8 [Database Management]: Database Applications - Data Mining General Terms Algorithms, Experimentation Keywords Graph, Structural Correlation, Hitting Time Corresponding author 1. INTRODUCTION The rise of the web, social networks, and bioinformatics has presented scientists with numerous graphs, each consisting of millions of nodes and edges. Hidden in these large datasets are the answers to important questions in network- ing, sociology, business, and biology. These graphs not only have topological structures, but also contain events/activities that occurred on their nodes. For example, an eBay cus- tomer could sell or bid a product. A Facebook user could play a Zynga game with his/her friends. This complex combination raises new research problems in graph data analysis [29, 22, 8, 2]. Among different events taking place in a network, some exhibit strong correlation with the network structure, while others do not. Such structural correlation might shed light on viral influence existing in the corresponding network, which is the key to many research problems in product marketing [7], online advertisement [3], and recommendation [16]. Figure 1 shows the distribution of three types of events over the same graph. We can easily incarnate Figure 1 into different application scenarios. For example, it could be three kinds of products that were bought by members in a social network. Dark nodes in Figure 1(A), 1(B) and 1(C) represent members who purchased products A, B and C, respectively. Intuitively, Figure 1 shows that in this net- work, people who bought product A (or B) are closer to each other. In contrary, black nodes for C seem to be randomly distributed. In this scenario, the network would be suitable for promoting A and B and we can promote A and B among people who have not bought them. While it is hard to derive deterministic relationship between sales and network struc- ture, it is possible to study how the sales is correlated to the structure. In fact, one can raise several interesting questions related to structure and events distributed over structure: 1. In regard to the sales of products A and B, which one is more related to the underlying social network? 2. Given two networks G and G ' for the same group of users, e.g., their email network and Facebook network, is the sales of product A more related to G than G '? 937 3. If we have snapshots of network G during different pe- riods, can we measure how product A was dispersed over the network over time? Was it purchased by a small circle of friends at the very beginning? In order to answer the above questions, we need to address the following research problems: 1. How to define and measure the correlation between the graph structure and events? 2. How to compute the measure efficiently in large graphs, if we want to rank all events according to the measure? Unfortunately, the classic association mining concept is not applicable in this setting since it only works on homoge- neous datasets like transactions and baskets [1, 30]. In this paper, we propose a novel measure to assess structural cor- relations in a graph. The measure aggregates the proximity among nodes on which the same event has occurred, using a proximity function such as hitting time [20]. We develop an efficient computation framework, gScore (Graph Structural Correlation Estimation), to quickly calculate correlation scores in large scale networks. By estimating the deviation from the expected correlation score of a random situation, our method is able to discover the events of nodes that are highly correlated with the graph structure. Our contributions. We propose a novel concept, structural correlation, to measure how an event is distributed over a graph and address a key research problem in ana- lyzing the relation between structure and contents. While many studies have demonstrated that social links could sig- nificantly influence the behavior of human beings [5, 17, 7], we suspect that such influence should be further scrutinized for more fine-grained knowledge: in which kind of social links (e.g., phone network, email network, employee net- work, friend network, etc) social influence is observed, and how strong, and for which kind of behaviors (e.g.,shopping, hobby, interest, and opinion). In this study, we quantify the correlation between link structure and human behav- iors, and make different behaviors ' correlations comparable using statistical significance. We discover that the corre- lation actually fluctuates dramatically with regard to link types, event types, and time, implying a need to further ex- amine the cause of the fluctuation. Note that in this work, we are not going to perform a causality study between cor- relation and influence [2]. We systematically introduce a framework to define and measure structural correlations in graphs. The principle is to aggregate the proximity among nodes which have the same event and compare the aggregated proximity against the sit- uations where these events are randomly distributed in the graph. This framework can integrate various graph proximity measures [6] such as hitting time [20], personalized PageRank [24, 14] and Katz [15]. We take hitting time as an example and propose a mod- ified version named Decayed Hitting Time (DHT) to better and faster calculate structural correlation. Scalable al- gorithms are developed using sampling and approximation techniques to calculate DHT for individual nodes and the average DHT for all the nodes which share the same event. We investigate the expectation and variance of the correlation when an event is randomly distributed over a graph and derive several important properties. These properties can help us to quickly estimate the deviation from random cases, thus making online computation of structural corre- lations possible. Our algorithm was tested in real networks including co-author DBLP network and social networks extracted from TaoBao.com, the largest online shopping net- work in China, with many exciting discoveries.",Ziyu Guan,"Dept. of Computer Science University of California Santa Barbara, CA, 93106, USA ",ziyuguan@cs.ucsb.edu,Jian Wu,"College of Computer Science Zhejiang University Hangzhou, 310027, China",ambuj@cs.ucsb.edu,Qing Zhang,"TaoBao.com 99 Huaxing Road Hangzhou, 310099, China",xyan@cs.ucsb.edu,Ambuj Singh,"Dept. of Computer Science University of California Santa Barbara, CA, 93106, USA ",wujian2000@zju.edu.cn,Xifeng Yan,"Dept. of Computer Science University of California Santa Barbara, CA, 93106, USA ",yunzheng@taobao.com,,,,,,,,,,,,,,,
20200408,1375,Aaron J. Elmore,"Department of Computer Science University of California, Santa Barbara Santa Barbara, CA 93106-5110, USA",aelmore@cs.ucsb.edu,,Zephyr: Live Migration in Shared Nothing Databases for Elastic Cloud Platforms,"Zephyr: Live Migration in Shared Nothing Databases for Elastic Cloud Platforms, Zephyr: Live Migration in Shared Nothing Databases for Elastic Cloud Platforms, Zephyr: Live Migration in Shared Nothing Databases for Elastic Cloud Platforms, Zephyr: Live Migration in Shared Nothing Databases for Elastic Cloud Platforms, Zephyr: Live Migration in Shared Nothing Databases for Elastic Cloud Platforms, ABSTRACT Multitenant data infrastructures for large cloud platforms hosting hundreds of thousands of applications face the challenge of serv- ing applications characterized by small data footprint and unpre- dictable load patterns. When such a platform is built on an elas- tic pay-per-use infrastructure, an added challenge is to minimize the system 's operating cost while guaranteeing the tenants ' service level agreements (SLA). Elastic load balancing is therefore an im- portant feature to enable scale-up during high load while scaling down when the load is low. Live migration, a technique to migrate tenants with minimal service interruption and no downtime, is critical to allow lightweight elastic scaling. We focus on the prob- lem of live migration in the database layer. We propose Zephyr, a technique to efficiently migrate a live database in a shared nothing transactional database architecture. Zephyr uses phases of on- demand pull and asynchronous push of data, requires minimal synchronization, results no service unavailability and few or no aborted transactions, minimizes the data transfer overhead, provides ACID guarantees during migration, and ensures correctness in the pres- ence of failures. We outline a prototype implementation using an open source relational database engine and an present a thorough evaluation using various transactional workloads. Zephyr 's efficiency is evident from the few tens of failed operations, 10-20% change in average transaction latency, minimal messaging, and no overhead during normal operation when migrating a live database. Categories and Subject Descriptors H.2.4 [Database Management]: Systems aRelational databases, Transaction processing; H.3.4 [Information Storage and Retrieval]: Systems and Software aDistributed systems General Terms Design, Experimentation, Performance, Reliability Keywords Cloud computing, multitenancy, elastic data management, database migration, shared nothing architectures 1. INTRODUCTION The increasing popularity of service oriented computing has seen hundreds of thousands of applications being deployed on various cloud platforms [15]. The sheer scale of the number of applica- tion databases, or tenants, and their small footprint (both in terms of size and load) mandate a shared infrastructure to minimize the operating cost [11, 19, 25, 26]. These applications often have un- predictable load patterns, such as flash crowds originating from a sudden and viral popularity, resulting in the tenants ' resource re- quirements changing with little notice. Load balancing is therefore an important feature to minimize the impact of a heavily loaded tenant on the other co-located tenants. Furthermore, a platform deployed on a pay-per-use infrastructure (like Amazon EC2) provides the potential to minimize the system 's operating cost. Elastic- ity, i.e. the ability to scale up to deal with high load while scaling down in periods of low load, is a critical feature to minimize the operating cost. Elastic load balancing is therefore a first class fea- ture in the design of modern database management systems for the cloud [11,12], and requires a low cost technique to migrate tenants between hosts, a feature referred to as live migration [8, 20].1 Our focus is the problem of live migration in the database layer supporting a multitenant cloud platform where the service provider manages the applications ' databases. Force.com, Microsoft Azure, and Google AppEngine are examples of such multitenant cloud platforms. Even though a number of techniques are prevalent to scale the DBMS layer, elasticity is often ignored primarily due to static infrastructure provisioning. In a multitenant platform built on an infrastructure as a service (IaaS) abstraction, elastic scal- ing allows minimizing the system 's operating cost leveraging the pay-per-use pricing. Most current DBMSs, however, only sup- port heavyweight techniques for elastic scale-up where adding new nodes requires manual intervention or long service disruption to migrate a tenant 's database to these newly added nodes. Therefore, to enable lightweight elasticity as a first class notion, live migration is a critical functionality. We present Zephyr,2 a technique for live migration in a shared nothing transactional database. Das et al. [13] proposed a solution for live database migration in a shared storage architecture while Curino et al. [10] outlined a possible solution for live migration in a shared nothing architecture. Zephyr is the first complete solution for live migration in a shared nothing database architecture. Zephyr minimizes service interruption for the tenant being migrated by introducing a synchronized dual mode that allows both the source and destination to simultaneously execute transactions for the tenant. Migration starts with the transfer of the tenant 's metadata to the destination which can then start serving new trans- actions, while the source completes the transactions that were ac- tive when migration started. Read/write access (called ownership) on database pages of the tenant is partitioned between the two nodes with the source node owning all pages at the start and the destination acquiring page ownership on-demand as transactions at the destination access those pages. The index structures are replicated at the source and destination and are immutable during migration. Lightweight synchronization between the source and the destina- tion, only during the short dual mode, guarantees serializability, while obviating the need for two phase commit [16]. Once the source node completes execution of all active transactions, migration completes with the ownership transfer of all database pages owned by the source to the destination node. Zephyr thus allows migration of individual tenant databases that share a database pro- cess at a node and where live VM migration [8] cannot be used. Zephyr guarantees no service disruption for other tenants, no system downtime, minimizes data transferred between the nodes, guarantees safe migration in the presence of failures, and ensures the strongest level of transaction isolation. Zephyr uses standard tree based indices and lock based concurrency control, thus allow- ing it to be used in a variety of DBMS implementations. Zephyr does not rely on replication in the database layer, thus providing greater flexibility in selecting the destination for migration, which might or might not have the tenant 's replica. However, considerable performance improvement is possible in the presence of replication when a tenant is migrated to one of the replicas. We implemented Zephyr in an open source RDBMS. Our evalu- ation using a variety of transactional workloads shows that Zephyr results in only a few tens of failed operations, compared to hun- dreds to thousands of failed transactions when using a simple heavy- weight migration technique. Zephyr results in no operational overhead during normal operation, minimal messaging overhead dur- ing migration, and between 10-20% increase in average transaction latency compared to an execution where no migration was performed. These results demonstrate the lightweight nature of Zephyr allowing live migration with minimal service interruption. The main contributions of this paper are as follows: ? We present Zephyr, the first complete end-to-end solution for live migration in a shared nothing database architecture. ? We present a detailed analysis of the guarantees provided by Zephyr, analyze the associated trade-offs, and prove safety and liveness guarantees in the presence of failures. ? We provide a detailed evaluation of our prototype evaluation using a variety of workloads that demonstrate interesting trade-offs in performance. The rest of the paper is organized as: Section 2 provides back-ground on multitenancy models, the system model used, migration cost measures, and describes some straightforward migration tech- niques. Section 3 describes Zephyr, Section 4 proves transaction correctness, and Section 5 discusses some extensions and optimiza- tions. Section 6 describes the details of a prototype implementation and Section 7 provides a detailed evaluation. Section 8 provides a survey of related literature and Section 9 concludes the paper.",Aaron J. Elmore,"Department of Computer Science University of California, Santa Barbara Santa Barbara, CA 93106-5110, USA",aelmore@cs.ucsb.edu,Sudipto Das,"Department of Computer Science University of California, Santa Barbara Santa Barbara, CA 93106-5110, USA",sudipto@cs.ucsb.edu,Divyakant Agrawal,"Department of Computer Science University of California, Santa Barbara Santa Barbara, CA 93106-5110, USA",agrawal@cs.ucsb.edu,Amr El Abbadi,"Department of Computer Science University of California, Santa Barbara Santa Barbara, CA 93106-5110, USA",amr@cs.ucsb.edu,,,,,,,,,,,,,,,,,,
20200409,122,Olga Poppe,"Worcester Polytechnic Institute, 100 Institute Road, Worcester, MA 01609, USA",opoppe@nec-labs.com,,Context-aware Event Stream Analytics,"Context-aware Event Stream Analytics, Context-aware Event Stream Analytics,  Context-aware Event Stream Analytics, Context-aware Event Stream Analytics, Context-aware Event Stream Analytics, ABSTRACT Complex event processing is a popular technology for con- tinuously monitoring high-volume event streams from health care to traffic management to detect complex compositions of events. These event compositions signify critical  ""appli- cation contexts "" from hygiene violations to traffic accidents. Certain event queries are only appropriate in particular con-texts. Yet state-of-the-art streaming engines tend to execute all event queries continuously regardless of the current application context. This wastes tremendous processing resources and thus leads to delayed reactions to critical situations. We have developed the first context-aware event process- ing solution, called CAESAR, which features the following key innovations. (1) The CAESAR model supports applica- tion contexts as first class citizens and associates appropriate event queries with them. (2) The CAESAR optimizer employs context-aware optimization strategies including con- text window push-down strategy and query workload shar- ing among overlapping contexts. (3) The CAESAR infras- tructure allows for lightweight event query suspension and activation driven by context windows. Our experimental study utilizing both the Linear Road stream benchmark as well as real-world data sets demonstrates that the context- aware event stream analytics consistently outperforms the state-of-the-art strategies by factor of 8 on average. 1. INTRODUCTION Complex Event Processing (CEP) has emerged as a prominent technology for supporting applications from financial fraud [30] to health care [32]. Traditionally, CEP systems consume event streams produced by smart digital devices like sensors and mobile phones and continuously evaluate the query workload to monitor the input event streams. In many stream-based applications, events convey particular application contexts such that the system reaction to an event may significantly vary depending on the current context. Therefore, some event queries may only need to be executed under certain circumstances while others can be safely suspended. The following examples highlight the challenges and opportunities of context-aware event stream processing that have been overlooked in the prior research. Motivating Example. Traffic has both a huge economic and environmental impact on our daily lives. Drivers trav- eling the 10-worst U.S. traffic corridors annually spend an average of 140 hours idling in traffic [2]. Due to pollution and noise, congestion in the USA 's 83 largest urban areas in 2010 led to a related public health cost of $18 billion [3]. Further, road traffic injuries caused an estimated 1.24 mil- lion deaths worldwide in 2010 [4]. An intelligent traffic control center could reduce these crippling impacts. The center receives vehicle position reports, analyzes them, infers the current situation in the mon- itored road segments and reacts instantaneously to ensure safe and smooth traffic flow. Early detection and prompt re- action to critical situations are eminently important. They prevent time and fuel waste, reduce pollution, avoid prop- erty damage and in some cases even save human lives. System reaction to a position report should thus be modulated depending on the current situation on the road (here referred to as context1). Indeed, if an accident is detected, all vehicles downstream should be warned and possibly al- ternative routes should be suggested (Figure 1). If a road segment becomes congested, drivers may be charged toll to discourage them from driving to control smooth traffic flow. If a road segment is clear, none of the above actions should take place. Clearly, current application contexts must be rapidly detected and continuously maintained to determine appropriate reactions of the system at all times. Conditions implying an application context can be com- plex. They are specified on both the event streams and the current contexts. For example, if over 50 cars per minute move with an average speed less then 40 mph and the current context is no congestion then the context deriving query updates the context to congestion for this road segment. To save resources and thus to ensure prompt system responsive- ness, such complex context detection should happen once. Its results must be available on-time and shared among all queries that belong to the detected context. In other words, context processing queries are dependent on the results of context deriving queries and a mechanism ensuring their cor- rect execution must be employed. The system responsiveness can be substantially improved by exploiting the optimization opportunities enabled by the application contests. (1) Only those event queries that are relevant in the current contexts should be executed. All ir- relevant computations should be suspended. (2) Workloads of overlapping contexts should be shared. Furthermore, application contexts break the application semantics into modules that facilitate the modular development and runtime maintenance of an event stream processing application. Challenges. To enable such event stream processing applications, the following challenges must be tackled: Context-aware specification model. As motivated above, event stream processing applications need to express rich semantics. In particular, they have to specify application con- texts as first class citizens and enable linkage of appropriate event queries to their respective context. Furthermore, this model must be in a convenient human-readable format to facilitate on-the-fly reconfiguration, easy maintenance and avoid fatal specification mistakes. Context-exploiting optimization techniques. To meet the demanding latency constraints of time-critical applications, this powerful context-aware application model must be trans- lated into an efficient physical query plan. This query plan must be optimized by exploiting the optimization opportu- nities enabled by context-aware event stream analytics. This is complicated by the fact that the duration of a context is unknown at compile time and potentially unbounded. Context-driven execution infrastructure. An efficient run- time execution infrastructure is required to support multiple concurrent contexts. To ensure correct query execution, the inter-dependencies between complex context deriving and context processing queries must be taken into account. State-of-the-Art. The challenges described above have so far not been addressed in a comprehensive fashion. Since the duration of a context varies, state-of-the-art win- dow semantics such as fixed-length tumbling and sliding win- dows [22, 8] are inadequate to model the proposed notion of a context. Classical predicate windows [15] have variable du- ration. However, conditions leading to an application con- text can be rather complex and thus resource-consuming, worse yet they can be dependent on the previous contexts (Figure 1). Since predicate windows are independent from each other, they fail to express context windows. While some event query languages (e.g., CQL [10], SASE [5, 34]) could be used to hard-code the equivalent of a con- text construct by queries that detect the context bounds. However, this approach is cumbersome and error-prone ? requiring the careful specification of multiple complex inter- dependent event queries [25]. Furthermore, no optimiza- tion techniques have been developed to exploit the benefits of context-awareness such as suspension of irrelevant event queries nor the sharing workloads of overlapping contexts. Business models [16, 28] focus on powerful modeling con- structs to capture the semantics of processes and in that sense express application contexts. However, these models, targeting business process specification, were not designed for event stream processing. Thus, they neglect its core peculiarities such as the event-driven nature of context de- tection achieving high performance analytics and the impor- tance of temporal windows and their processing techniques. The Proposed CAESAR Approach. In [25], we for- mally defined the first context-aware event query processing model for which we now design the Context-Aware Event Stream Analytics in Real time system, CAESAR for short. Our CAESAR model supports context windows as first- class citizens and associates appropriate event queries with each context window. Event queries that process events within a context are called context processing queries. Event queries that derive a context are called context deriving queries. Both types of queries operate within context windows, a new class of event query window we define. To achieve near real-time system responsiveness, the CAE- SAR model is transformed into a stream query plan com- posed of context-aware operators of the CAESAR algebra. This algebra serves as foundation for the CAESAR optimizer. The optimizer exploits the notion of context windows to avoid unnecessary computations by suspending those oper- ators which are irrelevant to the current context. Further- more, the optimizer saves computations by sharing work- loads of overlapping context windows. Finally, we built the CAESAR runtime infrastructure for correct yet efficient execution of inter-dependent context-aware event queries. Contributions can be summarized as follows: 1) We introduce a new notion of windows, called context windows, to enable context-aware event query process- ing critical to modeling event-based systems. The proposed human-readable context-aware CAESAR model significantly simplifies the specification of rich event-driven application semantics by explicit support of context windows2. It also opens new multi-query optimization opportunities by asso- ciating appropriate event queries with each context. 2) We define the CAESAR algebra for our context-aware event query processing. The CAESAR optimizer pushes the context windows down to suspend the execution of irrelevant operators. Furthermore, we propose the context window grouping algorithm that exploits the sharing opportunities from workloads of overlapping context windows. 3) We built the CAESAR runtime execution infrastruc- ture that guarantees correct and efficient execution of inter- dependent context deriving and context processing queries. 4) We evaluate the performance of the CAESAR system and its optimization strategies using the Linear Road stream benchmark [9] as well as the real world data set [26]. Our CAESAR system performs on average 8-fold faster than the context-independent solution for a wide range of cases. Outline. We start with preliminaries in Section 2 and introduce the CAESAR model in Section 3. We present our algebraic execution paradigm in Section 4 and its optimiza- tion techniques in Section 5. Section 6 is devoted to the run- time execution infrastructure. We conduct the performance study in Section 7. Related work is discussed in Section 8, and Section 9 concludes the article.",Olga Poppe,"Worcester Polytechnic Institute, 100 Institute Road, Worcester, MA 01609, USA",opoppe@nec-labs.com,Chuan Lei,"NEC Labs America, 10080 N Wolfe Rd, Cupertino, CA 95014, USA",rundenst@nec-labs.com,Elke A. Rundensteiner,"Worcester Polytechnic Institute, 100 Institute Road, Worcester, MA 01609, USA",dd@nec-labs.com,Dan Dougherty,"Worcester Polytechnic Institute, 100 Institute Road, Worcester, MA 01609, USA",chuan@nec-labs.com,,,,,,,,,,,,,,,,,,
20200410,1376,Sonia Bergamaschi,"University of Modena and Reggio Emilia, Italy",sonia.bergamaschi@unimore.it,,Keyword Search over Relational Databases: A Metadata Approach,"Keyword Search over Relational Databases: A Metadata Approach, Keyword Search over Relational Databases: A Metadata Approach, Keyword Search over Relational Databases: A Metadata Approach, Keyword Search over Relational Databases: A Metadata Approach, Keyword Search over Relational Databases: A Metadata Approach, ABSTRACT Keyword queries offer a convenient alternative to traditional SQL in querying relational databases with large, often unknown, schemas and instances. The challenge in answering such queries is to discover their intended semantics, construct the SQL queries that describe them and used them to retrieve the respective tuples. Existing approaches typically rely on indices built a-priori on the database content. This seriously limits their applicability if a-priori access to the database content is not possible. Examples include the online databases accessed through web interface, or the sources in information integration systems that operate behind wrappers with specific query capabilities. Furthermore, existing literature has not studied to its full extend the inter-dependencies across the ways the different keywords are mapped into the database values and schema elements. In this work, we describe a novel technique for translating keyword queries into SQL based on the Munkres (a.k.a. Hun- garian) algorithm. Our approach not only tackles the above two limitations, but it offers significant improvements in the identifica- tion of the semantically meaningful SQL queries that describe the intended keyword query semantics. We provide details of the tech- nique implementation and an extensive experimental evaluation. Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval aSearch process; Retrieval models; Query formulation General Terms Algorithms, Experimentation, Performance Keywords Semantic Keyword Search, Intensional Knowledge, Relational Databases, Metadata . 1. INTRODUCTION The more the relational data complexity is increasing and the user base is shifting towards the less technically skilled, the more the keyword searching is becoming an attractive alternative to tradi- tional SQL queries, mainly due to its simplicity. Unfortunately, this simplicity comes with the price of inherent ambiguity. Thus, the challenge of answering a keyword query over a relational database is to discover the database structures that contain the keywords and explore how these structures are inter-connected to form an answer. The discovered structures, alongside their inter-connections, are ac- tually representing in relational terms the semantic interpretation of the keyword query. Numerous studies and tools can already be found in the sci- entific literature. They include DISCOVER [16], DBXplorer [2], BANKS [1], SPARK [22], SQAK [31], and many others [21, 29, 27, 33, 34, 38]. Generally, these works consider the database as a network of interconnected tuples, they detect those containing the keywords in the query, they generate connected components based on how these tuples are associated, and they return these connected tuples as an answer to the query. To do so, specialized structures that index the database content [2] are used. By using these indices, they may directly retrieve the tuples of interest, or they may instead construct the queries expressions that retrieve these tuples when evaluated. This is the basic idea followed by the modern commer- cial database management systems supporting full-text search over their relational database. Unfortunately, existing techniques suffer from two main limi- tations. The first is that they require a-priori access to the data instance in order to build the indices that will locate the tuples re- lated to the given keywords at run time. This seriously limits their applicability if such access is not possible. Examples of such sit- uations include databases on the hidden web and sources located behind wrappers in data integration systems [19] that typically ex- pose only their schema information and lack notification mecha- nisms for their data updates. The second limitation is that no con- siderable attention has been paid to the inter-dependencies among the query keywords. The likelihood that a specific data structure represent the same semantics as a keyword in a user query does not only depend on the relationship between the keyword and the data structure, but also on the data to which the other keywords in the query are mapped. This is because despite the fact that a keyword query is a flat list of keywords, the meaning of each keyword is not independent of the meaning of the others, but they all collec- tively represent the intended concepts the user had in mind posing the query. Furthermore, not all the keywords represent instance values. Many are used as meta-data specification of the adjacent 565 keywords. Although there are already keyword based approaches on relational data that take into consideration metadata [21][31], they provide only partial solutions to the problem, and they only use the metadata as a way to improve their technique. In this work, we propose a novel technique for answering key- word queries over relational databases. The queries are translated into a number of SQL queries that capture the possible semantics of the keyword query. The generated SQL queries can be evaluated on the database, and their results serve as the answer to the keyword query. One of the novelties of the technique is that it is not based on an a-priori access to the database instances. Moreover, our ap- proach exploits the relative positions of the keywords in the query alongside auxiliary external knowledge in order to make a more educated guess of the semantics that most likely represent those of the keyword query, and then rank them accordingly. The strat- egy can not only be easily incorporated to many relational database management systems, but it can also be used as an enhancement of existing keyword searching techniques that utilize the database in- stance, offering them significant improvements over effectiveness and efficiency. An advantage of our approach is that it can be used to assist users browsing databases with large unknown schemas. It is of- ten the case that users formulate keyword queries without some specific semantics in mind but in an exploratory manner, mainly when they are neither fully aware of the type of information that is stored in a database, nor of the way this information is stored. The possible interpretations of a keyword query according to the schema and domain information of the database will be generated by our approach. In contrast to other keyword searching techniques on relational data that return sets of linked tuples, we can return the interpretations expressed in SQL. The study of these queries can reveal tables, attributes and join paths, providing the user with enough information to understand the kind of data that is stored in the database and the way this data is structured. Our key contributions are as follows: (i) we formally define the problem of keyword querying over relational databases that lack a- priori access to the database instance; (ii) we introduce the notion of a weight as a measure of the likelihood that the semantics of a keyword are represented by a database structure, i.e., a table, an attribute, or a value. We further distinguish the weights to intrinsic and contextual, to emphasize that this likelihood does not depend only on the meaning of the keyword semantics when the keyword is considered in isolation (intrinsic), but also on the way the seman- tics of the remaining, especially the neighbouring, keywords are represented in the data (contextual). (iii) we extend and exploit the Hungarian (a.k.a., Munkres) algorithm [7] to develop a technique for the systematic computation of the contextual weights that leads into to the generation and ranking of the different interpretations of a keyword query in terms of SQL; finally, (iv) we experimentally evaluate our approach on real application scenarios. The remainder of the paper is structured as follows: Section 2 provides a motivating example and Section 3 formally defines the problem. Section 4 describes our technique. Sections 5 - 7 provides details on our technical contributions, i.e. the computation of the intrinsic weights, the contextualization and the selection of the best mappings. The relationship of our approach to the related work is discussed in Section 8 and Section 9 describes our experimental evaluation and discusses our findings. Finally some conclusions are presented in Section 10.",Sonia Bergamaschi,"University of Modena and Reggio Emilia, Italy",sonia.bergamaschi@unimore.it,Elton Domnori,"University of Modena and Reggio Emilia, Italy",elton.domnori@unimore.it,Francesco Guerra,"University of Modena and Reggio Emilia, Italy",francesco.guerra@unimore.it,Raquel Trillo Lado,"University of Zaragoza, Spain",raqueltl@unizar.es,Yannis Velegrakis,"University of Trento, Italy",velgias@disi.unitn.eu,,,,,,,,,,,,,,,
