date,submitter_orcid,submitter_name,submitter_institution,submitter_email,submitter_title_doi,submitter_title,submitter_intro,submitter_author1_name,submitter_author1_institution,submitter_author1_email,submitter_author2_name,submitter_author2_institution,submitter_author2_email,submitter_author3_name,submitter_author3_institution,submitter_author3_email,submitter_author4_name,submitter_author4_institution,submitter_author4_email,submitter_author5_name,submitter_author5_institution,submitter_author5_email,submitter_author6_name,submitter_author6_institution,submitter_author6_email,submitter_author7_name,submitter_author7_institution,submitter_author7_email,submitter_author8_name,submitter_author8_institution,submitter_author8_email,submitter_author9_name,submitter_author9_institution,submitter_author9_email,submitter_author10_name,submitter_author10_institution,submitter_author10_email
20200101,326,Lu Qin,"The Chinese University of Hong Kong, Hong Kong, China",lqin@se.cuhk.edu.hk,,Diversifying Top-K Results,"ABSTRACT This paper proposes a general framework for matching similar subsequences in both time series and string databases. The matching results are pairs of query subsequences and database subsequences. The framework finds all possible pairs of similar subsequences if the distance measure satis- fies the ""consistency"" property, which is a property intro- duced in this paper. We show that most popular distance functions, such as the Euclidean distance, DTW, ERP, the Freche?t distance for time series, and the Hamming distance and Levenshtein distance for strings, are all ""consistent"". We also propose a generic index structure for metric spaces named ""reference net"". The reference net occupies O(n) space, where n is the size of the dataset and is optimized to work well with our framework. The experiments demon- strate the ability of our method to improve retrieval perfor- mance when combined with diverse distance measures. The experiments also illustrate that the reference net scales well in terms of space overhead and query time. 1. INTRODUCTION Sequence databases are used in many real-world applica- tions to store diverse types of information, such as DNA and protein data, wireless sensor observations, music and video streams, and financial data. Similarity-based search in such databases is an important functionality, that allows identi- fying, within large amounts of data, the few sequences that contain useful information for a specific task at hand. For example, identifying the most similar database matches for a query sequence can be useful for classification, forecasting, or retrieval of similar past events. The most straightforward way to compare the similarity between two sequences is to use a global similarity mea- sure, that computes an alignment matching the entire first sequence to the entire second sequence. However, in many scenarios it is desirable to perform subsequence matching, where, given two sequences Q and X, we want to identify pairs of subsequences SQ of Q and SX of X, such that the similarity between SQ and SX is high. When a large database of sequences is available, it is important to be able to identify, given a query Q, an optimally matching pair SQ and SX, where SX can be a subsequence of any database sequence. A well-known example of the need for subsequence match- ing is in comparisons of biological sequences. It is quite possible that two DNA sequences Q and X have a large Levenshtein distance [22] (also known as edit distance) be- tween them (e.g., a distance equal to 90% of the length of the sequences), while nonetheless containing subsequences SQ and SX that match at a very high level of statistical significance. Identifying these optimally matching subse- quences [34] helps biologists reason about the evolutionary relationship between Q and X, and possible similarities of functionality between those two pieces of genetic code. Similarly, subsequence matching can be useful in searching music databases, video databases, or databases of events and activities represented as time series. In all the above cases, while the entire query sequence may not have a good match in the database, there can be highly informative and statistically significant matches between subsequences of the query and subsequences of database sequences. Several methods have been proposed for efficient subse- quence matching in large sequence databases. However, all the proposed techniques are targeted to specific distance or similarity functions, and it is not clear how and when these techniques can be generalized and applied to other dis- tances. Especially, subsequence retrieval methods for string databases are difficult to be used for time-series databases. Furthermore, when a new distance function is proposed, we need to develop new techniques for efficient subsequence matching. In this paper we present a general framework, which can be applied to any arbitrary distance metric, as long as the metric satisfies a specific property that we call ""consistency"". Furthermore, we show that many well-known existing distance functions satisfy consistency. Thus, our framework can deal with both sequence types, i.e., strings and time series, including cases where each element of the sequence is a complex object. The framework in this paper consists of a number of steps: dataset segmentation, query segmentation, range query, can- didate generation, and subsequence retrieval. Brute-force search would require evaluating a total of O (|Q|2 |X|2) pairs of subsequences of Q and X. However our filtering method produces a shortlist of candidates after considering O (|Q| |X|) pairs of segments only. For the case where the distance is a metric, we also present a hierarchical reference 1579 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Articles from this volume were invited to present their results at The 38th International Conference on Very Large Data Bases, August 27th - 31st 2012, Istanbul, Turkey. Proceedings of the VLDB Endowment, Vol. 5, No. 11 Copyright 2012 VLDB Endowment 2150-8097/12/07... $ 10.00. net, a novel generic index structure that can be used within our framework to provide efficient query processing. Overall, this paper makes the following contributions: ? We propose a framework that, compared to alterna- tive methods, makes minimal assumptions about the underlying distance, and thus can be applied to a large variety of distance functions. ? We introduce the notion of âconsistencyâ as an im- portant property for distance measures applied to se- quences. ? We propose an efficient filtering method, which pro- duces a shortlist of candidates by matching only O(|Q| |X|) pairs of subsequences, whereas brute force would match O (|Q|2 |X|2) pairs of subsequences. ? We make this filtering method even faster, by using a generic indexing structure with linear space based on reference nets, that efficiently supports range similar- ity queries. ? Experiments demonstrate the ability of our method to provide good performance when combined with diverse metrics such as the Levenshtein distance for strings, and ERP [8] and the discrete Freche?t distance [11]) for time series. 2. RELATEDWORK Typically, the term ""sequences"" can refer to two different data types: strings and time-series. There has been a lot of work in subsequence retrieval for both time series and string databases. However, in almost all cases, existing methods concentrate on a specific distance function or specific type of queries. Here we review some of the recent works on subse- quence matching. Notice that this review is not exhaustive since the topic has received a lot of attention and a complete survey is beyond the scope of this paper. Time-series databases and efficient similarity retrieval have received a lot of attention in the last two decades. The first method for subsequence similarity retrieval under the Eu- clidean (L2?norm) distance appeared in the seminal paper of Faloutsos et al. [12]. The main idea is to use a sliding win- dow to create smaller sequences of fixed length and then use a dimensionality reduction technique to map each window to a small number of features that are indexed using a spa- tial index (e.g., R?-tree). Improvements of this technique have appeared in [28, 27] that improve both the window- based index construction and the query time using sliding windows on the query and not on the database. However, all these techniques are ap",Lu Qin,"The Chinese University of Hong Kong, Hong Kong, China",lqin@se.cuhk.edu.hk,Jeffrey Xu Yu,"The Chinese University of Hong Kong, Hong Kong, China",yu@se.cuhk.edu.hk,Lijun Chang,"The Chinese University of Hong Kong, Hong Kong, China",ljchang@se.cuhk.edu.hk,,,,,,,,,,,,,,,,,,,,,
20200102,897,Haohan Zhu,Department of Computer Science Boston University,zhu@cs.bu.edu,,A Generic Framework for Efficient and Effective Subsequence Retrieval,"ABSTRACT Top-k query processing finds a list of k results that have largest scores w.r.t the user given query, with the assumption that all the k results are independent to each other. In practice, some of the top-k results returned can be very similar to each other. As a re- sult some of the top-k results returned are redundant. In the lit- erature, diversified top-k search has been studied to return k re- sults that take both score and diversity into consideration. Most existing solutions on diversified top-k search assume that scores of all the search results are given, and some works solve the diver- sity problem on a specific problem and can hardly be extended to general cases. In this paper, we study the diversified top-k search problem. We define a general diversified top-k search problem that only considers the similarity of the search results themselves. We propose a framework, such that most existing solutions for top- k query processing can be extended easily to handle diversified top-k search, by simply applying three new functions, a sufficient stop condition sufficient(), a necessary stop condition necessary(), and an algorithm for diversified top-k search on the current set of generated results, div-search-current(). We propose three new algorithms, namely, div-astar, div-dp, and div-cut to solve the div-search-current() problem. div-astar is an A? based algorithm, div-dp is an algorithm that decomposes the results into components which are searched using div-astar independently and combined using dynamic programming. div-cut further decomposes the cur- rent set of generated results using cut points and combines the re- sults using sophisticated operations. We conducted extensive per- formance studies using two real datasets, enwiki and reuters. Our div-cut algorithm finds the optimal solution for diversified top-k search problem in seconds even for k as large as 2, 000. 1. INTRODUCTION Top-k queries are one of the most fundamental queries used in the IR and database areas. Given a user query, the top-k results of the query are a list of k results that have largest scores/relevances with respect to the user query, under the assumption that all of the k results are independent to each other. In some situations, for a cer- tain top-k query, some of the results returned can be very similar to each other. For example, if we search  íì§¸apple íì§¹ in Google image1, 7 out of the top-10 results returned are the logo of the Apple com- pany. In order to remove the redundancy in the results, and at the same time keep the quality of the top-k results, diversity should be considered in the top-k search problems. For top-k search algorithms. In the literature, most of them aim at finding an early stop condition, such that they can find the top- k results without exploring all the possible search results. Based on this, two frameworks are generally used, namely, the incremen- tal top-k framework and the bounding top-k framework. The in- cremental top-k framework outputs the results one by one in non- increasing order of their scores, and stops as soon as k results are generated. It aims to find a polynomial delay algorithm such that given the existing generated results, the next result with largest score can be generated in polynomial time w.r.t. the size of the input only [16, 15, 20, 14]. In the bounding top-k framework, re- sults are not necessarily generated in non-increasing order of their scores. It maintains a score upper bound for the unseen results ev- ery time when a new result is generated. The algorithm stops when the current k-th largest score is no smaller than the upper bound for the unseen results. The threshold algorithm based approaches [7, 9] fall in this framework and other approaches include [12, 17]. Diversity aware search has been studied in recent years. Most of the existing solutions that support diversity on top-k search results assume the ranking of all the search results are given in advance. Based on which, a diversity search algorithm is given to output k results based on a scoring function that takes both query relevance and diversity into consideration [6, 1, 11, 5, 2]. Other works give algorithms that solve the diversity problem for a special area, i.e., graph search [18], document search [22], etc. and can hardly be extended to support general top-k diversity search. In this paper, we propose a general framework to handle the di- versified top-k search problem. We keep the advantages for the existing top-k search algorithms, that can stop early without ex- ploring all search results, and at the same time, we take diversity into consideration. We show that any top-k search algorithm that can be used in the incremental top-k framework or the bounding top-k framework can be easily extended to handle diversified top- k search, by adding three new functions studied in this paper: a sufficient stop condition sufficient(), a necessary stop condition necessary(), and a diversity search function div-search-current(). All of them are application independent. The only assumption in our framework is that, given any two search results vi and vj , whether vi and vj are similar to each other can be decided, e.g., us- ing a similarity function sim(vi, vj) > íì§íì¨ for a user given threshold íì§íì¨ . We output a list of k results with maximum total scores such that 1 http://www.google.com/imghp 1124 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Articles from this volume were invited to present their results at The 38th International Conference on Very Large Data Bases, August 27th - 31st 2012, Istanbul, Turkey. Proceedings of the VLDB Endowment, Vol. 5, No. 11 Copyright 2012 VLDB Endowment 2150-8097/12/07... $ 10.00. no two of them are similar to each other. We make the following contributions in this paper. (1) We formalize the diversified top-k search problem. Based on our definition, the optimal solution only depend on the similarity of search results themselves, and no other information is needed. (2) We study two categories of algorithms generally used in finding top-k results with early stop in the literature, namely, the incre- mental top-k framework and the bounding top-k framework. We show both frameworks can be extended to diversified top-k search by simply adding three application independent functions studied in this paper, namely, a sufficient stop condition sufficient(), a nec- essary stop condition necessary(), and a diversity search function div-search-current(). The sufficient stop condition helps to early stop and the necessary stop condition helps to reduce the number of div-search-current() processes, since div-search-current() is usu- ally a costly operation. (3) We show that div-search-current() is an NP-Hard problem and is hard to be approximated. We propose three new algorithms, namely, div-astar, div-dp, and div-cut, to find the optimal solution for div-search-current(). div-astar is an A? based algorithm and is slow to handle a large number of results. div-dp decomposes the results into disconnected components in order to reduce the graph size to be searched using div-astar. Results in div-dp are com- bined using dynamic programming. div-cut further decomposes each component into several subgraphs to form a cptree, based on the cut points of each component. A tree based search is applied on cptree to find the optimal solution. (4) We conducted extensive performance studies using two real datasets, to test the performance of the three algorithms. Our div-cut approach can find the diversified top-k results within seconds when k is as large as 2, 000. The rest of this paper is organized as follows. Section 2 formally defines the diversified top-k search problem. Section 3 shows the two existing frameworks on general top-k search problems. Sec- tion 4 shows how to extend the two categories of top-k search ap- proaches to solve diversified top-k search, by defining a sufficient stop condition sufficient(), a necessary stop condition necessary(), and a diversified top-k search algorithm div-search-current() to search on the current result set. Section 5, 6, and 7 give three al- gorithms to solve the div-search-current() problem. We show our experimental results in Section 8, and introduce the related work in Section 9. Finally, we conclude our paper in Section 10. 2. PROBLEM DEFINITION We consider a list of results S = {v1, v2,  íì§  íì§  íì§ }. For each vi  íì¨ S, the score of vi is denoted as score(vi). For any two results vi  íì¨ S and vj  íì¨ S, there is a user defined similarity function sim(vi, vj) denoting the similarity between the two results vi and vj . Without loss of generality, we assume 0  íí sim(vi, vj)  íí 1 for any two results vi  íì¨ S and vj  íì¨ S, and sim(v, v) = 1 for any v  íì¨ S. Given an integer k where 1  íí k  íí |S|, the top-k results of S is a list of k results Sk that satisfy the following two conditions. 1) Sk  íì¨ S and |Sk| = k. 2) For any vi  íì¨ Sk and vj  íì¨ S ? Sk, score(vi)  íí score(vj). Here, S ? Sk is the set of results that are in S but not in Sk, i.e., S ? Sk = {v|v  íì¨ S, v / íì¨ Sk}. Given two results vi  íì¨ S and vj  íì¨ S, vi is similar to vj iff sim(vi, vj) > íì§íì¨ where íì§íì¨ is a user defined threshold, and 0 < íì§íì¨  íí 1. We use vi ? vj to define that vi is similar to vj . Definition 1 (Diversified Top-k Results) Given a list of search results S = {v1, v2,  íì§  íì§  íì§ }, and an integer k where 1  íí k  íí |S|, the v1 v3 v5 v2 v4 v6 10 6 8 7 7 1 v1 v3 v5 v2 v4 v6 10 6 8 7 7 1 K=3K=2 Figure 1: A sample diversity graph diversified top-k results of S, denoted as D(S), is a list of results that satisfy the following three conditions. 1) D(S)  íì¨ R and |D(S)|  íí k. 2) For any two results vi  íì¨ R and vj  íì¨ R and vi 6= vj , if vi ? vj , then {vi, vj} * D(S). 3) íì§2 v íì¨D(S) score(v) is maximized. Intuitively, D(S) is the set of at most k results, such that no two results are similar with each other, and the total score of the results is maximized. We use score(D(S)) to denote the total score of results in D(S), i.e., score(D(S)) = íì§2 v íì¨D(S) score(v). In this paper, we are to find the diversified top-k results. Our aim is to find a general approach, such that for any existing algo- rithm that returns the top-k results of a certain problem, it can be easily changed to return the diversified top-k results by applying our framework, in which the result set S is not necessarily to be computed in advanced but grows incrementally with an early stop condition. We first give the definition of the diversity graph. Definition 2 (Diversity Graph) Given a list of results S = {v1, v2,  íì§  íì§  íì§ }, the diversity graph of S, denoted as G(S) = (V,E), is an undirected graph such that for any result v  íì¨ R, there is a corresponding node v  íì¨ V , and for any two results vi  íì¨ S and vj  íì¨ R, there is an edge (vi, vj)  íì¨ E iff vi ? vj . We use V (G(S)) and E(G(S)) to denote the set of nodes and the set of edges in the diversity graph G(S) respectively, and use v.adj(G(S)) to denote the set of nodes that are adjacent to v in G(S). If the context is obvious, we use vi to denote both the result vi in S and the node vi in G(S), we use G to denote G(S), and we use D to denote D(S). Without loss of generality, we assume nodes in G(S) are arranged in non-increasing order of their scores, i.e., for any 1  íí i < j  íí |V (G(S))|, score(vi)  íí score(vj). The diversified top-k results D(S) can be equivalently defined as a subset of nodes in G(S), that satisfy the three conditions. 1) |D(S)|  íí k. 2) D(S) is an independent set of G(S). 3) score(D(S)) is maximized. Here, an independent set of a graph is a set of nodes in a graph, where no two nodes are adjacent. Example 1 Fig. 1 shows the diversity graph for 6 results S = {v1, v2,  íì§  íì§  íì§ , v6}. Suppose k = 2, the optimal solution D(S) includes two points v1 and v2 with score 18, as shown on the left part of Fig. 1. Suppose k = 3, the optimal solution D(S) includes three points v3, v4 and v5 with score 20, as shown on the right part of Fig. 1. In the following, we first show the two existing frameworks to solve top-k search problems, namely, the incremental top-k frame- work and the bounding top-k framework, which are most generally used in top-k search algorithms. Then we show the framework of 1125 Algorithm 1 incremental(k) 1: S  íì§  ?; 2: for i=1 to k do 3: v  íì§  incremental-next(); 4: if v = ? then 5: break; 6: S  íì§  S ? {v}; 7: return S; Algorithm 2 bounding(k) 1: S  íì§  ?; 2: unseen íì§  + íí; 3: while the k-th largest score of S < unseen do 4: v  íì§  bounding-next(); 5: if v = ? then 6: break; 7: S  íì§  S ? {v}; 8: update unseen; 9: return top-k results in S; our approach to extend the two frameworks to handle diversified top-k search. 3. TOP-K SEARCH FRAMEWORKS In the literature, the framework of most algorithms that find top- k results falls into two categories, namely, the incremental top-k framework and the bounding top-k framework. Incremental Top-k: In the incremental top-k framework, results are generated one by one by calling a procedure incremental-next(), with non-increasing order of their scores. The algorithm stops after k results are generated, and the k results are the final top-k results for the problem. The framework named incremental is shown in Algorithm 1. A lot of existing work fall into this category, e.g., finding top-k shortest paths in graphs, finding top-k steiner trees, communities and r-cliques in graphs, etc [16, 15, 20, 14]. A lot of works have been done to assume that the time complexity of each incremental-next() procedure to generate the next result with largest score is polynomial w.r.t. the size of the input only. Bounding Top-k: In the bounding top-k framework, results are generated one by one by calling a procedure bounding-next(), but not necessarily with non-increasing order of their scores. A bound unseen is defined to be the upper bound of the scores for the un- seen results. After each result is generated by bounding-next(), unseen is also updated to be a possibly smaller value. The algo- rithm stops when the k-th largest score of all generated results is no smaller than the upper bound for the unseen results unseen. The framework named bounding is shown in Algorithm 2. The thresh- old algorithm that is generally used to return top-k results falls into this category [7, 9]. Other works that fall into this category include [12, 17]. 4. DIVERSIFIED TOP-K SEARCH In this section, we show how to extend the incremental top-k framework incremental and bounding top-k framework bounding to handle diversified top-k search. We mainly focus on two tasks. First, a new early stop conditions is needed. Second, an algorithm that finds the diversified top-k results for the current generated re- sult set is needed. For the early stop condition, in the original al- gorithm, the stop condition for incremental is simply |S| = k and the stop condition for bounding is the current k-th largest score  íí unseen. Obviously, both of them cannot be applied to handle Algorithm 3 div-search(k) 1: S  íì§  ?; D(S) íì§  ?; 2: while sufficient() do 3: the code to update S (and unseen); 4: if necessary() then 5: D(S) íì§  div-search-current(G(S), k); 6: return D(S); diversified top-k search. Consider an extreme case, when the al- gorithm stops using the original stop condition, it is possible that all the results generated are similar to each other. Thus the current diversified top-k results only contain 1 result with the largest score. It is not the optimal solution because it is possible that an unseen result is not similar to the current one. Here, D(S) computed for the current generated result set S can be used to check the new stop condition, and if the new stop condition is satisfied, D(S) is the optimal solution for the diversified top-k search. We extend both incremental and bounding using the same frame- work, which is shown in Algorithm 3, by adding three new func- tions, a new sufficient stop condition sufficient(), a new necessary stop condition necessary() and an algorithm div-search-current() to search the diversified top-k results on the current generated re- sult set. The algorithm executes the code of the original top-k al- gorithm to update S and stops when sufficient() is satisfied. For incremental, the code is line 3-6 in algorithm 1, and for bounding, the code is line 4-8 in algorithm 2. After updating S, we construct the diversity graph G(S) on S based on the similarity function sim() for any given two results. If the necessary stop condition is satisfied, we find the diversified top-k results for the current result set S using div-search-current(). The necessary stop condition is used to reduce the number of calling div-search-current(), because div-search-current() is a costly work. In the following, we will in- troduce the sufficient stop condition, the necessary stop condition, and the search algorithm for current set. Sufficient Stop Condition: Given the current result set S, we need to calculate an upper bound best(S) for the possible optimal solu- tions considering both the current result set S and the unseen re- sults. Let Di(S) be the best diversified results of S with exactly i elements for 1  íí i  íí k, i.e., Di(S) is a subset of nodes in V (G(S)), that satisfies the following three conditions. 1) |Di(S)| = k. 2) Di(S) is an independent set of G(S). 3) score(Di(S)) is maximized. Lemma 1 Given Di(S) for 1  íí i  íí k and the score upper bound of all the unseen results u. The upper bound best(S) can be calcu- lated as follows. best(S) = max 1 ííi íík {score(Di(S)) + (k ? i) íì© u} (1) where u is the score of the last generated result v, score(v), for incremental and is the upper bound of the unseen results, unseen, for bounding. Proof Sketch: Suppose the final optimal solution is O, then we can divide O into two parts, O = O1 ? O2, where O1 is the set of generated results, and O2 is the set of unseen results. Suppose O1 has n1 elements and O2 has n2 elements. We have n1 + n2  íí k. Since O1 is the set of generated results, we have (1) score(O1)  íí score(Dn1(S)), since Dn1(S) is the optimal solution with n1 el- ements. We also have (2) score(O2)  íí n2  íì© u  íí (k ? n1)  íì© u, 1126 since (u) is the score upper bound for all unseen results. Com- bine (1) and (2), we have score(O) = score(O1) + score(O2)  íí score(Dn1(S)) + (k ? n1)  íì© u  íí max1 ííi íík{score(Di(S)) + (k? i) íì©u} = best(S). best(S) is an upper bound for the optimal solution. ? Having the score upper bound best(S) for the optimal solution, the sufficient stop condition for div-search can be defined as fol- lows. score(D(S))  íí best(S) (2) The following lemma shows that, after every iteration, div-search moves towards the sufficient stop condition. Lemma 2 For any S íí  íì¨ S, best(S íí)  íí best(S) and best(S íí)  íí best(S). Proof Sketch: Since S íí  íì¨ S, the best solution on S íí is a feasible solution on S, thus best(S íí)  íí best(S). Comparing to best(S íí), best(S) is calculated by changing some upper bounds u íí when cal- culating best(S íí) into the real scores no larger than u íí and chang- ing the other unseen upper bounds from u íí to u, where u  íí u íí is assumed by the original algorithm. Thus best(S íí)  íí best(S). ? Necessary Stop Condition: We discuss the necessary stop con- dition for div-search. The necessary stop condition is used as fol- lows. In each iteration, before invoking div-search-current(), if the necessary stop condition is not satisfied, then div-search-current() is not necessarily to be invoked in this iteration. Lemma 3 For div-search, if it can stop in a certain iteration, one of the following conditions should be satisfied before invoking the procedure div-search-current(): 1) The last generated result v = ?. 2) |S|  íí |S íí|+ k ?max{i|1  íí i  íí k,Di(S  íí) 6= ?} and the k-th largest score in S  íí u. Here S íí is the set of results when the last div-search-current() is invoked or ? if div-search-current() is never invoked. Proof Sketch: The first condition is trivial. Now suppose v 6= ?. For the second condition, when the k-th largest score in S < u, it is possible that a new result can be added that updates the k-th largest score, and thus improves the current best solution. Now we discuss |S|  íí |S íí| + k ? max{i|1  íí i  íí k,Di(S  íí) 6= ?}. max{i|1  íí i  íí k,Di(S  íí) 6= ?} is the size of the maximum independent set for G(S íí) if it is smaller than k, and k?max{i|1  íí i  íí k,Di(S  íí) 6= ?} is the minimum number of nodes needed to be added in order to generate a result of size k. If such a result does not exist, we cannot stop because we can always add some unseen nodes to any existing solution with a size smaller than k to make the score larger. As a result, we should add at least k ?max{i|1  íí i  íí k,Di(S  íí) 6= ?} nodes into S íí. ? Searching Current Set: The most important operation in our frame- work is the the algorithm div-search-current() to search the diver- sified top-k results for the current result set S. We first show the difficulties of the problems in this section and give three algorithms, namely div-astar, div-dp, and div-cut on div-search-current() in the next three sections respectively. The following lemma shows that finding the diversified top-k results is an NP-Hard problem. Lemma 4 Finding D(S) on G(S) is an NP-Hard problem. Proof Sketch: We consider a special case of the problem, where score(v) = 1 for all v  íì¨ V (G(S)), and k = |V (G(S))|. In such a case, finding Dk(R) on G(S) is equivalent to finding the v1 v2 v3 v0  íì§  íì§  íì§ u1 u2 u3  íì§  íì§  íì§ u100 v100 99 99 99 1 1 100 99 0.5 1 (a) The Greedy Solution v1 v2 v3 v0  íì§  íì§  íì§ u1 u2 u3  íì§  íì§  íì§ u100 v100 99 99 99 1 1 100 99 0.5 1 (b) The Optimal Solution Figure 2: The greedy algorithm div?astar div?dp div?cut NP NP NP NP NP NP NP NP NP NP NP NP NP NP NP NP NP NP NP NP NP NP NP NPNP Figure 3: Overview of three algorithms maximum independent set on graph G(S), which is an NP-Hard problem. Thus, the original problem is an NP-Hard problem. Greedy is Not Good: Given G(S) and k, a simple greedy algo- rithm to find D(S) works as follows. It processes in iterations. In each iteration, the node v with the maximum score is selected and put into D(S). After that, all the nodes that are adjacent to v in G(S) is removed from G(S). The process stops when G(S) is empty or D(S) contains k results. The quality of the greedy algorithm can be arbitrarily bad. The approximation ratio for the greedy algorithm is not bounded by a constant factor. Even for its special case, the maximum indepen- dent set problem is known to be hard to approximate in the litera- ture. We give an example. Fig. 2 shows a diversity graph with 201 nodes and 200 edges. Suppose k = 100. Using the greedy algo- rithm, the solution is shown in Fig. 2(a), where the selected results are marked gray. The score of the greedy solution is 199. The op- timal solution for the problem is shown in Fig. 2(b). The score of the optimal solution is 9, 900, which is nearly 50 times of the score of the greedy solution. In the following, we propose to find the optimal solution of D(S). We propose three algorithms, namely, div-astar, div-dp, and div-cut. div-astar searches the whole space S using the A? based heuris- tics by designing an upper bound function astar-bound(). Based on the NP-Hardness of the problem, div-astar can hardly handle problems with large diversity graph G. In our second div-dp al- gorithm, we decompose G into connected components. The size of each component can be much smaller than the original graph G, and is searched independently using div-astar. We combine the components using an efficient operation ? based on dynamic pro- gramming. In our third div-cut algorithm, we further decompose each connected component into subgraphs, where subgraphs are connected through a set of cut points. Each subgraph is searched independently for at most 4 times under different conditions. We combine the components using two efficient operations ? and ?. The general ideas of the three algorithms are illustrated in Fig. 3. 5. AN A? BASED APPROACH As discussed in Section 4, div-search-current(G(S), k) should return the optimal solution Di(S) for 1  íí i  íí k in order to find the early stop condition. For simplicity, we use D to denote the set of solutions, and we use D.solutioni to denote the optimal solution 1127 Algorithm 4 div-astar(G, k) Input: The diversity graph G, the top-k value. Output: Search result D. 1: H  íì§  ?; D  íì§  ?; 2: H.push((?, 0, 0, 0)); 3: for k íí = k down to 1 do 4: astar-search(G,H, D, k íí); 5: for all e  íì¨ H do 6: e.bound íì§  astar-bound(G, e, k íí); 7: update e inH; 8: return D; 9: procedure astar-search(G,H, D, k íí) 10: whileH 6= ? andH.top.bound > maxi íík íí{D.scorei} do 11: e íì§  H.pop(); 12: for i = e.pos+ 1 to |V (G)| do 13: if vi.adj(G) ? e.solution = ? then 14: e íí  íì§  (e.solution ? {vi}, i, e.score+ score(vi), 0); 15: e íí.bound íì§  astar-bound(G, e íí, k íí); 16: H.push(e íí); 17: update D using e íí.solution; 18: procedure astar-bound(G, e, k íí) 19: p íì§  |e.solution|; i íì§  e.pos+ 1; 20: bound íì§  e.score; 21: while p < k íí and i < |V (G)| do 22: if vi.adj(G) ? e.solution = ? then 23: bound íì§  bound+ score(vi); 24: p íì§  p+ 1; 25: i íì§  i+ 1; 26: return bound; with i results Di(S), and use D.scorei to denote the score for the optimal solution score(Di(S)). Our first algorithm is an A? based algorithm. The algorithm is shown in Algorithm 4. We define a max heap H to store the entries in the A? search. Each entry e  íì¨ H is with the form e = (solution, pos, score, bound). Each entry e is ranked in H according to e.bound, which is the estimated upper bound of the solution if we further expand it in the A? search. e.solution is the partial solution searched and e.pos is the position of the last searched node in e.solution. e.score is the score of the partial solu- tion, i.e., e.score = score(e.solution). The algorithm should return D.solutioni for all 1  íí i  íí k. Suppose we have an A ? algorithm that finds the optimal solution for a certain D.solutioni, the algo- rithm should be invoked k times to find the k solutions, which is costly. We show that after searching D.solutioni for a certain i, the partial solutions in H can be reused when searching D.solutionj for j < i. In the following, we first discuss the estimated upper bound for partial solutions. Then we discuss the A? algorithm to find the optimal solution D.solutioni for a certain i. At last, we discuss how the partial solutions in H can be reused to find the optimal solutions D.solutioni for all 1  íí i  íí k. Upper Bound Estimation: Given a partial solution e, for a cer- tain k íí, we show how to estimate the score upper bound if we ex- pand the partial solution to be a solution of at most k íí elements. The algorithm astar-bound is shown in Algorithm 4, line 18-26. The newly added nodes should at least satisfy the following two conditions: 1) they can not be one of e.solution, and 2) they are not adjacent to any node in e.solution. Under such conditions, we can just add the set of nodes with largest scores, and after adding the nodes, the total number of nodes is no larger than k íí. In or- der to satisfy condition 1), we visit nodes in G from the posi- tion e.pos + 1 (line 19). Since nodes in G are sorted in the non- increasing order of their scores, we add nodes one by one until the size p reaches k íí. For each node added, condition 2) can be checked using vi.adj(G) ? e.solution = ? (line 22). Lemma 5 astar-bound(G, e, k íí) finds the score upper bound for the partial solution e.solution to be expanded to a solution of at most k íí elements. Proof Sketch: Suppose we have removed all the nodes from G that are adjacent to at least one node in e.solution, then the func- tion astar-bound(G, e, k íí) calculates the upper bound by expand- ing e.solution using the set of nodes after position e.pos in G with largest scores. The optimal solution that e.solution can be ex- panded also selects the expanded nodes from the set of nodes after position e.pos but it may not select all with the largest scores since some of them may be adjacent to each other. Thus the optimal so- lution can not be larger than astar-bound(G, e, k íí). As a result, astar-bound(G, e, k íí) is a score upper bound for all expansions of e.solution. ? A? Search for a Certain k: To find the optimal solution for a certain k = k íí, the A? search algorithm astar-search is shown in Algorithm 4, line 9-17. It runs in iterations. In each iteration, the partial solution e with the largest estimated upper bound is popped out from H (line 11). e can then be expanded to new partial solu- tions by adding a new node into e.solution. The nodes are added from position e.pos + 1 in G since all nodes before the position has been processed (line 12). The newly added node vi should not be adjacent to one of e.solution(line 13), and after adding the new node, the upper bound of the new partial solution should be updated using astar-bound(), and the new partial solution should be pushed into H for further expansion (line 14-16). In line 17, suppose the",Haohan Zhu,Department of Computer Science Boston University,zhu@cs.bu.edu,George Kollios,Department of Computer Science Boston University,gkollios@cs.bu.edu,Vassilis Athitsos,Computer Science and Engineering Department University of Texas at Arlington,athitsos@uta.edu,,,,,,,,,,,,,,,,,,,,,
20200103,1387,Stefan Aulbach,"Technische Universit?t M?nchen, Germany",stefan.aulbach@in.tum.de,,A Comparison of Flexible Schemas for Software as a Service,"ABSTRACT A multi-tenant database system for Software as a Service (SaaS) should offer schemas that are flexible in that they can be extended for different versions of the application and dynamically modified while the system is on-line. This pa- per presents an experimental comparison of five techniques for implementing flexible schemas for SaaS. In three of these techniques, the databaseì°½íµownsì°½í¶the schema in that its struc- ture is explicitly defined in DDL. Included here is the com- monly-used mapping where each tenant is given their own private tables, which we take as the baseline, and a map- ping that employs Sparse Columns in Microsoft SQL Server. These techniques perform well, however they offer only lim- ited support for schema evolution in the presence of existing data. Moreover they do not scale beyond a certain level. In the other two techniques, the application ì°½íµownsì°½í¶ the schema in that it is mapped into generic structures in the database. Included here are XML in DB2 and Pivot Tables in HBase. These techniques give the application complete control over schema evolution, however they can produce a significant decrease in performance. We conclude that the ideal data- base for SaaS has not yet been developed and offer some suggestions as to how it should be designed. Categories and Subject Descriptors H.4 [Information Systems Applications]: Miscellaneous; H.2.1 [Information Systems]: Database Managementì°½í¬ Logical Design General Terms Design, Performance Keywords Multi-Tenancy, Software as a Service, Flexible Schemas, Ex- tensibility, Evolution Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGMODì°½í²09, June 29?July 2, 2009, Providence, Rhode Island, USA. Copyright 2009 ACM 978-1-60558-551-2/09/06 ...$5.00. 1. INTRODUCTION In the Software as a Service (SaaS) model, a service pro- vider owns and operates an application that is accessed by many businesses over the Internet. A key benefit of this model is that, by careful engineering, it is possible to lever- age economy of scale to reduce total cost of ownership rel- ative to on-premises solutions. Common practice in this regard is to consolidate multiple businesses into the same database to reduce operational expenditures, since there are fewer processes to manage, as well as capital expenditures, since resource utilization is increased. A multi-tenant database system for SaaS should offer sche- mas that are flexible in two respects. First, it should be possible to extend the base schema to support multiple spe- cialized versions of the application, e.g., for particular ver- tical industries or geographic regions. An extension may be private to an individual tenant or shared by multiple ten- ants. Second, it should be possible to dynamically evolve the base schema and its extensions while the database is on-line. Evolution of a tenant-owned extension should be totally ì°½íµself-serviceì°½í¶: the service provider should not be in- volved; otherwise operational costs will be too high. This paper presents an experimental comparison of five techniques for implementing flexible schemas for SaaS. In three of these techniques, the database ì°½íµownsì°½í¶ the schema in that its structure is explicitly defined in DDL: Private Tables: Each tenant is given their own private in- stance of the base tables that are extended as required. In contrast, in all of the other mappings, tenants share tables. We take Private Tables as the experimental baseline. Extension Tables: The extensions are vertically partitio- ned into separate tables that are joined to the base tables along a row ID column. Sparse Columns: Every extension field of every tenant is added to its associated base table as a Sparse Column. Our experiments here use Microsoft SQL Server 2008 [1]. To implement Sparse Columns efficiently, SQL Server uses a variant of the Interpreted Storage Format [4, 7], where a value is stored in the row together with an identifier for its column. Our experimental results show that these techniques per- form well, however they offer only limited support for schema evolution. DDL commands over existing data, if they are supported at all, consume considerable resources and neg- atively impact performance. In the on-line setting, the ap- 881 plication must be given control over when and how bulk data transformations occur. An additional issue is that these techniques do not scale beyond a certain level. In the other two techniques, the application ì°½íµownsì°½í¶ the schema in that it is mapped into generic structures in the database: XML: Each base table is augmented by a column that stores all extension fields for a tenant in a flat XML document. Since these documents necessarily vary by tenant, they are untyped. Our experiments here use pureXML in IBM DB2 [20]. Pivot Tables: Each value is stored along with an identifier for its column in a tall narrow table [2]. Our exper- iments here use HBase [11], which is an open source version of Google BigTable [6]. BigTable and HBase were originally designed to support the exploration of massive web data sets, but they are increasingly be- ing used to support enterprise applications [14]. The Pivot Table mapping into HBase that we employ is consistent with best practices. These two techniques give the application complete con- trol over schema evolution, however our experimental results show that they can produce a significant decrease in perfor- mance from the baseline. For XML, the decrease is greatest for reads, which require parsing the untyped documents and reassembling typed rows. The decrease is proportional to the number of extension fields. For Pivot Tables, the de- crease is more than an order of magnitude in some cases. Note that these results should not be taken as a negative statement about the quality of these systems, since they have not been optimized for our use case. Moreover, HBase is an early-stage open source project, not a mature com- mercial product. Our results are intended to give a general indication of the trade-offs in implementing flexible schemas. Several major SaaS vendors have developed mapping tech- niques in which the application owns the schema. This ap- proach has been elevated to a design principle whereby the application derives essential capabilities by managing the metadata itself [19, 23]. To achieve acceptable performance, these applications re-implement significant portions of the database, including indexing and query optimization, from the outside. We believe that databases should be enhanced to directly support the required capabilities. Our experiments are based on a multi-tenant database testbed that simulates a simple but realistic Customer Re- lationship Management (CRM) service. The workload con- tains single- and multi-row create, read, and update oper- ations as well as basic reporting tasks. The schema can be extended for individual tenants and it can evolve over time. Our previous work with a more limited version of this testbed (no extensions) showed that the performance of Pri- vate Tables degrades if there are too many tables [3]. This effect is due to the large amount of memory needed to hold the metadata as well as an inability to keep index pages in the buffer pool. In this paper, we create only a moderate number of tables, take Private Tables as the baseline, and use it to compare the other mappings. This paper is organized as follows. Section 2 describes our multi-tenant database testbed and the CRM application that it simulates. Section 3 describes the schema mapping techniques. Section 4 presents the results of our experi- ments. Section 5 concludes that the ideal database for SaaS LineItem Product Case Contract Lead Opportunity Asset Contact Campaign Account Figure 1: CRM Application Schema has not yet been developed and offers some suggestions as to how it should be designed. 2. MULTI-TENANT DATABASE TESTBED The experiments in this paper are based on a multi-tenant database testbed we have developed that can be adapted for different database configurations. Each configuration re- quires a plug-in to the testbed that transforms abstract ac- tions into operations that are specific to and optimized for the target database. The testbed simulates a simple but realistic CRM ser- vice. Figure 1 shows the entities and relationships in the base schema. The base entities are extended with additional fields of various types for each tenant. Tenants have different sizes and tenants with more data have more extension fields, ranging from 0 to 100. The characteristics of the dataset are modeled on salesforce.comì°½í²s published statistics [13]. The testbed has nine request classes. The distribution of these requests is controlled using a mechanism similar to TPCì°½í²s card decks. Select 1: Select all attributes of a single entity as if it was being displayed in a detail page in the browser. Select 50: Select all attributes of 50 entities as if they were being displayed in a list in the browser. Select 1000: Select all attributes of the first 1000 entities as if they were being exported through a Web Services interface. Reporting: Run one of five reporting queries that perform aggregation and/or parent-child-roll-ups. Insert 1: Insert one new entity instance as if it was being manually entered into the browser. Insert 50: Insert 50 new entity instances as if data were being synchronized through a Web Services interface. Insert 1750: Insert 1750 new entity instances as if data were being imported through a Web Services interface. Update 1: Update a single entity as if it was being modi- fied in an edit page in the browser. Update 100: Update 100 entity instances as if data were being synchronized through a Web Services interface. The testbed mimics a typical application serverì°½í²s behav- ior by creating a configurable number of connections to the database backend. To avoid blockings, the connections are distributed among a set of worker hosts, each of them han- dling a few connections only. Distributing these connec- tions among multiple hosts allows for modeling various sized, multi-threaded application servers. 882 3. SCHEMA MAPPING TECHNIQUES Within a SaaS application, each tenant has a logical sche- ma consisting of the base schema and a set of extensions. To implement multi-tenancy, the logical schemas from mul- tiple tenants are mapped into one physical schema in the database. The mapping layer transforms queries against the logical schemas into queries against the physical schema so multi-tenancy is transparent to application programmers. The physical schemas for the five mapping techniques stud- ied in this paper are illustrated in Figure 2. The example data set used in this Figure is most clearly shown in the Pri- vate Tables mapping (Figure 2(a)). There are three tenants ? 17, 35, and 42 ? each of which has an Account table with Account ID (Aid) and Name fields. Tenant 17 has extended the Account table with two fields for the health care indus- try: Hospital and Beds. Tenant 42 has extended the Account table with one field for the automotive industry: Dealers. In the Extension Tables mapping (Figure 2(b)), the industry extensions are split off into separate tables that are joined to the base Account table using a new Row number column (Row). Tenants share the tables using a tenant ID column (Tenant). This section describes the other three mappings in more detail. 3.1 Sparse Columns in Microsoft SQL Server Sparse Columns were originally developed to manage data such as parts catalogs where each item has only a few out of thousands of possible attributes. Storing such data in con- ventional tables with NULL values can decrease performance even with advanced optimizations for NULL handling. To implement Sparse Columns, SQL Server 2008 uses a variant of the Interpreted Storage Format [4, 7], where a value is stored in the row together with an identifier for its column. In our mapping for SaaS, the base tables are shared by all tenants and every extension field of every tenant is added to the corresponding base table as a Sparse Column, as il- lustrated in Figure 2(c). Sparse columns must be explicitly defined by a CREATE/ALTER TABLE statement in the DDL and, in this sense, are owned by the database. Nev- ertheless, the application must maintain its own description of the extensions, since the column names cannot be stati- cally embedded in the code. For writes, the application must ensure that each tenant uses only those columns that they have declared, since the namespace is global to all tenants. For reads, the application must do an explicit projection on the columns of interest, rather than doing a SELECT ?, to ensure that NULL values are treated correctly. Sparse Columns requires only a small, fixed number of tables, which gives it a performance advantage over Pri- vate Tables; [3] shows that having many tables negatively impacts performance. On the other hand, there is some overhead for managing Sparse Columns. As an example, the SQL Server documentation recommends using a Sparse Column for an INT field only if at least 64% of the values are NULL [15]. Both of these factors are reflected in the performance results presented in Section 4. 3.2 XML in IBM DB2 IBM pureXML was designed to allow processing of semi- structured data alongside of structured relational data [20]. The mapping for SaaS that we use follows the recommenda- tions in the pureXML documentation for supporting multi- tenancy [21]. The base tables are shared by all tenants and Account17 Aid Name Hospital Beds 1 Acme St. Mary 135 2 Gump State 1042 Account35 Aid Name 1 Ball Account42 Aid Name Dealers 1 Big 65 (a) Private Tables AccountExt Tenant Row Aid Name 17 0 1 Acme 17 1 2 Gump 35 0 1 Ball 42 0 1 Big HealthcareAccount Tenant Row Hospital Beds 17 0 St. Mary 135 17 1 State 1042 AutomotiveAccount Tenant Row Dealers 42 0 65 (b) Extension Tables Account Tenant Aid Name SPARSE 17 1 Acme Hospital St. Mary Bed 135 17 2 Gump Hospital State Bed 1042 35 1 Ball 42 1 Big Dealer 65 (c) Sparse Columns Account Tenant Aid Name Ext XML 17 1 Acme <ext><hospital>St. Mary</hospital> <beds>135</beds></ext> 17 2 Gump <ext><hospital>State</hospital> <beds>1042</beds></ext> 35 1 Ball 42 1 Big <ext><dealers>65</dealers></ext> (d) XML RowKey Account Contact 17Act1 [name:Acme, hospital:St. Mary, beds:135 ] 17Act2 [name:Gump, hospital:State, beds:1042 ] 17Ctc1 [íì¨ íì¨ íì¨ ] 17Ctc2 [íì¨ íì¨ íì¨ ] 35Act1 [name:Ball] 35Ctc1 [íì¨ íì¨ íì¨ ] 42Act1 [name:Big, dealers:65 ] (e) Pivot Tables Figure 2: Schema Mapping Techniques each base table is augmented by a column (Ext XML) that stores all extension fields for a tenant in a flat XML docu- ment, as illustrated in Figure 2(d). Since these documents necessarily vary by tenant, they are untyped. This repre- sentation keeps the documents as small as possible, which is an important consideration for performance [16]. pureXML offers a hybrid query language that provides native access to both the structured and semi-structured representations. Our testbed manipulates data in the struc- tured format, thus accessing extension data requires a corre- lated subquery to manage the XML. This subquery extracts the relevant extension fields using the XMLTABLE function which converts an XML document into a tabular format us- ing XPath. The query with the XMLTABLE function has 883 SELECT b.Tenant, b.Aid, b.Name, e.Dealers FROM Accounts b, XMLTABLE(ì°½í²i/extì°½í² PASSING b.Ext_XML AS ""i"" COLUMNS Dealers INTEGER PATH ì°½í²dealersì°½í² ) AS e WHERE Tenant = 42 AND Aid = 1; (a) Physical SELECT Query accounts tid,aid IXSCAN XSCANFETCH NLJOIN accounts RETURN (b) Query Execution Plan Figure 3: Correlated Subquery for XML in DB2 to be generated client- and query-specific to access clientsì°½í² extension fields relevant in the particular query. Figure 3(a) shows an example query against the physical schema that selects three base fields and one extension field; Figure 3(b) shows the associated query plan. In our testbed, rows are always accessed through base fields, hence there is no need to use the special XML indexes offered by pureXML [20]. To insert a new tuple with extension data, the application has to generate the appropriate XML document; our per- formance results generally include the time to perform this operation. Updates to extension fields are implemented us- ing XQuery 2.0 features to modify documents in place. 3.3 Pivot Tables in HBase HBase [11], which is an open source version of Google BigTable [6], was originally designed to support the explo- ration of massive web data sets. These systems are increas- ingly being used to support enterprise applications in a SaaS setting [14]. In an HBase table, columns are grouped into column fam- ilies. Column families must be explicitly defined in advance in the HBase ì°½íµDDLì°½í¶, for this reason they are owned by the database. There should not be more than tens of column families in a table and they should rarely be changed while the system is in operation. Columns within a column family may be created on-the-fly, hence they are owned by the ap- plication. Different rows in a table may use the same column family in different ways. All values in a column are stored as Strings. There may be an unbounded number of columns within a column family. Data in a column family is stored together on disk and in memory. Thus, a column family is essentially a Pivot Table; each value is stored along with an identifier for its column in a tall narrow table [2]. HBase was designed to scale out across a large farm of servers. Rows are range-partitioned across the servers by key. Applications define the key structure, therefore implic- itly control the distribution of data. Rows with the same key SELECT p.Name, COUNT(c.Case_id) AS cases FROM Products p, Assets a, Cases c WHERE c.Asset = a.Asset_id AND a.Product = p.Product_id GROUP BY p.Name ORDER BY cases DESC Figure 4: Logical Reporting Query prefix will be adjacent but, in general, may end up on differ- ent servers. The rows on each server are physically broken up into their column families. The mapping for SaaS that we use is illustrated in Fig- ure 2(e). In keeping with best practices for HBase, this map- ping ensures that data that is likely to be accessed within one query is clustered together. A single HBase table is used to store all tables for all tenants. The physical row key in HBase consists of the concatenation of the tenant ID, the name of the logical table, and the key of the row in the log- ical table. Each logical table is packed into its own column family, thus each row has values in only one column family. Within a column family, each column in the logical table is mapped into its own physical HBase column. Thus, since columns are dynamic, tenants may individually extend the base tables. The reporting queries in our testbed require join, sort and group operations, which are not currently provided by HBase. We therefore implemented these operators outside the database in an adaptation layer that runs in the client. The adaptation layer utilizes operations in the HBase client API such as update single-row, get single-row and multi-row scan with row-filter. As an example, consider the reporting query shown in Figure 4, which produces a list of all Prod- ucts with Cases by joining through Assets. To implement this query, our adaptation layer scans through all Cases for the given tenant and, for each one, retrieves the associated Asset and Product. It then groups and sorts the data for all Cases to produce the final result. In our experiments, HBase was configured to run on a sin- gle node and the Hadoop distributed map-reduce framework was not employed. In our experience, hundreds of tenants for an application like CRM can be managed by a database on a single commodity processor. In this setting, spreading the data for a tenant across multiple nodes and doing dis- tributed query processing would not be advantageous; the overhead for managing the distribution would nullify any benefits of parallelization. Of course, in addition to scal- ing up to handle many small tenants, the ideal SaaS data- base should also scale out to handle large tenants. But even in this case, map-reduce is problematic for queries such as the one in Figure 4, since it requires that data be clustered around Products. Other queries, such as pipeline reports on Opportunities, might require that the data be clustered in other ways. We conclude this section with several comments about the usage of HBase in our experiments. First, HBase offers only row-at-a-time transactions and we did not add a layer to extend the scope to the levels provided by the commer- cial databases. Second, compression of column families was turned off. Third, neither major nor minor compactions oc- curred during any of the experiments. Fourth, replication of data in the Hadoop file system was turned off. Fifth, column families were not pinned in memory. Sixth, the system was configured so that old attribute values were not maintained. 884  1  10  100  1000  10000  100000 Sel 1 Sel 50 Sel 1000 R eport Ins 1 Ins 50 Ins 1750 U pd 1 U pd 100 R e s p o n s e  T im e  [ m s e c ] Query Classes Private Tables Extension Tables Sparse Column (a) Overall  1  10  100  1000  10000  100000 Sel 1 Sel 50 Sel 1000 R eport Ins 1 Ins 50 Ins 1750 U pd 1 U pd 100 R e s p o n s e  T im e  [ m s e c ] Query Classes Priv. Table (Small Tenant) Priv. Table (Medium Tenant) Priv. Table (Large Tenant) Sparse (Small Tenant) Sparse (Medium Tenant) Sparse (Large Tenant) (b) By Tenant Size Figure 5: SQL Server Performance 4. EXPERIMENTAL RESULTS This section presents the results of our experiments on schema extensibility and evolution. To study schema evolu- tion, we issued a series of schema alteration statements dur- ing a run of the testbed and measured the drop in through- put. The experiments were run on Microsoft SQL Server 2008, IBM DB2 V.9.5 on Windows 2008, and HBase 0.19 on Linux 2.6.18 (CentOS 5.2). The database host was a VM on VMWare ESXi with 4 3.16 GHz vCPUs and 8 GB of RAM. 4.1 Microsoft SQL Server Figure 5(a) shows the results of running our testbed on Microsoft SQL Server using three different mappings: Pri- vate Tables, Extension Tables, and Sparse Columns. The horizontal axis shows the different request classes, as de- scribed in Section 2, and the vertical axis shows the response time in milliseconds on a log scale. In comparison to Private Tables, Extension Tables clearly exhibits the effects of vertical partitioning: wide reads (Sel 1, Sel 50, Sel 1000) are slower because an additional join is re- quired, while narrow reads (Report) are faster because some unnecessary loading of data is avoided. Updates (Upd 1, Upd 100) perform similarly to wide reads because our tests modify both base and extension fields. Extension Tables is faster for inserts because tables are shared among tenants so there is a greater likelihood of finding a page in the buffer pool with free space. Sparse Columns performs as well or better than Private Tables in most cases. The additional overhead for managing the Interpreted Storage Format appears to be offset by the fact that there are fewer tables. Sparse Columns performs worse for large inserts (Ins 1750), presumably because the implementation of the Interpreted Storage Format is tuned to favor reads over writes. Figure 5(b) shows a break down of the Private Table and Sparse Column results by tenant size. Recall from Section 2 that larger tenants have more extension fields, ranging from 0 to 100. The results show that the performance of both mappings decreases to some degree as the number of exten- sion fields goes up. SQL Server permits up to 30,000 Sparse Columns per ta- ble. Our standard configuration of the testbed has 195 ten- ants, which requires about 12,000 columns per table. We also tried a configuration with 390 tenants and about 24,000 columns per table and there was little performance degra- dation. The number of extension fields per tenant in our testbed is drawn from actual usage, so SQL Server is unlikely to be able to scale much beyond 400 tenants. As a point of comparison, salesforce.com maintains about 17,000 tenants in one (very large) database [13]. Figures 6(a) and 6(b) show the impact of schema evolu- tion on throughput in SQL Server. In these graphs, the horizontal axis is time in minutes and the vertical axis is transactions per minute. The overall trend of the lines is downward because data is inserted but not deleted during a run. Part way through each run, ALTER TABLE state- ments on 5 base tables were submitted. The first two lines in each graph show schema-only DDL statements: add a new column and increase the size of a VARCHAR column. The third line in each graph shows a DDL statement that affects existing data: decrease the size of a VARCHAR column. To implement this statement, SQL Server scans through the table and ensures that all values fit in the reduced size. A more realistic alteration would perform more work than this, so the results indicate a lower bound on the impact of evo- lution. The gray bar on each graph indicates the period during which this third operation took place. In the Private Tables case (Figure 6(a)), 975 ALTER TA- BLE statements were submitted, 5 for each of the 195 ten- ants. Individual schema-only alterations completed very rapidly, but nevertheless had an impact on throughput be- cause there were so many of them. Adding a new column took about 1 minute to complete while increasing the size of a VARCHAR column took about 3 minutes. Decreasing the size of a VARCHAR column took about 9 minutes and produced a significant decrease in throughput. The overall loss of throughput in each case is indicated by the amount of time it took to complete the run. In the Sparse Columns case (Figure 6(b)), the tables are shared and 5 ALTER TABLE statements were submitted. The schema-only changes completed almost immediately and had no impact on throughput. Decreasing the size of a VAR- CHAR column took about 2 minutes, during which through- 885  0  2000  4000  6000  8000  10000  12000  14000  16000  0 5 10 15 20 25 30 T ra n s a c ti o n s  p e r  M in u te Testbed runtime (min) Add new column Increase VARCHAR size Decrease VARCHAR size (a) Private Tables  0  2000  4000  6000  8000  10000  12000  14000  16000  0 5 10 15 20 25 30 T ra n s a c ti o n s  p e r  M in u te Testbed runtime (min) Add new column Increase VARCHAR size Decrease VARCHAR size (b) Sparse Columns Figure 6: SQL Server Throughput put dropped almost to zero. The overall loss of throughput was greater for Private Tables, as indicated by the amount of time it took to complete the runs. However the behavior of Private Tables is probably preferable in the SaaS setting be- cause the throughput drop is never as deep, thus the servers donì°½í²t need to be overprovisioned as much. In any case, nei- ther of these mappings is ideal in that the application should have more control over when such resource-intensive opera- tions occur. 4.2 IBM DB2 Figure 7(a) shows the results of running our testbed on IBM DB2 using three different mappings: Private Tables, Extension Tables, and XML using pureXML. The axes are the same as in Figure 5. In comparison to Private Tables, Extension Tables ex- hibits the same performance variations as in SQL Server. However XML produces a decrease in performance in most cases. The decrease is particularly severe for reads, which re- quire executing a correlated subquery containing an XQuery statement embedded in a call to the XMLTABLE function, as described in Section 3.2. Figure 7(b) shows a break down of the Private Table and XML results by tenant size. Re-  1  10  100  1000  10000  100000 Sel 1 Sel 50 Sel 1000 R eport Ins 1 Ins 50 Ins 1750 U pd 1 U pd 100 R e s p o n s e  T im e  [ m s e c ] Query Classes Private Tables Extension Tables XML (a) Overall  1  10  100  1000  10000  100000 Sel 1 Sel 50 Sel 1000 R eport Ins 1 Ins 50 Ins 1750 U pd 1 U pd 100 R e s p o n s e  T im e  [ m s e c ] Query Classes Private Table (Small Tenant) Private Table (Medium Tenant) Private Table (Large Tenant) XML (Small Tenant) XML (Medium Tenant) XML (Large Tenant) (b) By Tenant Size Figure 7: DB2 Performance call from Section 2 that larger tenants have more extension fields, ranging from 0 to 100. The results show that for reads, the performance decrease of XML is proportional to the number of extension fields. Note that in the Insert 1750 case, the results do not include the time to construct the XML document (for no particularly good reason) and there is no variation based on tenant size. XML gives the application complete control over schema evolution. In this setting, the application is responsible for performing any bulk transformations associated with schema alterations that impact existing data. To st",Stefan Aulbach,"Technische Universit?t M?nchen, Germany",stefan.aulbach@in.tum.de,Dean Jacobs,"SAP AG, Walldorf, Germany",dean.jacobs@sap.com,Alfons Kemper,"Technische Universit?t M?nchen, Germany",alfons.kemper@in.tum.de,Michael Seibold,"Technische Universit?t M?nchen, Germany",michael.seibold@in.tum.de,,,,,,,,,,,,,,,,,,
20200104,1126,Orestis Polychroniou,Columbia University,orestis@cs.columbia.edu,,A Comprehensive Study of Main-Memory Partitioning and its Application to Large-Scale Comparison- and Radix-Sort,"ABSTRACT Analytical database systems can achieve high throughput main-memory query execution by being aware of the dynam- ics of highly-parallel modern hardware. Such systems rely on partitioning to cluster or divide data into smaller pieces and thus achieve better parallelism and memory locality. This paper considers a comprehensive collection of variants of main-memory partitioning tuned for various layers of the memory hierarchy. We revisit the pitfalls of in-cache parti- tioning, and utilizing the crucial performance factors, we in- troduce new variants for partitioning out-of-cache. Besides non-in-place variants where linear extra space is used, we introduce large-scale in-place variants, and propose NUMA- aware partitioning that guarantees locality on multiple pro- cessors. Also, we make range partitioning comparably fast with hash or radix, by designing a novel cache-resident index to compute ranges. All variants are combined to build three NUMA-aware sorting algorithms: a stable LSB radix-sort; an in-place MSB radix-sort using different variants across memory layers; and a comparison-sort utilizing wide-fanout range partitioning and SIMD-optimal in-cache sorting. To the best of our knowledge, all three are the fastest to date on billion-scale inputs for both dense and sparse key domains. As shown for sorting, our work can serve as a tool for build- ing other operations (e.g., join, aggregation) by combining the most suitable variants that best meet the design goals. 1. INTRODUCTION The increasing main-memory capacity of contemporary hardware allows query execution to occur entirely in mem- ory. If the entire database also fits in RAM, analytical query workloads that are typically read-only need no disk access after the initial load, setting the memory bandwidth as the only performance bound. Since analytics are at the core of business intelligence tasks today, the need for high- throughput main-memory query execution is apparent. ?This work was supported by National Science Foundation grant IIS-0915956 and a gift from Oracle Corporation. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full cita- tion on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGMOD  íí14, June 22?27, 2014, Snowbird, UT, USA. Copyright 2014 ACM 978-1-4503-2376-5/14/06 ...$15.00. http://dx.doi.org/10.1145/2588555.2610522. To maximize memory bandwidth and capacity, a few CPUs can be combined in a shared-memory system using a fast interconnection. Such hardware combines the parallelism of multiple multi-core CPUs with a higher aggregate memory bandwidth. The shared-memory functionality is provided by the non-uniform-memory-access (NUMA) interconnection, adding an additional layer in the memory hierarchy. In a modern multi-core CPU, the best performance is achieved when all cores work in a shared-nothing fashion and the working set is small enough to fit in the fast (and private per core) caches. The same approach was more effi- cient even before the advent of the multi-core era [11], since random RAM accesses are too expensive out-of-cache. Query execution is decomposed into a series of operations, the most time consuming of which are typically joins and aggregations. To speed up these operations using hardware parallelism, we partition into small pieces using the keys, then process each piece independently. For instance, an effi- cient algorithm for joins is to hash partition in parallel until the input is split into cache resident pieces before we execute the join using a hash table [11]. In fact, even in-cache join can further partition into trivial parts with very few distinct items, before executing a nested loop to join them [7]. This paper considers a comprehensive menu of partition- ing options across several dimensions. The three types of partitioning are hash, radix and range partitioning, depend- ing on the function that takes the key as an input and out- puts the destination partition. Partitioning also depends on the layer of the memory hierarchy that it targets, namely in-cache, out-of-cache and across NUMA regions. Finally, we distinguish partitioning variants based on whether they use auxiliary space that is linear to the input size or not. Until recently, prior work used the in-cache versions of partitioning and, if parallel, the non-in-place variant, which can be trivially distributed across threads. In all cases, when the input is larger than the cache, the performance is throt- tled by TLB misses [11] and cache conflicts [14]. Satish et al. [14] suggested in-cache buffering to mitigate TLB misses and Wassenberg et al. [15] used non-temporal writes on cache line sized buffers to facilitate hardware write-combining. Efficient out-of-cache partitioning [14, 15] assumes free access to linear auxiliary space, to write the output. We introduce several in-place out-of-cache partitioning variants utilizing the same crucial performance factors: an in-place method analogous to the shared-nothing non-in-place method; a modified non-in-place method that generates the output as a list of blocks that overwrites the input; and a parallel non-in-place method that combines the previous two. 755 To support scaling to multiple CPUs, we consider how the NUMA layer affects partitioning performance and modify both in-place and non-in-place out-of-cache partitioning to guarantee minimal transfers across NUMA boundaries, also ensuring sequential accesses so that hardware pre-fetching can hide the latency of the NUMA interconnection [1]. All of the above algorithms target the data shuffling part of partitioning and implicitly assume that the partition func- tion is cheap and can be computed at virtually no cost. While this assumption holds for radix and hash partitioning given a suitable hash function choice, the cost of computing a range partition function is higher than the cost to transfer the tuple, especially when the number of partitions increases beyond the TLB capacity. The standard (and slow) imple- mentation is a binary search in a sorted array of delimiters that define the partition ranges. The slowdown is caused by the logarithmic number of cache loads. We introduce a specialized SIMD-based cache-resident index that speeds up range function computation up to 6 times and makes range partitioning a practical choice for many applications. All partitioning variants discussed in this paper are shown in Figure 1, where we also mark our contributions. We ap- ply all variants to design and implement three large-scale NUMA-aware sorting algorithms. We use sorting, rather than joins or aggregations for two reasons. First, because it is a wider problem that can be a sub-problem for both join and aggregation. Second, we can apply all partitioning variants to build unique sorting algorithms, such that each is more scalable in distinct cases depending on input size, key domain size, space requirements, and skew efficiency. The first sorting algorithm we propose is stable least- significant-bit (LSB) radix-sort based on non-in-place out- of-cache radix partitioning [14] where we add two innova- tions. We use hybrid range-radix partitioning to provide perfect load balancing, and guarantee that each tuple will cross NUMA boundaries at most once, even if the algorithm would by default re-organize the entire array in each pass. The second sorting algorithm we propose is an in-place most-significant-bit (MSB) radix-sort that uses all variants of in-place partitioning that we introduce, one for each dis- tinctive level in the memory hierarchy: shared out-of-cache in-place partitioning, shared-nothing out-of-cache partition- ing, and in-cache partitioning. We reuse the range-radix idea of LSB radix-sort for NUMA optimality and load balancing. The third sorting algorithm we propose is a comparison- sort that uses the newly optimized range partitioning. We perform very few out-of-cache range partitioning passes with a very wide fanout until we reach the cache, providing NUMA optimality. In the cache, we employ sorting [6] that scales to the SIMD length, modified to use the cache more effectively. radix hash range partition in-cache non-in-place out-of-cache out-of-cache in-place in-cache shared in block lists in segments sharedshared-nothing NUMA oblivious NUMA aware previously known our contributions Figure 1: Partitioning variants and contributions We use fixed length integer keys and payloads, typical of analytical database applications. We evaluate on both dense and sparse key domains. If order-preserving compression is used [12, 16], any sparse or dense domain with fixed or vari- able length data is compacted into a dense integer domain. Skewed workload distribution can reduce parallelism in some na?íì§¤íì§ve approaches. In our context, when we statically distribute partitions to threads, we ensure that the parti- tions are as balanced as possible. Specifically, we never use radix partitioning to any range of bits to divide the work- load. Instead, we combine range with radix partitioning to guarantee that, if specific bit ranges have very few distinct values, we can find delimiters that split the workload equally among threads, independently of the key value range. We summarize our contributions: ? We introduce several new variants for main-memory partitioning, most notably large-scale in-place parti- tioning and efficient range partitioning, and also guar- antee minimal NUMA transfers across multiple CPUs. ? We combine partitioning variants to design three sort- ing algorithms, all the fastest of their class for billion- scale inputs, and evaluate the best options depending on input size, key domain, available space, and skew. The rest of the paper is organized as follows. Section 2 outlines related work. In Section 3 we describe partitioning variants. In Section 4 we discuss sorting. Section 5 presents our experimental results, and we conclude in Section 6. 2. RELATED WORK We first outline related work on partitioning. Manegold et al. [11] identified the TLB thrashing problem when na?íì§¤íì§vely partitioning to a large number of outputs. Satish et al. [14] introduced efficient out-of-cache partitioning and Wassen- berg et al. [15] identified the significance of write-combining. Manegold et al. [11] proposed partitioning to cache-resident hash tables to join and Kim et al. [7] reused the same design on a multi-core CPU. Wu et al. [17] proposed hardware ac- celerated partitioning for performance and power efficiency. We briefly outline recent work on sorting for modern hard- ware, due to space constraints. Inoue et al. [6] proposed in- cache SIMD-vector comb-sort followed 2-way SIMD merg- ing. Chhugani et al. [5] proposed in-cache sorting networks followed by cyclic merging in buffers to avoid being memory bound. Kim et al. [7] compared sort-merge-join against hash join, projecting that sort-merge-join will eventually outper- form hash with wider SIMD. Satish et al. [14] compared radix-sort and merge-sort in CPUs and GPUs on multiple key domain sizes and concluded in favor of merge-sort. How- ever, the result is based on small arrays and only LSB radix- sort is considered. Wassenberg et al. [15] improved over Satish et al. [14] and claimed that radix-sort is better. Kim et al. [9] studied network-scale sorting maximizing network transfer with CPU computation overlap. Albutiu et al. [1] studied the NUMA effects using sort-merge-join on multiple CPUs with billion-scale arrays. Balkesen et al. [4] claimed that non-partitioning hash joins are competitive, but Balke- sen et al. [3] improved over Blanas et al. [4] and concluded that partitioning joins are generally faster, even without us- ing fast partitioning [14, 15]. Balkesen et al. [2] further im- proved joins on multiple CPUs using fast partitioning. Thus, on the fastest CPUs [3], partitioning appears to be the best choice for both hash joins and radix-sort-merge-joins. 756 3. PARTITIONING 3.1 In-Cache We start by considering the versions that best operate when the table fits in the cache. The non-in-place version (Algorithm 1) uses a separate array from the input to store the output, while the in-place version (Algorithm 2) uses one array for both input and output. Each partition is generated in a single segment. In-cache partitioning can be run in parallel, if the threads operate in a shared-nothing fashion. Algorithm 1 Non-in-place in-cache partitioning i íì§  0 // P : the number of partitions for p íì§  0 to P -1 do offset[p] íì§  i // point at the start of each partition i íì§  i + histogram[p] end for for iin  íì§  0 to |Tin|-1 do t íì§  Tin[iin] // Tin: the input table iout  íì§  offset[f(t.key)] + + // f : the partition function Tout[iout] íì§  t // Tout: the output table end for The simplest non-in-place version does only two random accesses per item. When operating in the cache, we need the output and the offset array to be cache-resident. A slightly more complicated version of the algorithm allows the parti- tioning to happen in-place, by swapping items across loca- tions. In short, we start by reading an item, find the correct partition and the output destination through the offset ar- ray, swap it with the item stored there, and continue for the new item until the cycle is closed. Each item is moved exactly once and we stop when the whole array is covered. Item swaps are performed in cycles of transfers, defined as swap cycles. When the items are processed low-to-high [1], the cycle starts by doing a read and then swaps until it reaches the same location it initially read from, to write back. This case occurs 1/P of time on average but requires branching. In Algorithm 2 below, the partitions are written high-to-low and swap cycles close when all items of a parti- tion have been placed, avoiding branching for every tuple. Algorithm 2 In-place in-cache partitioning i íì§  0 // P : the number of partitions for p íì§  0 to P -1 do i íì§  i + histogram[p] offset[p] íì§  i // point at the end of each partition end for p íì§  iend  íì§  0 while histogram[p] = 0 do p+ + // skip initial empty partitions end while repeat t íì§  T [iend] // T : the input & output table repeat p íì§  f(t.key) // f : the partition function i íì§  ??offset[p] T [i] íì§§ t // swap until i = iend repeat iend  íì§  iend + histogram[p+ +] // skip if empty until p = P or iend 6= offset[p] until p = P 3.2 Out-of-Cache Out-of-cache performance is throttled by increased cache conflicts [14] and cache pollution with output tuples [15]. TLB thrashing occurs when the number of partitions ex- ceeds the TLB capacity [11], unless the entire dataset can be placed in equally few large OS pages to be TLB resident. 3.2.1 Non-in-place To mitigate these problems, prior work [14] proposed using the cache as an intermediate buffer before writing back to memory. Also, when write backs occur, they bypass the higher cache levels entirely and avoid polluting the cache [15]. Recent work [2] uses the same basic technique for out- of-cache radix partitioning during hash join execution. Buffering data for each partition reduces the working set size and eliminates the TLB problem when operating in the buffer. TLB misses still occur, but 1/L of the time, if L is the number of tuples buffered for each partition before writing to output. If the buffer for each partition is exactly as big as a cache line, writing the full cache line to memory is accel- erated by write-combining and avoids polluting the higher cache levels with output data. The partitioning fanout is now bounded by the number of cache lines in the fast core- private cache, rather than the TLB entries. Buffer flushing is optimally done using wider registers [15]. To maximize the cache use, we use the last buffer slot to save the output offset and access one cache line per iteration (Algorithm 3). To extend the above method to multiple columns stored in separate arrays, the standard case in RAM-resident database data, we use one cache line per column in the buffer of each partition. A generic implementation can use one cache line per column and flush it separately depending on the column width. We can also interleave the columns in a single tuple and de-interleave the columns when the buffer is flushed. For example, when partitioning arrays of 32-bit keys and 32-bit payloads, we store 64-bit tuples in the cached buffer. Tuple (de-)interleaving can be accelerated using SIMD. Parallel execution of the non-in-place out-of-cache parti- tioning is trivial. The input can be split to equal pieces, one for each thread. By executing a prefix sum of all individ- ual histograms, one can ensure that each partition output is written in a distinct location. Threads are only synchronized after individual histograms are built. This is the only known technique for parallel partitioning on shared segments. Algorithm 3 Non-in-place out-of-cache partitioning iout  íì§  0 // P : the number of partitions for p íì§  0 to P -1 do buffer[p][L-1] íì§  iout // L: # of tuples per cache line iout  íì§  iout + histogram[p] end for for iin  íì§  0 to |Tin|-1 do t íì§  Tin[iin] // Tin/Tout: the input/output table p íì§  f(t.key) // f : the partition function iout  íì§  buffer[p][L-1] + + buffer[p][iout mod L] íì§  t if iout mod L = L-1 then for ibuf  íì§  0 to L-1 do Tout[iout + ibuf ? L] íì§  buffer[p][ibuf ] // no cache end for buffer[p][L-1] íì§  iout + 1 end if end for 757 3.2.2 In-place, Shared-Nothing Segments Adapting the out-of-cache buffering technique to in-place partitioning requires a more complicated approach. The ba- sic idea is to perform the swaps inside the buffer, so that the sparse RAM locations are accessed only 1/L of the time, re- ducing the overhead from TLB misses. Compared with non- in-place out-of-cache partitioning, which uses the buffer as an intermediate layer to group tuples before writing them, in-place out-of-cache partitioning performs all tuples swaps in the buffer, and accesses RAM one cache line at-a-time. Before the main partitioning loop starts, we load cache lines from all starting partition locations. Item swaps be- tween partitions occur using the last L tuples that are stored in the buffer. When a buffer has swapped all L items, the cache line is streamed to the RAM location it was loaded from and the buffer is re-filled with the next L items of the same partition. Thus, we operate in the buffer (L? 1)/L of the time and do not miss in the TLB. The offsets are stored inside the buffer and the last L items of each partition are handled differently to eliminate branching in the inner loop. If having T contiguous segments per partition (T is the number of threads) is acceptable, then we can run in-place partitioning in parallel. However, unlike the non-in-place variant, generating one segment per partition across threads is impossible with coarse-grained synchronization. Algorithm 4 In-place out-of-cache partitioning i íì§  0 // P : the number of partitions for p íì§  0 to P -1 do end[p] íì§  i i íì§  i + histogram[p] for ibuf  íì§  0 to L-1 do buffer[p][ibuf ] íì§  T [i? (i mod L) + ibuf ] end for item0[p] íì§  buffer[p][0] // save 1st item out of buffer buffer[p][0] íì§  i [...] // special handling for partitions smaller than L end for p íì§  0 while histogram[p] = 0 do p+ + // skip initial empty partitions end while t íì§  T [0] // T : the input & output table loop repeat p íì§  f(t.key) // f : the partition function i íì§  ??buffer[p][0] buffer[p][i mod L]  íì§§ t // swap until i mod L = 0 // L: # of tuples per cache line [...] // (rare) branch for end of partition (exits here) for ibuf  íì§  0 to L-1 do T [i] íì§  buffer[p][ibuf ] // no cache end for for ibuf  íì§  0 to L-1 do buffer[p][ibuf ] íì§  T [i? L] // cache end for t íì§  item0[p] item0[p] íì§  buffer[p][0] buffer[p][0] íì§  i if i ? end[p] < L then [...] // (rare) branch for last L items of partition end if end loop 3.2.3 In-place, List of Blocks For large-scale out-of-cache partitioning, the requirement of producing all partitions in P non-splitting segments can be relaxed. Instead of writing each partition output sequen- tially, we can write large blocks that only contain items from a single partition. When the block is full, we get a new block at some new available location. The block size must be large enough to amortize sequential writes, but not too large, in order to avoid external fragmentation from non-full blocks. To access data from a single partition only, we create a small linked list that connects all blocks that contain data of the same partition. While the access is not entirely se- quential as in the single segment case, the list hops after scanning each block are amortized by a sufficient block size. This method can be done in place, if we remove P  íì§B items from the start of the input and save it in private space (B is the block capacity in tuples). We start range partitioning the input from the (P  íì§B)-th tuple. By the time any partition is filled, the input pointer will have advanced enough for the output to safely use the space of input we read before, without overwriting tuples not yet read. At the end, we also add the data initially copied out back to the correct block lists. For each partition, only the last block of the block list can be non-full. Thus, unused space has an upper bound of P  íì§B and is negligible compared to size of the input. Block-based partitioning has a number of nice properties. First, it uses the fast non-in-place out-of-cache partition- ing. Second, it does not require the pre-computation of a histogram. Third, it can be done in place by ensuring no overlap between input and output data. Finally thread par- allelism is trivial; the only requirement is to connect the linked lists of blocks from all threads for each partition. 3.2.4 In-place, Shared Segments In order to partition and shuffle data in parallel inside the same segment, we need fine-grain synchronization. Since us- ing OS latches are overly expensive, we use atomic instruc- tions. Atomic fetch-and-add reads a memory location, in- crements it by some value and returns its previous value. Imagine an array of items and multiple threads where each item must be processed by exactly one thread. Each thread can safely use item at index i which is returned by invok- ing fetch-and-add(c,1) on a shared counter c. When done, the thread asks for the next item to process or terminates if i exceeds the number of items. We apply the same idea to in-place partitioning using one shared counter for each partition to represent the number of tuples swapped so far. We use fetch-and-add on the shared counter of partition p, to  íì§¸lock íì§¹ the cell of the next yet unread item of partition p. We store the first index that initiates the cycle. After swapping an arbitrary number of keys, when we return to the original partition p, we store the last tuple in the initial location. Only the P counters are shared across threads. As mentioned in Section 3.1, we define a swap cycle as a sequence of swaps that starts by reading a key from a specific partition and after a number of swaps, returns to the same partition to write a key in the initially read location. We cannot know in advance how large a swap cycle will be, thus we cannot lock all locations the cycle will go through before actually moving tuples. Imagine a scenario where the first partition has only one item found in the last cell of the array. Then, one thread would perform a single swap cycle covering all items before the last cell is reached and the cycle is closed. 758 To solve this first problem, threads lock only one location at a time for one swap. However, when close to comple- tion, multiple threads may compete for swap cycles, creating deadlocks. For example, assuming one item per partition, if thread t1 reads item kx (must go to lx) from location ly (must bring ky here) and thread t2 reads kz (must go to lz) from lx (must bring kx here), t1 will find no space for kx, be- cause the offset of partition X was incremented by t2 when it read kz. If t1 waits, t2 will reach ky. Then, a deadlock will occur, since t1 holds (kx, ly) and t2 holds (ky, lx). To solve this second problem and avoid waiting for others, when a thread finds a partition to be full, it records both the current key and the locked location that the swap cycle started from. In the above example, t1 records (kx, ly) and t2 records (ky, lx). A final fix step occurs  íì§¸offline íì§¹ and takes trivial time, as the number of such pairs is upper bounded by the number of partitions P , times the number of threads. So far, we presented a way for multiple threads to partition items in-place concurrently, but this solution is impractical if used as is. First, we make no use of buffering to improve out-of-cache performance and second, for each key we move to its destination, we update a shared variable triggering cache invalidations on every tuple move. To make this ap- proach practical, we change the unit of transfer from tuples to blocks. Each block must have a fixed size and all tuples must belong to the same partition. We generate such blocks using the technique described previously (see Section 3.2.3). Out-of-cache accesses are amortized by the block size, as is the synchronization cost of accessing shared variables. Algorithm 5 Synchronized in-place partitioning Pactive  íì§  {} // Pactive: set of yet unfinished partitions Tdeadlock  íì§  {} // Tdeadlock: set of tuple & location pairs i íì§  0 // P : the number of partitions for p íì§  0 to P -1 do Pactive  íì§  Pactive + {p} offset[p] íì§  i i íì§  i + histogram[p] end for while |Pactive| > 0 do p íì§  any  íì¨ Pactive i íì§  used[p] + + // atomic fetch-and-add if i  íí histogram[p] then Pactive  íì§  Pactive? {p} goto loop-end end if ibeg  íì§  i + offset[p] t íì§  T [ibeg] // T : the input & output table pnext  íì§  f(t.key) // f : the partition function while p 6= pnext do i íì§  used[p] + + // atomic fetch-and-add if i  íí histogram[pnext] then Tdeadlock  íì§  Tdeadlock + {t, iinit} goto loop-end end if i íì§  i + offset[pnext] T [i] íì§§ t // swap pnext  íì§  f(t.key) end while T [ibeg] íì§  t loop-end: end while [...] // handle tuples that could cause deadlock (Tdeadlock) 3.3 Across NUMA Moving RAM-resident data across multiple CPUs raises questions about the effectiveness of NUMA RAM transfers. Accessing remote memory locations goes through an inter- connection channel that issues operations to remote RAM modules, increasing the latency. Normally, random accesses are much slower than sequential access and the gap increases when the accesses reference remote RAM regions and go through the CPU interconnection. Prior work [1] proposed doing sequential accesses to remote memory, since hardware pre-fetching hides the latency. To avoid imbalanced use of the NUMA layer when all transfers are directed to a subset of CPUs, we can pre-schedule the transfers and supervise them via synchronization to ensure load balancing [10]. One way to make NUMA-oblivious code scale on multiple CPUs is to allocate both arrays to be physically interleaved across all RAM regions. The OS can support interleaved al- location, where the physical locations of a single array are in- terleaved across all NUMA regions. Randomization of page placement balances accesses across the NUMA interconnec- tion, but precludes NUMA locality. Thus, if we do random accesses, we pay the extra NUMA latency. Cache-line buffer- ing, used by out-of-cache partitioning to avoid TLB misses and facilitate write-combining, also mitigates the NUMA overhead. Still, we measured out-of-cache partitioning to be up to 55% slower on four NUMA regions on interleaved space. The overhead for single tuple random access is higher. A more NUMA-friendly allocation is to split space into large segments bound to a specific region. We can have one segment per thread or one segment per NUMA region. We use the second approach for sorting (see Section 4.1). 3.3.1 Non-in-place Using NUMA-bound segmented allocation for threads or CPUs and if extra space is allowed, we can ensure that all tuples will cross the NUMA boundaries at most once. We use shared-nothing partitioning locally and then use a sep- arate step to shuffle across CPUs. We can use the NUMA interconnection in a balanced way without manual sched- ules [10]. We distribute each segment across all threads of the destination CPU, and do the transfers in a per thread random order. Since some tuples are already on destination, the expected number of transfers is (x? 1)/x for x regions. The NUMA-oblivious partitioning might perform faster than the two step method of shared-nothing partitioning followed by NUMA shuffling, since out-of-cache partition- ing mitigates latencies. The decision to guarantee minimal transfers by incurring shuffling, depends on the hardware. 3.3.2 In-place Assuming NUMA-bound segmented allocation, the only in-place variant where threads do not work in a shared- nothing fashion is during block shuffling (see Section 3.2.4). During the phase of block shuffling on multiple NUMA re- gions, threads can read and write blocks from all regions, but all",Orestis Polychroniou,Columbia University,orestis@cs.columbia.edu,Kenneth A. Ross,Columbia University,kar@cs.columbia.edu,,,,,,,,,,,,,,,,,,,,,,,,
20200105,1388,Yang Cao,"RCBD and SKLSDE Lab, Beihang University",yang.cao@ed.ac.uk,,Making Pattern Queries Bounded in Big Graphs,"Abstract  It is cost-prohibitive to find matches Q(G) of a pattern query Q in a big graph G. We approach this by fetching a small subgraph GQ of G such that Q(GQ) = Q(G). We show that many practical patterns are effectively bounded under access constraints A commonly found in real life, such that GQ can be identified in time determined by Q and A only, independent of the size |G| of G. This holds no matter whether pattern queries are localized (e.g., via subgraph isomorphism) or non-localized (graph simulation). We provide algorithms to decide whether a pattern Q is effectively bounded, and if so, to generate a query plan that computes Q(G) by accessing GQ, in time independent of |G|. When Q is not effectively bounded, we give an algorithm to extend access constraints and make Q bounded in G. Using real-life data, we experimentally verify the effectiveness of the approach, e.g., about 60% of queries are effectively bounded for subgraph isomorphism, and for such queries our approach outperforms the conventional methods by 4 orders of magnitude. I. INTRODUCTION Given a pattern query Q and a graph G, graph pattern matching is to find the set Q(G) of matches of Q in G. It is used in, e.g., social marketing, knowledge discovery, mobile network analysis, intelligence analysis for identifying terrorist organizations [25], and the study of adolescent drug use [17]. When G is big, graph pattern matching is cost-prohibitive. Facebook has 1.26 billion nodes and 140 billion links in its social graph, about 300PB of user data [28]. When the size |G| of G is 1PB, a linear scan of G takes 1.9 days using SSD with scanning speed of 6GB/s. Worse still, graph pattern matching is intractable if it is defined with subgraph isomorphism [31], and it takes O((|V |+ |VQ|)(|E|+ |EQ|))-time if we use graph simulation [20], where |G| = |V |+|E| and |Q| = |VQ|+|EQ|. Can we still efficiently compute exact answers Q(G) when G is big while we have constrained resources, such as a single processor? We approach this by making big graphs small, capitalizing on a set A of access constraints, which are a combination of indices and simple cardinality constraints defined on the labels of neighboring nodes of G. We determine whether Q is effectively bounded under A, i.e., for all graphs G that satisfy A, there exists a subgraph GQ  íì¨ G such that (a) Q(GQ) = Q(G), and (b) the size |GQ| of GQ and the time for identifying GQ are both determined by A and Q only, independent of |G|. If Q is effectively bounded, we can generate a query plan that for all G satisfying A, computes Q(G) by accessing (visiting and fetching) a small GQ in time independent of |G|, no matter how big G is. Otherwise, we will identify extra access constraints on an input G and make Q bounded in G. A large number of real-life queries are effectively bounded under simple access constraints, as illustrated below. award year movie actressactor country 2011-2013u 1 u 2 u 3 u 4 u 5 u 6 Fig. 1. Pattern query Q0 on IMDb Example 1: Consider IMDb [22], a graph G0 in which nodes represent movies, casts, and awards from 1880 to 2014, and edges denote various relationships between the nodes. An example search on IMDb is to find pairs of first-billed actor and actress (main characters) from the same country who co- stared in a award-winning film released in 2011-2013. The search can be represented as a pattern query Q0 shown in Fig. 1. Graph pattern matching here is to find the set Q0(G0) of matches, i.e., subgraphs G íí of G0 that are isomorphic to Q0; we then extract and return actor-actress pairs from each match G íí. The challenge is that G0 is large: the IMDb graph has 5.1 million nodes and 19.5 million edges. Add to this that subgraph isomorphism is NP-complete. Not all is lost. Using simple aggregate queries one can readily find the following real-life cardinality constraints on the movie dataset from 1880?2014: (1) in each year, every award is presented to no more than 4 movies (C1); (2) each movie has at most 30 first-billed actors and actresses (C2), and each person has only one country of origin (C3); and (3) there are no more than 135 years (C4, i.e., 1880-2014), 24 major movie awards (C5) and 196 countries (C6) in total [22]. An index can be built on the labels and nodes of G0 for each of the constraints, yielding a set A0 of 8 access constraints. Under A0, pattern Q0 is effectively bounded. We can find Q0(G0) by accessing at most 17923 nodes and 35136 edges in G0, regardless of the size of G0, by the following query plan: (a) identify a set V1 of 135 year nodes, 24 award nodes and 196 country nodes, by using the indices for constraints C4-C6; (b) fetch a set V2 of at most 24 íì© 3 íì© 4 = 288 award-winning movies released in 2011?2013, with no more than 288 íì© 2 = 576 edges connecting movies to awards and years, by using those award and year nodes in V1 and the index for C1; (c) fetch a set V3 of at most (30+30)?288 = 17280 actors and actresses with 17280 edges, using V2 and the index for C2; (d) connect the actors and actresses in V3 to country nodes in V1, with at most 17280 edges by using the index for C3. Output (actor, actress) pairs connected to the same country in V1. The query plan visits at most 135 + 24 + 196 + 288 + 17280 = 17923 nodes, and 576 + 17280 + 17280 = 35136 978-1-4799-7964-6/15/$31.00 ? 2015 IEEE ICDE Conference 2015161 edges, using the cardinality constraints and indices in A0, as opposed to tens of millions of nodes and edges in IMDb. 2 This example tells us that graph pattern matching is feasible in big graphs within constrained resources, by making use of effectively bounded pattern queries. To develop a practical ap- proach out of the idea, several questions have to be answered. (1) Given a pattern query Q and a set A of access constraints, can we determine whether Q is effectively bounded under A? (2) If Q is effectively bounded, how can we generate a query plan to compute Q(G) in big G by accessing a bounded GQ? (3) If Q is not bounded, can we make it  íì§¸bounded íì§¹ in G by adding simple extra constraints? (4) Does the approach work on both localized queries (e.g., via subgraph isomorphism) and non-localized queries (via graph simulation)? Contributions. This paper aims to answer these questions for graph pattern matching. The main results are as follows. (1) We introduce effective boundedness for graph pattern queries (Section II). We formulate access constraints on graphs, and define effectively bounded pattern queries. We also show how to find simple access constraints from real-life data. (2) We characterize effectively bounded subgraph queries Q, i.e., patterns defined by subgraph isomorphism (Section III). We identify a sufficient and necessary condition to decide whether Q is effectively bounded under a set A of access con- straints. Using the condition, we develop a decision algorithm in O(|A||EQ|+||A|||VQ|2) time, where |Q| = |VQ|+|EQ|, and ||A|| is the number of constraints in A. The cost is independent of big graph G, and query Q is typically small in practice. (3) We provide an algorithm to generate query plans for effectively bounded subgraph queries (Section IV). After Q is found effectively bounded under A, the algorithm generates a query plan that, given a graph G that satisfies A, accesses a subgraph GQ of size independent of |G|, in O(|VQ||EQ||A|) time. Moreover, we show that the plan is worst-case-optimal, i.e., for each input Q and A, the largest GQ it finds from all graphs G that satisfy A is the minimum among all worst-case GQ identified by all other query plans. (4) If Q is not bounded under A, we make it instance-bounded (Section V). That is, for a given graph G that satisfies A, we find an extension AM of A such that under AM , we can find GQ  íì¨ G in time decided by AM and Q, and Q(GQ) = Q(G). We show that when the size of indices in AM is predefined, the problem for deciding the existence of AM is in low polynomial time (PTIME), but it is log-APX-hard to find a minimum AM . WhenAM is unbounded, all query loads can be made instance- bounded by adding simple access constraints. (5) We extend the study to simulation queries, i.e., patterns interpreted by graph simulation (Section VI). It is more chal- lenging to cope with the non-localized and recursive nature of simulation queries. Nonetheless, we provide a characterization of effectively bounded simulation queries. We also show that our algorithms for checking effective boundedness, generating query plans, and for making queries instance-bounded can be adapted to simulation queries, with the same complexity. (6) We experimentally evaluate our algorithms using real-life data (Section VII). We find that our approach is effective for both localized and non-localized queries: (a) on graphs G of billions of nodes and edges [1], our query plans outperform the conventional methods that computes Q(G) directly by 4 and 3 orders of magnitude on average, for subgraph and simulation queries, respectively, accessing at most 0.0032% of the data in G; (b) 60% (resp. 33%) of subgraph (resp. simulation) queries are effectively bounded under simple access constraints; and (c) all queries can be made instance-bounded in G by extend- ing constraints and accessing 0.016% of extra data in G; and 95% become instance-bounded by accessing at most 0.009% extra data. Our algorithms are efficient: they take at most 37ms to decide whether Q is effectively bounded and to generate an optimal query plan for all Q and constraints tested. This work is the first effort to study effectively bounded graph queries, from fundamental problems to practical algo- rithms. It suggests an approach to querying graphs: (1) given a query Q, we check whether Q is effectively bounded under a set A of access constraints; (2) if so, we generate a query plan that given a graph G satisfying A, computes Q(G) by accessing GQ of size independent of |G|, no matter how big G grows; (3) if not, we make Q instance-bounded in G with extra simple constraints. The approach works for both localized subgraph queries and non-localized simulation queries. Given the prohibitive cost of querying big graphs, this approach helps even when only limited queries are effectively bounded. In fact, we find that many queries on real-life datasets are actually effectively bounded under very simple access constraints. Moreover, when a finite set of queries is not effectively bounded, we can make them instance-bounded. All proofs of the results of the paper can be found in [3]. Related Work. We categorize related works as follows. Effective boundedness. The study of effective boundedness traces back to scale independence. The latter was proposed [5] to approximately answer relational aggregate queries under certain conditions, for key/value stores. It aims to guarantee that a bounded amount of work is required to execute all queries in an application, regardless of the size of the underlying data. The idea was formalized in [12], along with a notion of access constraints for relational queries. Recently, the notion of [12] is revised in [10] by requiring that the amount of data accessed (i.e., GQ) can be identified in time determined by query Q and access constraints A only, referred to as effective boundedness; it is characterized for SPC queries [10]. This work differs from the previous work in the following. (1) We introduce access constraints on graph data, to specify cardinality constraints on the labels of neighboring nodes, and guide us to retrieve small subgraphs GQ. (2) Under such constraints, we formalize and characterize the effective bound- edness of graph patterns, an issue harder than its counterpart for relational queries [10], [12]. (3) We propose instance boundedness for queries that are not effectively bounded. Resource-bounded and anytime algorithms. Related are also resource-bounded [16] and anytime algorithms [32]. The former study reachability queries and personalized pattern queries, in which some pattern nodes are designated to match 162 fixed nodes in a graph G. It is to compute approximate answers by accessing no more than íì§íì§|G| nodes and edges in G, for íì§íì§  íì¨ (0, 1) [16]. Anytime algorithms [32] allow users either to specify a budget on resources (e.g., running time; known as contract algorithms [33]), or to terminate the run of the algorithms at any time and get intermediate answers (known as interruptible algorithms [19]). Contract anytime algorithms have been explored for (a) budgeted search such as bounded- cost planning [4], [29], [30], [32] under a user-specified budget; and (b) graph search via subgraph isomorphism, to find intermediate approximate answers within the budget, either by assigning dynamically maintained budgets and costs to nodes during the traversal [8], or by deciding search orders based on the frequencies of certain features in queries and graphs [27]. This work differs from the prior work as follows. (1) We aim to compute exact answers for pattern queries in big graphs, as opposed to heuristic answers that may not have a provable accuracy bound. (2) We characterize what pattern queries can be answered exactly within a cost independent of the size of big graph, based on access constraints; in contrast, the prior work does not study under what budget accurate answers are warranted by using the semantics of the data. (3) We study general pattern queries, which may be either localized or non- localized, and may not be personalized [16]. Graph indexing and compression. There are typically two ways to reduce search space. (1) Graph indexing uses pre- computed global information of G to compute distance [11], shortest paths [18] or substructure matching [26]. (2) Graph compression computes a summary Gc of a big graph G and uses Gc to answer all queries posed on G [7], [13], [24]. In contrast to the prior work, (1) we compute exact answers rather than heuristic. (2) Instead of using the same graph Gc to answer all queries posed on G, we adopt a dynamic reduction scheme that finds a subgraph GQ of G for each query Q. Since GQ consists of only the information needed for answering Q, it allows us to compute Q(G) by using GQ much smaller than Gc and hence, much less resources. (3) When Q is effectively bounded, for all graphs G we can find GQ of size independent of |G|; in contrast, |Gc| may be proportional to |G|. Making big graphs small. There have been other techniques for reducing a big graph into small ones, e.g., distribute query answering [23], pattern matching using views [15], and incremental pattern matching [14]. These are complementary to this work and can be readily combined with ours, e.g., our methods can be readily adapted to distributed settings. II. EFFECTIVELY BOUNDED GRAPH PATTERN QUERIES In this section we define access schema on graphs and effectively bounded graph pattern queries. We start with a review of graphs and patterns. Assume an alphabet íì§ííª of labels. Graphs. A data graph is a node-labeled directed graph G = (V,E, f, íì§íì§¯), where (1) V is a finite set of nodes; (2) E  íì¨ V íì©V is a set of edges, in which (v, v íí) denotes the edge from v to v íí; (3) f() is a function such that for each node v in V , f(v) is a label in íì§ííª, e.g., year; and (4) íì§íì§¯(v) is the attribute value of f(v), e.g., year = 2011. u 1 Q 1 A B  íì§ u 2 v 1 v 2 v 3 v 2n A AB B G 1 Cu3 Du4 Cv2n+1 Dv2n+2 Fig. 2. Pattern query Q1 and data graph G1 We write G as (V,E) or (V,E, f) when it is clear from the context. The size of G, denoted by |G|, is defined to be the total number of nodes and edges in G, i.e., |G| = |V | + |E|. Remark. To simplify the discussion, we do not explicitly define edge labels. Nonetheless, our techniques can be readily adapted to edge labels: for each labeled edge e, we can insert a  íì§¸dummy íì§¹ node to represent e, carrying e  íís label. Labeled set. For a set S  íì¨ íì§ííª of labels, we say that VS  íì¨ V is a S-labeled set of G if (a) |VS | = |S| and (b) for each label lS in S, there exists a node v in VS such that f(v) = lS . In particular, when S = ?, the S-labeled set in G is ?. Common neighbors. A node v is called a neighbor of another node v íí in G if either (v, v íí) or (v íí, v) is an edge in G. We say that v is a common neighbor of a set VS of nodes in G if for all nodes v íí in VS , v is a neighbor of v íí. In particular, when VS is ?, all nodes of G are common neighbors of VS . Subgraphs. Graph Gs = (Vs, Es, fs, íì§íì§¯s) is a subgraph of G if Vs  íì¨ V , Es  íì¨ E, and for each (v, v íí)  íì¨ Es, v  íì¨ Vs and v íí  íì¨ Vs, and for each v  íì¨ Vs, fs(v) = f(v) and íì§íì§¯s(v) = íì§íì§¯(v). Pattern queries. A pattern query Q is a directed graph (VQ, EQ, fQ, gQ), where (1) VQ, EQ and fQ are analogous to their counterparts in data graphs; and (2) for each node u in VQ, gQ(u) is the predicate of u, defined as a conjunction of atomic formulas of the form fQ(u) op c, where c is a constant, and op is one of =, >, <,  íí and  íí. For instance, in pattern Q0 of Fig. 1, gQ(year) = year  íí 2011  íì© year  íí 2013. We simply write Q as (VQ, EQ) or (VQ, EQ, fQ). We consider two semantics of graph pattern matching. Subgraph queries. A match of Q in G via subgraph isomor- phism [31] is a subgraph G íí(V  íí, E íí, f  íí) of G that is isomorphic to Q, i.e., there exists a bijective function h from VQ to V  íí such that (a) (u, u íí) is in EQ if and only if (h(u), h(u íí))  íì¨ E íí, and (b) for each u  íì¨ VQ, fQ(u) = f  íí(h(u)) and gQ(íì§íì§¯(h(u))) evaluates to true, where gQ(íì§íì§¯(h(u))) substitutes íì§íì§¯(h(u)) for fQ(u) in gQ(u). Here Q(G) is the set of all matches of Q in G. Simulation queries. A match of Q in G via graph simula- tion [20] is a binary match relation R  íì¨ VQ íì©V such that (a) for each (u, v)  íì¨ R, fQ(u) = f(v) and gQ(íì§íì§¯(v)) evaluates to true, where gQ(íì§íì§¯(v)) substitutes íì§íì§¯(v) for fQ(u) in gQ(u); (b) for each node u in VQ, there exists a node v in V such that (i) (u, v)  íì¨ R, and (ii) for any edge (u, u íí) in Q, there exists an edge (v, v íí) in G such that (u íí, v íí)  íì¨ R. For any Q and G, there exists a unique maximum match relation RM via graph simulation (possibly empty) [20]. Here Q(G) is defined to be RM . Simulation queries are widely used in social community analysis and social marketing [9]. 163 Data locality. A query Q is localized if for any graph G that matches Q, any node u and neighbor u íí of u in Q, and for any match v of u in G, there must exist a match v íí of u íí in G such that v íí is a neighbor of v in G. Subgraph queries are localized. In contrast, simulation queries are non-localized. Example 2: Consider a simulation query Q1 and graph G1 shown in Fig. 2, where G1 matches Q1. Then Q1 is not localized: u2 matches v2, . . . , v2n?2 and v2n, but for all k  íì¨ [2, n], v2k?2 has no neighbor in G that matches the neighbor u3 of u2 in Q. To decide whether u2 matches v2, we have to inspect all the nodes on an unbounded cycle in G1. 2 We will study effective boundedness for subgraph queries in Sections III?V, and then extend the results to non-localized simulation queries in Section VI. To formalize effectively bounded patterns, we first define access constraints on graphs. Access schema on graphs. An access schema A is a set of access constraints of the following form: S  íì§ (l, N), where S  íì¨ íì§ííª is a (possibly empty) set of labels, l is a label in íì§ííª, and N is a natural number. A graph G(V,E, f) satisfies the access constraint if ? for any S-labeled set VS of nodes in V , there exist at most N common neighbors of VS with label l; and ? there exists an index on S for l such that for any S-labeled set VS in G, it finds all common neighbors of VS labeled with l in O(N)-time, independent of |G|. We say that G satisfies access schema A, denoted by G |= A, if G satisfies all the access constraints in A. An access constraint is a combination of (a) a cardinality constraint and (b) an index on the labels of neighboring nodes. It tells us that for any S-node labeled set VS , there exist a bounded number of common neighbors Vl labeled with l and moreover, Vl can be efficiently retrieved with the index. Two special types of access constraints are as follows: (1) |S| = 0 (i.e., ?  íì§ (l, N)): for any G that satisfies the constraint, there exist at most N nodes in G labeled l; and (2) |S| = 1 (i.e., l  íì§ (l íí, N)): for any G that satisfies the access constraint and for each node v labeled with l in G, at most N neighbors of v are labeled with l íí. Intuitively, constraints of type (1) are global cardinality constraints on all nodes labeled l, and those of type (2) state cardinality constraints on l íí-neighbors of each l-labeled node. Example 3: Constraints C1-C6 on IMDb given in Example 1 can be expressed as access constraints ?i (for i  íì¨ [1, 6]): ?1: (year, award) íì§ (movie, 4); ?4: ?  íì§ (year, 135); ?2: movie íì§ (actors/actress, 30); ?5: ?  íì§ (award, 24); ?3: actor/actress íì§ (country, 1); ?6: ?  íì§ (country, 196). Here ?2 denotes a pair movie  íì§ (actors, 30) and movie  íì§ (actress, 30) of access constraints; similarly for ?3. Note that ?4 ? ?6 are constraints of type (1); ?2 ? ?3 are of type (2); and ?1 has the general form: for any pair of year and award nodes, there are at most 4 movie nodes connected to both, i.e., an award is given to at most 4 movies each year. We use A0 to denote the set of these access constraints. 2 Effectively bounded patterns. A pattern query Q is effectively bounded under an access schema A if for all graphs G that satisfy A, there exists a subgraph GQ of G such that (a) Q(GQ) = Q(G); and (b) GQ can be identified in time that is determined by Q and A only, not by |G|. By (b), |GQ| is also independent of the size |G| of G. Intuitively, Q is effectively bounded under A if for all graphs G that satisfy A, Q(G) can be computed by accessing a bounded GQ rather than the entire G, and moreover, GQ can be efficiently accessed by using access constraints of A. For instance, as shown in Example 1, query Q0 is effec- tively bounded under the access schema A0 of Example 3. Discovering access constraints. From experiments with real- life data we find that many practical queries are effectively bounded under simple access constraints S  íì§ (l, N) when |S| is at most 3. We discover access constraints as follows. (1) Degree bounds: if each node with label l has degree at most N , then for any label l íí, l íì§ (l íí, N) is an access constraint. (2) Constraints of type (1): such global constraints are quite common, e.g., ?6 on IMDb: ?  íì§ (country, 196). (3) Functional dependencies (FDs): our familiar FDs X  íì§ A are access constraints of the form X  íì§ (A, 1), e.g., movie íì§ year is an access constraint of type (2): movie  íì§ (year, 1). Such constraints can be discovered by shredding a graph into relations and then using available FD discovery tools. (4) Aggregate queries: such queries allow us to discover the semantics of the data, e.g., grouping by (year, country, genre) we find (year, country, genre)  íì§ (movie, 1800), i.e., each country releases at most 1800 movies per year in each genre. Maintaining access constraints. The indices in an access schema can be incrementally and locally maintained in re- sponse to changes to the underlying graph G. It suffices to inspect ?G  íì¨ NbG(?G), where ?G is the set of nodes and edges deleted or inserted, and NbG(?G) is the set of neighbors of those nodes in ?G, regardless of how big G is. III. EFFECTIVE BOUNDEDNESS OF SUBGRAPH QUERIES To make practical use of effective boundedness, we first answer the following question, denoted by EBnd(Q,A): ? Input: A pattern query Q(VQ, EQ), an access schema A. ? Question: Is Q effectively bounded under A? We start with subgraph queries. The good news is that (a) there exists a sufficient and necessary condition, i.e., a characterization, for deciding whether a subgraph query Q is effectively bounded under A; and better still, (b) EBnd(Q,A) is decidable in low polynomial time in the size of Q and A, independent of any data graph. 164 We prove these results in the rest of the section. A. Characterizing the Effective Boundedness The effective boundedness of subgraph queries is charac- terized in terms of a notion of coverage, given as follows. The node cover of A on Q, denoted by VCov(Q,A), is the set of nodes in Q computed inductively as follows: (a) if ?  íì§ (l, N) is in A, then for each node u in Q with label l, u  íì¨ VCov(Q,A); and (b) if S  íì§ (l, N) is in A, then for each S-labeled set VS in Q, if VS  íì¨ VCov(Q,A), then all common neighbors of VS in Q that are labeled with l are also in VCov(Q,A). Intuitively, a node u is covered by A if in any graph G sat- isfying A, there exist a bounded number of candidate matches of u, and the candidates can be retrieved by using indices in A. Obviously, (a) u is covered if its candidates are bounded by type (1) constraints. (b) If for some ? = S  íì§ (l, N) in A, u is labeled with l and is a common neighbor of VS that is covered by A, then u is covered by A, since its candidates are bounded (by N and the bounds on candidate matches of VS), and can be retrieved by using the index of ?. The edge cover of A on Q, denoted by ECov(Q,A), is the set of edges in Q defined as follows: (u1, u2) is in ECov(Q,A) if and only if there exist an access constraint S  íì§ (l, N) in A and a S-labeled set VS in Q such that (1) u1 (resp. u2) is in VS and VS  íì¨ VCov(Q,A) and (2) fQ(u2) = l (resp. fQ(u1) = l). Intuitively, (u1, u2) is in ECov(Q,A) if one of u1 and u2 is covered by A and the other has a bounded number of candidate matches by S  íì§ (l, N). Thus, we can verify their matches in a graph G by accessing a bounded number of edges. Note that VCov(Q,A)  íì¨ VQ and ECov(Q,A)  íì¨ EQ. The node and edge covers characterize effectively bounded subgraph queries (see [3] for a proof, which uses three lemmas and the data locality of subgraph queries). Theorem 1: A subgraph query Q is effectively bounded under an access schema A if and only if (iff) VCov(Q,A) = VQ and ECov(Q,A) = EQ. 2 Example 4: For query Q0(V0, E0) of Fig. 1 and access schema A0 of Example 3, one can verify that VCov(Q0,A0) = V0 and ECov(Q0,A0) = E0. From this and Theorem 1 it follows that Q0 is effectively bounded under A0. 2 B. Checking Effectively Bounded Subgraph Queries Capitalizing on the characterization, we show that whether Q is effectively bounded under A can be efficiently decided. Theorem 2: For subgraph queries Q, EBnd(Q,A) is in (1) O(|A||EQ|+ ||A|||VQ|2) time in general; and (2) O(|A||EQ|+ |VQ|2) time when either ? for each node in Q, its parents have distinct labels; or ? all access constraints in A are of type (1) or (2). 2 Algorithm EBChk Input: A subgraph query Q and an access schema A. Output:  íì§¸yes íì§¹ if Q is effectively bounded and  íì§¸no íì§¹ otherwise. 1. for each S  íì§ (l, N) in A (S 6= ?) do 2. find all V? uS 7 íì§ (u,N) in Q and add them to íì§íí; /*f(u) = l*/ 3. B := {v  íì¨ VQ | ?  íì§ (fQ(v), N) is in A}; 4. C := B; /*Initialize VCov(Q,A)*/ 5. InitAuxi(L, ct); /*Initialize auxiliary structures*/ 6. while B is not empty do 7. v = B.pop(); 8. for each íì§íì¨ in L[v] do 9. Update (ct[íì§íì¨]); /*Update counter ct[íì§íì¨]*/ 10. if ct[íì§íì¨] = ? and u 6 íì¨ C do /*suppose íì§íì¨: V? uS 7 íì§ (u,N)*/ 11. B := B  íì¨ {u}; C := C  íì¨ {u}; 12. if VQ  íì¨ C and all edges in Q are in ECov(Q,A) then 13. return  íì§¸yes íì§¹; 14. return  íì§¸no íì§¹; Fig. 3. Algorithm EBChk Here |A| denotes the total length of access constraints in A, ||A|| is the number of constraints in A, and a node u íí is a parent of u in Q if there exists an edge from u íí to u in Q. Algorithm. We prove Theorem 2 by providing a checking algorithm. The algorithm is denoted by EBChk and shown in Fig. 3. Given a subgraph query Q(VQ, EQ) and an access schema A, it checks whether (a) VQ  íì¨ VCov(Q,A) and (b) EQ  íì¨ ECov(Q,A); it returns  íì§¸yes íì§¹ if so, by Theorem 1. To check these conditions, we actualize A on Q: for each S  íì§ (l, N) in A (S 6= ?), and each node u in Q with fQ(u) = l, the actualized constraint is V? uS 7 íì§ (u,N), where V? uS is the maximum set of neighbors of u in Q such that (a) there exists a S-labeled set VS  íì¨ V? uS and (b) for each u íí in V? uS , fQ(u íí)  íì¨ S. Actualized constraints help us deduce VCov(Q,A): a node u of Q is in VCov(Q,A) if and only if either ? there exists ?  íì§ (l, N) in A and fQ(u) = l; or ? V? uS 7 íì§ (u,N) and there exists a S-labeled set of Q that is a subset of V? uS  íì¨© VCov(Q,A). When VCov(Q,A) is in place, we can easily check whether EQ  íì¨ ECov(Q,A) by definition and using the actualized constraints, without explicitly computing ECov(Q,A). We next present the details of algorithm EBChk. Auxiliary structures. EBChk uses three auxiliary structures. (1) It maintains a set B of nodes in Q that are in VCov(Q,A) but it remains to be checked whether other nodes can be deduced from them. Initially, B includes nodes whose labels are covered by type (1) constraints in A (line 3). EBChk uses B to control the while loop (lines 5-10): it terminates when B = ?, i.e., all candidates for VCov(Q,A) are found. (2) For each node v, EBChk uses an inverted index L[v] to store all actualized constraints V? uS 7 íì§ (u,N) such that v  íì¨ V? uS . That is, L[v] indexes these constraintsthat can be used on v. (3) For each actualized constraint íì§íì¨ = V? uS 7 íì§ (u,N), EBChk maintains a set ct[íì§íì¨] to keep track of those labels of S that are not covered by nodes in V? uS  íì¨© VCov(Q,A) yet. Initially, ct[íì§íì¨] = S. When ct[íì§íì¨] is empty, EBChk concludes that there 165 is a S-labeled subset of V? uS covered by VCov(Q,A), and thus deduces that u should also be in",Yang Cao,"RCBD and SKLSDE Lab, Beihang University",yang.cao@ed.ac.uk,Wenfei Fan,University of Edinburgh,wenfei@inf.ed.ac.uk,Jinpeng Huai,"RCBD and SKLSDE Lab, Beihang University",huaijp@buaa.edu.cn,Ruizhe Huang,University of Edinburgh,s1335233@sms.ed.ac.uk,,,,,,,,,,,,,,,,,,
20200106,1389,Abdeltawab M. Hendawi,"Department of Computer Science and Engineering, University of Minnesota, Minneapolis, MN, USA",hendawi@cs.umn.edu,,Predictive Tree: An Efficient Index for Predictive Queries On Road Networks,"Abstract Predictive queries on moving objects offer an im- portant category of location-aware services based on the objects-expected future locations. A wide range of applications utilize this type of services, e.g., traffic management systems, location-based advertising, and ride sharing systems. This paper proposes a novel index structure, named Predictive tree (P-tree), for process- ing predictive queries against moving objects on road networks. The predictive tree: (1) provides a generic infrastructure for answering the common types of predictive queries including predictive point, range, KNN, and aggregate queries, (2) updates the probabilistic prediction of the object's future locations dynamically and incrementally as the object moves around on the road network, and (3) provides an extensible mechanism to customize the probability assignments of the object's expected future locations, with the help of user defined functions. The proposed index enables the evaluation of predictive queries in the absence of the objectsì°½í² historical trajectories. Based solely on the connectivity of the road network graph and assuming that the object follows the shortest route to destination, the predictive tree determines the reachable nodes of a moving object within a specified time window T in the future. The predictive tree prunes the space around each moving object in order to reduce computation, and increase system efficiency. Tunable threshold parameters control the behavior of the predictive trees by trading the maximum prediction time and the details of the reported results on one side for the computation and memory overheads on the other side. The predictive tree is integrated in the context of the iRoad system in two different query processing modes, namely, the precomputed query result mode, and the on-demand query result mode. Extensive experimental results based on large scale real and synthetic datasets confirm that the predictive tree achieves better accuracy compared to the existing related work, and scales up to support a large number of moving objects and heavy predictive query workloads. I. INTRODUCTION The availability of hundreds of millions of smart phones [6] in usersì°½í² hands during their movements in daily lives fired the explosion of a vast number of location aware services [5], [11], [20], [29]. Predictive queries [10], [12], [13] offer a fundamental type of location-based services based on usersì°½í² future locations. Common types of predictive spatial queries include predictive range query, e.g., ì°½íµfind all hotels that are This work is partially supported by the National Science Foundation, USA, under Grants IIS-0952977 and IIS-1218168. located within two miles from a userì°½í²s anticipated location after 30 minutesì°½íµ, predictive KNN query, e.g., ì°½íµfind the three taxis that are closest to a userì°½í²s location within the next 10 minutesì°½íµ, and predictive aggregate query, e.g., ì°½íµfind the number of cars expected to be around the stadium during the next 20 minutesì°½íµ. In fact, Predictive queries are beneficial in various types of real applications such as (1) traffic management, to predict areas with high traffic in the next half hour, so appropriate decisions are taken before congestion appears, (2) location- aware advertising, to distribute coupons and sales promotions to customers more likely to show up around a certain store during the sale time in the next hour, (3) routing services, to take into consideration the predicted traffic on each road segment to find the shortest path of a userì°½í²s trip starting after 15 minutes from the present time, (4) ride sharing systems, to match the drivers that will pass by a riderì°½í²s location within few minutes, and (5) store finders, to recommend the closest restaurants to a userì°½í²s predicted destination in 15 minutes. In this paper, we address the problem of how to process pre- dictive queries for moving objects on road networks efficiently. To this end, we introduce a novel index structure, named the Predictive tree (P-tree), proposed to precompute the predicted moving objects around each node in the underlying road network graph over time. The predictive tree is best described as generic and extensible, from a functionality perspective, dynamic and tunable from a performance perspective. A. Challenges Existing studies on predictive query processing have gone a long way in advancing predictive location-based services. However, existing techniques suffer from both functional limitations and performance deficiencies. From a functional perspective, they suffer from one or more of the following limitations: (1) They consider an Euclidean space [9], [28], [25], [30] where objects can move freely in a two dimensional space. Yet, practical predictive location-based services target moving objects on road networks as described by the motivat- ing applications earlier in this section. (2) Many techniques utilize prediction models that must be trained using a massive 978-1-4799-7964-6/15/$31.00 ? 2015 IEEE ICDE Conference 20151215 a mount of objectsì°½í² historical trajectories in order to produce accurate predictions [2], [9], [13], [15], [24], [28]. However, practical scenarios and industrial experience reveal that such historical data is not easily obtainable for many reasons, either due to usersì°½í² privacy and data confidentiality on one side or due to the unavailability of historical data in rural areas on the other side. (3) Most of the previous solutions were designed to support a specific query type only, e.g., [13], [25], [30] support predictive range query, [3], [22], [30] support predictive KNN query, and [9], [24] support predictive aggregate query. B. Approach Before summarizing the contributions of the proposed pre- dictive tree index, we briefly highlight the basic idea of the index in order to build the proper context. Once an object starts a trip, we construct a predictive tree for this object such that the objectì°½í²s start node in the road network graph becomes the root of the tree. The predictive tree consists of the nodes reachable within a certain time frame T from the objectì°½í²s start location. More specifically, we assume that moving objects follow shortest paths during their travel from source to destination [16], [18]. Hence, we organize the nodes inside the predictive tree according to the shortest path from the objectì°½í²s start node, which is marked as the root of the tree. Accordingly, each branch from the root node to any node in the tree represents the shortest route from the root to this node. Then, our prediction is based on a probability assignment model that assigns a probability value to each node inside the objectì°½í²s predictive tree. In general, the probability assignment is made according to the nodeì°½í²s position in the tree, the travel time between the object and this node and the number of the sibling nodes. In practice, the probability assignment process is tricky and varies from one application to another. At each node in the given road network that is indexed in R-tree, we keep track of a list of objects predicted to appear in this node, along with their probabilities, and travel time cost from the objectsì°½í² current locations to this node. This list represents a raw precomputed answer that can be customized according to the type of the received query (i.e., point, range or kNN predictive query) at query processing time. When an object moves from its current node to a different node, we incrementally update the predictive tree by pruning all nodes in the tree that are no longer accessible through a shortest route from the objectì°½í²s new location. Mostly, this pruning shrinks the number of possible destinations, yet, increases the focus of the prediction. Consequently, the precomputed answer at each node in the object predictive tree is updated to accommodate the effect of the objectì°½í²s movements. This update is reflected to the original nodes in the road network. When a predictive query is received, we fetch the up-to-date answer from the node of interest and compile it according to the query type. To adjust the behavior of the predictive tree and, hence, control the overall predictive query processing performance, we leverage two tunable parameters, a maximum time T and a probability threshold P . These parameters compromise between the maximum prediction time a predictive tree can support and the details in the reported query results on one side, and system resources overheads, i.e., CPU and memory, on the other side. The proposed predictive tree is implemented within the iRoad framework. The iRoad offers two query processing modes of leveraging the predictive tree to control the inter- action between its components: (1) the precomputed query result mode, in which the predicted results are computed and materialized in advance; and (2) the on-demand query result mode which is a lazy approach that postpones all computation till a query is received. C. Contributions In general, the contributions of this paper can be summa- rized as follows: ? We propose a novel data structure named Predictive tree (P-tree) that supports predictive queries against moving objects on road networks. ? We introduce a probability model that computes the like- lihood of a node in the road network being a destination to a moving object. The probability model is introduced to the predictive tree as a user defined function and is handled as a black box by the index construction and maintenance algorithms. ? We introduce two tunable parameters T and P that are experimentally proved to be efficient tools to control the predictive tree index, the system performance, the prediction time, and the results details as well. ? We provide an incremental approach to update the pre- dictive tree as objects move around. Hence, we utilize the existing index structure and incur minimal cost in response to the movement of the object. ? We propose the iRoad framework that leverages the introduced predictive tree to support a wide variety of predictive queries including predictive point, range, and KNN queries. ? we provide an experimental evidence based on real and synthetic data that our introduced index structure is efficient in terms of query processing, scalable in terms of supporting large number of moving objects and heavy query workloads, and achieves a high-quality prediction without the need to reveal objectsì°½í² historical data. The remainder of this paper is organized as follows. Sec- tion II sets the preliminaries and defines our problem. Sec- tion III presents the iRoad system. Section IV describes the the predictive tree and its associated construction, maintenance and querying algorithms. Experimental results are presented in Section V. The study of related work is given in Section VI. Finally, Section VII concludes the paper. II. PRELIMINARIES In this section, we formalize the basic predictive query we address in this paper. Then, we define different types of predictive queries that the predictive tree can support within the iRoad framework. After that, we explain the intuition of the leveraged prediction model. 1216 Fig. 1. iRoad System Architecture A. Basic Query In this paper, we focus on addressing the predictive point query as our basic query on the road network. In this query, we want to find out the moving objects with their corresponding probabilities that are expected to be around a specified query node in the road network within a future time period. The example of such query could be like, ì°½íµFind out all the cars that may pass by my location in the next 10 minsì°½í¶. The predictive point query we address in this paper can be formalized as: ì°½íµGiven (1) a set of moving objects O, (2) a road network graph G(N, E, W), where N is the set of nodes, E is the set of edges, and W is the edge weights, i.e., travel times, and (3) a predictive point query Q(n, t), where n ì°½í°í N, and t is a future time period, we aim to find the set of objects R ì°½í°í O expected to show up around the node n within the future time t. The returned result should identify the objects along with their probabilities to show up at the node of interest. For example, within the next 30 mins, object o1 is expected to be at node n3 with probability 0.8, R(Q(n3,30)) = {< o1,0.8>}. B. Extensions We consider the aforementioned predictive point query as a building block upon which our framework can be extended to support other types of predictive queries including: (i) Predictive range query, where a user defines a query region that might contain more than one node and asks for the list of objects expected to be inside the boundaries of that region within a specified future time, (ii) Predictive KNN query to find out the most likely K objects expected to be around the node of interest within a certain time period, and (iii) Predictive aggregate query to return the number of objects predicted to be within a given location in the next specified time duration. C. Prediction Model Our prediction model employed by the introduced predictive tree index structure is based on two corner stones. (1) The assumption that objects follow the shortest paths in their routing trips. The intuition behind this assumption is based on the fact that in most cases, the moving objects on road networks, e.g., vehicles, travel through shortest routes to their destinations [16], [18]. In fact, this assumption is aligned with the observation in [4] that moving objects do not use random paths when traveling through the road network, rather they follow optimized ones, e.g., fastest route. As a result, this (a) Network & Objects (b) Predictive Trees Integrated With R-Tree Fig. 2. Example Of The Proposed Index Structure model prevents the looping case that appears in the traditional turn-by-turn probability model and assigns a probability value for the moving object to turn when facing an intersection [13]. (2)The probability assignment model that assigns a proba- bility value to each node inside the objectì°½í²s predictive tree. In fact, the probability assignment is affected by the nodeì°½í²s position with respect to the root of the tree, the travel time cost between the object in its current location to this node, and the number of the sibling nodes. In general, our predictive tree is designed to work with different probability assignment models. For example, a possible probability model can give higher values to nodes in business areas, e.g., down town, rather than those in the suburbs. In our default probability model, each node in the predictive tree has a value equal to one divided by the number of nodes accessible from the root within a certain time range. III. THE IROAD SYSTEM The proposed predictive tree is implemented in the context of the iRoad System. More precisely, the predictive tree and its construction, maintenance and querying algorithms form the core of the iRoad System. The iRoad System is a scalable framework for predictive query processing and analysis on road networks. The architecture of the iRoad system consists of three main modules, namely, the state manager, the pre- dictive tree builder and the query processor, Figure 1. In this section, we present an overview of the iRoad System and give a brief description of its key components. Moreover, we focus on the interaction and workflow between these components under both the precomputed query result mode and the on- demand query result mode. A. State Manager The state manager is a user facing module that receives a stream of location updates from the moving objects being monitored by the system. The state manager maintains the following data structures. (1) An R-tree [7] that is generated on the underlying road network graph. It differs from the conventional R-tree in that at each leaf node, i.e., a node in the road network, in addition to storing the corresponding MBR, it also keeps track of two lists: (a) current objects that records the pointers to the objects around this node, and (b) 1217 predicted objects that maintains the predicted results of the objects that most likely to show up around that node. (2) A trajectory buffer that stores the most recent one or more nodes in the road network that are visited by the moving object in its ongoing trip. (3) A predictive tree such that root of a predictive tree is the current location of the moving object. Figure 2(a) gives an example of a set of objects moving on a road network, while Figure 2(b) depicts how the predictive trees are integrated within the basic data structures layout to facilitate the processing of predictive queries. As we mentioned, the system can be running under either (1) a precomputed query result mode or (2) an on-demand query result mode. The first is the default mode inside the iRoad framework. In either modes, upon the receipt of a location update of a moving object, the R-tree is consulted and the new location is mapped to its closest node Nnew in the road network. If the new node Nnew is the same as the objectì°½í²s old node Nold, the object movement is not significant enough to change the systemì°½í²s state and no further action is taken. Otherwise, the object has moved to a different node and an evaluation of the impact of the objectì°½í²s movement is triggered in the system. We differentiate between the precomputed and the on-demand query result modes as follows. Precomputed query result mode: In this mode, the predic- tive tree builder is invoked immediately once the moving ob- ject changes its current node and, consequently, the predictive tree is either constructed from scratch or updated in response to the objectì°½í²s movement. Remember that the predictive tree is constructed from scratch if the incoming location update belongs to a new object that is being examined by the system for the first time. Also, the predictive tree is constructed from scratch if Nnew is not a child of the root of the objectì°½í²s in- hand predictive tree. As will be described in Section IV, this case happens if the object decided not to follow the shortest path, e.g., made a u-turn or started a new trip. Otherwise, the tree is incrementally maintained. Note that, in this mode, the trajectory buffer data structure boils down to one single node (i.e., the current node) of the moving object because of the eagerness to update the predictive tree with the receipt of every location update. Hence, the past trajectory is entirely factored in the predictive tree. On-demand query result mode: In this mode, the tra- jectory buffer stores all nodes the moving object passed by since the start of its current trip. Initially, We do not perform any computation until a query is received. Then, we identify the vicinity nodes within the time range determined by the query. Those nodes might contribute in the predicted results. For each object in these nodes, we construct its predictive tree and run a series of updates according to the list of passed nodes in its trajectory buffer, Figure 3. For example, in this figure, nodes A, G, and E are within the time range specified in the query at node B. Then, we construct the predictive tree for each object, O1, O2, O3, in those nodes and update them according the passed nodes by each one. Obviously, O1, O3 will contribute in the predicted objects at node B, while O2 will not contribute as node B is no longer a possible destination Fig. 3. On-Demand Approach for O2 based on its trajectory buffer. Then, we get rid of any data structure, i.e., the predictive trees and predicted results, directly once the query processing is completed and the results are carried back to the query issuer. We ending by adding the objectì°½í²s current node Nnew, i.e,. Node B in this example, to the objectì°½í²s trajectory buffer. B. Predictive Tree Builder The predictive tree builder is the component that encom- passes the predictive tree construction and maintenance algo- rithms. It takes as input, (1) the moving objectì°½í²s trajectory buffer, (2) the moving objectì°½í²s current predictive tree (if exists), (3) the tunable parameters (T and P) that trade the prediction length and accuracy for systemì°½í²s resources, and (4) a user defined probability assigned function. The predictive tree builder reflects the most recent movements of the object (as recorded in the objectì°½í²s trajectory buffer) to the objectì°½í²s predictive tree. Upon the completion of a successful invocation of the tree builder, an up-to-date predictive tree rooted at the objectì°½í²s current location is obtained and the objectì°½í²s trajectory buffer is modified to accommodate the objectì°½í²s current node. The predictive tree builder is invoked in two different ways. In a precomputed query result mode, the builder is invoked by the state manager upon the receipt of every location update. The state manager pushes the incoming location update of a moving object Oi to the predictive tree builder that eagerly reflects the location update in the predictive tree of Oi. Afterwards, the tree builder updates the precomputed query results at every node in the road network that is on the shortest path route from the object Oiì°½í²s current location. In an on-demand query result mode, the predictive tree builder is invoked by the query processor once a query Q is received. The predictive tree builder consults the road network graph and retrieves a list of nodes Nvicinity that are within the time distance determined by the query Q. Then, it pulls, from the state manager, the predictive trees and the trajectory buffers of moving objects whose current nodes are in Nvicinity . In other words, lazy or selective processing of moving objects that are believed to affect the query result is carried over without taking the burden of updating the predictive tree of every single moving object in the system. C. Query processor The main goal behind predictive query processing in the iRoad system is to be generic and to provide an infrastructure for various query types. This goal is achieved by mapping a query type to a set of nodes (Nvicinity) in the road network graph such that the query result is satisfied by predictions 1218 Algorithm 1 Predictive Tree Construction Input: Node n, T ime Range T , Road Network Graph G(N,E,W ) 1: Step 1. Initialize the data structures 2: Set n as the root of the Predictive Tree PT 3: Visited nodes list NL $ ? 4: Min-Heap MH $ ? 5: for all Edge ei connected with n do 6: Insert the node ni ì°½í°í ei íì§¸C MH 7: end for 8: Step 2. Expand the road network and create the predictive tree 9: while the minimum time range Tmin in MH < T do 10: Get the node nmin with Tmin from MH 11: if The nmin /ì°½í°í NL then 12: Insert nmin íì§¸C PT 13: Insert nmin íì§¸C NL 14: for all Edge ej connected with nmin do 15: Insert the node nj íì§¸C MH 16: end for 17: end if 18: end while 19: Return PT associated with these nodes. For predictive point queries as an example, the point query is answered using the information associated with the road network node that is closest to the query point. For predictive range queries, all nodes in the range are considered to compute the query result. For predictive KNN queries, we sort those predicted objects associated with Nvicinity based on their probabilities. Nvicinity is rationally expanded till K objects are retrieved, if visible. In a precomputed query result mode, generic results are prepared in advance and are held in memory. The process is triggered by an update in objectì°½í²s location and the precom- puted results are constructed/updated for all nodes along the shortest path route of that object. Therefore, most of the work is done during the location update time. Upon the receipt of a query, the query processor fetches the precomputed results only from nodes in Nvicinity , adapts them according to the type of the received query and gives a low latency response back to the user. In an on-demand query result mode, nothing is precomputed in advance and all computation will be performed after the receipt of the userì°½í²s query. Nvicinity is identified and the pre- dictive tree of objects whose current node belong to Nvicinity are constructed/upadted as described earlier in this section. Then, the results are collected and adapted to the query type in a similar way to the precomputed result approach. IV. PREDICTIVE TREE In this section, we describe the proposed predictive tree index structure that is leveraged inside the iRoad framework to process predictive queries based on the predicted destinations of the moving objects within a time period T . We first introduce the main idea and the motivation to build the predictive tree. After that, we provide a detailed description for the two main operations in the predictive tree: 1) predictive tree construction, and 2) predictive tree maintenance. The idea of the predictive tree is to identify all the possible destinations that a moving object could visit in a time period T by traveling through the shortest paths. As there may only exist one shortest path from a start node to a destination node, we can guarantee it will be a tree structure (i.e., without any loop). The intuition for constructing the predictive tree with a time boundary T is based on two real facts: 1) most of the moving objects travel through shortest path to their destinations [16], [18], and 2) majority of the real life trips are within a time period, e.g., 19 minutes [17], [18]. As a result, we only need to care about the possible destinations reachable through a shortest route from the objectì°½í²s start location within a bounded time period. Based on that, we build the predictive tree to hold only the accessible nodes around a moving object and assign a probability for each one of them. The predictive trees leveraged in the iRoad system signif- icantly improves the predictive query processing efficiency for two main reasons. 1) The possible destinations of the prediction shrinks as a result of using the time boundary T . Yet, prediction computation is performed on few number of nodes instead of millions of nodes in the underlying road network, e.g., road network of California state in USA has about 1,965,206 nodes and 5,533,214 edges [23]. 2) Inside the predictive tree, we maintain only those nodes with probability higher than a certain probability threshold parameter P , e.g., 10%. By doing this, we cut down the computation overhead consumed for continuously maintaining the predicted results at each node in the predictive trees. Moreover, we control iRoad to focus on those nodes that more likely to be reached by a moving object. Yet, the query reported results can be more reasonable to users. A. Predictive Tree Construction Main idea. When a moving object starts its trip on the road network, we build a predictive tree based on its starting location to predict its possible destinations within a certain time frame T . We propose a best-first network expansion algorithm for constructing predictive tree for time period T , e.g., 30 minutes. We set the objectì°½í²s initial node as the start node, then, we visit the nodes and edges on the road network that are reachable using a shortest path from this start node [21]. The algorithm proceeds to traverse and process the edges in the road network based on the travel time cost from the start node until all the costs to the remaining edges are over T . Algorithm. The pseudo code for the predictive tree con- struction algorithm is given in Figure 1. The algorithm takes the road network G = {N,E,W}, a starting node n and a time range T as input. The algorithm consists of two main steps: ? Initialization. We first initialize the predictive tree under construction by setting the start node n as the root of the tree. We also create the visited nodes list NL to store the nodes that have been processed by the algorithm so far. An empty min-heap, MH , is employed to order the nodes based on its distance to the root node n. After that, we insert the nodes that are directly connected with the 1219 Fig. 4. Example of Constructing And Expanding The Predictive Tree Started At Node A. root node n into the min-heap MH , (Lines from 2 to 7 in Algorithm 1). ? Expansion. We continuously pop the node nmin that is the closest to the root node from the min-heap. Then, we check if that node has been visited by our algorithm before, which means there was a shorter path from the root to this node nmin. If visited nodes list NL does not contain nmin, we insert the node nmin to it as well as a child to the current expanding branch of the predictive tree PT . After that, we insert to the min-heap MH the node nj that is connected with the yet processed node nmin for further expansion. The algorithm stops when the distance between the next closest node in the min- heap is over the boundary T , (Lines from 8 to 18 in Algorithm 1). Example. Figure 4 gives an example for constructing a predictive tree for node A from the given road network. For this example, we set the time period T to 20 minutes. Figure 4(a) gives the original road network structure, where circles represent nodes and lines between nodes represent edges and the number on each edge represents the time cost to traverse that edge. In the first iteration, we start by setting the root of the tree to node A. Then, we insert nodes B and C into the min-heap, as they are the connected ones to the root node A, Figure 4(b). After that, we expand the closest child to the root, B, where we insert D and E into the min- heap MH and put B in the predictive t",Abdeltawab M. Hendawi,"Department of Computer Science and Engineering, University of Minnesota, Minneapolis, MN, USA",hendawi@cs.umn.edu,Jie Bao,"Department of Computer Science and Engineering, University of Minnesota, Minneapolis, MN, USA",baojie@cs.umn.edu,Mohamed F. Mokbel,"Department of Computer Science and Engineering, University of Minnesota, Minneapolis, MN, USA",mokbel@cs.umn.edu,Mohamed Ali,"Institute of Technology, University of Washington, Tacoma , WA, USA",mhali@uw.edu,,,,,,,,,,,,,,,,,,
20200107,1390,Michael Mattig,"Department of Mathematics and Computer Science University of Marburg, Germany",mattig@mathematik.uni-marburg.de,,Kernel-Based Cardinality Estimation on Metric Data,"ABSTRACT The efficient management of metric data is extremely important in many challenging applications as they occur e.g. in the life sciences. Here, data typically cannot be represented in a vec- tor space. Instead, a distance function only allows comparing individual elements with each other to support distance queries. As high-dimensional data suffers strongly from the curse of di- mensionality, distance-based techniques also allow for better handling of such data. This has already led to the development of a plethora of metric indexing and processing techniques. So far, the important problem of cardinality estimation on metric data has not been addressed in the literature. Standard vector-based techniques like histograms require an expensive and error-prone embedding. Thus, random sampling seems to be the best choice for selectivity estimation so far, but errors are very high for mod- erately small queries. In this paper, we present a native cardinality estimation technique for distance queries on metric data based on kernel-density estimation. The basic idea is to apply kernels to the one-dimensional distance function among metric objects and to use novel global and local bandwidth optimization methods. Our results on real-world data sets show the clear advantage of our method in comparison to its competitors. 1 INTRODUCTION Statistics about the distribution of data in a database are used for two very important aspects of data management: query optimiza- tion and data exploration. In query optimization, they allow esti- mating the costs of operations, choosing appropriate algorithms, and computing the order of joins. For very large databases, where computations take a very long time, small in-memory statistics can deliver approximate answers. Those are often sufficient to determine whether it is worth further investigating the data in a particular direction. While one- and multidimensional vector data is very common in traditional applications, there are many domains for which data is in a metric space only. This means data is not describable by a d-dimensional vector, instead there exists only a metric mea- suring distances between pairs of objects. Examples include the life sciences, where e.g. proteins are usually described by their geometrical structure or at least a sequence of amino acids. Mul- timedia data comes in different datatypes such as JPEG or MPEG which are also not appropriate for a relational representation. In such domains there is a severe lack of native statistical sup- port. Thus, a standard approach is to transform metric data into a multidimensional vector space and to apply one of the standard estimation techniques [18]. There are two serious opposing ef- fects. First, a metric embedding causes in general a considerable information loss. In order to alleviate this, the number of dimen- sions needs to be sufficiently high. Second, the well-known curse ? 2018 Copyright held by the owner/author(s). Published in Proceedings of the 21st International Conference on Extending Database Technology (EDBT), March 26-29, 2018, ISBN 978-3-89318-078-3 on OpenProceedings.org. Distribution of this paper is permitted under the terms of the Creative Commons license CC-by-nc-nd 4.0. of dimensionality is already noticeable for a moderate number of dimensions. Thus, statistics provide only accurate results for low-dimensional vector spaces [1]. In this paper, we present the first native method for cardinality estimation of distance queries in metric spaces. The basic idea is to consider the distances of objects in a metric space and to use kernel techniques to estimate the underlying distance distri- bution. By tuning the bandwidth of the kernels and the kernel function, we obtain a robust estimator for the cardinality of dis- tance queries in metric spaces. Moreover, our approach is also beneficial for high-dimensional vector spaces by treating them as metric spaces, thus considering only the distance among objects, to overcome the shortcomings of standard vectorial statistics. The main contributions of this paper are: ? We show the deficiencies of traditional cardinality estima- tion techniques on metric data sets. ? We present the first effective and efficient method for cardinality estimation in metric space. ? Extensive experiments on real-world data show the valid- ity of our approach. The rest of the paper is structured as follows. Section 2 de- scribes several applications for cardinality estimation in metric spaces, formally defines the problem, and emphasizes the dif- ferences of vector and metric data. Section 3 presents related work in the areas of cardinality estimation in general, techniques for embedding metric data into vector space and kernel-based techniques for cardinality estimation. Section 4 presents our distance-based kernel estimator approach in metric space. Sec- tion 5 describes our methods for global and local bandwidth optimization. Section 6 presents our experimental findings. Fi- nally, Section 7 concludes the paper. 2 PRELIMINARIES We first give a formal description of the problem of cardinality estimation on metric data. Then, we discuss several applications that greatly benefit from a suitable solution to this problem. Fi- nally, we discuss the fundamental differences between vector and metric data that lead to the ineffectiveness of established methods. 2.1 Problem Specification Let X be a set of N objects {x1, . . . ,xN }  íì¨ X. These objects are all of a certain type, in particular a type which can differ from Rn . Moreover a distance function distX : X  íì© X  íì§ R+ is given which fulfills the three properties of a metric, namely (a) identity of indiscernibles: distX(x ,y) = 0íì§íì§ x = y, (b) symmetry: distX(x ,y) = distX(y,x) and (c) triangle inequality: distX(x , z)  íí distX(x ,y)+distX(y, z), with x ,y, z  íì¨ X. We will refer to the combination of X and distX as metric data. In mathematics the pair (X,distX) is called metric space. Cardinality estimation for metric data can be formalized as follows: Given a distance queryQ = (xQ , rQ ), with object xQ  íì¨ X     Series ISSN: 2367-2005 349 10.5441/002/edbt.2018.31 and distance rQ  íì¨ R+, efficiently approximate the cardinality of the set {x  íì¨ X | distX(xQ ,x)  íí rQ }. We will refer to the distance rQ as query radius. The true cardinality is denoted by c(Q) and the estimated cardinality by c?(Q). The goal is to minimize the error of the estimation, but also the construction costs and the size, as well as the query time of the estimator. Note that the actual cardinality can, of course, be calculated by computing the distance to every other item in the data set. This requires a linear number of distance calculations. Each of them can be very costly as, e.g., in video dissimilarity. Hence, we want to minimize the computational costs induced by an estimator. 2.2 Applications Cardinality estimation in metric spaces has many applications. Among others we want to mention the domain of Machine Learn- ing and Data Mining, where algorithms are usually based on a distance measure. The most prominent example is the so-called k-nearest neighbor (kNN) classifier which can be used to classify any kind of data, if it is endowed with a distance measure. The basic idea is to retrieve thek objects from a database which have the smallest distances to a certain query object. Assuming that the objects within the database carry a certain class label (e.g. customers of an insurance with label churn or no churn), the query is classified with the majority label from the set of the k nearest neighbors. kNN classifiers are known to be inefficient since for each run of such an algorithm a complete scan of the data set is required. To accelerate this algorithm typically metric index structures [28] are employed that allow the efficient retrieval of elements within a given distance of a query object. However, it is hard to specify the radius for the corresponding queries since the kNN classifier requires rather the set of k nearest neighbors than a certain set of neighbors exhibiting a certain maximal distance to the query object. For calculating the minimum radius, leading to a retrieval of k results, cardinality estimation can be used. For example, [14] make use of cardinality estimation to assign objects to different Locality Sensitive Hashing tables of different radii. This improves kNN queries on data sets where the distances of the k nearest neighbours of items vary greatly over the data set. A single LSH table could not sufficiently answer such queries. Another example is the estimation of densities, e.g. for Bayes Classifiers, where the density around an element x is propor- tional to the amount of elements in a database that are in a small vicinity to x . This vicinity is typically specified by a certain small distance. Obviously, a reliable cardinality estimation approach would increase the efficiency of such classifiers enormously. If a query is expressed as a conjunction of multiple proximity predicates, each of them with a different distance function, car- dinality estimation is useful for computing an efficient order of their computation. In an optimal execution plan queries should be applied in an order that leads to quickly decreasing result sets. Cardinality estimation can be used to answer exactly this question and to find an order in which the different distance measures are to be applied. Applications in which such scenarios occur are, e.g., pharmaceutical chemistry, where different dis- tance measures covering certain requirements are applied onto protein and/or ligand databases to get the final result in form of a very small set of therapeutically effective drugs. In general, this is a metric scenario, since proteins cannot be described on the structural level by vectors without a considerable loss of information. 2.3 Vector Data vs. Metric Data In order to emphasize the fundamental differences of metric data to vector data that lead to the in-applicability of established methods, we now briefly review important properties of a vector space. We limit our discussion to vector spaces over the real numbers. Here, ad-dimensional vector space consists of elements x = (x1, ...,xd )with a real value xi  íì¨ R called coordinate for each dimension. Individual elements can be added to each other and multiplied with scalar values v  íì¨ R. This, e.g., allows to compute the mean of multiple elements which is not possible in a metric space. Thus one of the most basic data summarization operators is not available in a metric space. Furthermore, the coordinates of the vector allow determining the location of an element with respect to other elements. Such a direction cannot be determined in a metric space. A set of vectors can be ordered globally by component-wise sorting or by a space-filling curve [27] that better preserves the proximity of subsequent elements. In contrast, elements in a metric space can only be ordered based on the distance to a single reference object. Furthermore, it is straight-forward to divide a vector space into a finite number of distinct subsets by incrementally subdividing the space along the dimensions. In a metric space such subsets have to be defined using a center object and a radius. In general, such partitions will overlap if the complete data space should be covered. A vector space has ameasure that allows calculating the vol- ume of subspaces and their intersections. In particular, this is the foundation for the definition of a density and a distribution of a data set. The notions of volume, density and distribution are not available in a metric space. Finally, in a vector space, the costs of distance calculations between elements is linear in the number of dimension if an Lp norm (typically p = 2 for Euclidean distance) is used. In a metric space a distance function can be arbitrarily complex, such as e.g. the edit distance between two strings which has a quadratic runtime. Furthermore, we can calculate a bounding box of a vector data set in linear time by finding the minimum and maximum value for each dimension. In contrast, finding the maximum distance between elements in a metric space requires a quadratic number of distance computations. In summary, metric data lacks most of the tools available in traditional scenarios for cardinality estimation. This makes most established methods infeasible as we discuss in the Section 3. However, as discussed previously, metric data appears in many different applications naturally. Furthermore, it supports distance queries, which are also highly relevant for vector data [4]. As our experiments will show later, using distance-based techniques helps lowering the impact of the curse of dimensionality. 3 RELATEDWORK The most basic idea for estimating the size of a query result is to perform the query on a sample of the data and scale up the resulting cardinality by the sample  íís fraction of the total data size. Using Reservoir Sampling [32], a random data selection can be computed in linear time. We can apply this method also on metric data. However, small sample sizes result in underestimates often equal to zero because metric spaces are sparse. Histograms are the most popular technique for cardinality estimation in database systems [18]. They divide a domain into multiple buckets and store the number of contained elements. When estimating the cardinality within a given query range, they 350 approximate the actual cardinality usually by assuming a uniform distribution within the buckets. Computing optimal histograms that minimize the error induced by this assumption is NP-hard [25]. The most prominent example of an efficient heuristic is MinSkew [2]. It recursively subdivides the space by splitting the most skewed bucket until the desired number of buckets is reached. Other techniques like rkHist [11] and R-V histogram [1] start from the leaves of a spatial index structure and merge them together for limiting the amount of buckets. The introduced histograms are, however, not applicable for metric data. There is no straight-forward criterion for subdivid- ing a metric space into a finite amount of disjoint buckets. The missing notion of uniform distribution within a bucket and the unavailability of a volume measure make the incorporation of such buckets into a cardinality estimate impossible. It is possi- ble to transform metric data into vector data in order to build a spatial histogram, though. We can then extract the cardinality estimate for a distance query by calculating the intersection of the query (in form of a hyper-sphere) with the histogram buckets. However, such a transformation into a vector space is costly and introduces an error in form of distance distortions. Compression techniques like wavelets and cosine transfor- mations are also suitable for cardinality estimation [24]. Both techniques are applicable to multi-dimensional vector data and are shown to provide accurate results. They approximate the actual data distribution by means of a basis function and several coefficients, thus drastically reducing the amount of data. The cardinality estimate is computed as a cumulative joint distribu- tion of the individual dimensions of the data set. However, in a metric space we are not able to use these techniques as the data has no such dimensions and there is no notion of a distribution. Another method for approximate query processing is Local Sensitive Hashing (LSH). LSH performs very well on data from a high-dimensional vector space. It is for example used for approx- imate similarity search [15] and thus related to distance queries in metric spaces. There has been work in cardinality estimation of similarity joins using LSH [21]. Also, multiple LSH indexes with different radii can be used for cardinality estimation by counting collisions of hash buckets [14]. However, LSH requires a similarity-preserving hash function which does not universally exist for metric data. A more recent approach uses Machine Learning [4] for car- dinality estimation. It is, to the best of our knowledge, the only method supporting distance queries. The query-driven approach learns to differentiate several prototype queries and predicts the cardinality of unseen queries by assignment to a prototype and subsequent interpolation using regression. The optimization of the query prototypes is performed via gradient descent where the prototype query is moved across the data space. This manip- ulation of a query object is not possible in a metric space. Thus, like the other approaches, this approach is infeasible for metric data, unless it is mapped into a vector space first. A distance preserving mapping of data from a metric space to a vector space is called embedding. The goal is to find for each xi  íì¨ X an embedding yi  íì¨ Rd , such that the induced stress [19] on the distances is minimized. This stress measure incorporates the deviations of the resulting distances among objects with respect to the original distances. There are different approaches available to embed metric data into a vector space [3]. One prominent example is Multidimen- sional scaling (MDS) [20]. It tries to preserve the pairwise dis- tances in vector space by using such a stress function [19] and minimizing it subsequently. This minimization can be performed by eigendecomposition or gradient descent. However, both meth- ods are expensive to compute, and thus, not suitable for very large data sets. Landmark MDS [9] was introduced as an alternative to MDS for big data scenarios. It uses samples of the data called land- marks and applies MDS on them. The remaining points are then embedded based on the distances to the l landmark elements. Kernel estimators [26] are a competitor of histograms which exhibit a fast convergence for 1-dimensional data [7] and have been generalized to multi-dimensional data [16]. Note, that both approaches do not support distance queries on metric data. Here, samples distribute their weight using a kernel function K , e.g. Epanechnikov [12] or Gaussian. This weight corresponds to the probability of data points existing in the vicinity of the sample. One approximates the underlying probability density function f? of a data set at the evaluation point x by using a set of samples S and summing up over all samples: f? (x) = 1 |S |  íì§h íì§2 s  íì¨S K( x?s h ) = 1 |S | íì§2 s  íì¨S Kh (x ? s). Here, h is the smoothing-factor called band- width. The cardinality estimate results from integrating the ker- nel density function within a given rectangle query and scaling the result up. In a d-dimensional vector space typically product kernels are used where the density function is integrated for each dimension separately. This is only feasible for rectangular queries and not for distance queries. Hence, the application of existing kernel-density estimators for distance queries on met- ric data embedded into a vector space is not straight-forward. Approximating the distance query as a hyper-sphere introduces an error that is also influenced by the curse of dimensionality. Our approach makes use of kernels, but we avoid the curse of dimensionality by using the one-dimensional distance function. The choice of the actual kernel function is considered to be of low impact according to the literature [8]. Nevertheless, we consider different kernel functions in the experiment section of this paper. However, the selection of the kernel bandwidth h has a much more crucial impact on the resulting estimator quality. There are two general approaches for the bandwidth selection: global and locally adaptive methods [31]. Using a global (fixed) bandwidth means that all samples and evaluation points use the same bandwidth. One method of obtaining this bandwidth is by minimizing the mean integrated squared error (MISE) [30]. In contrast to traditional applications, the underlying distribution that shall be fitted by the kernel estimator is known in cardinality estimation. It is given by the data itself. This enables other opti- mization techniques than those used in the statistics literature. Recent work [17] used a gradient descent based approach to find the optimal bandwidth for a given set of training queries. They fit a global bandwidth for each dimension of the vector space. However, a global bandwidth is usually not optimal, as the result- ing estimator oversmoothes the distribution in dense regions and undersmoothes in sparse regions of the data set. While the au- thors of [17] were able to exploit the different distributions in the individual dimensions, we found the error of a global bandwidth for different query sizes in our metric scenario to be significantly high. Furthermore, a gradient descent based approach to band- width estimation turned out to get stuck in local optima of poor quality in our experiments. We thus also investigate locally adap- tive kernel estimators that vary the bandwidth either based on 351 ?? ?? ?????(??, ?) ????? ? 0 ???????? Figure 1: The incorporation of a kernel-sample s into the cardinality estimation for a query Q = (xQ , rQ ). The omit- ted y-axis corresponds to the probability density. Algorithm 1: Generic Kernel Estimation Algorithm Input :Kernel function Kh : R íì§ R+, centered at 0 Optimized bandwidths B : X  íì© X  íì© R+  íì§ R+ Samples S  íì¨ X  íì¨ X Total data set size |X | Distance function distX : X  íì© X  íì§ R+ Query Q = (xQ , rQ ) with object and radius Output :Estimated cardinality c?(Q) 1 total  íì§  0.0; 2 foreach s  íì¨ S do 3 h  íì§  B(s,xQ , rQ ); 4 s?  íì§  distX(xQ , s); 5 contribution  íì§   íì§¼ rQ 0 Kh (x ? s?) dx ; 6 total  íì§  total + contribution; 7 end 8 probability  íì§  total/|S |; 9 return ?probability  íì§ |X |?; the sample point or the evaluation point. The latter is also called balloon estimator [30]. Other work in kernel-based techniques for cardinality estima- tion in vector spaces focuses also on improving the efficiency of the estimation process. One approach is reducing the number of samples to a so-called coreset [33] that maximizes both quality and efficiency of the estimator. In the scope of this paper we do not yet consider such improvements but focus on demonstrating the general applicability of kernel estimators to this new scenario of metric data. 4 DISTANCE-BASED KERNEL ESTIMATORS Kernel estimators allow us to overcome a fundamental problem of using a sample directly for estimating the cardinality of a query result. Namely that the information is concentrated at a sample point. In contrast to a histogram we also get a continuous distribution. In a metric space it is, however, not straight-forward how we can apply a kernel function on a sample point, as there are no dimensions in which they could gradually distribute the mass of a sample. The central idea of our proposed technique is therefore to apply the kernel function on the distance to a sample point in order to incorporate the probability of elements in the vicinity fractionally. In the following we show how to incorporate a sample point into the cardinality estimate. Here, the query Q = (xQ , rQ ) with object xQ and radius rQ is located at distance s? B distX(xQ , s) 0.00 0.10 ? 1 0 1 2 3 Bandwidth M ed ia n  of  R el at iv e  E rr or s 0.00 0.10 0 50 0 15 00 25 00 Bandwidth Su m  o f S qu ar ed  E rr or s Figure 2: Influence of the bandwidth on the estimation er- ror on the Moby data set (cf. Section 6) for a fixed query size. The left-hand side shows the median of the relative errors (Equation (2)). The right-hand side shows the sum of squared errors (measureMLS ). from the sample point s . As depicted in Figure 1, we introduce an axis expressing the distance to xQ . For that wemap xQ to x?Q B 0, the origin of the axis. The sample point s is then mapped onto s? . The kernel function Kh is then centered at point s? by subtracting s? from its argument. We take the area under the curve of the kernel function between x?Q and rQ as the contribution of this sample to the cardinality estimate. Algorithm 1 shows the full estimation process. For each sam- ple point we calculate the contribution and compute the sum. For this we first compute the optimized bandwidth by calling the function B for the given sample point and query with object and radius. In case of a global bandwidth, this function ignores the parameters and always returns the same bandwidth. In case of a locally adaptive approach, it either uses the sample or evaluation point (query) to obtain a specific bandwidth. We detail algorithms for computing the bandwidth in the next section. Given the opti- mized bandwidth, the distance between sample and query object, and the radius, we calculate the contribution of the sample to the running total . After all samples are processed, the probability is then the total divided by the number of samples, see line 8. Finally, we scale the resulting probability up by the total data set size and return this value as the cardinality estimate. The general workflow of our technique consists of (1) collect- ing a set S of samples, (2) determining the optimal bandwidths B and (3) applying Algorithm 1 to estimate the cardinality of new queries. In the following we present the process of optimizing the bandwidths. 5 BANDWIDTH OPTIMIZATION It is well-known [31] that the bandwidth of a kernel function has a crucial impact on the resulting cardinality estimate. A too small bandwidth leads to undersmoothing, a too large bandwidth to oversmoothing. The two edge cases are an infinitely small bandwidth that converges to sampling and an infinitely large bandwidth that converges to a uniform distribution. We thus take particular care of finding an optimal value. We distinguish between a global bandwidth for all samples and queries, and locally adaptive methods where the bandwidth is individually fitted to accommodate for sparser and denser regions of the data space. 352 5.1 Global The computation of the optimal global bandwidth for a kernel function and a given data set is an optimization problem. We first formalize this problem and then present our optimization strategy. 5.1.1 Optimization Problem. We want to find a bandwidth h that minimizes the error of estimates for future queries on the given data set. As we do not know the future queries, we extract a set of training queries Q from the data set and minimize the error for these queries. Afterwards, we validate the performance against an independent set of test queries that we extracted from the data set beforehand. We formally define the optimization problem for a fixed kernel function as arg min h ErrorX (h,Q) , (1) where h is the bandwidth,X is the data set and ErrorX a function that computes the error of the queries Q on X for the given bandwidth h. We define an appropriate error measure for Equation (1) in two steps. First, we define an auxiliary function errorX (h,Q) B c?h (Q) ? c(Q) c(Q) , (2) where c?h (Q) is the estimated cardinality using bandwidth h and c(Q) the actual cardinality of queryQ on data setX . This measure differs slightly from the common relative error metric, as we do not take the absolute value in the numerator. This allows us to assess over- and underestimates separately. It returns values in the interval [?1, íí]. Two values are of particular interest: ?1 indicates that the estimator returns simply a result of zero even though there are results contained in the query. On the other hand, an error of zero indicates a perfect result: the estimated cardinality is equal to the true number of elements the query returns. There is no upper bound for our measure. However, one should notice, that a value of 1 means already an overestimation by a factor of 2. To compute the error of a set of queries Q we combine the errors errorX (Q) of the individual queriesQ  íì¨ Q using ameasure M : R |Q |  íì§ R+. M computes for a set of errors E a single value that is then subject to minimization. Two examples for M are the deviation of the median error from zero, and the sum of squared errors (LS for least squares): Mmedian (E) B | median(E) | MLS (E) B íì§2 e  íì¨E e2 . For M  íì¨ {Mmedian ,MLS }, the final optimization problem is defined as arg min h ErrorX (h,Q) = arg min h M({errorX (h,Q) | Q  íì¨ Q}) (3) 5.1.2 Optimization Strategy. The minimization of the error function (3) requires an efficient and robust optimization method. Figure 2 shows the relationship between bandwidth and error for an example data set. On the left-hand side of the plot we ob- serve that starting from an infinitely small bandwidth results first underestimate the true cardinality. A higher bandwidth reduces the error to a certain degree. At some point the bandwidth over- smoothes the distribution, leading to very high overestimations. The right-hand side shows the mean squared errors. While the general trend of the error function is clearly visible, we can also see that the results are noisy. This poses a difficult to find global optimum as the multitude of local optima has to be overcome. A method that has shown to be very effective in practice are Evolution Strategies. An Evolution Strategy (ES) is a global numeric optimization approach inspired by the Darwinian theory of natural selection. We implemented the approach of Beyer and Schwefel [6]. Here, ? parents produce another set of íì§íì§¬ offspring. From the thus ob- tained set of ? + íì§íì§¬ individuals the best ? individuals a",Michael Mattig,"Department of Mathematics and Computer Science University of Marburg, Germany",mattig@mathematik.uni-marburg.de,Thomas Fober,"Department of Mathematics and Computer Science University of Marburg, Germany",thomas@mathematik.uni-marburg.de,Christian Beilschmidt,"Department of Mathematics and Computer Science University of Marburg, Germany",beilschmidt@mathematik.uni-marburg.de,Bernhard Seeger,"Department of Mathematics and Computer Science University of Marburg, Germany",seeger@mathematik.uni-marburg.de,,,,,,,,,,,,,,,,,,
20200108,1391,Michael Vollmer,"Karlsruhe Institute of Technology (KIT) Karlsruhe, Germany",michael.vollmer@kit.edu,,Iterative Estimation of Mutual Information with Error Bounds,"ABSTRACT Mutual Information (MI) is an established measure for linear and nonlinear dependencies between two variables. Estimating MI is nontrivial and requires notable computation power for high estimation quality. While some estimation techniques allow trad- ing result quality for lower runtimes, this tradeoff is fixed per task and cannot be adjusted. If the available time is unknown in advance or is overestimated, one may need to abort the esti- mation without any result. Conversely, when there are several estimation tasks, and one wants to budget computation time between them, there currently is no efficient way to adjust it dynamically based on certain targets, e.g., high MI values or MI values close to a constant. In this article, we present an itera- tive estimator of MI. Our method offers an estimate with low quality near-instantly and improves this estimate in fine grained steps with more computation time. The estimate also converges towards the result of a conventional estimator. We prove that the time complexity for this convergence is only slightly slower than non-iterative estimation. Additionally, with each step our estimator also tightens statistical guarantees regarding the con- vergence result, i.e., confidence intervals, progressively. These also serve as quality indicators for early estimates and allow to reliably discern between attribute pairs with weak and strong dependencies. Our experiments show that these guarantees can also be used to execute threshold queries faster compared to non-iterative estimation. 1 INTRODUCTION Motivation. Detecting and quantifying dependencies between variables is an essential task in the database community [10, 13, 20, 30]. Conventional methods such as correlation coefficients and covariance matrices only detect linear or monotonous depen- dencies.Mutual Information (MI) in turn is an index that captures any linear and nonlinear dependency [1, 5]. Probability distri- butions of the variables in question serve as input to compute the MI. For real-world data however, these distributions are not available. In this case, MI must be estimated based on samples. Various estimators for MI have been proposed [15, 23, 33], and some offer good results even for small samples [15]. However, continuous variables with an unknown distribution continue to be challenging, since their multivariate distribution is substituted only by a limited sample. A prominent approach for estimation of MI between continuous variables without assumption of the distribution is the nearest-neighbor based method by Kraskov et al. (KSG) [19]. While good estimators are available, they are very rigid in their time requirements and regarding the estimation quality. Once the computation has started, they impose a fixed time requirement and do not yield aby preliminary result if they are terminated ? 2019 Copyright held by the owner/author(s). Published in Proceedings of the 22nd International Conference on Extending Database Technology (EDBT), March 26-29, 2019, ISBN 978-3-89318-081-3 on OpenProceedings.org. Distribution of this paper is permitted under the terms of the Creative Commons license CC-by-nc-nd 4.0. M u tu al  I n fo rm at io n Runtime MIT MIfin tT t tfin Figure 1: MI estimation with dynamic time allocation. prematurely. They also are unable to exploit easier queries like whether the MI value is above a certain threshold but instead determined the value. Such features are highly relevant for high- dimensional data and data streams with irregular arrival rate as we showcase with the following two scenarios. Scenario 1. Consider a modern production plant with smart meters installed on each machine. A first step in data exploration is determining which attributes are strongly dependent. For in- stance dependencies among currents or energy consumption may offer insights into production sequences. For this first step, a query like Which pairs of measurements have a MI value above the thresholdMIT ? often suffices. With conventional MI estima- tors, each pair either induces high computational costs, or results are uncertain because of low estimation quality. Scenario 2. Think of a database with financial data and its real- time analysis. To maintain a diverse portfolio, it is important to track the relationships between stocks. Because bids and trades happen irregularly, new information and market prices arrive at irregular speed. Thus, it is not known how much time is available to monitor stock relationships in the presence of incoming data. Current MI estimators cannot adapt during execution. They risk not producing a result in time, or estimates are of low quality. To improve upon these shortcomings, we study estimation of MI with dynamic allocation of computation time. Ideally, such an estimator should not only offer preliminary results, but also indicate its remaining uncertainty. Figure 1 shows exemplary pro- gression over time of such an estimator based on our experiments with real data. The black line indicates the preliminary estimate after a certain runtime, and the gray area shows the (expected) maximum error of the preliminary estimate. To obtain the defin- itive result MIfin, a user would require time tfin. However, he could also stop the estimator as soon as the estimate is above a threshold MIT with certainty, or he can use the preliminary result available after time t. In this work, we focus on iterative estimation of MI in order to offer this functionality. Here, iterative means quickly providing an estimate, but with the option to improve the estimation if there is time left. In other words, improving the estimate with     Series ISSN: 2367-2005 73 10.5441/002/edbt.2019.08 some time available is what we call an iteration. At the same time, an iterative estimator can terminate the estimation, i.e., stop iterating, when the result is good enough. For efficiency, it is important that computations from previous iterations remain useful and are not repeated or discarded in a later iteration. So far, efficient iterative estimators for MI do not exist. Challenges. The most significant feature of an estimator is its quality of estimation. This is even more so for iterative methods because both preliminary and final estimation quality are important. In other words, the estimate should already be useful after a few iterations, and estimation quality must level up to the one of conventional estimators after many iterations. Ideally, this convergence should happen after a known, finite number of iter- ations. In this article, we target at respective formal guarantees. Next, the quality of preliminary estimates is crucial for us- ability. Determining if a preliminary result is good enough or interesting enough to merit additional computation time requires some information on its certainty. The number of iterations alone is insufficient, as the result quality depends on many other fac- tors such as data characteristics, required accuracy and time con- straints. Instead, each estimate requires an individual indicator of the uncertainty remaining. While the time spent to improve the estimate iteratively is committed dynamically, it must of course be used efficiently. Many conventional estimators use data structures that are ex- pensive to build and cheap to use, such as space-partitioning trees [19, 31, 32]. Such an upfront activity is undesirable for an iterative estimator whose first estimate must arrive soon. At the same time, runtime and scalability do remain important charac- teristics of the estimator. In other words, an iterative estimator must feature guaranteed efficiency for both individual iterations and final estimates. Contributions. In this article, we present IMIE, our Iterative Mutual Information Estimator. To prove its practical usefulness, we establish several features both formally and experimentally. Quality of Estimation. In Section 4, we propose a design for IMIE such that estimates converge to the same value as with the KSG. To make early iterations useful, IMIE also offers statistical error bounds for its early estimates. More precisely, an early estimate provides a confidence interval for the final estimate. We describe the specifics and the statistical soundness in Section 4.3. Complexity. We study the time complexity of initialization and of individual iterations of IMIE. In Section 5 we establish an amor- tized time complexity for IMIE and the nearest-neighbor search used. This complexity is competitive with existing non-iterative estimators. To be precise, we show that iterating IMIE until con- vergence is only slightly slower in terms of time complexity than computing the KSG directly with optimal algorithms. Experimental Validation. We show that IMIE complements the formal guarantees established so far with good actual perfor- mance. To do so, we perform extensive experiments using both synthetic and real data sets in Section 6. On the one hand, we show that the concrete runtime and estimation results of IMIE are comparable to the ones of conventional estimation methods. On the other hand, the experiments show the practical benefits of the early results from IMIE. For instance, IMIE finds attribute pairs above a threshold value significantly faster than non-iterative estimators. 2 RELATEDWORK Iterative estimation ofMI is interesting from two perspectives. On the one hand, it is methodically interesting, as it can be considered an anytime algorithm. On the other hand, it is interesting to consider the benefits it provides over current methods in different settings. Important application scenarios are dependency analysis in high dimensional data and data streams, cf. Scenario 1 and 2. Anytime Algorithms. Anytime algorithms [36] use available time to increase their result quality. One can obtain a low-quality result after a short time and a better one when waiting longer. In data analysis, anytime algorithms exist for clustering [22], classification [35] and outlier detection [2]. So far however, there is no anytime algorithm to estimate MI. So while there is no direct competitor, IMIE extends the set of tools available as anytime algorithms. Additionally, there has been more general work on the optimal use of available anytime algorithms [11, 18], which may improve the performance of IMIE in larger systems. MI on Data Streams. Estimating MI on streams has received some attention recently. The MISE framework [14] summarizes a bivariate stream such that the MI for arbitrary time frames can be queried. To this end, MISE offers parameters for the balance between accuracy of older queries and resource requirements both in terms of memory and computation time. In contrast, the DIMID estimator [4] processes a bivariate stream as sliding win- dow for monitoring tasks. This approach provides fast updates between time steps by approximation with random projection. MI estimation in sliding windows has also been the focus of [32]. That paper provides lower bounds for estimates using Equa- tion 5 both in general and for updates in sliding windows. It also features two dynamic data structures, DEMI and ADEMI, to main- tain such estimates using either simple or complex algorithms and data structures. These approaches have limitations. First, they all impose the necessary execution time, i.e., one cannot adapt this time after the start of stream processing. If the rate of new items increases, the estimator may be unable to keep up. If it decreases, the es- timator cannot use this time to improve results. Second, the ap- proaches are all focused on bivariate streams. While MI is defined for exactly two variables, the number of attribute pairs grows quadratically in the number of dimensions. In contrast, the only information IMIE maintains on a stream is based on individual di- mensions and thus scales linearly with the dimensionality. Third, the approximate results of MISE and DIMID are difficult to use. Their estimation quality is only known on average; this average defines the perceived quality of individual estimates. So if one estimate has a very small error, it is less likely to be appreciated, while the error of a particularly bad estimate may be assumed to be smaller. Dependencies in High Dimensional Data. Even though MI is de- fined for exactly two variables, it hasmany applicationswith high- dimensional data. Prominent ones are image registration [25], which uses MI between two high-dimensional variables, and fea- ture selection [24], which targets at the MI between attributes and a classification label. But estimating the MI between all pairs of attributes has received little attention, despite being the non- linear equivalent of correlation matrices. [26] uses a different approach, i.e., kernel density estimation, and removes redundant computations that arise when using this estimator for each pair. This approach has a worse computational complexity than a pair- wise application of the KSG estimator, without offering better 74 0 1 2 3 4 5 6 7 8 9 1 2 4 3 5 MCy1 (p3) = 2 MCx1 (p3) = 3 y1 (p3) x1 (p3)6 7 p3 p1 p2 p4 p5 p6 X = { 1, 3, 4, 5, 8}6 Y = { 1, 3, 4, 5, 7} 2, x y Figure 2: Illustration of terms used for the KSG. results [15, 23]. While both scale quadratically in the number of attributes, their approach is also quadratic in the number of points. The complexity of the KSG in turn is (n logn) [32]. Ad- ditionally, it does not expose any parameter to modify the result quality. Consequently, there would not be any benefit of a direct experimental comparison with IMIE. 3 FUNDAMENTALS We first cover the background of MI and its estimation. Mutual Information. Shannon has introduced the notion of entropy [28] to quantify the expected information gained from ob- serving a value of a random variable.H (X ) stands for the entropy of a random variable X . The expected information of observing two random variables X and Y is the joint entropy H (X ,Y ). Mu- tual Information quantifies the amount of information that is shared or redundant between the two variables. It is defined as I (X ;Y ) = H (X ) + H (Y ) ? H (X ;Y ). (1) With the definition of entropy for continuous variables [6], the MI of two continuous random variables is I (X ;Y ) =  X  Y pXY (x,y) log ( pXY (x,y) pX (x)pY (y) ) dx dy, (2) where pX ,pY and pXY are the marginal and joint probability density functions of X and Y . The type of logarithm used in Equation 2 determines the unit of measurement. In this work we use the natural logarithm. This means that MI is measured in the natural unit of information (nat). Estimation. One can perceive many sources of data, e.g., smart meters or market prices, as random variables with unknown dis- tribution. Since Equation 2 requires probability density functions, we cannot compute the MI of such sources exactly. Instead, we can only estimate the MI based on available samples. The popular estimator that will serve as foundation of our work is the one by Kraskov, St?gbauer and Grassberger [19], which we call KSG. It is based on the estimator for probability densities by Loftsgaarden and Quesenberry [21], which Kozachenko and Leonenko have studied further in the context of entropy [17]. In the following, we briefly review the terms and computation of the KSG. Let P = {p1 = (xp1 ,yp1 ), . . . ,pn = (xpn ,ypn )}  R 2 be a sample from a random variable with two attributes. Figure 2 illustrates the notions that we define in the following using the sample P = {(1, 5), (6, 1), (5, 4), (4, 7), (3, 3), (8, 2)}. Let X = {xp1 , . . . , xpn } and Y = {yp1 , . . . ,ypn } be the set of values per attribute. For each point p  P , its k  N+ nearest neighbors in P using the maximum distance form the set kNN (p). More formally, it is kNN (p) = argmin S (P\{p }) s .t . |S |=k max s S ~p, s~, (3) with ~p, s~ = max(|xp ? xs |, |yp ? ys |). We define the largest distance between xp and any x-value among the k nearest neigh- bors of p as xk (p) = maxs kNN (p) |xp ?xs |. We use this distance xk (p) to define the x-marginal count MCxk (p) = |{x  (X \ xp ) : |x ? xp |   x k (p)}|, (4) which is the number of points whose x-value is close to p. In Figure 2, vertical dashed lines mark the area of points whose x-values are at least as close as the nearest neighbor of p3. Since this area contains three points excluding p3, it is M x 1 (p3) = 3. The distance  y k (p) and the y-marginal count MC x k (p) are defined analogously. Note that xk (p) and  y k (p) may differ, which results in differently sized areas for the marginal counts, as seen in Figure 2. Using these counts, the KSG estimate is defined as I? (P) = ? (n)+? (k)? 1 k ? 1 n n2. i=1 ? ( MCxk (pi ) ) +? ( MC y k (pi ) ) , (5) where ? is the digamma function. This is ? (z) = ?C + 2.z?1 t=1 1 t for z  N+ and C ? 0.577 being the Euler-Mascheroni constant. While k is a parameter of this estimator, it is generally rec- ommended [15, 16, 19] to use a small k , that is k  10. Gao et al. [9] have proven that the KSG is a consistent estimator for fixed k , that is, it converges towards the true value with increasing sample size. 4 ITERATIVE ESTIMATION In this section we present IMIE, our iterative estimator for MI. The core concept of our approach is considering the KSG estimate itself as the mean of a random variable with a finite population. Using subsamples of this population for early estimates offers beneficial properties such as an expected value equal to the KSG estimate and convergence to the KSG for large sample sizes. We first present IMIE and its underlying data structure as well as the algorithms for the initialization and for subsequent iterations. Then we describe our approach for nearest neighbor search, which is better for iterative algorithms than the standard procedures. Finally, we describe the statistical bounds that IMIE provides with its estimates. 4.1 IMIE For brevity, we introduce some notation in addition to the one from Section 3. For a pointp  P , we define the pointwise estimate (p) = ? ( MCxk (p) ) +? ( MC y k (p) ) . (6) The set of all pointwise estimates is  = {(p1), . . . ,(pn )}. Seeing  as a finite population of size n with mean ? , Equation 5 can be rewritten as I? (P) = ? (n) +? (k) ? 1 k ? ? . (7) Using a (random) subsample ?  , its mean ?? is an (unbiased) estimation of ? . This in turn yields an (unbiased) estimate of I? (P), I?? (P) = ? (n) +? (k) ? 1 k ? ?? . (8) 75 Data Structure 1: IMIE struct { Point[] P Real Mean, Var Int k,m Int[] OrderR , Orderx , Ordery Real Offset }; Algorithm 2: Init (P,k) 1 Persist k and P O(n) 2 Mean, Var,m $ 0 O(1) 3 OrderR , Orderx , Ordery $ (0, 1, . . . , |P | ? 1) O(n) 4 Sort Orderx and Ordery O(n logn) 5 Offset$ ? (|P |) +? (k) ? 1k O(1) The variance  2? of our subsample serves as a quality indicator of this approximation, which we further discuss in Section 4.3. The idea of IMIE is to maintain a subsample ? and use I?? (P) to estimate I? (P). Each iteration then increases the sample size of ? by one, to improve the estimate. Starting with an empty set, this means there are exactly |P | iterations before IMIE yields exactly the same result as the KSG, i.e., I?? (P) = I? (P). Data Structure. IMIE uses and stores P and k as well as some additional information listed in Data Structure 1. In the following we use the zero-indexed array notation P[i] = pi+1. Contrary to the original data sample P , we do not store ? explicitly. In- stead we store its mean Mean, its variance Var and size, which is the number of performed iterationsm. To maintain the current variance efficiently, we use the online algorithm by Welford [34]. To ensure that ? is a random subsample of , we need to draw without replacement. To this end, IMIE maintains an array of indices OrderR , where index i at position j means that (pi ) is added to ? in the j-th iteration. The positions of this array are randomly swapped during iterations to perform the random se- lection. This enables a fast selection of a random element without replacement in each iteration. In addition, we maintain two ar- rays Orderx and Ordery containing references to all points in P ordered by their x- and y-value, respectively. For instance, in- dex i at Orderx [0] means that pi has the smallest x-value in P , i.e., pi = argminpP xp . These ordered arrays are used to find nearest neighbors, as described in Section 4.2. Finally, we store the Offset = ? (n) + ? (k) ? 1k . With this, the (preliminary) MI estimate is available as I?? (P) = Offset ?Mean. Methods. We now present the two methods Init and Iterate. See Algorithms 2 and 3, together with amortized time complexi- ties, derived in Section 5. Init ensures the proper state of Data Structure 1 before the first iteration, i.e., preparing all variables assuming that |? | = 0. Observe that Init is a straightforward method for the simple case of static data with two attributes. For other scenarios, such as high-dimensional or streaming data, some adjustments to the initialization may be appropriate, as discussed in Section 5.3. Iterate increases the size of sample ? by one. This requires computing (p) for a random p  P with (p) < ?. Iterate consists of three phases. In the first one (Lines 1-3), we select a random point p of P that has not been selected earlier. After Algorithm 3: Iterate 1 ID$ Draw random integer from [m,n ? 1] O(1) 2 Swap values of OrderR [m] and OrderR [ID] O(1) 3 p $ P[OrderR [m]] O(1) 4 kNN (p) $ NNSearch(p) (see Algorithm 4) O(  n) 5 Compute xk (p),  y k (p) O(1) 6 ComputeMCxk (p),MC y k (p) O(logn) 7 (p) $ ? ( MCxk (p) ) +? ( MC y k (p) ) O(1) 8 m $m + 1 O(1) 9 Diff old $ (p) ? Mean O(1) 10 Mean$ Mean + Diff old m O(1) 11 Diff new $ (p) ? Mean O(1) 12 Var $ Var(m?1)+Diff old Diff new m O(1) m ? 1 iterations, we swap the index at position m of OrderR with the index at a random position behindm ? 1. This ensures that we do not use any index twice, since positions before m are not considered, and that each unused index has the same probability of being selected. This random swap is one step of the Fisher-Yates Shuffle in the version of Durstenfeld [8], which fully randomizes the order of a sequence. The second phase (Lines 4-7) computes (p) using the ordered lists Orderx and Ordery . The last phase (Lines 8-12) performs the online algorithm [34] to maintain mean and variance of a sample, in our case ?. Example 4.1. Disregarding the dashed lines for now, Figure 3 illustrates the state of Data Structure 1 after initialization and before the first iteration. For the first iteration, we draw an in- teger ID from {0, . . . ,n ? 1}. Suppose that we drew 5. We swap the content of OrderR [0] and OrderR [5]. OrderR [0] now contains 6. This means that this iteration adds (p6) to our implicit sam- ple ?. We then determine its nearest neighbor 1NN (p6) = {p15}, the distances x 1 (p6) and  y 1 (p6) as well as the marginal counts MCx 1 (p6) = 1 andMC y 1 (p6) = 3. The dashed lines in Figure 3 illus- trate the area of counted points in x and y-direction, respectively, identically to Figure 2. It follows that (p6) = ? (1)+? (3) = 0.346. Substituting the appropriate variables, the remaining values are set accordingly, i.e.,m = 0+ 1 = 1,Mean = 0+ 0.346 1 = 0.346 and Var = 00+00.346 1 = 0. The second iteration is analogous, draw- ing ID = 6 at random from {1, . . . ,n ? 1}, thus choosing p7. Its nearest neighbor is p8, and the marginal counts areMC x 1 (p7) = 1 andMC y 1 (p7) = 6, cf. the dashed lines in Figure 4. As a result, it is (p7) = ? (1)+? (6) = 1.129. Analogously to the first iteration, the remaining values arem = 1+1 = 2,Mean = 0.346+ 0.783 2 = 0.738 and Var = 01+0.7830.391 2 = 0.153. Figure 4 graphs the state of Data Structure 1 after both iterations, and the new MI estimate is 1.164 ? 0.738 = 0.426. 4.2 Nearest-Neighbor Search A computation-intensive step in Iterate is the computation of nearest neighbors, which also is a key step for static estima- tion with the KSG. The classic solution [19, 31] is using space- partitioning trees, which are optimal in terms of computational complexity [32]. This efficiency is achieved because the slow tree construction is performed once, and each nearest-neighbor search afterwards is fast. Contrary to the traditional KSG esti- mation, it is not known beforehand how many nearest-neighbor searches IMIE performs. Constructing such a tree for IMIE would 76 Mean = 0 Var = 0.153 m = 0 X Y p1 p7 p13p2 p9 p14 p3 9 2 5 10 4 14 3 1 121115 6 16 8 713 92 5104 14 3112 1115 6 168713 P Orderx Ordery OrderR k = 1 Offset = 1.164p12p4 p10 p8 p11 p15 p5 p6 p16 92 5 104 1431 1211 156 1687 13 Figure 3: State of IMIE after initialization. X Y p1 p7 p13p2 p9 p14 p3 9 2 5 10 4 14 3 1 121115 6 16 8 713 92 5104 14 3112 1115 6 168713 P Orderx Ordery OrderR k = 1 Offset = 1.164p12p4 p10 p8 Mean = 0.738 Var = 0 m = 2 925 104 143 1 1211 156 1687 13 p11 p15 p5 p6 p16 Figure 4: State of IMIE after two iterations ((p6) and (p7)). not only delay the first estimate, but may also be an inefficient choice overall if only few iterations take place. The opposite, i.e., searching nearest neighbors without any preparation, is a linear search. Each iteration would then require time linear in the num- ber of data points. Since IMIE should offer both fast iterations and preliminary estimates after a short time, our approach is a compromise between these two options. The general idea is to use sorted arrays to perform a guided linear search that offers a good amortized time complexity (cf. Section 5). In the following, we elaborate on our NNSearch approach. Let p be the point whose nearest neighbor we are searching for and q the nearest neighbor we have found so far. Then any point r with |xp ?xr | > ~p?q~ cannot be a nearest neighbor with the maximum norm. This means that we only have to consider the interval [xp ? ~p?q~, xp + ~p?q~] in the sorted array Orderx . When we find a closer point during the search, this interval gets smaller, and fewer points need to be considered. For the y-values, this is analogous. To reduce the number of worst-case scenarios, we perform this search simultaneously in both directions and terminate when either one terminates. See Algorithm 4 for an algorithmic description of NNSearch. Example 4.2. Figure 5 illustrates an exemplary run of this procedure for k = 1. The figure shows four states corresponding to the variables of NNSearch(p) after 0, . . . , 3 loops. The query point p is the filled square, and a projection of the points to their x- and y-coordinates is shown at the bottom and the left side, respectively. These projections indicate the order of points in Orderx and Ordery , respectively. Each state after the first loop also illustrates the variables of NNSearch. The nearest neighbor found so far is marked with a circle and is labeled NN , and the distance max = ~p ? NN ~ is used for the dashed lines that highlight the remaining area of nearest neighbor candidates. Points accessed via Orderx in a previous iteration are marked with a diagonal stripe from the upper left to the lower right. This is done analogously for Ordery . Each loop considers the next loops = 0 X Y p loops = 0 X Y max ?y? ?y+ ?x+ ?x? p 1 NN NN loops = 0 X Y ?y+ ?y? ?x+ ?x? p 2 max NN loops = 0 X Y ?y+ ?y? ?x+ ?x? p 3 max Figure 5: Illustration of Algorithm 4 for each loop. unmarked point in both directions for both Orderx and Ordery . Additionally, the small arrows illustrate the minimal distances ?? for any further point accessed when iterating over Orderx or Ordery in the respective direction. After the third loop, the arrows of ?y+ and ?y? both exceed the area of the remaining candidates, represented by the dashed lines. This means that all relevant candidates have been considered via Ordery , and that the current nearest neighbor is correct. 4.3 Statistical Quality Indicators Finally we present statistical guarantees for early estimates by IMIE. Since ? is a subsample of , statistical tests with ?? and  2 ? yield statistically significant assertions regarding ? . Equations 7 and 8 give way to analogous assertions for I? (P). Theorem 4.3 ([27]). Let  be a finite population of size n with mean ? and a variance  2  . When drawing an i.i.d. sample ? of size m from , the sample mean ?? has an expected value of E(?? ) = ? and a variance of  2?? =  2 m ( n?m n?1 ) . Proof. See [27].  While the classic version of the Central Limit Theorem is not formulated for finite populations, it has been proven that some variations are applicable, and that ?? is approximately normally distributed [27]. In other words, drawing a sample of sizem with a sample mean ? is as likely as drawing ? from N(? ,?? ) with ?? =   2?? . So we can estimate the probability that a sample mean ?? is off by more than a specified value ? > 0 by using the cumulative distribution function  of the standard normal distributionN(0, 1). This is illustrated in Figure 6 and is formally described as Pr[|?? ? ? | ? ?] = 2   ( ?? ?? ) . (9) 77 Algorithm 4: NNSearch(p) 1 ix , iy $ index of p in Orderx , Ordery , respectively 2 ?x+,?x?,?y+,?y?, loops$ 0 3 max $ 4 NN $ {} 5 while min(?x?,?x+) < max m",Michael Vollmer,"Karlsruhe Institute of Technology (KIT) Karlsruhe, Germany",michael.vollmer@kit.edu,Klemens B?hm,"Karlsruhe Institute of Technology (KIT) Karlsruhe, Germany",klemens.boehm@kit.edu,,,,,,,,,,,,,,,,,,,,,,,,
20200109,1393,Chris Mayfield,"Purdue University West Lafayette, Indiana, USA",cmayfiel@cs.purdue.edu,,ERACER: A Database Approach for Statistical Inference and Data Cleaning,"ERACER: A Database Approach for Statistical Inference and Data Cleaning, ERACER: A Database Approach for Statistical Inference and Data Cleaning, ERACER: A Database Approach for Statistical Inference and Data Cleaning, ERACER: A Database Approach for Statistical Inference and Data Cleaning, ERACER: A Database Approach for Statistical Inference and Data Cleaning, ABSTRACT Real-world databases often contain syntactic and semantic errors, in spite of integrity constraints and other safety measures incorporated into modern DBMSs. We present ERACER, an iterative statistical framework for inferring missing information and correcting such errors automatically. Our approach is based on belief propagation and relational dependency networks, and includes an efficient ap- proximate inference algorithm that is easily implemented in standard DBMSs using SQL and user defined functions. The system performs the inference and cleansing tasks in an integrated manner, using shrinkage techniques to infer correct values accurately even in the presence of dirty data. We evaluate the proposed methods empirically on multiple synthetic and real-world data sets. The results show that our framework achieves accuracy comparable to a baseline statistical method using Bayesian networks with exact inference. However, our framework has wider applicability than the Bayesian network baseline, due to its ability to reason with complex, cyclic relational dependencies. Categories and Subject Descriptors H.2.8 [Database Applications]: Statistical Databases; H.4 [Information Systems Applications]: Miscellaneous General Terms Algorithms, Experimentation, Performance Keywords Relational dependency network, approximate inference, dis- crete convolution, linear regression, outlier detection 1. INTRODUCTION Although the database community has produced a large amount of research on integrity constraints and other safety measures to ensure the quality of information stored in relational database management systems (DBMSs), real-world databases often contain a significant amount of non-trivial errors. These errors, both syntactic and semantic, are gen- erally subtle mistakes which are difficult or even impossible to express using the general types of constraints available in modern DBMSs. In addition, quality control on data in- put is decreasing as collaborative efforts increase, with the Internet facilitating widespread data exchange, collection, and integration activities. Clearly, there is an increasing need for new approaches to data cleaning for the purpose of maintaining quality in relational databases. Data cleaning (or cleansing, scrubbing, etc.) is the process of identifying and repairing incorrect or corrupt records in a database. The goal is not only to bring the database into a consistent state (i.e., with respect to domain or in- tegrity constraints), but also to ensure an accurate and com- plete representation of the real-world constructs to which the data refer. Two surveys of common techniques and general challenges in this research area include [16] and [22]. Removing impurities from data is traditionally an engineering problem, where ad-hoc tools made up of low-level rules and manually-tuned algorithms are designed for specific tasks. However, recent work has shown the the effectiveness of ap- plying techniques from machine learning and data mining for the purpose of data cleaning [7]. In particular, statis- tical methods make it possible to automate the cleansing process for a variety of domains. For this work we develop statistical methods for cleaning relational databases with the following characteristics: ? Incomplete and erroneous: There are both (1) missing values to be filled in, and (2) corrupted val- ues to be identified. This goes beyond traditional statistical methods which make assumptions about the reliability of the non-missing values. ? Correlated attributes: The values of different at- tributes are correlated, both within and across tuples (involving perhaps multiple relations). Much of the prior work in data cleaning concentrates on values within a single tuple or relation. ? High-level dependencies: The attributes with large domains (i.e., many possible values), exhibit higherlevel dependencies among sets of similar values (for categorical variables) or a numerical functional depen- dency (for continuous variables). As an example of this type of domain, consider the task of inferring missing birth and death years of individuals in genealogical databases. The individuals are related through 75 parent-child relationships and the birth and death years of an individual are correlated due to life expectancies. In ad- dition, the birth dates of parents and children are correlated due to expected parenting ages. Furthermore, since life ex- pectancies and parenthood ages are likely to be similar over time, the dependencies do not need to be modeled for specific birth years. Instead they can be modeled as a higher-level functional dependency such as birth year = parent 's birth year + . A statistical method can learn these dependencies from the available data and then use them to infer missing values automatically. As another example, consider the task of inferring missing data in sensor networks. There are often relationships among the different types of measurements in the same sen- sor (e.g., temperature and humidity), as well as relationships among the measurements of neighboring sensors due to spa- tial locality. Again, a statistical method could learn these dependencies from observed data and then use them to infer missing values (e.g., due to battery loss) and/or clean corrupt values (e.g., due to sensor malfunction). Such a method can also be used for anomaly detection and intrusion detec- tion systems. This paper introduces ERACER, a database-centric statistical framework for integrated data cleaning and imputa- tion. The core techniques are based on belief propagation [20] and relational dependency networks [18]. We show how to implement the inference and cleaning processes efficiently at the database level. This eliminates the expensive process of migrating the data to and from statistical software such as R or Matlab, which is particularly useful when the amount of data aor limited processing time and resources aprevents a more extensive analysis. In contrast to prior work that cleans values within a single tuple, our approach exploits the graphical structure of the data to propagate inferences throughout the database. As a result the imputation and cleaning tasks go hand in hand: additional information in the database helps identify errors more accurately, and cor- rected data values improve the quality of inference for the missing values.",Chris Mayfield,"Purdue University West Lafayette, Indiana, USA",cmayfiel@cs.purdue.edu,Jennifer Neville,"Purdue University West Lafayette, Indiana, USA",neville@cs.purdue.edu,Sunil Prabhakar,"Purdue University West Lafayette, Indiana, USA",sunil@cs.purdue.edu,,,,,,,,,,,,,,,,,,,,,
20200110,1394,Alexander Hall,"Google, Inc.",alexhall@google.com,,Processing a Trillion Cells per Mouse Click,"Processing a Trillion Cells per Mouse Click, Processing a Trillion Cells per Mouse Click, Processing a Trillion Cells per Mouse Click, Processing a Trillion Cells per Mouse Click, Processing a Trillion Cells per Mouse Click, ABSTRACT Column-oriented database systems have been a real game changer for the industry in recent years. Highly tuned and performant systems have evolved that provide users with the possibility of answering ad hoc queries over large datasets in an interactive manner. In this paper we present the column-oriented datastore developed as one of the central components of PowerDrill1. It combines the advantages of columnar data layout with other known techniques (such as using composite range partitions) and extensive algorithmic engineering on key data structures. The main goal of the latter being to reduce the main memory footprint and to increase the efficiency in processing typical user queries. In this combination we achieve large speed-ups. These enable a highly interactive Web UI where it is common that a single mouse click leads to processing a trillion values in the underlying dataset. 1. INTRODUCTION In the last decade, large companies have been placing an ever increasing importance on mining their in-house databases; often recognizing them as one of their core assets. With this and with dataset sizes growing at an enormous pace, it comes as no surprise that the interest in column- oriented databases (column-stores) has grown equally. This spawned several dozens of research papers and at least a dozen of new column-store start-ups, cf. [2]. This is in ad- dition to well established offerings, e.g., by MonetDB [25], Netezza [26], or QlikTech [30]. Since 2011 all major commer- cial database vendors actually provide column-store tech- nologies (cf. [25]). Typically, these products are deployed to import existing databases into the respective column-store. An OLAP or OLTP, i.e., SQL, interface is provided to then mine the data interactively. The key advantage shared by these systems is that column-oriented storage enables reading only data for  relevant columns. Obviously, in denormalized datasets with often several thousands of columns this can make a huge difference compared to the the row-wise storage used by most database systems. Moreover, columnar formats compress very well, thus leading to less I/O and main memory usage. At Google multiple frameworks have been developed to support data analysis at a very large scale. Best known and most widely used are MapReduce [13] and Dremel [23]. Both are highly distributed systems processing requests on thou- sands of machines. The latter is a column-store providing interactive query speeds for ad hoc SQL-like queries. In this paper we present an alternative column-store de- veloped at Google as part of the PowerDrill project. For typical user queries originating from an interactive Web UI (developed as part of the same project) it gives a perfor- mance boost of 10?100x compared to traditional column- stores which do full scans of the data. Background Before diving into the subject matter, we give a little back- ground about the PowerDrill system and how it is used for data analysis at Google. Its most visible part is an interactive Web UI making heavy use of AJAX with the help of the Google Web Toolkit [16]. It enables data visualization and exploration with flexible drill down capabilities. In the back- ground, the ""engine"" provides an abstraction layer for the UI based on SQL: the user constructs charts via drag'n'drop op- erations, they get translated to group-by SQL queries, which the engine parses and processes. It can send out such queries to different backends, e.g., Dremel, or execute them directly on data stored, e.g., in CSV files, record-io files (binary for- mat based on protocol buffers [29]), or in Bigtable [10]. The third large part of the project is the column-store presented in this paper. The Web UI is very versatile; it allows to select arbitrary dimensions, measures, and computed values for grouping and filtering. The dimensions can have a large number of distinct values, such as strings representing Google searches. A user can quickly drill down to values of interest, e.g., all German searches from yesterday afternoon that contain the word ""auto"", by restricting a set of charts to these values. For these reasons, pre-aggregation or indexing of data does not help and we need to query the raw data directly. The nature of the use cases enabled by this UI demand for high availability and low latency. Examples of such use cases include: Responding to customer requests, spam anal- ysis, dealing with alerts in highly critical revenue systems, or monitoring and assessing changes to production systems. The system has been in production since end of 2008 and  was made available for internal users across all of Google mid 2009. Each month it is used by more than 800 users sending out about 4 million SQL queries. After a hard day's work, one of our top users has spent over 6 hours in the UI, triggering up to 12 thousand queries. When using our column-store as a backend, this may amount to scanning as much as 525 trillion cells in (hypothetical) full scans. The column-store developed as part of PowerDrill is tai- lored to support a few selected datasets and tuned for speed on typical queries resulting from users interacting with the UI. Compared to Dremel which supports thousands of dif- ferent datasets (streaming the data from a distributed file system such as GFS [15]), our column-store relies on having as much data in memory as possible. PowerDrill can run interactive single queries over more rows than Dremel, how- ever the total amount of data it can serve is much smaller, since data is kept mostly in memory, whereas Dremel uses a distributed file system. This and several other important distinctions, enable han- dling very large amounts of data in interactive queries. Consider a typical use case such as triggering 20 SQL queries with a single mouse click in the UI. In our production sys- tem on average these queries process 782 billion cells in 30-40 seconds (under 2 seconds per query), several orders of mag- nitude faster than what a more traditional approach as used by Dremel could provide. Contributions The main contributions presented in this paper: ? We describe how-unlike in most column-stores-the data is partitioned and organized in an import phase (Section 2.2). This enables skipping and caching large parts of the data: on average in production 92.41% is skipped and 5.02% cached, leaving only 2.66% to be scanned (see also Section 6). ? We present the basic data-structures used in Section 2.3. Their main goal is to support the partitioned layout of the data and to enable quick skipping of chunks of data. For optimal usage it is assumed they can be held in memory. Experiments show that these simple data-structures also directly give performance benefits of around 100x or more on full scans, compared to two row-wise stor- age formats and Dremel's column-store (Section 2.5). Note that for these experiments we do not partition the data at import. When dropping the ""in memory"" assumption, a still impressive factor of 30x can be achieved. ? In Section 3 we present several successive ""algorithmic engineering"" choices to improve key data-structures. The aim being to reduce the memory footprint for certain typical cases. We pin-point the effects of individ- ual optimizations with experiments measuring mem- ory usage. E.g., for the important case of a field with many distinct values, we obtain a reduction of 16x. ? In Section 4 we describe how queries may be computed in a distributed manner on a cluster of machines. In Section 5 we present selected extensions and finally in Section 6 the highly distributed setup of the actual productionized system running on over 1000 machines. We give measurements concerning the usage in prac- tice which show the positive effect of the partitioning (enabling to skip or cache large parts of the data). Related Work For an introduction to OLAP and basic techniques applied in data-warehouse applications, see the Chaudhuri and Dayal [11]. To obtain an overview of recent work on column-store architectures, please see the concise review [2] and references therein. The excellent PhD thesis by Abadi [1] can serve as a more in-depth introduction and overview of the topic. Recent research in this area includes, e.g., work on how super-scalar CPU architectures affect query processing [9], tuple reconstruction [17], compression in column-stores [34, 9, 3], and a comparison to traditional row-wise storage [4]. Kersten et al. [20] give a more open ended outlook on inter- esting future research directions. The plethora of open-source and commercial column-store systems, e.g., [34, 25, 26, 30, 36] further demonstrates the effectiveness of this paradigm. Melnik et al. [23] recently have introduced Dremel to a wider audience. As mentioned, its power lies in providing interactive responses to ad hoc SQL queries over thousands of datasets. It achieves this by streaming over petabytes of data (stored, e.g., on GFS [15]) in a highly distributed and efficient manner. This is also a key difference to the column-store presented in this paper which heavily relies on having as much data in memory as possible and therefore only is used for a few selected data sources. Melnik et al. also give a nice overview of data anlysis at Google and how interactive approaches like Dremel's complement the offline MapReduce [13] framework. Skipping over data in the context of colum-stores has been explored by other authors, e.g., Slezak et al. [32] or Mo- erkotte [24]. We give some details on these approaches in comparison to ours in Section 2.1. Reordering rows to improve the compression of column- wise stored data has been investigated, e.g., by [18, 21, 3]. We give some details on this at the end of Section 3. Notation and Simplifying Assumptions For the remainder of the paper we will only consider im- porting and processing data from single tables; which, e.g., correspond to log files at Google in the ""protocol buffers"" format [29] or result from denormalizing a set of relational tables in a database. We refer to such an instance as table or just the data which has columns (also referred to as fields) and rows (also referred to as records). In order to store pro- tocol buffer records with nested and repeated records (i.e., lists of sub-records), PowerDrill supports a nested relational model, cf. [5]. For ease of exposition, in the following we focus on unstructured / flat records as opposed to records which may, e.g., contain lists.",Alexander Hall,"Google, Inc.",alexhall@google.com,Olaf Bachmann,"Google, Inc.",olafb@google.com,Robert Bu ?ssow,"Google, Inc.",buessow@google.com,Silviu Ga ?nceanu,"Google, Inc.",silviu@google.com,Marc Nunkesser,"Google, Inc.",marcnunkesser@google.com,,,,,,,,,,,,,,,
