date,submitter_orcid,submitter_name,submitter_institution,submitter_email,submitter_title_doi,submitter_title,submitter_intro,submitter_author1_name,submitter_author1_institution,submitter_author1_email,submitter_author2_name,submitter_author2_institution,submitter_author2_email,submitter_author3_name,submitter_author3_institution,submitter_author3_email,submitter_author4_name,submitter_author4_institution,submitter_author4_email,submitter_author5_name,submitter_author5_institution,submitter_author5_email,submitter_author6_name,submitter_author6_institution,submitter_author6_email,submitter_author7_name,submitter_author7_institution,submitter_author7_email,submitter_author8_name,submitter_author8_institution,submitter_author8_email,submitter_author9_name,submitter_author9_institution,submitter_author9_email,submitter_author10_name,submitter_author10_institution,submitter_author10_email
20200101,326,Lu Qin,"The Chinese University of Hong Kong, Hong Kong, China",lqin@se.cuhk.edu.hk,,Diversifying Top-K Results,"ABSTRACT This paper proposes a general framework for matching similar subsequences in both time series and string databases. The matching results are pairs of query subsequences and database subsequences. The framework finds all possible pairs of similar subsequences if the distance measure satis- fies the ""consistency"" property, which is a property intro- duced in this paper. We show that most popular distance functions, such as the Euclidean distance, DTW, ERP, the Freche?t distance for time series, and the Hamming distance and Levenshtein distance for strings, are all ""consistent"". We also propose a generic index structure for metric spaces named ""reference net"". The reference net occupies O(n) space, where n is the size of the dataset and is optimized to work well with our framework. The experiments demon- strate the ability of our method to improve retrieval perfor- mance when combined with diverse distance measures. The experiments also illustrate that the reference net scales well in terms of space overhead and query time. 1. INTRODUCTION Sequence databases are used in many real-world applica- tions to store diverse types of information, such as DNA and protein data, wireless sensor observations, music and video streams, and financial data. Similarity-based search in such databases is an important functionality, that allows identi- fying, within large amounts of data, the few sequences that contain useful information for a specific task at hand. For example, identifying the most similar database matches for a query sequence can be useful for classification, forecasting, or retrieval of similar past events. The most straightforward way to compare the similarity between two sequences is to use a global similarity mea- sure, that computes an alignment matching the entire first sequence to the entire second sequence. However, in many scenarios it is desirable to perform subsequence matching, where, given two sequences Q and X, we want to identify pairs of subsequences SQ of Q and SX of X, such that the similarity between SQ and SX is high. When a large database of sequences is available, it is important to be able to identify, given a query Q, an optimally matching pair SQ and SX, where SX can be a subsequence of any database sequence. A well-known example of the need for subsequence match- ing is in comparisons of biological sequences. It is quite possible that two DNA sequences Q and X have a large Levenshtein distance [22] (also known as edit distance) be- tween them (e.g., a distance equal to 90% of the length of the sequences), while nonetheless containing subsequences SQ and SX that match at a very high level of statistical significance. Identifying these optimally matching subse- quences [34] helps biologists reason about the evolutionary relationship between Q and X, and possible similarities of functionality between those two pieces of genetic code. Similarly, subsequence matching can be useful in searching music databases, video databases, or databases of events and activities represented as time series. In all the above cases, while the entire query sequence may not have a good match in the database, there can be highly informative and statistically significant matches between subsequences of the query and subsequences of database sequences. Several methods have been proposed for efficient subse- quence matching in large sequence databases. However, all the proposed techniques are targeted to specific distance or similarity functions, and it is not clear how and when these techniques can be generalized and applied to other dis- tances. Especially, subsequence retrieval methods for string databases are difficult to be used for time-series databases. Furthermore, when a new distance function is proposed, we need to develop new techniques for efficient subsequence matching. In this paper we present a general framework, which can be applied to any arbitrary distance metric, as long as the metric satisfies a specific property that we call ""consistency"". Furthermore, we show that many well-known existing distance functions satisfy consistency. Thus, our framework can deal with both sequence types, i.e., strings and time series, including cases where each element of the sequence is a complex object. The framework in this paper consists of a number of steps: dataset segmentation, query segmentation, range query, can- didate generation, and subsequence retrieval. Brute-force search would require evaluating a total of O (|Q|2 |X|2) pairs of subsequences of Q and X. However our filtering method produces a shortlist of candidates after considering O (|Q| |X|) pairs of segments only. For the case where the distance is a metric, we also present a hierarchical reference 1579 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Articles from this volume were invited to present their results at The 38th International Conference on Very Large Data Bases, August 27th - 31st 2012, Istanbul, Turkey. Proceedings of the VLDB Endowment, Vol. 5, No. 11 Copyright 2012 VLDB Endowment 2150-8097/12/07... $ 10.00. net, a novel generic index structure that can be used within our framework to provide efficient query processing. Overall, this paper makes the following contributions: ? We propose a framework that, compared to alterna- tive methods, makes minimal assumptions about the underlying distance, and thus can be applied to a large variety of distance functions. ? We introduce the notion of âconsistencyâ as an im- portant property for distance measures applied to se- quences. ? We propose an efficient filtering method, which pro- duces a shortlist of candidates by matching only O(|Q| |X|) pairs of subsequences, whereas brute force would match O (|Q|2 |X|2) pairs of subsequences. ? We make this filtering method even faster, by using a generic indexing structure with linear space based on reference nets, that efficiently supports range similar- ity queries. ? Experiments demonstrate the ability of our method to provide good performance when combined with diverse metrics such as the Levenshtein distance for strings, and ERP [8] and the discrete Freche?t distance [11]) for time series. 2. RELATEDWORK Typically, the term ""sequences"" can refer to two different data types: strings and time-series. There has been a lot of work in subsequence retrieval for both time series and string databases. However, in almost all cases, existing methods concentrate on a specific distance function or specific type of queries. Here we review some of the recent works on subse- quence matching. Notice that this review is not exhaustive since the topic has received a lot of attention and a complete survey is beyond the scope of this paper. Time-series databases and efficient similarity retrieval have received a lot of attention in the last two decades. The first method for subsequence similarity retrieval under the Eu- clidean (L2?norm) distance appeared in the seminal paper of Faloutsos et al. [12]. The main idea is to use a sliding win- dow to create smaller sequences of fixed length and then use a dimensionality reduction technique to map each window to a small number of features that are indexed using a spa- tial index (e.g., R?-tree). Improvements of this technique have appeared in [28, 27] that improve both the window- based index construction and the query time using sliding windows on the query and not on the database. However, all these techniques are ap",Lu Qin,"The Chinese University of Hong Kong, Hong Kong, China",lqin@se.cuhk.edu.hk,Jeffrey Xu Yu,"The Chinese University of Hong Kong, Hong Kong, China",yu@se.cuhk.edu.hk,Lijun Chang,"The Chinese University of Hong Kong, Hong Kong, China",ljchang@se.cuhk.edu.hk,,,,,,,,,,,,,,,,,,,,,
20200102,897,Haohan Zhu,Department of Computer Science Boston University,zhu@cs.bu.edu,,A Generic Framework for Efficient and Effective Subsequence Retrieval,"ABSTRACT Top-k query processing finds a list of k results that have largest scores w.r.t the user given query, with the assumption that all the k results are independent to each other. In practice, some of the top-k results returned can be very similar to each other. As a re- sult some of the top-k results returned are redundant. In the lit- erature, diversified top-k search has been studied to return k re- sults that take both score and diversity into consideration. Most existing solutions on diversified top-k search assume that scores of all the search results are given, and some works solve the diver- sity problem on a specific problem and can hardly be extended to general cases. In this paper, we study the diversified top-k search problem. We define a general diversified top-k search problem that only considers the similarity of the search results themselves. We propose a framework, such that most existing solutions for top- k query processing can be extended easily to handle diversified top-k search, by simply applying three new functions, a sufficient stop condition sufficient(), a necessary stop condition necessary(), and an algorithm for diversified top-k search on the current set of generated results, div-search-current(). We propose three new algorithms, namely, div-astar, div-dp, and div-cut to solve the div-search-current() problem. div-astar is an A? based algorithm, div-dp is an algorithm that decomposes the results into components which are searched using div-astar independently and combined using dynamic programming. div-cut further decomposes the cur- rent set of generated results using cut points and combines the re- sults using sophisticated operations. We conducted extensive per- formance studies using two real datasets, enwiki and reuters. Our div-cut algorithm finds the optimal solution for diversified top-k search problem in seconds even for k as large as 2, 000. 1. INTRODUCTION Top-k queries are one of the most fundamental queries used in the IR and database areas. Given a user query, the top-k results of the query are a list of k results that have largest scores/relevances with respect to the user query, under the assumption that all of the k results are independent to each other. In some situations, for a cer- tain top-k query, some of the results returned can be very similar to each other. For example, if we search  íì§¸apple íì§¹ in Google image1, 7 out of the top-10 results returned are the logo of the Apple com- pany. In order to remove the redundancy in the results, and at the same time keep the quality of the top-k results, diversity should be considered in the top-k search problems. For top-k search algorithms. In the literature, most of them aim at finding an early stop condition, such that they can find the top- k results without exploring all the possible search results. Based on this, two frameworks are generally used, namely, the incremen- tal top-k framework and the bounding top-k framework. The in- cremental top-k framework outputs the results one by one in non- increasing order of their scores, and stops as soon as k results are generated. It aims to find a polynomial delay algorithm such that given the existing generated results, the next result with largest score can be generated in polynomial time w.r.t. the size of the input only [16, 15, 20, 14]. In the bounding top-k framework, re- sults are not necessarily generated in non-increasing order of their scores. It maintains a score upper bound for the unseen results ev- ery time when a new result is generated. The algorithm stops when the current k-th largest score is no smaller than the upper bound for the unseen results. The threshold algorithm based approaches [7, 9] fall in this framework and other approaches include [12, 17]. Diversity aware search has been studied in recent years. Most of the existing solutions that support diversity on top-k search results assume the ranking of all the search results are given in advance. Based on which, a diversity search algorithm is given to output k results based on a scoring function that takes both query relevance and diversity into consideration [6, 1, 11, 5, 2]. Other works give algorithms that solve the diversity problem for a special area, i.e., graph search [18], document search [22], etc. and can hardly be extended to support general top-k diversity search. In this paper, we propose a general framework to handle the di- versified top-k search problem. We keep the advantages for the existing top-k search algorithms, that can stop early without ex- ploring all search results, and at the same time, we take diversity into consideration. We show that any top-k search algorithm that can be used in the incremental top-k framework or the bounding top-k framework can be easily extended to handle diversified top- k search, by adding three new functions studied in this paper: a sufficient stop condition sufficient(), a necessary stop condition necessary(), and a diversity search function div-search-current(). All of them are application independent. The only assumption in our framework is that, given any two search results vi and vj , whether vi and vj are similar to each other can be decided, e.g., us- ing a similarity function sim(vi, vj) > íì§íì¨ for a user given threshold íì§íì¨ . We output a list of k results with maximum total scores such that 1 http://www.google.com/imghp 1124 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Articles from this volume were invited to present their results at The 38th International Conference on Very Large Data Bases, August 27th - 31st 2012, Istanbul, Turkey. Proceedings of the VLDB Endowment, Vol. 5, No. 11 Copyright 2012 VLDB Endowment 2150-8097/12/07... $ 10.00. no two of them are similar to each other. We make the following contributions in this paper. (1) We formalize the diversified top-k search problem. Based on our definition, the optimal solution only depend on the similarity of search results themselves, and no other information is needed. (2) We study two categories of algorithms generally used in finding top-k results with early stop in the literature, namely, the incre- mental top-k framework and the bounding top-k framework. We show both frameworks can be extended to diversified top-k search by simply adding three application independent functions studied in this paper, namely, a sufficient stop condition sufficient(), a nec- essary stop condition necessary(), and a diversity search function div-search-current(). The sufficient stop condition helps to early stop and the necessary stop condition helps to reduce the number of div-search-current() processes, since div-search-current() is usu- ally a costly operation. (3) We show that div-search-current() is an NP-Hard problem and is hard to be approximated. We propose three new algorithms, namely, div-astar, div-dp, and div-cut, to find the optimal solution for div-search-current(). div-astar is an A? based algorithm and is slow to handle a large number of results. div-dp decomposes the results into disconnected components in order to reduce the graph size to be searched using div-astar. Results in div-dp are com- bined using dynamic programming. div-cut further decomposes each component into several subgraphs to form a cptree, based on the cut points of each component. A tree based search is applied on cptree to find the optimal solution. (4) We conducted extensive performance studies using two real datasets, to test the performance of the three algorithms. Our div-cut approach can find the diversified top-k results within seconds when k is as large as 2, 000. The rest of this paper is organized as follows. Section 2 formally defines the diversified top-k search problem. Section 3 shows the two existing frameworks on general top-k search problems. Sec- tion 4 shows how to extend the two categories of top-k search ap- proaches to solve diversified top-k search, by defining a sufficient stop condition sufficient(), a necessary stop condition necessary(), and a diversified top-k search algorithm div-search-current() to search on the current result set. Section 5, 6, and 7 give three al- gorithms to solve the div-search-current() problem. We show our experimental results in Section 8, and introduce the related work in Section 9. Finally, we conclude our paper in Section 10. 2. PROBLEM DEFINITION We consider a list of results S = {v1, v2,  íì§  íì§  íì§ }. For each vi  íì¨ S, the score of vi is denoted as score(vi). For any two results vi  íì¨ S and vj  íì¨ S, there is a user defined similarity function sim(vi, vj) denoting the similarity between the two results vi and vj . Without loss of generality, we assume 0  íí sim(vi, vj)  íí 1 for any two results vi  íì¨ S and vj  íì¨ S, and sim(v, v) = 1 for any v  íì¨ S. Given an integer k where 1  íí k  íí |S|, the top-k results of S is a list of k results Sk that satisfy the following two conditions. 1) Sk  íì¨ S and |Sk| = k. 2) For any vi  íì¨ Sk and vj  íì¨ S ? Sk, score(vi)  íí score(vj). Here, S ? Sk is the set of results that are in S but not in Sk, i.e., S ? Sk = {v|v  íì¨ S, v / íì¨ Sk}. Given two results vi  íì¨ S and vj  íì¨ S, vi is similar to vj iff sim(vi, vj) > íì§íì¨ where íì§íì¨ is a user defined threshold, and 0 < íì§íì¨  íí 1. We use vi ? vj to define that vi is similar to vj . Definition 1 (Diversified Top-k Results) Given a list of search results S = {v1, v2,  íì§  íì§  íì§ }, and an integer k where 1  íí k  íí |S|, the v1 v3 v5 v2 v4 v6 10 6 8 7 7 1 v1 v3 v5 v2 v4 v6 10 6 8 7 7 1 K=3K=2 Figure 1: A sample diversity graph diversified top-k results of S, denoted as D(S), is a list of results that satisfy the following three conditions. 1) D(S)  íì¨ R and |D(S)|  íí k. 2) For any two results vi  íì¨ R and vj  íì¨ R and vi 6= vj , if vi ? vj , then {vi, vj} * D(S). 3) íì§2 v íì¨D(S) score(v) is maximized. Intuitively, D(S) is the set of at most k results, such that no two results are similar with each other, and the total score of the results is maximized. We use score(D(S)) to denote the total score of results in D(S), i.e., score(D(S)) = íì§2 v íì¨D(S) score(v). In this paper, we are to find the diversified top-k results. Our aim is to find a general approach, such that for any existing algo- rithm that returns the top-k results of a certain problem, it can be easily changed to return the diversified top-k results by applying our framework, in which the result set S is not necessarily to be computed in advanced but grows incrementally with an early stop condition. We first give the definition of the diversity graph. Definition 2 (Diversity Graph) Given a list of results S = {v1, v2,  íì§  íì§  íì§ }, the diversity graph of S, denoted as G(S) = (V,E), is an undirected graph such that for any result v  íì¨ R, there is a corresponding node v  íì¨ V , and for any two results vi  íì¨ S and vj  íì¨ R, there is an edge (vi, vj)  íì¨ E iff vi ? vj . We use V (G(S)) and E(G(S)) to denote the set of nodes and the set of edges in the diversity graph G(S) respectively, and use v.adj(G(S)) to denote the set of nodes that are adjacent to v in G(S). If the context is obvious, we use vi to denote both the result vi in S and the node vi in G(S), we use G to denote G(S), and we use D to denote D(S). Without loss of generality, we assume nodes in G(S) are arranged in non-increasing order of their scores, i.e., for any 1  íí i < j  íí |V (G(S))|, score(vi)  íí score(vj). The diversified top-k results D(S) can be equivalently defined as a subset of nodes in G(S), that satisfy the three conditions. 1) |D(S)|  íí k. 2) D(S) is an independent set of G(S). 3) score(D(S)) is maximized. Here, an independent set of a graph is a set of nodes in a graph, where no two nodes are adjacent. Example 1 Fig. 1 shows the diversity graph for 6 results S = {v1, v2,  íì§  íì§  íì§ , v6}. Suppose k = 2, the optimal solution D(S) includes two points v1 and v2 with score 18, as shown on the left part of Fig. 1. Suppose k = 3, the optimal solution D(S) includes three points v3, v4 and v5 with score 20, as shown on the right part of Fig. 1. In the following, we first show the two existing frameworks to solve top-k search problems, namely, the incremental top-k frame- work and the bounding top-k framework, which are most generally used in top-k search algorithms. Then we show the framework of 1125 Algorithm 1 incremental(k) 1: S  íì§  ?; 2: for i=1 to k do 3: v  íì§  incremental-next(); 4: if v = ? then 5: break; 6: S  íì§  S ? {v}; 7: return S; Algorithm 2 bounding(k) 1: S  íì§  ?; 2: unseen íì§  + íí; 3: while the k-th largest score of S < unseen do 4: v  íì§  bounding-next(); 5: if v = ? then 6: break; 7: S  íì§  S ? {v}; 8: update unseen; 9: return top-k results in S; our approach to extend the two frameworks to handle diversified top-k search. 3. TOP-K SEARCH FRAMEWORKS In the literature, the framework of most algorithms that find top- k results falls into two categories, namely, the incremental top-k framework and the bounding top-k framework. Incremental Top-k: In the incremental top-k framework, results are generated one by one by calling a procedure incremental-next(), with non-increasing order of their scores. The algorithm stops after k results are generated, and the k results are the final top-k results for the problem. The framework named incremental is shown in Algorithm 1. A lot of existing work fall into this category, e.g., finding top-k shortest paths in graphs, finding top-k steiner trees, communities and r-cliques in graphs, etc [16, 15, 20, 14]. A lot of works have been done to assume that the time complexity of each incremental-next() procedure to generate the next result with largest score is polynomial w.r.t. the size of the input only. Bounding Top-k: In the bounding top-k framework, results are generated one by one by calling a procedure bounding-next(), but not necessarily with non-increasing order of their scores. A bound unseen is defined to be the upper bound of the scores for the un- seen results. After each result is generated by bounding-next(), unseen is also updated to be a possibly smaller value. The algo- rithm stops when the k-th largest score of all generated results is no smaller than the upper bound for the unseen results unseen. The framework named bounding is shown in Algorithm 2. The thresh- old algorithm that is generally used to return top-k results falls into this category [7, 9]. Other works that fall into this category include [12, 17]. 4. DIVERSIFIED TOP-K SEARCH In this section, we show how to extend the incremental top-k framework incremental and bounding top-k framework bounding to handle diversified top-k search. We mainly focus on two tasks. First, a new early stop conditions is needed. Second, an algorithm that finds the diversified top-k results for the current generated re- sult set is needed. For the early stop condition, in the original al- gorithm, the stop condition for incremental is simply |S| = k and the stop condition for bounding is the current k-th largest score  íí unseen. Obviously, both of them cannot be applied to handle Algorithm 3 div-search(k) 1: S  íì§  ?; D(S) íì§  ?; 2: while sufficient() do 3: the code to update S (and unseen); 4: if necessary() then 5: D(S) íì§  div-search-current(G(S), k); 6: return D(S); diversified top-k search. Consider an extreme case, when the al- gorithm stops using the original stop condition, it is possible that all the results generated are similar to each other. Thus the current diversified top-k results only contain 1 result with the largest score. It is not the optimal solution because it is possible that an unseen result is not similar to the current one. Here, D(S) computed for the current generated result set S can be used to check the new stop condition, and if the new stop condition is satisfied, D(S) is the optimal solution for the diversified top-k search. We extend both incremental and bounding using the same frame- work, which is shown in Algorithm 3, by adding three new func- tions, a new sufficient stop condition sufficient(), a new necessary stop condition necessary() and an algorithm div-search-current() to search the diversified top-k results on the current generated re- sult set. The algorithm executes the code of the original top-k al- gorithm to update S and stops when sufficient() is satisfied. For incremental, the code is line 3-6 in algorithm 1, and for bounding, the code is line 4-8 in algorithm 2. After updating S, we construct the diversity graph G(S) on S based on the similarity function sim() for any given two results. If the necessary stop condition is satisfied, we find the diversified top-k results for the current result set S using div-search-current(). The necessary stop condition is used to reduce the number of calling div-search-current(), because div-search-current() is a costly work. In the following, we will in- troduce the sufficient stop condition, the necessary stop condition, and the search algorithm for current set. Sufficient Stop Condition: Given the current result set S, we need to calculate an upper bound best(S) for the possible optimal solu- tions considering both the current result set S and the unseen re- sults. Let Di(S) be the best diversified results of S with exactly i elements for 1  íí i  íí k, i.e., Di(S) is a subset of nodes in V (G(S)), that satisfies the following three conditions. 1) |Di(S)| = k. 2) Di(S) is an independent set of G(S). 3) score(Di(S)) is maximized. Lemma 1 Given Di(S) for 1  íí i  íí k and the score upper bound of all the unseen results u. The upper bound best(S) can be calcu- lated as follows. best(S) = max 1 ííi íík {score(Di(S)) + (k ? i) íì© u} (1) where u is the score of the last generated result v, score(v), for incremental and is the upper bound of the unseen results, unseen, for bounding. Proof Sketch: Suppose the final optimal solution is O, then we can divide O into two parts, O = O1 ? O2, where O1 is the set of generated results, and O2 is the set of unseen results. Suppose O1 has n1 elements and O2 has n2 elements. We have n1 + n2  íí k. Since O1 is the set of generated results, we have (1) score(O1)  íí score(Dn1(S)), since Dn1(S) is the optimal solution with n1 el- ements. We also have (2) score(O2)  íí n2  íì© u  íí (k ? n1)  íì© u, 1126 since (u) is the score upper bound for all unseen results. Com- bine (1) and (2), we have score(O) = score(O1) + score(O2)  íí score(Dn1(S)) + (k ? n1)  íì© u  íí max1 ííi íík{score(Di(S)) + (k? i) íì©u} = best(S). best(S) is an upper bound for the optimal solution. ? Having the score upper bound best(S) for the optimal solution, the sufficient stop condition for div-search can be defined as fol- lows. score(D(S))  íí best(S) (2) The following lemma shows that, after every iteration, div-search moves towards the sufficient stop condition. Lemma 2 For any S íí  íì¨ S, best(S íí)  íí best(S) and best(S íí)  íí best(S). Proof Sketch: Since S íí  íì¨ S, the best solution on S íí is a feasible solution on S, thus best(S íí)  íí best(S). Comparing to best(S íí), best(S) is calculated by changing some upper bounds u íí when cal- culating best(S íí) into the real scores no larger than u íí and chang- ing the other unseen upper bounds from u íí to u, where u  íí u íí is assumed by the original algorithm. Thus best(S íí)  íí best(S). ? Necessary Stop Condition: We discuss the necessary stop con- dition for div-search. The necessary stop condition is used as fol- lows. In each iteration, before invoking div-search-current(), if the necessary stop condition is not satisfied, then div-search-current() is not necessarily to be invoked in this iteration. Lemma 3 For div-search, if it can stop in a certain iteration, one of the following conditions should be satisfied before invoking the procedure div-search-current(): 1) The last generated result v = ?. 2) |S|  íí |S íí|+ k ?max{i|1  íí i  íí k,Di(S  íí) 6= ?} and the k-th largest score in S  íí u. Here S íí is the set of results when the last div-search-current() is invoked or ? if div-search-current() is never invoked. Proof Sketch: The first condition is trivial. Now suppose v 6= ?. For the second condition, when the k-th largest score in S < u, it is possible that a new result can be added that updates the k-th largest score, and thus improves the current best solution. Now we discuss |S|  íí |S íí| + k ? max{i|1  íí i  íí k,Di(S  íí) 6= ?}. max{i|1  íí i  íí k,Di(S  íí) 6= ?} is the size of the maximum independent set for G(S íí) if it is smaller than k, and k?max{i|1  íí i  íí k,Di(S  íí) 6= ?} is the minimum number of nodes needed to be added in order to generate a result of size k. If such a result does not exist, we cannot stop because we can always add some unseen nodes to any existing solution with a size smaller than k to make the score larger. As a result, we should add at least k ?max{i|1  íí i  íí k,Di(S  íí) 6= ?} nodes into S íí. ? Searching Current Set: The most important operation in our frame- work is the the algorithm div-search-current() to search the diver- sified top-k results for the current result set S. We first show the difficulties of the problems in this section and give three algorithms, namely div-astar, div-dp, and div-cut on div-search-current() in the next three sections respectively. The following lemma shows that finding the diversified top-k results is an NP-Hard problem. Lemma 4 Finding D(S) on G(S) is an NP-Hard problem. Proof Sketch: We consider a special case of the problem, where score(v) = 1 for all v  íì¨ V (G(S)), and k = |V (G(S))|. In such a case, finding Dk(R) on G(S) is equivalent to finding the v1 v2 v3 v0  íì§  íì§  íì§ u1 u2 u3  íì§  íì§  íì§ u100 v100 99 99 99 1 1 100 99 0.5 1 (a) The Greedy Solution v1 v2 v3 v0  íì§  íì§  íì§ u1 u2 u3  íì§  íì§  íì§ u100 v100 99 99 99 1 1 100 99 0.5 1 (b) The Optimal Solution Figure 2: The greedy algorithm div?astar div?dp div?cut NP NP NP NP NP NP NP NP NP NP NP NP NP NP NP NP NP NP NP NP NP NP NP NPNP Figure 3: Overview of three algorithms maximum independent set on graph G(S), which is an NP-Hard problem. Thus, the original problem is an NP-Hard problem. Greedy is Not Good: Given G(S) and k, a simple greedy algo- rithm to find D(S) works as follows. It processes in iterations. In each iteration, the node v with the maximum score is selected and put into D(S). After that, all the nodes that are adjacent to v in G(S) is removed from G(S). The process stops when G(S) is empty or D(S) contains k results. The quality of the greedy algorithm can be arbitrarily bad. The approximation ratio for the greedy algorithm is not bounded by a constant factor. Even for its special case, the maximum indepen- dent set problem is known to be hard to approximate in the litera- ture. We give an example. Fig. 2 shows a diversity graph with 201 nodes and 200 edges. Suppose k = 100. Using the greedy algo- rithm, the solution is shown in Fig. 2(a), where the selected results are marked gray. The score of the greedy solution is 199. The op- timal solution for the problem is shown in Fig. 2(b). The score of the optimal solution is 9, 900, which is nearly 50 times of the score of the greedy solution. In the following, we propose to find the optimal solution of D(S). We propose three algorithms, namely, div-astar, div-dp, and div-cut. div-astar searches the whole space S using the A? based heuris- tics by designing an upper bound function astar-bound(). Based on the NP-Hardness of the problem, div-astar can hardly handle problems with large diversity graph G. In our second div-dp al- gorithm, we decompose G into connected components. The size of each component can be much smaller than the original graph G, and is searched independently using div-astar. We combine the components using an efficient operation ? based on dynamic pro- gramming. In our third div-cut algorithm, we further decompose each connected component into subgraphs, where subgraphs are connected through a set of cut points. Each subgraph is searched independently for at most 4 times under different conditions. We combine the components using two efficient operations ? and ?. The general ideas of the three algorithms are illustrated in Fig. 3. 5. AN A? BASED APPROACH As discussed in Section 4, div-search-current(G(S), k) should return the optimal solution Di(S) for 1  íí i  íí k in order to find the early stop condition. For simplicity, we use D to denote the set of solutions, and we use D.solutioni to denote the optimal solution 1127 Algorithm 4 div-astar(G, k) Input: The diversity graph G, the top-k value. Output: Search result D. 1: H  íì§  ?; D  íì§  ?; 2: H.push((?, 0, 0, 0)); 3: for k íí = k down to 1 do 4: astar-search(G,H, D, k íí); 5: for all e  íì¨ H do 6: e.bound íì§  astar-bound(G, e, k íí); 7: update e inH; 8: return D; 9: procedure astar-search(G,H, D, k íí) 10: whileH 6= ? andH.top.bound > maxi íík íí{D.scorei} do 11: e íì§  H.pop(); 12: for i = e.pos+ 1 to |V (G)| do 13: if vi.adj(G) ? e.solution = ? then 14: e íí  íì§  (e.solution ? {vi}, i, e.score+ score(vi), 0); 15: e íí.bound íì§  astar-bound(G, e íí, k íí); 16: H.push(e íí); 17: update D using e íí.solution; 18: procedure astar-bound(G, e, k íí) 19: p íì§  |e.solution|; i íì§  e.pos+ 1; 20: bound íì§  e.score; 21: while p < k íí and i < |V (G)| do 22: if vi.adj(G) ? e.solution = ? then 23: bound íì§  bound+ score(vi); 24: p íì§  p+ 1; 25: i íì§  i+ 1; 26: return bound; with i results Di(S), and use D.scorei to denote the score for the optimal solution score(Di(S)). Our first algorithm is an A? based algorithm. The algorithm is shown in Algorithm 4. We define a max heap H to store the entries in the A? search. Each entry e  íì¨ H is with the form e = (solution, pos, score, bound). Each entry e is ranked in H according to e.bound, which is the estimated upper bound of the solution if we further expand it in the A? search. e.solution is the partial solution searched and e.pos is the position of the last searched node in e.solution. e.score is the score of the partial solu- tion, i.e., e.score = score(e.solution). The algorithm should return D.solutioni for all 1  íí i  íí k. Suppose we have an A ? algorithm that finds the optimal solution for a certain D.solutioni, the algo- rithm should be invoked k times to find the k solutions, which is costly. We show that after searching D.solutioni for a certain i, the partial solutions in H can be reused when searching D.solutionj for j < i. In the following, we first discuss the estimated upper bound for partial solutions. Then we discuss the A? algorithm to find the optimal solution D.solutioni for a certain i. At last, we discuss how the partial solutions in H can be reused to find the optimal solutions D.solutioni for all 1  íí i  íí k. Upper Bound Estimation: Given a partial solution e, for a cer- tain k íí, we show how to estimate the score upper bound if we ex- pand the partial solution to be a solution of at most k íí elements. The algorithm astar-bound is shown in Algorithm 4, line 18-26. The newly added nodes should at least satisfy the following two conditions: 1) they can not be one of e.solution, and 2) they are not adjacent to any node in e.solution. Under such conditions, we can just add the set of nodes with largest scores, and after adding the nodes, the total number of nodes is no larger than k íí. In or- der to satisfy condition 1), we visit nodes in G from the posi- tion e.pos + 1 (line 19). Since nodes in G are sorted in the non- increasing order of their scores, we add nodes one by one until the size p reaches k íí. For each node added, condition 2) can be checked using vi.adj(G) ? e.solution = ? (line 22). Lemma 5 astar-bound(G, e, k íí) finds the score upper bound for the partial solution e.solution to be expanded to a solution of at most k íí elements. Proof Sketch: Suppose we have removed all the nodes from G that are adjacent to at least one node in e.solution, then the func- tion astar-bound(G, e, k íí) calculates the upper bound by expand- ing e.solution using the set of nodes after position e.pos in G with largest scores. The optimal solution that e.solution can be ex- panded also selects the expanded nodes from the set of nodes after position e.pos but it may not select all with the largest scores since some of them may be adjacent to each other. Thus the optimal so- lution can not be larger than astar-bound(G, e, k íí). As a result, astar-bound(G, e, k íí) is a score upper bound for all expansions of e.solution. ? A? Search for a Certain k: To find the optimal solution for a certain k = k íí, the A? search algorithm astar-search is shown in Algorithm 4, line 9-17. It runs in iterations. In each iteration, the partial solution e with the largest estimated upper bound is popped out from H (line 11). e can then be expanded to new partial solu- tions by adding a new node into e.solution. The nodes are added from position e.pos + 1 in G since all nodes before the position has been processed (line 12). The newly added node vi should not be adjacent to one of e.solution(line 13), and after adding the new node, the upper bound of the new partial solution should be updated using astar-bound(), and the new partial solution should be pushed into H for further expansion (line 14-16). In line 17, suppose the",Haohan Zhu,Department of Computer Science Boston University,zhu@cs.bu.edu,George Kollios,Department of Computer Science Boston University,gkollios@cs.bu.edu,Vassilis Athitsos,Computer Science and Engineering Department University of Texas at Arlington,athitsos@uta.edu,,,,,,,,,,,,,,,,,,,,,
20200103,1387,Stefan Aulbach,"Technische Universit?t M?nchen, Germany",stefan.aulbach@in.tum.de,,A Comparison of Flexible Schemas for Software as a Service,"ABSTRACT A multi-tenant database system for Software as a Service (SaaS) should offer schemas that are flexible in that they can be extended for different versions of the application and dynamically modified while the system is on-line. This pa- per presents an experimental comparison of five techniques for implementing flexible schemas for SaaS. In three of these techniques, the databaseì°½íµownsì°½í¶the schema in that its struc- ture is explicitly defined in DDL. Included here is the com- monly-used mapping where each tenant is given their own private tables, which we take as the baseline, and a map- ping that employs Sparse Columns in Microsoft SQL Server. These techniques perform well, however they offer only lim- ited support for schema evolution in the presence of existing data. Moreover they do not scale beyond a certain level. In the other two techniques, the application ì°½íµownsì°½í¶ the schema in that it is mapped into generic structures in the database. Included here are XML in DB2 and Pivot Tables in HBase. These techniques give the application complete control over schema evolution, however they can produce a significant decrease in performance. We conclude that the ideal data- base for SaaS has not yet been developed and offer some suggestions as to how it should be designed. Categories and Subject Descriptors H.4 [Information Systems Applications]: Miscellaneous; H.2.1 [Information Systems]: Database Managementì°½í¬ Logical Design General Terms Design, Performance Keywords Multi-Tenancy, Software as a Service, Flexible Schemas, Ex- tensibility, Evolution Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGMODì°½í²09, June 29?July 2, 2009, Providence, Rhode Island, USA. Copyright 2009 ACM 978-1-60558-551-2/09/06 ...$5.00. 1. INTRODUCTION In the Software as a Service (SaaS) model, a service pro- vider owns and operates an application that is accessed by many businesses over the Internet. A key benefit of this model is that, by careful engineering, it is possible to lever- age economy of scale to reduce total cost of ownership rel- ative to on-premises solutions. Common practice in this regard is to consolidate multiple businesses into the same database to reduce operational expenditures, since there are fewer processes to manage, as well as capital expenditures, since resource utilization is increased. A multi-tenant database system for SaaS should offer sche- mas that are flexible in two respects. First, it should be possible to extend the base schema to support multiple spe- cialized versions of the application, e.g., for particular ver- tical industries or geographic regions. An extension may be private to an individual tenant or shared by multiple ten- ants. Second, it should be possible to dynamically evolve the base schema and its extensions while the database is on-line. Evolution of a tenant-owned extension should be totally ì°½íµself-serviceì°½í¶: the service provider should not be in- volved; otherwise operational costs will be too high. This paper presents an experimental comparison of five techniques for implementing flexible schemas for SaaS. In three of these techniques, the database ì°½íµownsì°½í¶ the schema in that its structure is explicitly defined in DDL: Private Tables: Each tenant is given their own private in- stance of the base tables that are extended as required. In contrast, in all of the other mappings, tenants share tables. We take Private Tables as the experimental baseline. Extension Tables: The extensions are vertically partitio- ned into separate tables that are joined to the base tables along a row ID column. Sparse Columns: Every extension field of every tenant is added to its associated base table as a Sparse Column. Our experiments here use Microsoft SQL Server 2008 [1]. To implement Sparse Columns efficiently, SQL Server uses a variant of the Interpreted Storage Format [4, 7], where a value is stored in the row together with an identifier for its column. Our experimental results show that these techniques per- form well, however they offer only limited support for schema evolution. DDL commands over existing data, if they are supported at all, consume considerable resources and neg- atively impact performance. In the on-line setting, the ap- 881 plication must be given control over when and how bulk data transformations occur. An additional issue is that these techniques do not scale beyond a certain level. In the other two techniques, the application ì°½íµownsì°½í¶ the schema in that it is mapped into generic structures in the database: XML: Each base table is augmented by a column that stores all extension fields for a tenant in a flat XML document. Since these documents necessarily vary by tenant, they are untyped. Our experiments here use pureXML in IBM DB2 [20]. Pivot Tables: Each value is stored along with an identifier for its column in a tall narrow table [2]. Our exper- iments here use HBase [11], which is an open source version of Google BigTable [6]. BigTable and HBase were originally designed to support the exploration of massive web data sets, but they are increasingly be- ing used to support enterprise applications [14]. The Pivot Table mapping into HBase that we employ is consistent with best practices. These two techniques give the application complete con- trol over schema evolution, however our experimental results show that they can produce a significant decrease in perfor- mance from the baseline. For XML, the decrease is greatest for reads, which require parsing the untyped documents and reassembling typed rows. The decrease is proportional to the number of extension fields. For Pivot Tables, the de- crease is more than an order of magnitude in some cases. Note that these results should not be taken as a negative statement about the quality of these systems, since they have not been optimized for our use case. Moreover, HBase is an early-stage open source project, not a mature com- mercial product. Our results are intended to give a general indication of the trade-offs in implementing flexible schemas. Several major SaaS vendors have developed mapping tech- niques in which the application owns the schema. This ap- proach has been elevated to a design principle whereby the application derives essential capabilities by managing the metadata itself [19, 23]. To achieve acceptable performance, these applications re-implement significant portions of the database, including indexing and query optimization, from the outside. We believe that databases should be enhanced to directly support the required capabilities. Our experiments are based on a multi-tenant database testbed that simulates a simple but realistic Customer Re- lationship Management (CRM) service. The workload con- tains single- and multi-row create, read, and update oper- ations as well as basic reporting tasks. The schema can be extended for individual tenants and it can evolve over time. Our previous work with a more limited version of this testbed (no extensions) showed that the performance of Pri- vate Tables degrades if there are too many tables [3]. This effect is due to the large amount of memory needed to hold the metadata as well as an inability to keep index pages in the buffer pool. In this paper, we create only a moderate number of tables, take Private Tables as the baseline, and use it to compare the other mappings. This paper is organized as follows. Section 2 describes our multi-tenant database testbed and the CRM application that it simulates. Section 3 describes the schema mapping techniques. Section 4 presents the results of our experi- ments. Section 5 concludes that the ideal database for SaaS LineItem Product Case Contract Lead Opportunity Asset Contact Campaign Account Figure 1: CRM Application Schema has not yet been developed and offers some suggestions as to how it should be designed. 2. MULTI-TENANT DATABASE TESTBED The experiments in this paper are based on a multi-tenant database testbed we have developed that can be adapted for different database configurations. Each configuration re- quires a plug-in to the testbed that transforms abstract ac- tions into operations that are specific to and optimized for the target database. The testbed simulates a simple but realistic CRM ser- vice. Figure 1 shows the entities and relationships in the base schema. The base entities are extended with additional fields of various types for each tenant. Tenants have different sizes and tenants with more data have more extension fields, ranging from 0 to 100. The characteristics of the dataset are modeled on salesforce.comì°½í²s published statistics [13]. The testbed has nine request classes. The distribution of these requests is controlled using a mechanism similar to TPCì°½í²s card decks. Select 1: Select all attributes of a single entity as if it was being displayed in a detail page in the browser. Select 50: Select all attributes of 50 entities as if they were being displayed in a list in the browser. Select 1000: Select all attributes of the first 1000 entities as if they were being exported through a Web Services interface. Reporting: Run one of five reporting queries that perform aggregation and/or parent-child-roll-ups. Insert 1: Insert one new entity instance as if it was being manually entered into the browser. Insert 50: Insert 50 new entity instances as if data were being synchronized through a Web Services interface. Insert 1750: Insert 1750 new entity instances as if data were being imported through a Web Services interface. Update 1: Update a single entity as if it was being modi- fied in an edit page in the browser. Update 100: Update 100 entity instances as if data were being synchronized through a Web Services interface. The testbed mimics a typical application serverì°½í²s behav- ior by creating a configurable number of connections to the database backend. To avoid blockings, the connections are distributed among a set of worker hosts, each of them han- dling a few connections only. Distributing these connec- tions among multiple hosts allows for modeling various sized, multi-threaded application servers. 882 3. SCHEMA MAPPING TECHNIQUES Within a SaaS application, each tenant has a logical sche- ma consisting of the base schema and a set of extensions. To implement multi-tenancy, the logical schemas from mul- tiple tenants are mapped into one physical schema in the database. The mapping layer transforms queries against the logical schemas into queries against the physical schema so multi-tenancy is transparent to application programmers. The physical schemas for the five mapping techniques stud- ied in this paper are illustrated in Figure 2. The example data set used in this Figure is most clearly shown in the Pri- vate Tables mapping (Figure 2(a)). There are three tenants ? 17, 35, and 42 ? each of which has an Account table with Account ID (Aid) and Name fields. Tenant 17 has extended the Account table with two fields for the health care indus- try: Hospital and Beds. Tenant 42 has extended the Account table with one field for the automotive industry: Dealers. In the Extension Tables mapping (Figure 2(b)), the industry extensions are split off into separate tables that are joined to the base Account table using a new Row number column (Row). Tenants share the tables using a tenant ID column (Tenant). This section describes the other three mappings in more detail. 3.1 Sparse Columns in Microsoft SQL Server Sparse Columns were originally developed to manage data such as parts catalogs where each item has only a few out of thousands of possible attributes. Storing such data in con- ventional tables with NULL values can decrease performance even with advanced optimizations for NULL handling. To implement Sparse Columns, SQL Server 2008 uses a variant of the Interpreted Storage Format [4, 7], where a value is stored in the row together with an identifier for its column. In our mapping for SaaS, the base tables are shared by all tenants and every extension field of every tenant is added to the corresponding base table as a Sparse Column, as il- lustrated in Figure 2(c). Sparse columns must be explicitly defined by a CREATE/ALTER TABLE statement in the DDL and, in this sense, are owned by the database. Nev- ertheless, the application must maintain its own description of the extensions, since the column names cannot be stati- cally embedded in the code. For writes, the application must ensure that each tenant uses only those columns that they have declared, since the namespace is global to all tenants. For reads, the application must do an explicit projection on the columns of interest, rather than doing a SELECT ?, to ensure that NULL values are treated correctly. Sparse Columns requires only a small, fixed number of tables, which gives it a performance advantage over Pri- vate Tables; [3] shows that having many tables negatively impacts performance. On the other hand, there is some overhead for managing Sparse Columns. As an example, the SQL Server documentation recommends using a Sparse Column for an INT field only if at least 64% of the values are NULL [15]. Both of these factors are reflected in the performance results presented in Section 4. 3.2 XML in IBM DB2 IBM pureXML was designed to allow processing of semi- structured data alongside of structured relational data [20]. The mapping for SaaS that we use follows the recommenda- tions in the pureXML documentation for supporting multi- tenancy [21]. The base tables are shared by all tenants and Account17 Aid Name Hospital Beds 1 Acme St. Mary 135 2 Gump State 1042 Account35 Aid Name 1 Ball Account42 Aid Name Dealers 1 Big 65 (a) Private Tables AccountExt Tenant Row Aid Name 17 0 1 Acme 17 1 2 Gump 35 0 1 Ball 42 0 1 Big HealthcareAccount Tenant Row Hospital Beds 17 0 St. Mary 135 17 1 State 1042 AutomotiveAccount Tenant Row Dealers 42 0 65 (b) Extension Tables Account Tenant Aid Name SPARSE 17 1 Acme Hospital St. Mary Bed 135 17 2 Gump Hospital State Bed 1042 35 1 Ball 42 1 Big Dealer 65 (c) Sparse Columns Account Tenant Aid Name Ext XML 17 1 Acme <ext><hospital>St. Mary</hospital> <beds>135</beds></ext> 17 2 Gump <ext><hospital>State</hospital> <beds>1042</beds></ext> 35 1 Ball 42 1 Big <ext><dealers>65</dealers></ext> (d) XML RowKey Account Contact 17Act1 [name:Acme, hospital:St. Mary, beds:135 ] 17Act2 [name:Gump, hospital:State, beds:1042 ] 17Ctc1 [íì¨ íì¨ íì¨ ] 17Ctc2 [íì¨ íì¨ íì¨ ] 35Act1 [name:Ball] 35Ctc1 [íì¨ íì¨ íì¨ ] 42Act1 [name:Big, dealers:65 ] (e) Pivot Tables Figure 2: Schema Mapping Techniques each base table is augmented by a column (Ext XML) that stores all extension fields for a tenant in a flat XML docu- ment, as illustrated in Figure 2(d). Since these documents necessarily vary by tenant, they are untyped. This repre- sentation keeps the documents as small as possible, which is an important consideration for performance [16]. pureXML offers a hybrid query language that provides native access to both the structured and semi-structured representations. Our testbed manipulates data in the struc- tured format, thus accessing extension data requires a corre- lated subquery to manage the XML. This subquery extracts the relevant extension fields using the XMLTABLE function which converts an XML document into a tabular format us- ing XPath. The query with the XMLTABLE function has 883 SELECT b.Tenant, b.Aid, b.Name, e.Dealers FROM Accounts b, XMLTABLE(ì°½í²i/extì°½í² PASSING b.Ext_XML AS ""i"" COLUMNS Dealers INTEGER PATH ì°½í²dealersì°½í² ) AS e WHERE Tenant = 42 AND Aid = 1; (a) Physical SELECT Query accounts tid,aid IXSCAN XSCANFETCH NLJOIN accounts RETURN (b) Query Execution Plan Figure 3: Correlated Subquery for XML in DB2 to be generated client- and query-specific to access clientsì°½í² extension fields relevant in the particular query. Figure 3(a) shows an example query against the physical schema that selects three base fields and one extension field; Figure 3(b) shows the associated query plan. In our testbed, rows are always accessed through base fields, hence there is no need to use the special XML indexes offered by pureXML [20]. To insert a new tuple with extension data, the application has to generate the appropriate XML document; our per- formance results generally include the time to perform this operation. Updates to extension fields are implemented us- ing XQuery 2.0 features to modify documents in place. 3.3 Pivot Tables in HBase HBase [11], which is an open source version of Google BigTable [6], was originally designed to support the explo- ration of massive web data sets. These systems are increas- ingly being used to support enterprise applications in a SaaS setting [14]. In an HBase table, columns are grouped into column fam- ilies. Column families must be explicitly defined in advance in the HBase ì°½íµDDLì°½í¶, for this reason they are owned by the database. There should not be more than tens of column families in a table and they should rarely be changed while the system is in operation. Columns within a column family may be created on-the-fly, hence they are owned by the ap- plication. Different rows in a table may use the same column family in different ways. All values in a column are stored as Strings. There may be an unbounded number of columns within a column family. Data in a column family is stored together on disk and in memory. Thus, a column family is essentially a Pivot Table; each value is stored along with an identifier for its column in a tall narrow table [2]. HBase was designed to scale out across a large farm of servers. Rows are range-partitioned across the servers by key. Applications define the key structure, therefore implic- itly control the distribution of data. Rows with the same key SELECT p.Name, COUNT(c.Case_id) AS cases FROM Products p, Assets a, Cases c WHERE c.Asset = a.Asset_id AND a.Product = p.Product_id GROUP BY p.Name ORDER BY cases DESC Figure 4: Logical Reporting Query prefix will be adjacent but, in general, may end up on differ- ent servers. The rows on each server are physically broken up into their column families. The mapping for SaaS that we use is illustrated in Fig- ure 2(e). In keeping with best practices for HBase, this map- ping ensures that data that is likely to be accessed within one query is clustered together. A single HBase table is used to store all tables for all tenants. The physical row key in HBase consists of the concatenation of the tenant ID, the name of the logical table, and the key of the row in the log- ical table. Each logical table is packed into its own column family, thus each row has values in only one column family. Within a column family, each column in the logical table is mapped into its own physical HBase column. Thus, since columns are dynamic, tenants may individually extend the base tables. The reporting queries in our testbed require join, sort and group operations, which are not currently provided by HBase. We therefore implemented these operators outside the database in an adaptation layer that runs in the client. The adaptation layer utilizes operations in the HBase client API such as update single-row, get single-row and multi-row scan with row-filter. As an example, consider the reporting query shown in Figure 4, which produces a list of all Prod- ucts with Cases by joining through Assets. To implement this query, our adaptation layer scans through all Cases for the given tenant and, for each one, retrieves the associated Asset and Product. It then groups and sorts the data for all Cases to produce the final result. In our experiments, HBase was configured to run on a sin- gle node and the Hadoop distributed map-reduce framework was not employed. In our experience, hundreds of tenants for an application like CRM can be managed by a database on a single commodity processor. In this setting, spreading the data for a tenant across multiple nodes and doing dis- tributed query processing would not be advantageous; the overhead for managing the distribution would nullify any benefits of parallelization. Of course, in addition to scal- ing up to handle many small tenants, the ideal SaaS data- base should also scale out to handle large tenants. But even in this case, map-reduce is problematic for queries such as the one in Figure 4, since it requires that data be clustered around Products. Other queries, such as pipeline reports on Opportunities, might require that the data be clustered in other ways. We conclude this section with several comments about the usage of HBase in our experiments. First, HBase offers only row-at-a-time transactions and we did not add a layer to extend the scope to the levels provided by the commer- cial databases. Second, compression of column families was turned off. Third, neither major nor minor compactions oc- curred during any of the experiments. Fourth, replication of data in the Hadoop file system was turned off. Fifth, column families were not pinned in memory. Sixth, the system was configured so that old attribute values were not maintained. 884  1  10  100  1000  10000  100000 Sel 1 Sel 50 Sel 1000 R eport Ins 1 Ins 50 Ins 1750 U pd 1 U pd 100 R e s p o n s e  T im e  [ m s e c ] Query Classes Private Tables Extension Tables Sparse Column (a) Overall  1  10  100  1000  10000  100000 Sel 1 Sel 50 Sel 1000 R eport Ins 1 Ins 50 Ins 1750 U pd 1 U pd 100 R e s p o n s e  T im e  [ m s e c ] Query Classes Priv. Table (Small Tenant) Priv. Table (Medium Tenant) Priv. Table (Large Tenant) Sparse (Small Tenant) Sparse (Medium Tenant) Sparse (Large Tenant) (b) By Tenant Size Figure 5: SQL Server Performance 4. EXPERIMENTAL RESULTS This section presents the results of our experiments on schema extensibility and evolution. To study schema evolu- tion, we issued a series of schema alteration statements dur- ing a run of the testbed and measured the drop in through- put. The experiments were run on Microsoft SQL Server 2008, IBM DB2 V.9.5 on Windows 2008, and HBase 0.19 on Linux 2.6.18 (CentOS 5.2). The database host was a VM on VMWare ESXi with 4 3.16 GHz vCPUs and 8 GB of RAM. 4.1 Microsoft SQL Server Figure 5(a) shows the results of running our testbed on Microsoft SQL Server using three different mappings: Pri- vate Tables, Extension Tables, and Sparse Columns. The horizontal axis shows the different request classes, as de- scribed in Section 2, and the vertical axis shows the response time in milliseconds on a log scale. In comparison to Private Tables, Extension Tables clearly exhibits the effects of vertical partitioning: wide reads (Sel 1, Sel 50, Sel 1000) are slower because an additional join is re- quired, while narrow reads (Report) are faster because some unnecessary loading of data is avoided. Updates (Upd 1, Upd 100) perform similarly to wide reads because our tests modify both base and extension fields. Extension Tables is faster for inserts because tables are shared among tenants so there is a greater likelihood of finding a page in the buffer pool with free space. Sparse Columns performs as well or better than Private Tables in most cases. The additional overhead for managing the Interpreted Storage Format appears to be offset by the fact that there are fewer tables. Sparse Columns performs worse for large inserts (Ins 1750), presumably because the implementation of the Interpreted Storage Format is tuned to favor reads over writes. Figure 5(b) shows a break down of the Private Table and Sparse Column results by tenant size. Recall from Section 2 that larger tenants have more extension fields, ranging from 0 to 100. The results show that the performance of both mappings decreases to some degree as the number of exten- sion fields goes up. SQL Server permits up to 30,000 Sparse Columns per ta- ble. Our standard configuration of the testbed has 195 ten- ants, which requires about 12,000 columns per table. We also tried a configuration with 390 tenants and about 24,000 columns per table and there was little performance degra- dation. The number of extension fields per tenant in our testbed is drawn from actual usage, so SQL Server is unlikely to be able to scale much beyond 400 tenants. As a point of comparison, salesforce.com maintains about 17,000 tenants in one (very large) database [13]. Figures 6(a) and 6(b) show the impact of schema evolu- tion on throughput in SQL Server. In these graphs, the horizontal axis is time in minutes and the vertical axis is transactions per minute. The overall trend of the lines is downward because data is inserted but not deleted during a run. Part way through each run, ALTER TABLE state- ments on 5 base tables were submitted. The first two lines in each graph show schema-only DDL statements: add a new column and increase the size of a VARCHAR column. The third line in each graph shows a DDL statement that affects existing data: decrease the size of a VARCHAR column. To implement this statement, SQL Server scans through the table and ensures that all values fit in the reduced size. A more realistic alteration would perform more work than this, so the results indicate a lower bound on the impact of evo- lution. The gray bar on each graph indicates the period during which this third operation took place. In the Private Tables case (Figure 6(a)), 975 ALTER TA- BLE statements were submitted, 5 for each of the 195 ten- ants. Individual schema-only alterations completed very rapidly, but nevertheless had an impact on throughput be- cause there were so many of them. Adding a new column took about 1 minute to complete while increasing the size of a VARCHAR column took about 3 minutes. Decreasing the size of a VARCHAR column took about 9 minutes and produced a significant decrease in throughput. The overall loss of throughput in each case is indicated by the amount of time it took to complete the run. In the Sparse Columns case (Figure 6(b)), the tables are shared and 5 ALTER TABLE statements were submitted. The schema-only changes completed almost immediately and had no impact on throughput. Decreasing the size of a VAR- CHAR column took about 2 minutes, during which through- 885  0  2000  4000  6000  8000  10000  12000  14000  16000  0 5 10 15 20 25 30 T ra n s a c ti o n s  p e r  M in u te Testbed runtime (min) Add new column Increase VARCHAR size Decrease VARCHAR size (a) Private Tables  0  2000  4000  6000  8000  10000  12000  14000  16000  0 5 10 15 20 25 30 T ra n s a c ti o n s  p e r  M in u te Testbed runtime (min) Add new column Increase VARCHAR size Decrease VARCHAR size (b) Sparse Columns Figure 6: SQL Server Throughput put dropped almost to zero. The overall loss of throughput was greater for Private Tables, as indicated by the amount of time it took to complete the runs. However the behavior of Private Tables is probably preferable in the SaaS setting be- cause the throughput drop is never as deep, thus the servers donì°½í²t need to be overprovisioned as much. In any case, nei- ther of these mappings is ideal in that the application should have more control over when such resource-intensive opera- tions occur. 4.2 IBM DB2 Figure 7(a) shows the results of running our testbed on IBM DB2 using three different mappings: Private Tables, Extension Tables, and XML using pureXML. The axes are the same as in Figure 5. In comparison to Private Tables, Extension Tables ex- hibits the same performance variations as in SQL Server. However XML produces a decrease in performance in most cases. The decrease is particularly severe for reads, which re- quire executing a correlated subquery containing an XQuery statement embedded in a call to the XMLTABLE function, as described in Section 3.2. Figure 7(b) shows a break down of the Private Table and XML results by tenant size. Re-  1  10  100  1000  10000  100000 Sel 1 Sel 50 Sel 1000 R eport Ins 1 Ins 50 Ins 1750 U pd 1 U pd 100 R e s p o n s e  T im e  [ m s e c ] Query Classes Private Tables Extension Tables XML (a) Overall  1  10  100  1000  10000  100000 Sel 1 Sel 50 Sel 1000 R eport Ins 1 Ins 50 Ins 1750 U pd 1 U pd 100 R e s p o n s e  T im e  [ m s e c ] Query Classes Private Table (Small Tenant) Private Table (Medium Tenant) Private Table (Large Tenant) XML (Small Tenant) XML (Medium Tenant) XML (Large Tenant) (b) By Tenant Size Figure 7: DB2 Performance call from Section 2 that larger tenants have more extension fields, ranging from 0 to 100. The results show that for reads, the performance decrease of XML is proportional to the number of extension fields. Note that in the Insert 1750 case, the results do not include the time to construct the XML document (for no particularly good reason) and there is no variation based on tenant size. XML gives the application complete control over schema evolution. In this setting, the application is responsible for performing any bulk transformations associated with schema alterations that impact existing data. To st",Stefan Aulbach,"Technische Universit?t M?nchen, Germany",stefan.aulbach@in.tum.de,Dean Jacobs,"SAP AG, Walldorf, Germany",dean.jacobs@sap.com,Alfons Kemper,"Technische Universit?t M?nchen, Germany",alfons.kemper@in.tum.de,Michael Seibold,"Technische Universit?t M?nchen, Germany",michael.seibold@in.tum.de,,,,,,,,,,,,,,,,,,
20200104,1126,Orestis Polychroniou,Columbia University,orestis@cs.columbia.edu,,A Comprehensive Study of Main-Memory Partitioning and its Application to Large-Scale Comparison- and Radix-Sort,"ABSTRACT Analytical database systems can achieve high throughput main-memory query execution by being aware of the dynam- ics of highly-parallel modern hardware. Such systems rely on partitioning to cluster or divide data into smaller pieces and thus achieve better parallelism and memory locality. This paper considers a comprehensive collection of variants of main-memory partitioning tuned for various layers of the memory hierarchy. We revisit the pitfalls of in-cache parti- tioning, and utilizing the crucial performance factors, we in- troduce new variants for partitioning out-of-cache. Besides non-in-place variants where linear extra space is used, we introduce large-scale in-place variants, and propose NUMA- aware partitioning that guarantees locality on multiple pro- cessors. Also, we make range partitioning comparably fast with hash or radix, by designing a novel cache-resident index to compute ranges. All variants are combined to build three NUMA-aware sorting algorithms: a stable LSB radix-sort; an in-place MSB radix-sort using different variants across memory layers; and a comparison-sort utilizing wide-fanout range partitioning and SIMD-optimal in-cache sorting. To the best of our knowledge, all three are the fastest to date on billion-scale inputs for both dense and sparse key domains. As shown for sorting, our work can serve as a tool for build- ing other operations (e.g., join, aggregation) by combining the most suitable variants that best meet the design goals. 1. INTRODUCTION The increasing main-memory capacity of contemporary hardware allows query execution to occur entirely in mem- ory. If the entire database also fits in RAM, analytical query workloads that are typically read-only need no disk access after the initial load, setting the memory bandwidth as the only performance bound. Since analytics are at the core of business intelligence tasks today, the need for high- throughput main-memory query execution is apparent. ?This work was supported by National Science Foundation grant IIS-0915956 and a gift from Oracle Corporation. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full cita- tion on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGMOD  íí14, June 22?27, 2014, Snowbird, UT, USA. Copyright 2014 ACM 978-1-4503-2376-5/14/06 ...$15.00. http://dx.doi.org/10.1145/2588555.2610522. To maximize memory bandwidth and capacity, a few CPUs can be combined in a shared-memory system using a fast interconnection. Such hardware combines the parallelism of multiple multi-core CPUs with a higher aggregate memory bandwidth. The shared-memory functionality is provided by the non-uniform-memory-access (NUMA) interconnection, adding an additional layer in the memory hierarchy. In a modern multi-core CPU, the best performance is achieved when all cores work in a shared-nothing fashion and the working set is small enough to fit in the fast (and private per core) caches. The same approach was more effi- cient even before the advent of the multi-core era [11], since random RAM accesses are too expensive out-of-cache. Query execution is decomposed into a series of operations, the most time consuming of which are typically joins and aggregations. To speed up these operations using hardware parallelism, we partition into small pieces using the keys, then process each piece independently. For instance, an effi- cient algorithm for joins is to hash partition in parallel until the input is split into cache resident pieces before we execute the join using a hash table [11]. In fact, even in-cache join can further partition into trivial parts with very few distinct items, before executing a nested loop to join them [7]. This paper considers a comprehensive menu of partition- ing options across several dimensions. The three types of partitioning are hash, radix and range partitioning, depend- ing on the function that takes the key as an input and out- puts the destination partition. Partitioning also depends on the layer of the memory hierarchy that it targets, namely in-cache, out-of-cache and across NUMA regions. Finally, we distinguish partitioning variants based on whether they use auxiliary space that is linear to the input size or not. Until recently, prior work used the in-cache versions of partitioning and, if parallel, the non-in-place variant, which can be trivially distributed across threads. In all cases, when the input is larger than the cache, the performance is throt- tled by TLB misses [11] and cache conflicts [14]. Satish et al. [14] suggested in-cache buffering to mitigate TLB misses and Wassenberg et al. [15] used non-temporal writes on cache line sized buffers to facilitate hardware write-combining. Efficient out-of-cache partitioning [14, 15] assumes free access to linear auxiliary space, to write the output. We introduce several in-place out-of-cache partitioning variants utilizing the same crucial performance factors: an in-place method analogous to the shared-nothing non-in-place method; a modified non-in-place method that generates the output as a list of blocks that overwrites the input; and a parallel non-in-place method that combines the previous two. 755 To support scaling to multiple CPUs, we consider how the NUMA layer affects partitioning performance and modify both in-place and non-in-place out-of-cache partitioning to guarantee minimal transfers across NUMA boundaries, also ensuring sequential accesses so that hardware pre-fetching can hide the latency of the NUMA interconnection [1]. All of the above algorithms target the data shuffling part of partitioning and implicitly assume that the partition func- tion is cheap and can be computed at virtually no cost. While this assumption holds for radix and hash partitioning given a suitable hash function choice, the cost of computing a range partition function is higher than the cost to transfer the tuple, especially when the number of partitions increases beyond the TLB capacity. The standard (and slow) imple- mentation is a binary search in a sorted array of delimiters that define the partition ranges. The slowdown is caused by the logarithmic number of cache loads. We introduce a specialized SIMD-based cache-resident index that speeds up range function computation up to 6 times and makes range partitioning a practical choice for many applications. All partitioning variants discussed in this paper are shown in Figure 1, where we also mark our contributions. We ap- ply all variants to design and implement three large-scale NUMA-aware sorting algorithms. We use sorting, rather than joins or aggregations for two reasons. First, because it is a wider problem that can be a sub-problem for both join and aggregation. Second, we can apply all partitioning variants to build unique sorting algorithms, such that each is more scalable in distinct cases depending on input size, key domain size, space requirements, and skew efficiency. The first sorting algorithm we propose is stable least- significant-bit (LSB) radix-sort based on non-in-place out- of-cache radix partitioning [14] where we add two innova- tions. We use hybrid range-radix partitioning to provide perfect load balancing, and guarantee that each tuple will cross NUMA boundaries at most once, even if the algorithm would by default re-organize the entire array in each pass. The second sorting algorithm we propose is an in-place most-significant-bit (MSB) radix-sort that uses all variants of in-place partitioning that we introduce, one for each dis- tinctive level in the memory hierarchy: shared out-of-cache in-place partitioning, shared-nothing out-of-cache partition- ing, and in-cache partitioning. We reuse the range-radix idea of LSB radix-sort for NUMA optimality and load balancing. The third sorting algorithm we propose is a comparison- sort that uses the newly optimized range partitioning. We perform very few out-of-cache range partitioning passes with a very wide fanout until we reach the cache, providing NUMA optimality. In the cache, we employ sorting [6] that scales to the SIMD length, modified to use the cache more effectively. radix hash range partition in-cache non-in-place out-of-cache out-of-cache in-place in-cache shared in block lists in segments sharedshared-nothing NUMA oblivious NUMA aware previously known our contributions Figure 1: Partitioning variants and contributions We use fixed length integer keys and payloads, typical of analytical database applications. We evaluate on both dense and sparse key domains. If order-preserving compression is used [12, 16], any sparse or dense domain with fixed or vari- able length data is compacted into a dense integer domain. Skewed workload distribution can reduce parallelism in some na?íì§¤íì§ve approaches. In our context, when we statically distribute partitions to threads, we ensure that the parti- tions are as balanced as possible. Specifically, we never use radix partitioning to any range of bits to divide the work- load. Instead, we combine range with radix partitioning to guarantee that, if specific bit ranges have very few distinct values, we can find delimiters that split the workload equally among threads, independently of the key value range. We summarize our contributions: ? We introduce several new variants for main-memory partitioning, most notably large-scale in-place parti- tioning and efficient range partitioning, and also guar- antee minimal NUMA transfers across multiple CPUs. ? We combine partitioning variants to design three sort- ing algorithms, all the fastest of their class for billion- scale inputs, and evaluate the best options depending on input size, key domain, available space, and skew. The rest of the paper is organized as follows. Section 2 outlines related work. In Section 3 we describe partitioning variants. In Section 4 we discuss sorting. Section 5 presents our experimental results, and we conclude in Section 6. 2. RELATED WORK We first outline related work on partitioning. Manegold et al. [11] identified the TLB thrashing problem when na?íì§¤íì§vely partitioning to a large number of outputs. Satish et al. [14] introduced efficient out-of-cache partitioning and Wassen- berg et al. [15] identified the significance of write-combining. Manegold et al. [11] proposed partitioning to cache-resident hash tables to join and Kim et al. [7] reused the same design on a multi-core CPU. Wu et al. [17] proposed hardware ac- celerated partitioning for performance and power efficiency. We briefly outline recent work on sorting for modern hard- ware, due to space constraints. Inoue et al. [6] proposed in- cache SIMD-vector comb-sort followed 2-way SIMD merg- ing. Chhugani et al. [5] proposed in-cache sorting networks followed by cyclic merging in buffers to avoid being memory bound. Kim et al. [7] compared sort-merge-join against hash join, projecting that sort-merge-join will eventually outper- form hash with wider SIMD. Satish et al. [14] compared radix-sort and merge-sort in CPUs and GPUs on multiple key domain sizes and concluded in favor of merge-sort. How- ever, the result is based on small arrays and only LSB radix- sort is considered. Wassenberg et al. [15] improved over Satish et al. [14] and claimed that radix-sort is better. Kim et al. [9] studied network-scale sorting maximizing network transfer with CPU computation overlap. Albutiu et al. [1] studied the NUMA effects using sort-merge-join on multiple CPUs with billion-scale arrays. Balkesen et al. [4] claimed that non-partitioning hash joins are competitive, but Balke- sen et al. [3] improved over Blanas et al. [4] and concluded that partitioning joins are generally faster, even without us- ing fast partitioning [14, 15]. Balkesen et al. [2] further im- proved joins on multiple CPUs using fast partitioning. Thus, on the fastest CPUs [3], partitioning appears to be the best choice for both hash joins and radix-sort-merge-joins. 756 3. PARTITIONING 3.1 In-Cache We start by considering the versions that best operate when the table fits in the cache. The non-in-place version (Algorithm 1) uses a separate array from the input to store the output, while the in-place version (Algorithm 2) uses one array for both input and output. Each partition is generated in a single segment. In-cache partitioning can be run in parallel, if the threads operate in a shared-nothing fashion. Algorithm 1 Non-in-place in-cache partitioning i íì§  0 // P : the number of partitions for p íì§  0 to P -1 do offset[p] íì§  i // point at the start of each partition i íì§  i + histogram[p] end for for iin  íì§  0 to |Tin|-1 do t íì§  Tin[iin] // Tin: the input table iout  íì§  offset[f(t.key)] + + // f : the partition function Tout[iout] íì§  t // Tout: the output table end for The simplest non-in-place version does only two random accesses per item. When operating in the cache, we need the output and the offset array to be cache-resident. A slightly more complicated version of the algorithm allows the parti- tioning to happen in-place, by swapping items across loca- tions. In short, we start by reading an item, find the correct partition and the output destination through the offset ar- ray, swap it with the item stored there, and continue for the new item until the cycle is closed. Each item is moved exactly once and we stop when the whole array is covered. Item swaps are performed in cycles of transfers, defined as swap cycles. When the items are processed low-to-high [1], the cycle starts by doing a read and then swaps until it reaches the same location it initially read from, to write back. This case occurs 1/P of time on average but requires branching. In Algorithm 2 below, the partitions are written high-to-low and swap cycles close when all items of a parti- tion have been placed, avoiding branching for every tuple. Algorithm 2 In-place in-cache partitioning i íì§  0 // P : the number of partitions for p íì§  0 to P -1 do i íì§  i + histogram[p] offset[p] íì§  i // point at the end of each partition end for p íì§  iend  íì§  0 while histogram[p] = 0 do p+ + // skip initial empty partitions end while repeat t íì§  T [iend] // T : the input & output table repeat p íì§  f(t.key) // f : the partition function i íì§  ??offset[p] T [i] íì§§ t // swap until i = iend repeat iend  íì§  iend + histogram[p+ +] // skip if empty until p = P or iend 6= offset[p] until p = P 3.2 Out-of-Cache Out-of-cache performance is throttled by increased cache conflicts [14] and cache pollution with output tuples [15]. TLB thrashing occurs when the number of partitions ex- ceeds the TLB capacity [11], unless the entire dataset can be placed in equally few large OS pages to be TLB resident. 3.2.1 Non-in-place To mitigate these problems, prior work [14] proposed using the cache as an intermediate buffer before writing back to memory. Also, when write backs occur, they bypass the higher cache levels entirely and avoid polluting the cache [15]. Recent work [2] uses the same basic technique for out- of-cache radix partitioning during hash join execution. Buffering data for each partition reduces the working set size and eliminates the TLB problem when operating in the buffer. TLB misses still occur, but 1/L of the time, if L is the number of tuples buffered for each partition before writing to output. If the buffer for each partition is exactly as big as a cache line, writing the full cache line to memory is accel- erated by write-combining and avoids polluting the higher cache levels with output data. The partitioning fanout is now bounded by the number of cache lines in the fast core- private cache, rather than the TLB entries. Buffer flushing is optimally done using wider registers [15]. To maximize the cache use, we use the last buffer slot to save the output offset and access one cache line per iteration (Algorithm 3). To extend the above method to multiple columns stored in separate arrays, the standard case in RAM-resident database data, we use one cache line per column in the buffer of each partition. A generic implementation can use one cache line per column and flush it separately depending on the column width. We can also interleave the columns in a single tuple and de-interleave the columns when the buffer is flushed. For example, when partitioning arrays of 32-bit keys and 32-bit payloads, we store 64-bit tuples in the cached buffer. Tuple (de-)interleaving can be accelerated using SIMD. Parallel execution of the non-in-place out-of-cache parti- tioning is trivial. The input can be split to equal pieces, one for each thread. By executing a prefix sum of all individ- ual histograms, one can ensure that each partition output is written in a distinct location. Threads are only synchronized after individual histograms are built. This is the only known technique for parallel partitioning on shared segments. Algorithm 3 Non-in-place out-of-cache partitioning iout  íì§  0 // P : the number of partitions for p íì§  0 to P -1 do buffer[p][L-1] íì§  iout // L: # of tuples per cache line iout  íì§  iout + histogram[p] end for for iin  íì§  0 to |Tin|-1 do t íì§  Tin[iin] // Tin/Tout: the input/output table p íì§  f(t.key) // f : the partition function iout  íì§  buffer[p][L-1] + + buffer[p][iout mod L] íì§  t if iout mod L = L-1 then for ibuf  íì§  0 to L-1 do Tout[iout + ibuf ? L] íì§  buffer[p][ibuf ] // no cache end for buffer[p][L-1] íì§  iout + 1 end if end for 757 3.2.2 In-place, Shared-Nothing Segments Adapting the out-of-cache buffering technique to in-place partitioning requires a more complicated approach. The ba- sic idea is to perform the swaps inside the buffer, so that the sparse RAM locations are accessed only 1/L of the time, re- ducing the overhead from TLB misses. Compared with non- in-place out-of-cache partitioning, which uses the buffer as an intermediate layer to group tuples before writing them, in-place out-of-cache partitioning performs all tuples swaps in the buffer, and accesses RAM one cache line at-a-time. Before the main partitioning loop starts, we load cache lines from all starting partition locations. Item swaps be- tween partitions occur using the last L tuples that are stored in the buffer. When a buffer has swapped all L items, the cache line is streamed to the RAM location it was loaded from and the buffer is re-filled with the next L items of the same partition. Thus, we operate in the buffer (L? 1)/L of the time and do not miss in the TLB. The offsets are stored inside the buffer and the last L items of each partition are handled differently to eliminate branching in the inner loop. If having T contiguous segments per partition (T is the number of threads) is acceptable, then we can run in-place partitioning in parallel. However, unlike the non-in-place variant, generating one segment per partition across threads is impossible with coarse-grained synchronization. Algorithm 4 In-place out-of-cache partitioning i íì§  0 // P : the number of partitions for p íì§  0 to P -1 do end[p] íì§  i i íì§  i + histogram[p] for ibuf  íì§  0 to L-1 do buffer[p][ibuf ] íì§  T [i? (i mod L) + ibuf ] end for item0[p] íì§  buffer[p][0] // save 1st item out of buffer buffer[p][0] íì§  i [...] // special handling for partitions smaller than L end for p íì§  0 while histogram[p] = 0 do p+ + // skip initial empty partitions end while t íì§  T [0] // T : the input & output table loop repeat p íì§  f(t.key) // f : the partition function i íì§  ??buffer[p][0] buffer[p][i mod L]  íì§§ t // swap until i mod L = 0 // L: # of tuples per cache line [...] // (rare) branch for end of partition (exits here) for ibuf  íì§  0 to L-1 do T [i] íì§  buffer[p][ibuf ] // no cache end for for ibuf  íì§  0 to L-1 do buffer[p][ibuf ] íì§  T [i? L] // cache end for t íì§  item0[p] item0[p] íì§  buffer[p][0] buffer[p][0] íì§  i if i ? end[p] < L then [...] // (rare) branch for last L items of partition end if end loop 3.2.3 In-place, List of Blocks For large-scale out-of-cache partitioning, the requirement of producing all partitions in P non-splitting segments can be relaxed. Instead of writing each partition output sequen- tially, we can write large blocks that only contain items from a single partition. When the block is full, we get a new block at some new available location. The block size must be large enough to amortize sequential writes, but not too large, in order to avoid external fragmentation from non-full blocks. To access data from a single partition only, we create a small linked list that connects all blocks that contain data of the same partition. While the access is not entirely se- quential as in the single segment case, the list hops after scanning each block are amortized by a sufficient block size. This method can be done in place, if we remove P  íì§B items from the start of the input and save it in private space (B is the block capacity in tuples). We start range partitioning the input from the (P  íì§B)-th tuple. By the time any partition is filled, the input pointer will have advanced enough for the output to safely use the space of input we read before, without overwriting tuples not yet read. At the end, we also add the data initially copied out back to the correct block lists. For each partition, only the last block of the block list can be non-full. Thus, unused space has an upper bound of P  íì§B and is negligible compared to size of the input. Block-based partitioning has a number of nice properties. First, it uses the fast non-in-place out-of-cache partition- ing. Second, it does not require the pre-computation of a histogram. Third, it can be done in place by ensuring no overlap between input and output data. Finally thread par- allelism is trivial; the only requirement is to connect the linked lists of blocks from all threads for each partition. 3.2.4 In-place, Shared Segments In order to partition and shuffle data in parallel inside the same segment, we need fine-grain synchronization. Since us- ing OS latches are overly expensive, we use atomic instruc- tions. Atomic fetch-and-add reads a memory location, in- crements it by some value and returns its previous value. Imagine an array of items and multiple threads where each item must be processed by exactly one thread. Each thread can safely use item at index i which is returned by invok- ing fetch-and-add(c,1) on a shared counter c. When done, the thread asks for the next item to process or terminates if i exceeds the number of items. We apply the same idea to in-place partitioning using one shared counter for each partition to represent the number of tuples swapped so far. We use fetch-and-add on the shared counter of partition p, to  íì§¸lock íì§¹ the cell of the next yet unread item of partition p. We store the first index that initiates the cycle. After swapping an arbitrary number of keys, when we return to the original partition p, we store the last tuple in the initial location. Only the P counters are shared across threads. As mentioned in Section 3.1, we define a swap cycle as a sequence of swaps that starts by reading a key from a specific partition and after a number of swaps, returns to the same partition to write a key in the initially read location. We cannot know in advance how large a swap cycle will be, thus we cannot lock all locations the cycle will go through before actually moving tuples. Imagine a scenario where the first partition has only one item found in the last cell of the array. Then, one thread would perform a single swap cycle covering all items before the last cell is reached and the cycle is closed. 758 To solve this first problem, threads lock only one location at a time for one swap. However, when close to comple- tion, multiple threads may compete for swap cycles, creating deadlocks. For example, assuming one item per partition, if thread t1 reads item kx (must go to lx) from location ly (must bring ky here) and thread t2 reads kz (must go to lz) from lx (must bring kx here), t1 will find no space for kx, be- cause the offset of partition X was incremented by t2 when it read kz. If t1 waits, t2 will reach ky. Then, a deadlock will occur, since t1 holds (kx, ly) and t2 holds (ky, lx). To solve this second problem and avoid waiting for others, when a thread finds a partition to be full, it records both the current key and the locked location that the swap cycle started from. In the above example, t1 records (kx, ly) and t2 records (ky, lx). A final fix step occurs  íì§¸offline íì§¹ and takes trivial time, as the number of such pairs is upper bounded by the number of partitions P , times the number of threads. So far, we presented a way for multiple threads to partition items in-place concurrently, but this solution is impractical if used as is. First, we make no use of buffering to improve out-of-cache performance and second, for each key we move to its destination, we update a shared variable triggering cache invalidations on every tuple move. To make this ap- proach practical, we change the unit of transfer from tuples to blocks. Each block must have a fixed size and all tuples must belong to the same partition. We generate such blocks using the technique described previously (see Section 3.2.3). Out-of-cache accesses are amortized by the block size, as is the synchronization cost of accessing shared variables. Algorithm 5 Synchronized in-place partitioning Pactive  íì§  {} // Pactive: set of yet unfinished partitions Tdeadlock  íì§  {} // Tdeadlock: set of tuple & location pairs i íì§  0 // P : the number of partitions for p íì§  0 to P -1 do Pactive  íì§  Pactive + {p} offset[p] íì§  i i íì§  i + histogram[p] end for while |Pactive| > 0 do p íì§  any  íì¨ Pactive i íì§  used[p] + + // atomic fetch-and-add if i  íí histogram[p] then Pactive  íì§  Pactive? {p} goto loop-end end if ibeg  íì§  i + offset[p] t íì§  T [ibeg] // T : the input & output table pnext  íì§  f(t.key) // f : the partition function while p 6= pnext do i íì§  used[p] + + // atomic fetch-and-add if i  íí histogram[pnext] then Tdeadlock  íì§  Tdeadlock + {t, iinit} goto loop-end end if i íì§  i + offset[pnext] T [i] íì§§ t // swap pnext  íì§  f(t.key) end while T [ibeg] íì§  t loop-end: end while [...] // handle tuples that could cause deadlock (Tdeadlock) 3.3 Across NUMA Moving RAM-resident data across multiple CPUs raises questions about the effectiveness of NUMA RAM transfers. Accessing remote memory locations goes through an inter- connection channel that issues operations to remote RAM modules, increasing the latency. Normally, random accesses are much slower than sequential access and the gap increases when the accesses reference remote RAM regions and go through the CPU interconnection. Prior work [1] proposed doing sequential accesses to remote memory, since hardware pre-fetching hides the latency. To avoid imbalanced use of the NUMA layer when all transfers are directed to a subset of CPUs, we can pre-schedule the transfers and supervise them via synchronization to ensure load balancing [10]. One way to make NUMA-oblivious code scale on multiple CPUs is to allocate both arrays to be physically interleaved across all RAM regions. The OS can support interleaved al- location, where the physical locations of a single array are in- terleaved across all NUMA regions. Randomization of page placement balances accesses across the NUMA interconnec- tion, but precludes NUMA locality. Thus, if we do random accesses, we pay the extra NUMA latency. Cache-line buffer- ing, used by out-of-cache partitioning to avoid TLB misses and facilitate write-combining, also mitigates the NUMA overhead. Still, we measured out-of-cache partitioning to be up to 55% slower on four NUMA regions on interleaved space. The overhead for single tuple random access is higher. A more NUMA-friendly allocation is to split space into large segments bound to a specific region. We can have one segment per thread or one segment per NUMA region. We use the second approach for sorting (see Section 4.1). 3.3.1 Non-in-place Using NUMA-bound segmented allocation for threads or CPUs and if extra space is allowed, we can ensure that all tuples will cross the NUMA boundaries at most once. We use shared-nothing partitioning locally and then use a sep- arate step to shuffle across CPUs. We can use the NUMA interconnection in a balanced way without manual sched- ules [10]. We distribute each segment across all threads of the destination CPU, and do the transfers in a per thread random order. Since some tuples are already on destination, the expected number of transfers is (x? 1)/x for x regions. The NUMA-oblivious partitioning might perform faster than the two step method of shared-nothing partitioning followed by NUMA shuffling, since out-of-cache partition- ing mitigates latencies. The decision to guarantee minimal transfers by incurring shuffling, depends on the hardware. 3.3.2 In-place Assuming NUMA-bound segmented allocation, the only in-place variant where threads do not work in a shared- nothing fashion is during block shuffling (see Section 3.2.4). During the phase of block shuffling on multiple NUMA re- gions, threads can read and write blocks from all regions, but all",Orestis Polychroniou,Columbia University,orestis@cs.columbia.edu,Kenneth A. Ross,Columbia University,kar@cs.columbia.edu,,,,,,,,,,,,,,,,,,,,,,,,
20200105,1388,Yang Cao,"RCBD and SKLSDE Lab, Beihang University",yang.cao@ed.ac.uk,,Making Pattern Queries Bounded in Big Graphs,"Abstract  It is cost-prohibitive to find matches Q(G) of a pattern query Q in a big graph G. We approach this by fetching a small subgraph GQ of G such that Q(GQ) = Q(G). We show that many practical patterns are effectively bounded under access constraints A commonly found in real life, such that GQ can be identified in time determined by Q and A only, independent of the size |G| of G. This holds no matter whether pattern queries are localized (e.g., via subgraph isomorphism) or non-localized (graph simulation). We provide algorithms to decide whether a pattern Q is effectively bounded, and if so, to generate a query plan that computes Q(G) by accessing GQ, in time independent of |G|. When Q is not effectively bounded, we give an algorithm to extend access constraints and make Q bounded in G. Using real-life data, we experimentally verify the effectiveness of the approach, e.g., about 60% of queries are effectively bounded for subgraph isomorphism, and for such queries our approach outperforms the conventional methods by 4 orders of magnitude. I. INTRODUCTION Given a pattern query Q and a graph G, graph pattern matching is to find the set Q(G) of matches of Q in G. It is used in, e.g., social marketing, knowledge discovery, mobile network analysis, intelligence analysis for identifying terrorist organizations [25], and the study of adolescent drug use [17]. When G is big, graph pattern matching is cost-prohibitive. Facebook has 1.26 billion nodes and 140 billion links in its social graph, about 300PB of user data [28]. When the size |G| of G is 1PB, a linear scan of G takes 1.9 days using SSD with scanning speed of 6GB/s. Worse still, graph pattern matching is intractable if it is defined with subgraph isomorphism [31], and it takes O((|V |+ |VQ|)(|E|+ |EQ|))-time if we use graph simulation [20], where |G| = |V |+|E| and |Q| = |VQ|+|EQ|. Can we still efficiently compute exact answers Q(G) when G is big while we have constrained resources, such as a single processor? We approach this by making big graphs small, capitalizing on a set A of access constraints, which are a combination of indices and simple cardinality constraints defined on the labels of neighboring nodes of G. We determine whether Q is effectively bounded under A, i.e., for all graphs G that satisfy A, there exists a subgraph GQ  íì¨ G such that (a) Q(GQ) = Q(G), and (b) the size |GQ| of GQ and the time for identifying GQ are both determined by A and Q only, independent of |G|. If Q is effectively bounded, we can generate a query plan that for all G satisfying A, computes Q(G) by accessing (visiting and fetching) a small GQ in time independent of |G|, no matter how big G is. Otherwise, we will identify extra access constraints on an input G and make Q bounded in G. A large number of real-life queries are effectively bounded under simple access constraints, as illustrated below. award year movie actressactor country 2011-2013u 1 u 2 u 3 u 4 u 5 u 6 Fig. 1. Pattern query Q0 on IMDb Example 1: Consider IMDb [22], a graph G0 in which nodes represent movies, casts, and awards from 1880 to 2014, and edges denote various relationships between the nodes. An example search on IMDb is to find pairs of first-billed actor and actress (main characters) from the same country who co- stared in a award-winning film released in 2011-2013. The search can be represented as a pattern query Q0 shown in Fig. 1. Graph pattern matching here is to find the set Q0(G0) of matches, i.e., subgraphs G íí of G0 that are isomorphic to Q0; we then extract and return actor-actress pairs from each match G íí. The challenge is that G0 is large: the IMDb graph has 5.1 million nodes and 19.5 million edges. Add to this that subgraph isomorphism is NP-complete. Not all is lost. Using simple aggregate queries one can readily find the following real-life cardinality constraints on the movie dataset from 1880?2014: (1) in each year, every award is presented to no more than 4 movies (C1); (2) each movie has at most 30 first-billed actors and actresses (C2), and each person has only one country of origin (C3); and (3) there are no more than 135 years (C4, i.e., 1880-2014), 24 major movie awards (C5) and 196 countries (C6) in total [22]. An index can be built on the labels and nodes of G0 for each of the constraints, yielding a set A0 of 8 access constraints. Under A0, pattern Q0 is effectively bounded. We can find Q0(G0) by accessing at most 17923 nodes and 35136 edges in G0, regardless of the size of G0, by the following query plan: (a) identify a set V1 of 135 year nodes, 24 award nodes and 196 country nodes, by using the indices for constraints C4-C6; (b) fetch a set V2 of at most 24 íì© 3 íì© 4 = 288 award-winning movies released in 2011?2013, with no more than 288 íì© 2 = 576 edges connecting movies to awards and years, by using those award and year nodes in V1 and the index for C1; (c) fetch a set V3 of at most (30+30)?288 = 17280 actors and actresses with 17280 edges, using V2 and the index for C2; (d) connect the actors and actresses in V3 to country nodes in V1, with at most 17280 edges by using the index for C3. Output (actor, actress) pairs connected to the same country in V1. The query plan visits at most 135 + 24 + 196 + 288 + 17280 = 17923 nodes, and 576 + 17280 + 17280 = 35136 978-1-4799-7964-6/15/$31.00 ? 2015 IEEE ICDE Conference 2015161 edges, using the cardinality constraints and indices in A0, as opposed to tens of millions of nodes and edges in IMDb. 2 This example tells us that graph pattern matching is feasible in big graphs within constrained resources, by making use of effectively bounded pattern queries. To develop a practical ap- proach out of the idea, several questions have to be answered. (1) Given a pattern query Q and a set A of access constraints, can we determine whether Q is effectively bounded under A? (2) If Q is effectively bounded, how can we generate a query plan to compute Q(G) in big G by accessing a bounded GQ? (3) If Q is not bounded, can we make it  íì§¸bounded íì§¹ in G by adding simple extra constraints? (4) Does the approach work on both localized queries (e.g., via subgraph isomorphism) and non-localized queries (via graph simulation)? Contributions. This paper aims to answer these questions for graph pattern matching. The main results are as follows. (1) We introduce effective boundedness for graph pattern queries (Section II). We formulate access constraints on graphs, and define effectively bounded pattern queries. We also show how to find simple access constraints from real-life data. (2) We characterize effectively bounded subgraph queries Q, i.e., patterns defined by subgraph isomorphism (Section III). We identify a sufficient and necessary condition to decide whether Q is effectively bounded under a set A of access con- straints. Using the condition, we develop a decision algorithm in O(|A||EQ|+||A|||VQ|2) time, where |Q| = |VQ|+|EQ|, and ||A|| is the number of constraints in A. The cost is independent of big graph G, and query Q is typically small in practice. (3) We provide an algorithm to generate query plans for effectively bounded subgraph queries (Section IV). After Q is found effectively bounded under A, the algorithm generates a query plan that, given a graph G that satisfies A, accesses a subgraph GQ of size independent of |G|, in O(|VQ||EQ||A|) time. Moreover, we show that the plan is worst-case-optimal, i.e., for each input Q and A, the largest GQ it finds from all graphs G that satisfy A is the minimum among all worst-case GQ identified by all other query plans. (4) If Q is not bounded under A, we make it instance-bounded (Section V). That is, for a given graph G that satisfies A, we find an extension AM of A such that under AM , we can find GQ  íì¨ G in time decided by AM and Q, and Q(GQ) = Q(G). We show that when the size of indices in AM is predefined, the problem for deciding the existence of AM is in low polynomial time (PTIME), but it is log-APX-hard to find a minimum AM . WhenAM is unbounded, all query loads can be made instance- bounded by adding simple access constraints. (5) We extend the study to simulation queries, i.e., patterns interpreted by graph simulation (Section VI). It is more chal- lenging to cope with the non-localized and recursive nature of simulation queries. Nonetheless, we provide a characterization of effectively bounded simulation queries. We also show that our algorithms for checking effective boundedness, generating query plans, and for making queries instance-bounded can be adapted to simulation queries, with the same complexity. (6) We experimentally evaluate our algorithms using real-life data (Section VII). We find that our approach is effective for both localized and non-localized queries: (a) on graphs G of billions of nodes and edges [1], our query plans outperform the conventional methods that computes Q(G) directly by 4 and 3 orders of magnitude on average, for subgraph and simulation queries, respectively, accessing at most 0.0032% of the data in G; (b) 60% (resp. 33%) of subgraph (resp. simulation) queries are effectively bounded under simple access constraints; and (c) all queries can be made instance-bounded in G by extend- ing constraints and accessing 0.016% of extra data in G; and 95% become instance-bounded by accessing at most 0.009% extra data. Our algorithms are efficient: they take at most 37ms to decide whether Q is effectively bounded and to generate an optimal query plan for all Q and constraints tested. This work is the first effort to study effectively bounded graph queries, from fundamental problems to practical algo- rithms. It suggests an approach to querying graphs: (1) given a query Q, we check whether Q is effectively bounded under a set A of access constraints; (2) if so, we generate a query plan that given a graph G satisfying A, computes Q(G) by accessing GQ of size independent of |G|, no matter how big G grows; (3) if not, we make Q instance-bounded in G with extra simple constraints. The approach works for both localized subgraph queries and non-localized simulation queries. Given the prohibitive cost of querying big graphs, this approach helps even when only limited queries are effectively bounded. In fact, we find that many queries on real-life datasets are actually effectively bounded under very simple access constraints. Moreover, when a finite set of queries is not effectively bounded, we can make them instance-bounded. All proofs of the results of the paper can be found in [3]. Related Work. We categorize related works as follows. Effective boundedness. The study of effective boundedness traces back to scale independence. The latter was proposed [5] to approximately answer relational aggregate queries under certain conditions, for key/value stores. It aims to guarantee that a bounded amount of work is required to execute all queries in an application, regardless of the size of the underlying data. The idea was formalized in [12], along with a notion of access constraints for relational queries. Recently, the notion of [12] is revised in [10] by requiring that the amount of data accessed (i.e., GQ) can be identified in time determined by query Q and access constraints A only, referred to as effective boundedness; it is characterized for SPC queries [10]. This work differs from the previous work in the following. (1) We introduce access constraints on graph data, to specify cardinality constraints on the labels of neighboring nodes, and guide us to retrieve small subgraphs GQ. (2) Under such constraints, we formalize and characterize the effective bound- edness of graph patterns, an issue harder than its counterpart for relational queries [10], [12]. (3) We propose instance boundedness for queries that are not effectively bounded. Resource-bounded and anytime algorithms. Related are also resource-bounded [16] and anytime algorithms [32]. The former study reachability queries and personalized pattern queries, in which some pattern nodes are designated to match 162 fixed nodes in a graph G. It is to compute approximate answers by accessing no more than íì§íì§|G| nodes and edges in G, for íì§íì§  íì¨ (0, 1) [16]. Anytime algorithms [32] allow users either to specify a budget on resources (e.g., running time; known as contract algorithms [33]), or to terminate the run of the algorithms at any time and get intermediate answers (known as interruptible algorithms [19]). Contract anytime algorithms have been explored for (a) budgeted search such as bounded- cost planning [4], [29], [30], [32] under a user-specified budget; and (b) graph search via subgraph isomorphism, to find intermediate approximate answers within the budget, either by assigning dynamically maintained budgets and costs to nodes during the traversal [8], or by deciding search orders based on the frequencies of certain features in queries and graphs [27]. This work differs from the prior work as follows. (1) We aim to compute exact answers for pattern queries in big graphs, as opposed to heuristic answers that may not have a provable accuracy bound. (2) We characterize what pattern queries can be answered exactly within a cost independent of the size of big graph, based on access constraints; in contrast, the prior work does not study under what budget accurate answers are warranted by using the semantics of the data. (3) We study general pattern queries, which may be either localized or non- localized, and may not be personalized [16]. Graph indexing and compression. There are typically two ways to reduce search space. (1) Graph indexing uses pre- computed global information of G to compute distance [11], shortest paths [18] or substructure matching [26]. (2) Graph compression computes a summary Gc of a big graph G and uses Gc to answer all queries posed on G [7], [13], [24]. In contrast to the prior work, (1) we compute exact answers rather than heuristic. (2) Instead of using the same graph Gc to answer all queries posed on G, we adopt a dynamic reduction scheme that finds a subgraph GQ of G for each query Q. Since GQ consists of only the information needed for answering Q, it allows us to compute Q(G) by using GQ much smaller than Gc and hence, much less resources. (3) When Q is effectively bounded, for all graphs G we can find GQ of size independent of |G|; in contrast, |Gc| may be proportional to |G|. Making big graphs small. There have been other techniques for reducing a big graph into small ones, e.g., distribute query answering [23], pattern matching using views [15], and incremental pattern matching [14]. These are complementary to this work and can be readily combined with ours, e.g., our methods can be readily adapted to distributed settings. II. EFFECTIVELY BOUNDED GRAPH PATTERN QUERIES In this section we define access schema on graphs and effectively bounded graph pattern queries. We start with a review of graphs and patterns. Assume an alphabet íì§ííª of labels. Graphs. A data graph is a node-labeled directed graph G = (V,E, f, íì§íì§¯), where (1) V is a finite set of nodes; (2) E  íì¨ V íì©V is a set of edges, in which (v, v íí) denotes the edge from v to v íí; (3) f() is a function such that for each node v in V , f(v) is a label in íì§ííª, e.g., year; and (4) íì§íì§¯(v) is the attribute value of f(v), e.g., year = 2011. u 1 Q 1 A B  íì§ u 2 v 1 v 2 v 3 v 2n A AB B G 1 Cu3 Du4 Cv2n+1 Dv2n+2 Fig. 2. Pattern query Q1 and data graph G1 We write G as (V,E) or (V,E, f) when it is clear from the context. The size of G, denoted by |G|, is defined to be the total number of nodes and edges in G, i.e., |G| = |V | + |E|. Remark. To simplify the discussion, we do not explicitly define edge labels. Nonetheless, our techniques can be readily adapted to edge labels: for each labeled edge e, we can insert a  íì§¸dummy íì§¹ node to represent e, carrying e  íís label. Labeled set. For a set S  íì¨ íì§ííª of labels, we say that VS  íì¨ V is a S-labeled set of G if (a) |VS | = |S| and (b) for each label lS in S, there exists a node v in VS such that f(v) = lS . In particular, when S = ?, the S-labeled set in G is ?. Common neighbors. A node v is called a neighbor of another node v íí in G if either (v, v íí) or (v íí, v) is an edge in G. We say that v is a common neighbor of a set VS of nodes in G if for all nodes v íí in VS , v is a neighbor of v íí. In particular, when VS is ?, all nodes of G are common neighbors of VS . Subgraphs. Graph Gs = (Vs, Es, fs, íì§íì§¯s) is a subgraph of G if Vs  íì¨ V , Es  íì¨ E, and for each (v, v íí)  íì¨ Es, v  íì¨ Vs and v íí  íì¨ Vs, and for each v  íì¨ Vs, fs(v) = f(v) and íì§íì§¯s(v) = íì§íì§¯(v). Pattern queries. A pattern query Q is a directed graph (VQ, EQ, fQ, gQ), where (1) VQ, EQ and fQ are analogous to their counterparts in data graphs; and (2) for each node u in VQ, gQ(u) is the predicate of u, defined as a conjunction of atomic formulas of the form fQ(u) op c, where c is a constant, and op is one of =, >, <,  íí and  íí. For instance, in pattern Q0 of Fig. 1, gQ(year) = year  íí 2011  íì© year  íí 2013. We simply write Q as (VQ, EQ) or (VQ, EQ, fQ). We consider two semantics of graph pattern matching. Subgraph queries. A match of Q in G via subgraph isomor- phism [31] is a subgraph G íí(V  íí, E íí, f  íí) of G that is isomorphic to Q, i.e., there exists a bijective function h from VQ to V  íí such that (a) (u, u íí) is in EQ if and only if (h(u), h(u íí))  íì¨ E íí, and (b) for each u  íì¨ VQ, fQ(u) = f  íí(h(u)) and gQ(íì§íì§¯(h(u))) evaluates to true, where gQ(íì§íì§¯(h(u))) substitutes íì§íì§¯(h(u)) for fQ(u) in gQ(u). Here Q(G) is the set of all matches of Q in G. Simulation queries. A match of Q in G via graph simula- tion [20] is a binary match relation R  íì¨ VQ íì©V such that (a) for each (u, v)  íì¨ R, fQ(u) = f(v) and gQ(íì§íì§¯(v)) evaluates to true, where gQ(íì§íì§¯(v)) substitutes íì§íì§¯(v) for fQ(u) in gQ(u); (b) for each node u in VQ, there exists a node v in V such that (i) (u, v)  íì¨ R, and (ii) for any edge (u, u íí) in Q, there exists an edge (v, v íí) in G such that (u íí, v íí)  íì¨ R. For any Q and G, there exists a unique maximum match relation RM via graph simulation (possibly empty) [20]. Here Q(G) is defined to be RM . Simulation queries are widely used in social community analysis and social marketing [9]. 163 Data locality. A query Q is localized if for any graph G that matches Q, any node u and neighbor u íí of u in Q, and for any match v of u in G, there must exist a match v íí of u íí in G such that v íí is a neighbor of v in G. Subgraph queries are localized. In contrast, simulation queries are non-localized. Example 2: Consider a simulation query Q1 and graph G1 shown in Fig. 2, where G1 matches Q1. Then Q1 is not localized: u2 matches v2, . . . , v2n?2 and v2n, but for all k  íì¨ [2, n], v2k?2 has no neighbor in G that matches the neighbor u3 of u2 in Q. To decide whether u2 matches v2, we have to inspect all the nodes on an unbounded cycle in G1. 2 We will study effective boundedness for subgraph queries in Sections III?V, and then extend the results to non-localized simulation queries in Section VI. To formalize effectively bounded patterns, we first define access constraints on graphs. Access schema on graphs. An access schema A is a set of access constraints of the following form: S  íì§ (l, N), where S  íì¨ íì§ííª is a (possibly empty) set of labels, l is a label in íì§ííª, and N is a natural number. A graph G(V,E, f) satisfies the access constraint if ? for any S-labeled set VS of nodes in V , there exist at most N common neighbors of VS with label l; and ? there exists an index on S for l such that for any S-labeled set VS in G, it finds all common neighbors of VS labeled with l in O(N)-time, independent of |G|. We say that G satisfies access schema A, denoted by G |= A, if G satisfies all the access constraints in A. An access constraint is a combination of (a) a cardinality constraint and (b) an index on the labels of neighboring nodes. It tells us that for any S-node labeled set VS , there exist a bounded number of common neighbors Vl labeled with l and moreover, Vl can be efficiently retrieved with the index. Two special types of access constraints are as follows: (1) |S| = 0 (i.e., ?  íì§ (l, N)): for any G that satisfies the constraint, there exist at most N nodes in G labeled l; and (2) |S| = 1 (i.e., l  íì§ (l íí, N)): for any G that satisfies the access constraint and for each node v labeled with l in G, at most N neighbors of v are labeled with l íí. Intuitively, constraints of type (1) are global cardinality constraints on all nodes labeled l, and those of type (2) state cardinality constraints on l íí-neighbors of each l-labeled node. Example 3: Constraints C1-C6 on IMDb given in Example 1 can be expressed as access constraints ?i (for i  íì¨ [1, 6]): ?1: (year, award) íì§ (movie, 4); ?4: ?  íì§ (year, 135); ?2: movie íì§ (actors/actress, 30); ?5: ?  íì§ (award, 24); ?3: actor/actress íì§ (country, 1); ?6: ?  íì§ (country, 196). Here ?2 denotes a pair movie  íì§ (actors, 30) and movie  íì§ (actress, 30) of access constraints; similarly for ?3. Note that ?4 ? ?6 are constraints of type (1); ?2 ? ?3 are of type (2); and ?1 has the general form: for any pair of year and award nodes, there are at most 4 movie nodes connected to both, i.e., an award is given to at most 4 movies each year. We use A0 to denote the set of these access constraints. 2 Effectively bounded patterns. A pattern query Q is effectively bounded under an access schema A if for all graphs G that satisfy A, there exists a subgraph GQ of G such that (a) Q(GQ) = Q(G); and (b) GQ can be identified in time that is determined by Q and A only, not by |G|. By (b), |GQ| is also independent of the size |G| of G. Intuitively, Q is effectively bounded under A if for all graphs G that satisfy A, Q(G) can be computed by accessing a bounded GQ rather than the entire G, and moreover, GQ can be efficiently accessed by using access constraints of A. For instance, as shown in Example 1, query Q0 is effec- tively bounded under the access schema A0 of Example 3. Discovering access constraints. From experiments with real- life data we find that many practical queries are effectively bounded under simple access constraints S  íì§ (l, N) when |S| is at most 3. We discover access constraints as follows. (1) Degree bounds: if each node with label l has degree at most N , then for any label l íí, l íì§ (l íí, N) is an access constraint. (2) Constraints of type (1): such global constraints are quite common, e.g., ?6 on IMDb: ?  íì§ (country, 196). (3) Functional dependencies (FDs): our familiar FDs X  íì§ A are access constraints of the form X  íì§ (A, 1), e.g., movie íì§ year is an access constraint of type (2): movie  íì§ (year, 1). Such constraints can be discovered by shredding a graph into relations and then using available FD discovery tools. (4) Aggregate queries: such queries allow us to discover the semantics of the data, e.g., grouping by (year, country, genre) we find (year, country, genre)  íì§ (movie, 1800), i.e., each country releases at most 1800 movies per year in each genre. Maintaining access constraints. The indices in an access schema can be incrementally and locally maintained in re- sponse to changes to the underlying graph G. It suffices to inspect ?G  íì¨ NbG(?G), where ?G is the set of nodes and edges deleted or inserted, and NbG(?G) is the set of neighbors of those nodes in ?G, regardless of how big G is. III. EFFECTIVE BOUNDEDNESS OF SUBGRAPH QUERIES To make practical use of effective boundedness, we first answer the following question, denoted by EBnd(Q,A): ? Input: A pattern query Q(VQ, EQ), an access schema A. ? Question: Is Q effectively bounded under A? We start with subgraph queries. The good news is that (a) there exists a sufficient and necessary condition, i.e., a characterization, for deciding whether a subgraph query Q is effectively bounded under A; and better still, (b) EBnd(Q,A) is decidable in low polynomial time in the size of Q and A, independent of any data graph. 164 We prove these results in the rest of the section. A. Characterizing the Effective Boundedness The effective boundedness of subgraph queries is charac- terized in terms of a notion of coverage, given as follows. The node cover of A on Q, denoted by VCov(Q,A), is the set of nodes in Q computed inductively as follows: (a) if ?  íì§ (l, N) is in A, then for each node u in Q with label l, u  íì¨ VCov(Q,A); and (b) if S  íì§ (l, N) is in A, then for each S-labeled set VS in Q, if VS  íì¨ VCov(Q,A), then all common neighbors of VS in Q that are labeled with l are also in VCov(Q,A). Intuitively, a node u is covered by A if in any graph G sat- isfying A, there exist a bounded number of candidate matches of u, and the candidates can be retrieved by using indices in A. Obviously, (a) u is covered if its candidates are bounded by type (1) constraints. (b) If for some ? = S  íì§ (l, N) in A, u is labeled with l and is a common neighbor of VS that is covered by A, then u is covered by A, since its candidates are bounded (by N and the bounds on candidate matches of VS), and can be retrieved by using the index of ?. The edge cover of A on Q, denoted by ECov(Q,A), is the set of edges in Q defined as follows: (u1, u2) is in ECov(Q,A) if and only if there exist an access constraint S  íì§ (l, N) in A and a S-labeled set VS in Q such that (1) u1 (resp. u2) is in VS and VS  íì¨ VCov(Q,A) and (2) fQ(u2) = l (resp. fQ(u1) = l). Intuitively, (u1, u2) is in ECov(Q,A) if one of u1 and u2 is covered by A and the other has a bounded number of candidate matches by S  íì§ (l, N). Thus, we can verify their matches in a graph G by accessing a bounded number of edges. Note that VCov(Q,A)  íì¨ VQ and ECov(Q,A)  íì¨ EQ. The node and edge covers characterize effectively bounded subgraph queries (see [3] for a proof, which uses three lemmas and the data locality of subgraph queries). Theorem 1: A subgraph query Q is effectively bounded under an access schema A if and only if (iff) VCov(Q,A) = VQ and ECov(Q,A) = EQ. 2 Example 4: For query Q0(V0, E0) of Fig. 1 and access schema A0 of Example 3, one can verify that VCov(Q0,A0) = V0 and ECov(Q0,A0) = E0. From this and Theorem 1 it follows that Q0 is effectively bounded under A0. 2 B. Checking Effectively Bounded Subgraph Queries Capitalizing on the characterization, we show that whether Q is effectively bounded under A can be efficiently decided. Theorem 2: For subgraph queries Q, EBnd(Q,A) is in (1) O(|A||EQ|+ ||A|||VQ|2) time in general; and (2) O(|A||EQ|+ |VQ|2) time when either ? for each node in Q, its parents have distinct labels; or ? all access constraints in A are of type (1) or (2). 2 Algorithm EBChk Input: A subgraph query Q and an access schema A. Output:  íì§¸yes íì§¹ if Q is effectively bounded and  íì§¸no íì§¹ otherwise. 1. for each S  íì§ (l, N) in A (S 6= ?) do 2. find all V? uS 7 íì§ (u,N) in Q and add them to íì§íí; /*f(u) = l*/ 3. B := {v  íì¨ VQ | ?  íì§ (fQ(v), N) is in A}; 4. C := B; /*Initialize VCov(Q,A)*/ 5. InitAuxi(L, ct); /*Initialize auxiliary structures*/ 6. while B is not empty do 7. v = B.pop(); 8. for each íì§íì¨ in L[v] do 9. Update (ct[íì§íì¨]); /*Update counter ct[íì§íì¨]*/ 10. if ct[íì§íì¨] = ? and u 6 íì¨ C do /*suppose íì§íì¨: V? uS 7 íì§ (u,N)*/ 11. B := B  íì¨ {u}; C := C  íì¨ {u}; 12. if VQ  íì¨ C and all edges in Q are in ECov(Q,A) then 13. return  íì§¸yes íì§¹; 14. return  íì§¸no íì§¹; Fig. 3. Algorithm EBChk Here |A| denotes the total length of access constraints in A, ||A|| is the number of constraints in A, and a node u íí is a parent of u in Q if there exists an edge from u íí to u in Q. Algorithm. We prove Theorem 2 by providing a checking algorithm. The algorithm is denoted by EBChk and shown in Fig. 3. Given a subgraph query Q(VQ, EQ) and an access schema A, it checks whether (a) VQ  íì¨ VCov(Q,A) and (b) EQ  íì¨ ECov(Q,A); it returns  íì§¸yes íì§¹ if so, by Theorem 1. To check these conditions, we actualize A on Q: for each S  íì§ (l, N) in A (S 6= ?), and each node u in Q with fQ(u) = l, the actualized constraint is V? uS 7 íì§ (u,N), where V? uS is the maximum set of neighbors of u in Q such that (a) there exists a S-labeled set VS  íì¨ V? uS and (b) for each u íí in V? uS , fQ(u íí)  íì¨ S. Actualized constraints help us deduce VCov(Q,A): a node u of Q is in VCov(Q,A) if and only if either ? there exists ?  íì§ (l, N) in A and fQ(u) = l; or ? V? uS 7 íì§ (u,N) and there exists a S-labeled set of Q that is a subset of V? uS  íì¨© VCov(Q,A). When VCov(Q,A) is in place, we can easily check whether EQ  íì¨ ECov(Q,A) by definition and using the actualized constraints, without explicitly computing ECov(Q,A). We next present the details of algorithm EBChk. Auxiliary structures. EBChk uses three auxiliary structures. (1) It maintains a set B of nodes in Q that are in VCov(Q,A) but it remains to be checked whether other nodes can be deduced from them. Initially, B includes nodes whose labels are covered by type (1) constraints in A (line 3). EBChk uses B to control the while loop (lines 5-10): it terminates when B = ?, i.e., all candidates for VCov(Q,A) are found. (2) For each node v, EBChk uses an inverted index L[v] to store all actualized constraints V? uS 7 íì§ (u,N) such that v  íì¨ V? uS . That is, L[v] indexes these constraintsthat can be used on v. (3) For each actualized constraint íì§íì¨ = V? uS 7 íì§ (u,N), EBChk maintains a set ct[íì§íì¨] to keep track of those labels of S that are not covered by nodes in V? uS  íì¨© VCov(Q,A) yet. Initially, ct[íì§íì¨] = S. When ct[íì§íì¨] is empty, EBChk concludes that there 165 is a S-labeled subset of V? uS covered by VCov(Q,A), and thus deduces that u should also be in",Yang Cao,"RCBD and SKLSDE Lab, Beihang University",yang.cao@ed.ac.uk,Wenfei Fan,University of Edinburgh,wenfei@inf.ed.ac.uk,Jinpeng Huai,"RCBD and SKLSDE Lab, Beihang University",huaijp@buaa.edu.cn,Ruizhe Huang,University of Edinburgh,s1335233@sms.ed.ac.uk,,,,,,,,,,,,,,,,,,
20200106,1389,Abdeltawab M. Hendawi,"Department of Computer Science and Engineering, University of Minnesota, Minneapolis, MN, USA",hendawi@cs.umn.edu,,Predictive Tree: An Efficient Index for Predictive Queries On Road Networks,"Abstract Predictive queries on moving objects offer an im- portant category of location-aware services based on the objects-expected future locations. A wide range of applications utilize this type of services, e.g., traffic management systems, location-based advertising, and ride sharing systems. This paper proposes a novel index structure, named Predictive tree (P-tree), for process- ing predictive queries against moving objects on road networks. The predictive tree: (1) provides a generic infrastructure for answering the common types of predictive queries including predictive point, range, KNN, and aggregate queries, (2) updates the probabilistic prediction of the object's future locations dynamically and incrementally as the object moves around on the road network, and (3) provides an extensible mechanism to customize the probability assignments of the object's expected future locations, with the help of user defined functions. The proposed index enables the evaluation of predictive queries in the absence of the objectsì°½í² historical trajectories. Based solely on the connectivity of the road network graph and assuming that the object follows the shortest route to destination, the predictive tree determines the reachable nodes of a moving object within a specified time window T in the future. The predictive tree prunes the space around each moving object in order to reduce computation, and increase system efficiency. Tunable threshold parameters control the behavior of the predictive trees by trading the maximum prediction time and the details of the reported results on one side for the computation and memory overheads on the other side. The predictive tree is integrated in the context of the iRoad system in two different query processing modes, namely, the precomputed query result mode, and the on-demand query result mode. Extensive experimental results based on large scale real and synthetic datasets confirm that the predictive tree achieves better accuracy compared to the existing related work, and scales up to support a large number of moving objects and heavy predictive query workloads. I. INTRODUCTION The availability of hundreds of millions of smart phones [6] in usersì°½í² hands during their movements in daily lives fired the explosion of a vast number of location aware services [5], [11], [20], [29]. Predictive queries [10], [12], [13] offer a fundamental type of location-based services based on usersì°½í² future locations. Common types of predictive spatial queries include predictive range query, e.g., ì°½íµfind all hotels that are This work is partially supported by the National Science Foundation, USA, under Grants IIS-0952977 and IIS-1218168. located within two miles from a userì°½í²s anticipated location after 30 minutesì°½íµ, predictive KNN query, e.g., ì°½íµfind the three taxis that are closest to a userì°½í²s location within the next 10 minutesì°½íµ, and predictive aggregate query, e.g., ì°½íµfind the number of cars expected to be around the stadium during the next 20 minutesì°½íµ. In fact, Predictive queries are beneficial in various types of real applications such as (1) traffic management, to predict areas with high traffic in the next half hour, so appropriate decisions are taken before congestion appears, (2) location- aware advertising, to distribute coupons and sales promotions to customers more likely to show up around a certain store during the sale time in the next hour, (3) routing services, to take into consideration the predicted traffic on each road segment to find the shortest path of a userì°½í²s trip starting after 15 minutes from the present time, (4) ride sharing systems, to match the drivers that will pass by a riderì°½í²s location within few minutes, and (5) store finders, to recommend the closest restaurants to a userì°½í²s predicted destination in 15 minutes. In this paper, we address the problem of how to process pre- dictive queries for moving objects on road networks efficiently. To this end, we introduce a novel index structure, named the Predictive tree (P-tree), proposed to precompute the predicted moving objects around each node in the underlying road network graph over time. The predictive tree is best described as generic and extensible, from a functionality perspective, dynamic and tunable from a performance perspective. A. Challenges Existing studies on predictive query processing have gone a long way in advancing predictive location-based services. However, existing techniques suffer from both functional limitations and performance deficiencies. From a functional perspective, they suffer from one or more of the following limitations: (1) They consider an Euclidean space [9], [28], [25], [30] where objects can move freely in a two dimensional space. Yet, practical predictive location-based services target moving objects on road networks as described by the motivat- ing applications earlier in this section. (2) Many techniques utilize prediction models that must be trained using a massive 978-1-4799-7964-6/15/$31.00 ? 2015 IEEE ICDE Conference 20151215 a mount of objectsì°½í² historical trajectories in order to produce accurate predictions [2], [9], [13], [15], [24], [28]. However, practical scenarios and industrial experience reveal that such historical data is not easily obtainable for many reasons, either due to usersì°½í² privacy and data confidentiality on one side or due to the unavailability of historical data in rural areas on the other side. (3) Most of the previous solutions were designed to support a specific query type only, e.g., [13], [25], [30] support predictive range query, [3], [22], [30] support predictive KNN query, and [9], [24] support predictive aggregate query. B. Approach Before summarizing the contributions of the proposed pre- dictive tree index, we briefly highlight the basic idea of the index in order to build the proper context. Once an object starts a trip, we construct a predictive tree for this object such that the objectì°½í²s start node in the road network graph becomes the root of the tree. The predictive tree consists of the nodes reachable within a certain time frame T from the objectì°½í²s start location. More specifically, we assume that moving objects follow shortest paths during their travel from source to destination [16], [18]. Hence, we organize the nodes inside the predictive tree according to the shortest path from the objectì°½í²s start node, which is marked as the root of the tree. Accordingly, each branch from the root node to any node in the tree represents the shortest route from the root to this node. Then, our prediction is based on a probability assignment model that assigns a probability value to each node inside the objectì°½í²s predictive tree. In general, the probability assignment is made according to the nodeì°½í²s position in the tree, the travel time between the object and this node and the number of the sibling nodes. In practice, the probability assignment process is tricky and varies from one application to another. At each node in the given road network that is indexed in R-tree, we keep track of a list of objects predicted to appear in this node, along with their probabilities, and travel time cost from the objectsì°½í² current locations to this node. This list represents a raw precomputed answer that can be customized according to the type of the received query (i.e., point, range or kNN predictive query) at query processing time. When an object moves from its current node to a different node, we incrementally update the predictive tree by pruning all nodes in the tree that are no longer accessible through a shortest route from the objectì°½í²s new location. Mostly, this pruning shrinks the number of possible destinations, yet, increases the focus of the prediction. Consequently, the precomputed answer at each node in the object predictive tree is updated to accommodate the effect of the objectì°½í²s movements. This update is reflected to the original nodes in the road network. When a predictive query is received, we fetch the up-to-date answer from the node of interest and compile it according to the query type. To adjust the behavior of the predictive tree and, hence, control the overall predictive query processing performance, we leverage two tunable parameters, a maximum time T and a probability threshold P . These parameters compromise between the maximum prediction time a predictive tree can support and the details in the reported query results on one side, and system resources overheads, i.e., CPU and memory, on the other side. The proposed predictive tree is implemented within the iRoad framework. The iRoad offers two query processing modes of leveraging the predictive tree to control the inter- action between its components: (1) the precomputed query result mode, in which the predicted results are computed and materialized in advance; and (2) the on-demand query result mode which is a lazy approach that postpones all computation till a query is received. C. Contributions In general, the contributions of this paper can be summa- rized as follows: ? We propose a novel data structure named Predictive tree (P-tree) that supports predictive queries against moving objects on road networks. ? We introduce a probability model that computes the like- lihood of a node in the road network being a destination to a moving object. The probability model is introduced to the predictive tree as a user defined function and is handled as a black box by the index construction and maintenance algorithms. ? We introduce two tunable parameters T and P that are experimentally proved to be efficient tools to control the predictive tree index, the system performance, the prediction time, and the results details as well. ? We provide an incremental approach to update the pre- dictive tree as objects move around. Hence, we utilize the existing index structure and incur minimal cost in response to the movement of the object. ? We propose the iRoad framework that leverages the introduced predictive tree to support a wide variety of predictive queries including predictive point, range, and KNN queries. ? we provide an experimental evidence based on real and synthetic data that our introduced index structure is efficient in terms of query processing, scalable in terms of supporting large number of moving objects and heavy query workloads, and achieves a high-quality prediction without the need to reveal objectsì°½í² historical data. The remainder of this paper is organized as follows. Sec- tion II sets the preliminaries and defines our problem. Sec- tion III presents the iRoad system. Section IV describes the the predictive tree and its associated construction, maintenance and querying algorithms. Experimental results are presented in Section V. The study of related work is given in Section VI. Finally, Section VII concludes the paper. II. PRELIMINARIES In this section, we formalize the basic predictive query we address in this paper. Then, we define different types of predictive queries that the predictive tree can support within the iRoad framework. After that, we explain the intuition of the leveraged prediction model. 1216 Fig. 1. iRoad System Architecture A. Basic Query In this paper, we focus on addressing the predictive point query as our basic query on the road network. In this query, we want to find out the moving objects with their corresponding probabilities that are expected to be around a specified query node in the road network within a future time period. The example of such query could be like, ì°½íµFind out all the cars that may pass by my location in the next 10 minsì°½í¶. The predictive point query we address in this paper can be formalized as: ì°½íµGiven (1) a set of moving objects O, (2) a road network graph G(N, E, W), where N is the set of nodes, E is the set of edges, and W is the edge weights, i.e., travel times, and (3) a predictive point query Q(n, t), where n ì°½í°í N, and t is a future time period, we aim to find the set of objects R ì°½í°í O expected to show up around the node n within the future time t. The returned result should identify the objects along with their probabilities to show up at the node of interest. For example, within the next 30 mins, object o1 is expected to be at node n3 with probability 0.8, R(Q(n3,30)) = {< o1,0.8>}. B. Extensions We consider the aforementioned predictive point query as a building block upon which our framework can be extended to support other types of predictive queries including: (i) Predictive range query, where a user defines a query region that might contain more than one node and asks for the list of objects expected to be inside the boundaries of that region within a specified future time, (ii) Predictive KNN query to find out the most likely K objects expected to be around the node of interest within a certain time period, and (iii) Predictive aggregate query to return the number of objects predicted to be within a given location in the next specified time duration. C. Prediction Model Our prediction model employed by the introduced predictive tree index structure is based on two corner stones. (1) The assumption that objects follow the shortest paths in their routing trips. The intuition behind this assumption is based on the fact that in most cases, the moving objects on road networks, e.g., vehicles, travel through shortest routes to their destinations [16], [18]. In fact, this assumption is aligned with the observation in [4] that moving objects do not use random paths when traveling through the road network, rather they follow optimized ones, e.g., fastest route. As a result, this (a) Network & Objects (b) Predictive Trees Integrated With R-Tree Fig. 2. Example Of The Proposed Index Structure model prevents the looping case that appears in the traditional turn-by-turn probability model and assigns a probability value for the moving object to turn when facing an intersection [13]. (2)The probability assignment model that assigns a proba- bility value to each node inside the objectì°½í²s predictive tree. In fact, the probability assignment is affected by the nodeì°½í²s position with respect to the root of the tree, the travel time cost between the object in its current location to this node, and the number of the sibling nodes. In general, our predictive tree is designed to work with different probability assignment models. For example, a possible probability model can give higher values to nodes in business areas, e.g., down town, rather than those in the suburbs. In our default probability model, each node in the predictive tree has a value equal to one divided by the number of nodes accessible from the root within a certain time range. III. THE IROAD SYSTEM The proposed predictive tree is implemented in the context of the iRoad System. More precisely, the predictive tree and its construction, maintenance and querying algorithms form the core of the iRoad System. The iRoad System is a scalable framework for predictive query processing and analysis on road networks. The architecture of the iRoad system consists of three main modules, namely, the state manager, the pre- dictive tree builder and the query processor, Figure 1. In this section, we present an overview of the iRoad System and give a brief description of its key components. Moreover, we focus on the interaction and workflow between these components under both the precomputed query result mode and the on- demand query result mode. A. State Manager The state manager is a user facing module that receives a stream of location updates from the moving objects being monitored by the system. The state manager maintains the following data structures. (1) An R-tree [7] that is generated on the underlying road network graph. It differs from the conventional R-tree in that at each leaf node, i.e., a node in the road network, in addition to storing the corresponding MBR, it also keeps track of two lists: (a) current objects that records the pointers to the objects around this node, and (b) 1217 predicted objects that maintains the predicted results of the objects that most likely to show up around that node. (2) A trajectory buffer that stores the most recent one or more nodes in the road network that are visited by the moving object in its ongoing trip. (3) A predictive tree such that root of a predictive tree is the current location of the moving object. Figure 2(a) gives an example of a set of objects moving on a road network, while Figure 2(b) depicts how the predictive trees are integrated within the basic data structures layout to facilitate the processing of predictive queries. As we mentioned, the system can be running under either (1) a precomputed query result mode or (2) an on-demand query result mode. The first is the default mode inside the iRoad framework. In either modes, upon the receipt of a location update of a moving object, the R-tree is consulted and the new location is mapped to its closest node Nnew in the road network. If the new node Nnew is the same as the objectì°½í²s old node Nold, the object movement is not significant enough to change the systemì°½í²s state and no further action is taken. Otherwise, the object has moved to a different node and an evaluation of the impact of the objectì°½í²s movement is triggered in the system. We differentiate between the precomputed and the on-demand query result modes as follows. Precomputed query result mode: In this mode, the predic- tive tree builder is invoked immediately once the moving ob- ject changes its current node and, consequently, the predictive tree is either constructed from scratch or updated in response to the objectì°½í²s movement. Remember that the predictive tree is constructed from scratch if the incoming location update belongs to a new object that is being examined by the system for the first time. Also, the predictive tree is constructed from scratch if Nnew is not a child of the root of the objectì°½í²s in- hand predictive tree. As will be described in Section IV, this case happens if the object decided not to follow the shortest path, e.g., made a u-turn or started a new trip. Otherwise, the tree is incrementally maintained. Note that, in this mode, the trajectory buffer data structure boils down to one single node (i.e., the current node) of the moving object because of the eagerness to update the predictive tree with the receipt of every location update. Hence, the past trajectory is entirely factored in the predictive tree. On-demand query result mode: In this mode, the tra- jectory buffer stores all nodes the moving object passed by since the start of its current trip. Initially, We do not perform any computation until a query is received. Then, we identify the vicinity nodes within the time range determined by the query. Those nodes might contribute in the predicted results. For each object in these nodes, we construct its predictive tree and run a series of updates according to the list of passed nodes in its trajectory buffer, Figure 3. For example, in this figure, nodes A, G, and E are within the time range specified in the query at node B. Then, we construct the predictive tree for each object, O1, O2, O3, in those nodes and update them according the passed nodes by each one. Obviously, O1, O3 will contribute in the predicted objects at node B, while O2 will not contribute as node B is no longer a possible destination Fig. 3. On-Demand Approach for O2 based on its trajectory buffer. Then, we get rid of any data structure, i.e., the predictive trees and predicted results, directly once the query processing is completed and the results are carried back to the query issuer. We ending by adding the objectì°½í²s current node Nnew, i.e,. Node B in this example, to the objectì°½í²s trajectory buffer. B. Predictive Tree Builder The predictive tree builder is the component that encom- passes the predictive tree construction and maintenance algo- rithms. It takes as input, (1) the moving objectì°½í²s trajectory buffer, (2) the moving objectì°½í²s current predictive tree (if exists), (3) the tunable parameters (T and P) that trade the prediction length and accuracy for systemì°½í²s resources, and (4) a user defined probability assigned function. The predictive tree builder reflects the most recent movements of the object (as recorded in the objectì°½í²s trajectory buffer) to the objectì°½í²s predictive tree. Upon the completion of a successful invocation of the tree builder, an up-to-date predictive tree rooted at the objectì°½í²s current location is obtained and the objectì°½í²s trajectory buffer is modified to accommodate the objectì°½í²s current node. The predictive tree builder is invoked in two different ways. In a precomputed query result mode, the builder is invoked by the state manager upon the receipt of every location update. The state manager pushes the incoming location update of a moving object Oi to the predictive tree builder that eagerly reflects the location update in the predictive tree of Oi. Afterwards, the tree builder updates the precomputed query results at every node in the road network that is on the shortest path route from the object Oiì°½í²s current location. In an on-demand query result mode, the predictive tree builder is invoked by the query processor once a query Q is received. The predictive tree builder consults the road network graph and retrieves a list of nodes Nvicinity that are within the time distance determined by the query Q. Then, it pulls, from the state manager, the predictive trees and the trajectory buffers of moving objects whose current nodes are in Nvicinity . In other words, lazy or selective processing of moving objects that are believed to affect the query result is carried over without taking the burden of updating the predictive tree of every single moving object in the system. C. Query processor The main goal behind predictive query processing in the iRoad system is to be generic and to provide an infrastructure for various query types. This goal is achieved by mapping a query type to a set of nodes (Nvicinity) in the road network graph such that the query result is satisfied by predictions 1218 Algorithm 1 Predictive Tree Construction Input: Node n, T ime Range T , Road Network Graph G(N,E,W ) 1: Step 1. Initialize the data structures 2: Set n as the root of the Predictive Tree PT 3: Visited nodes list NL $ ? 4: Min-Heap MH $ ? 5: for all Edge ei connected with n do 6: Insert the node ni ì°½í°í ei íì§¸C MH 7: end for 8: Step 2. Expand the road network and create the predictive tree 9: while the minimum time range Tmin in MH < T do 10: Get the node nmin with Tmin from MH 11: if The nmin /ì°½í°í NL then 12: Insert nmin íì§¸C PT 13: Insert nmin íì§¸C NL 14: for all Edge ej connected with nmin do 15: Insert the node nj íì§¸C MH 16: end for 17: end if 18: end while 19: Return PT associated with these nodes. For predictive point queries as an example, the point query is answered using the information associated with the road network node that is closest to the query point. For predictive range queries, all nodes in the range are considered to compute the query result. For predictive KNN queries, we sort those predicted objects associated with Nvicinity based on their probabilities. Nvicinity is rationally expanded till K objects are retrieved, if visible. In a precomputed query result mode, generic results are prepared in advance and are held in memory. The process is triggered by an update in objectì°½í²s location and the precom- puted results are constructed/updated for all nodes along the shortest path route of that object. Therefore, most of the work is done during the location update time. Upon the receipt of a query, the query processor fetches the precomputed results only from nodes in Nvicinity , adapts them according to the type of the received query and gives a low latency response back to the user. In an on-demand query result mode, nothing is precomputed in advance and all computation will be performed after the receipt of the userì°½í²s query. Nvicinity is identified and the pre- dictive tree of objects whose current node belong to Nvicinity are constructed/upadted as described earlier in this section. Then, the results are collected and adapted to the query type in a similar way to the precomputed result approach. IV. PREDICTIVE TREE In this section, we describe the proposed predictive tree index structure that is leveraged inside the iRoad framework to process predictive queries based on the predicted destinations of the moving objects within a time period T . We first introduce the main idea and the motivation to build the predictive tree. After that, we provide a detailed description for the two main operations in the predictive tree: 1) predictive tree construction, and 2) predictive tree maintenance. The idea of the predictive tree is to identify all the possible destinations that a moving object could visit in a time period T by traveling through the shortest paths. As there may only exist one shortest path from a start node to a destination node, we can guarantee it will be a tree structure (i.e., without any loop). The intuition for constructing the predictive tree with a time boundary T is based on two real facts: 1) most of the moving objects travel through shortest path to their destinations [16], [18], and 2) majority of the real life trips are within a time period, e.g., 19 minutes [17], [18]. As a result, we only need to care about the possible destinations reachable through a shortest route from the objectì°½í²s start location within a bounded time period. Based on that, we build the predictive tree to hold only the accessible nodes around a moving object and assign a probability for each one of them. The predictive trees leveraged in the iRoad system signif- icantly improves the predictive query processing efficiency for two main reasons. 1) The possible destinations of the prediction shrinks as a result of using the time boundary T . Yet, prediction computation is performed on few number of nodes instead of millions of nodes in the underlying road network, e.g., road network of California state in USA has about 1,965,206 nodes and 5,533,214 edges [23]. 2) Inside the predictive tree, we maintain only those nodes with probability higher than a certain probability threshold parameter P , e.g., 10%. By doing this, we cut down the computation overhead consumed for continuously maintaining the predicted results at each node in the predictive trees. Moreover, we control iRoad to focus on those nodes that more likely to be reached by a moving object. Yet, the query reported results can be more reasonable to users. A. Predictive Tree Construction Main idea. When a moving object starts its trip on the road network, we build a predictive tree based on its starting location to predict its possible destinations within a certain time frame T . We propose a best-first network expansion algorithm for constructing predictive tree for time period T , e.g., 30 minutes. We set the objectì°½í²s initial node as the start node, then, we visit the nodes and edges on the road network that are reachable using a shortest path from this start node [21]. The algorithm proceeds to traverse and process the edges in the road network based on the travel time cost from the start node until all the costs to the remaining edges are over T . Algorithm. The pseudo code for the predictive tree con- struction algorithm is given in Figure 1. The algorithm takes the road network G = {N,E,W}, a starting node n and a time range T as input. The algorithm consists of two main steps: ? Initialization. We first initialize the predictive tree under construction by setting the start node n as the root of the tree. We also create the visited nodes list NL to store the nodes that have been processed by the algorithm so far. An empty min-heap, MH , is employed to order the nodes based on its distance to the root node n. After that, we insert the nodes that are directly connected with the 1219 Fig. 4. Example of Constructing And Expanding The Predictive Tree Started At Node A. root node n into the min-heap MH , (Lines from 2 to 7 in Algorithm 1). ? Expansion. We continuously pop the node nmin that is the closest to the root node from the min-heap. Then, we check if that node has been visited by our algorithm before, which means there was a shorter path from the root to this node nmin. If visited nodes list NL does not contain nmin, we insert the node nmin to it as well as a child to the current expanding branch of the predictive tree PT . After that, we insert to the min-heap MH the node nj that is connected with the yet processed node nmin for further expansion. The algorithm stops when the distance between the next closest node in the min- heap is over the boundary T , (Lines from 8 to 18 in Algorithm 1). Example. Figure 4 gives an example for constructing a predictive tree for node A from the given road network. For this example, we set the time period T to 20 minutes. Figure 4(a) gives the original road network structure, where circles represent nodes and lines between nodes represent edges and the number on each edge represents the time cost to traverse that edge. In the first iteration, we start by setting the root of the tree to node A. Then, we insert nodes B and C into the min-heap, as they are the connected ones to the root node A, Figure 4(b). After that, we expand the closest child to the root, B, where we insert D and E into the min- heap MH and put B in the predictive t",Abdeltawab M. Hendawi,"Department of Computer Science and Engineering, University of Minnesota, Minneapolis, MN, USA",hendawi@cs.umn.edu,Jie Bao,"Department of Computer Science and Engineering, University of Minnesota, Minneapolis, MN, USA",baojie@cs.umn.edu,Mohamed F. Mokbel,"Department of Computer Science and Engineering, University of Minnesota, Minneapolis, MN, USA",mokbel@cs.umn.edu,Mohamed Ali,"Institute of Technology, University of Washington, Tacoma , WA, USA",mhali@uw.edu,,,,,,,,,,,,,,,,,,
20200107,1390,Michael Mattig,"Department of Mathematics and Computer Science University of Marburg, Germany",mattig@mathematik.uni-marburg.de,,Kernel-Based Cardinality Estimation on Metric Data,"ABSTRACT The efficient management of metric data is extremely important in many challenging applications as they occur e.g. in the life sciences. Here, data typically cannot be represented in a vec- tor space. Instead, a distance function only allows comparing individual elements with each other to support distance queries. As high-dimensional data suffers strongly from the curse of di- mensionality, distance-based techniques also allow for better handling of such data. This has already led to the development of a plethora of metric indexing and processing techniques. So far, the important problem of cardinality estimation on metric data has not been addressed in the literature. Standard vector-based techniques like histograms require an expensive and error-prone embedding. Thus, random sampling seems to be the best choice for selectivity estimation so far, but errors are very high for mod- erately small queries. In this paper, we present a native cardinality estimation technique for distance queries on metric data based on kernel-density estimation. The basic idea is to apply kernels to the one-dimensional distance function among metric objects and to use novel global and local bandwidth optimization methods. Our results on real-world data sets show the clear advantage of our method in comparison to its competitors. 1 INTRODUCTION Statistics about the distribution of data in a database are used for two very important aspects of data management: query optimiza- tion and data exploration. In query optimization, they allow esti- mating the costs of operations, choosing appropriate algorithms, and computing the order of joins. For very large databases, where computations take a very long time, small in-memory statistics can deliver approximate answers. Those are often sufficient to determine whether it is worth further investigating the data in a particular direction. While one- and multidimensional vector data is very common in traditional applications, there are many domains for which data is in a metric space only. This means data is not describable by a d-dimensional vector, instead there exists only a metric mea- suring distances between pairs of objects. Examples include the life sciences, where e.g. proteins are usually described by their geometrical structure or at least a sequence of amino acids. Mul- timedia data comes in different datatypes such as JPEG or MPEG which are also not appropriate for a relational representation. In such domains there is a severe lack of native statistical sup- port. Thus, a standard approach is to transform metric data into a multidimensional vector space and to apply one of the standard estimation techniques [18]. There are two serious opposing ef- fects. First, a metric embedding causes in general a considerable information loss. In order to alleviate this, the number of dimen- sions needs to be sufficiently high. Second, the well-known curse ? 2018 Copyright held by the owner/author(s). Published in Proceedings of the 21st International Conference on Extending Database Technology (EDBT), March 26-29, 2018, ISBN 978-3-89318-078-3 on OpenProceedings.org. Distribution of this paper is permitted under the terms of the Creative Commons license CC-by-nc-nd 4.0. of dimensionality is already noticeable for a moderate number of dimensions. Thus, statistics provide only accurate results for low-dimensional vector spaces [1]. In this paper, we present the first native method for cardinality estimation of distance queries in metric spaces. The basic idea is to consider the distances of objects in a metric space and to use kernel techniques to estimate the underlying distance distri- bution. By tuning the bandwidth of the kernels and the kernel function, we obtain a robust estimator for the cardinality of dis- tance queries in metric spaces. Moreover, our approach is also beneficial for high-dimensional vector spaces by treating them as metric spaces, thus considering only the distance among objects, to overcome the shortcomings of standard vectorial statistics. The main contributions of this paper are: ? We show the deficiencies of traditional cardinality estima- tion techniques on metric data sets. ? We present the first effective and efficient method for cardinality estimation in metric space. ? Extensive experiments on real-world data show the valid- ity of our approach. The rest of the paper is structured as follows. Section 2 de- scribes several applications for cardinality estimation in metric spaces, formally defines the problem, and emphasizes the dif- ferences of vector and metric data. Section 3 presents related work in the areas of cardinality estimation in general, techniques for embedding metric data into vector space and kernel-based techniques for cardinality estimation. Section 4 presents our distance-based kernel estimator approach in metric space. Sec- tion 5 describes our methods for global and local bandwidth optimization. Section 6 presents our experimental findings. Fi- nally, Section 7 concludes the paper. 2 PRELIMINARIES We first give a formal description of the problem of cardinality estimation on metric data. Then, we discuss several applications that greatly benefit from a suitable solution to this problem. Fi- nally, we discuss the fundamental differences between vector and metric data that lead to the ineffectiveness of established methods. 2.1 Problem Specification Let X be a set of N objects {x1, . . . ,xN }  íì¨ X. These objects are all of a certain type, in particular a type which can differ from Rn . Moreover a distance function distX : X  íì© X  íì§ R+ is given which fulfills the three properties of a metric, namely (a) identity of indiscernibles: distX(x ,y) = 0íì§íì§ x = y, (b) symmetry: distX(x ,y) = distX(y,x) and (c) triangle inequality: distX(x , z)  íí distX(x ,y)+distX(y, z), with x ,y, z  íì¨ X. We will refer to the combination of X and distX as metric data. In mathematics the pair (X,distX) is called metric space. Cardinality estimation for metric data can be formalized as follows: Given a distance queryQ = (xQ , rQ ), with object xQ  íì¨ X     Series ISSN: 2367-2005 349 10.5441/002/edbt.2018.31 and distance rQ  íì¨ R+, efficiently approximate the cardinality of the set {x  íì¨ X | distX(xQ ,x)  íí rQ }. We will refer to the distance rQ as query radius. The true cardinality is denoted by c(Q) and the estimated cardinality by c?(Q). The goal is to minimize the error of the estimation, but also the construction costs and the size, as well as the query time of the estimator. Note that the actual cardinality can, of course, be calculated by computing the distance to every other item in the data set. This requires a linear number of distance calculations. Each of them can be very costly as, e.g., in video dissimilarity. Hence, we want to minimize the computational costs induced by an estimator. 2.2 Applications Cardinality estimation in metric spaces has many applications. Among others we want to mention the domain of Machine Learn- ing and Data Mining, where algorithms are usually based on a distance measure. The most prominent example is the so-called k-nearest neighbor (kNN) classifier which can be used to classify any kind of data, if it is endowed with a distance measure. The basic idea is to retrieve thek objects from a database which have the smallest distances to a certain query object. Assuming that the objects within the database carry a certain class label (e.g. customers of an insurance with label churn or no churn), the query is classified with the majority label from the set of the k nearest neighbors. kNN classifiers are known to be inefficient since for each run of such an algorithm a complete scan of the data set is required. To accelerate this algorithm typically metric index structures [28] are employed that allow the efficient retrieval of elements within a given distance of a query object. However, it is hard to specify the radius for the corresponding queries since the kNN classifier requires rather the set of k nearest neighbors than a certain set of neighbors exhibiting a certain maximal distance to the query object. For calculating the minimum radius, leading to a retrieval of k results, cardinality estimation can be used. For example, [14] make use of cardinality estimation to assign objects to different Locality Sensitive Hashing tables of different radii. This improves kNN queries on data sets where the distances of the k nearest neighbours of items vary greatly over the data set. A single LSH table could not sufficiently answer such queries. Another example is the estimation of densities, e.g. for Bayes Classifiers, where the density around an element x is propor- tional to the amount of elements in a database that are in a small vicinity to x . This vicinity is typically specified by a certain small distance. Obviously, a reliable cardinality estimation approach would increase the efficiency of such classifiers enormously. If a query is expressed as a conjunction of multiple proximity predicates, each of them with a different distance function, car- dinality estimation is useful for computing an efficient order of their computation. In an optimal execution plan queries should be applied in an order that leads to quickly decreasing result sets. Cardinality estimation can be used to answer exactly this question and to find an order in which the different distance measures are to be applied. Applications in which such scenarios occur are, e.g., pharmaceutical chemistry, where different dis- tance measures covering certain requirements are applied onto protein and/or ligand databases to get the final result in form of a very small set of therapeutically effective drugs. In general, this is a metric scenario, since proteins cannot be described on the structural level by vectors without a considerable loss of information. 2.3 Vector Data vs. Metric Data In order to emphasize the fundamental differences of metric data to vector data that lead to the in-applicability of established methods, we now briefly review important properties of a vector space. We limit our discussion to vector spaces over the real numbers. Here, ad-dimensional vector space consists of elements x = (x1, ...,xd )with a real value xi  íì¨ R called coordinate for each dimension. Individual elements can be added to each other and multiplied with scalar values v  íì¨ R. This, e.g., allows to compute the mean of multiple elements which is not possible in a metric space. Thus one of the most basic data summarization operators is not available in a metric space. Furthermore, the coordinates of the vector allow determining the location of an element with respect to other elements. Such a direction cannot be determined in a metric space. A set of vectors can be ordered globally by component-wise sorting or by a space-filling curve [27] that better preserves the proximity of subsequent elements. In contrast, elements in a metric space can only be ordered based on the distance to a single reference object. Furthermore, it is straight-forward to divide a vector space into a finite number of distinct subsets by incrementally subdividing the space along the dimensions. In a metric space such subsets have to be defined using a center object and a radius. In general, such partitions will overlap if the complete data space should be covered. A vector space has ameasure that allows calculating the vol- ume of subspaces and their intersections. In particular, this is the foundation for the definition of a density and a distribution of a data set. The notions of volume, density and distribution are not available in a metric space. Finally, in a vector space, the costs of distance calculations between elements is linear in the number of dimension if an Lp norm (typically p = 2 for Euclidean distance) is used. In a metric space a distance function can be arbitrarily complex, such as e.g. the edit distance between two strings which has a quadratic runtime. Furthermore, we can calculate a bounding box of a vector data set in linear time by finding the minimum and maximum value for each dimension. In contrast, finding the maximum distance between elements in a metric space requires a quadratic number of distance computations. In summary, metric data lacks most of the tools available in traditional scenarios for cardinality estimation. This makes most established methods infeasible as we discuss in the Section 3. However, as discussed previously, metric data appears in many different applications naturally. Furthermore, it supports distance queries, which are also highly relevant for vector data [4]. As our experiments will show later, using distance-based techniques helps lowering the impact of the curse of dimensionality. 3 RELATEDWORK The most basic idea for estimating the size of a query result is to perform the query on a sample of the data and scale up the resulting cardinality by the sample  íís fraction of the total data size. Using Reservoir Sampling [32], a random data selection can be computed in linear time. We can apply this method also on metric data. However, small sample sizes result in underestimates often equal to zero because metric spaces are sparse. Histograms are the most popular technique for cardinality estimation in database systems [18]. They divide a domain into multiple buckets and store the number of contained elements. When estimating the cardinality within a given query range, they 350 approximate the actual cardinality usually by assuming a uniform distribution within the buckets. Computing optimal histograms that minimize the error induced by this assumption is NP-hard [25]. The most prominent example of an efficient heuristic is MinSkew [2]. It recursively subdivides the space by splitting the most skewed bucket until the desired number of buckets is reached. Other techniques like rkHist [11] and R-V histogram [1] start from the leaves of a spatial index structure and merge them together for limiting the amount of buckets. The introduced histograms are, however, not applicable for metric data. There is no straight-forward criterion for subdivid- ing a metric space into a finite amount of disjoint buckets. The missing notion of uniform distribution within a bucket and the unavailability of a volume measure make the incorporation of such buckets into a cardinality estimate impossible. It is possi- ble to transform metric data into vector data in order to build a spatial histogram, though. We can then extract the cardinality estimate for a distance query by calculating the intersection of the query (in form of a hyper-sphere) with the histogram buckets. However, such a transformation into a vector space is costly and introduces an error in form of distance distortions. Compression techniques like wavelets and cosine transfor- mations are also suitable for cardinality estimation [24]. Both techniques are applicable to multi-dimensional vector data and are shown to provide accurate results. They approximate the actual data distribution by means of a basis function and several coefficients, thus drastically reducing the amount of data. The cardinality estimate is computed as a cumulative joint distribu- tion of the individual dimensions of the data set. However, in a metric space we are not able to use these techniques as the data has no such dimensions and there is no notion of a distribution. Another method for approximate query processing is Local Sensitive Hashing (LSH). LSH performs very well on data from a high-dimensional vector space. It is for example used for approx- imate similarity search [15] and thus related to distance queries in metric spaces. There has been work in cardinality estimation of similarity joins using LSH [21]. Also, multiple LSH indexes with different radii can be used for cardinality estimation by counting collisions of hash buckets [14]. However, LSH requires a similarity-preserving hash function which does not universally exist for metric data. A more recent approach uses Machine Learning [4] for car- dinality estimation. It is, to the best of our knowledge, the only method supporting distance queries. The query-driven approach learns to differentiate several prototype queries and predicts the cardinality of unseen queries by assignment to a prototype and subsequent interpolation using regression. The optimization of the query prototypes is performed via gradient descent where the prototype query is moved across the data space. This manip- ulation of a query object is not possible in a metric space. Thus, like the other approaches, this approach is infeasible for metric data, unless it is mapped into a vector space first. A distance preserving mapping of data from a metric space to a vector space is called embedding. The goal is to find for each xi  íì¨ X an embedding yi  íì¨ Rd , such that the induced stress [19] on the distances is minimized. This stress measure incorporates the deviations of the resulting distances among objects with respect to the original distances. There are different approaches available to embed metric data into a vector space [3]. One prominent example is Multidimen- sional scaling (MDS) [20]. It tries to preserve the pairwise dis- tances in vector space by using such a stress function [19] and minimizing it subsequently. This minimization can be performed by eigendecomposition or gradient descent. However, both meth- ods are expensive to compute, and thus, not suitable for very large data sets. Landmark MDS [9] was introduced as an alternative to MDS for big data scenarios. It uses samples of the data called land- marks and applies MDS on them. The remaining points are then embedded based on the distances to the l landmark elements. Kernel estimators [26] are a competitor of histograms which exhibit a fast convergence for 1-dimensional data [7] and have been generalized to multi-dimensional data [16]. Note, that both approaches do not support distance queries on metric data. Here, samples distribute their weight using a kernel function K , e.g. Epanechnikov [12] or Gaussian. This weight corresponds to the probability of data points existing in the vicinity of the sample. One approximates the underlying probability density function f? of a data set at the evaluation point x by using a set of samples S and summing up over all samples: f? (x) = 1 |S |  íì§h íì§2 s  íì¨S K( x?s h ) = 1 |S | íì§2 s  íì¨S Kh (x ? s). Here, h is the smoothing-factor called band- width. The cardinality estimate results from integrating the ker- nel density function within a given rectangle query and scaling the result up. In a d-dimensional vector space typically product kernels are used where the density function is integrated for each dimension separately. This is only feasible for rectangular queries and not for distance queries. Hence, the application of existing kernel-density estimators for distance queries on met- ric data embedded into a vector space is not straight-forward. Approximating the distance query as a hyper-sphere introduces an error that is also influenced by the curse of dimensionality. Our approach makes use of kernels, but we avoid the curse of dimensionality by using the one-dimensional distance function. The choice of the actual kernel function is considered to be of low impact according to the literature [8]. Nevertheless, we consider different kernel functions in the experiment section of this paper. However, the selection of the kernel bandwidth h has a much more crucial impact on the resulting estimator quality. There are two general approaches for the bandwidth selection: global and locally adaptive methods [31]. Using a global (fixed) bandwidth means that all samples and evaluation points use the same bandwidth. One method of obtaining this bandwidth is by minimizing the mean integrated squared error (MISE) [30]. In contrast to traditional applications, the underlying distribution that shall be fitted by the kernel estimator is known in cardinality estimation. It is given by the data itself. This enables other opti- mization techniques than those used in the statistics literature. Recent work [17] used a gradient descent based approach to find the optimal bandwidth for a given set of training queries. They fit a global bandwidth for each dimension of the vector space. However, a global bandwidth is usually not optimal, as the result- ing estimator oversmoothes the distribution in dense regions and undersmoothes in sparse regions of the data set. While the au- thors of [17] were able to exploit the different distributions in the individual dimensions, we found the error of a global bandwidth for different query sizes in our metric scenario to be significantly high. Furthermore, a gradient descent based approach to band- width estimation turned out to get stuck in local optima of poor quality in our experiments. We thus also investigate locally adap- tive kernel estimators that vary the bandwidth either based on 351 ?? ?? ?????(??, ?) ????? ? 0 ???????? Figure 1: The incorporation of a kernel-sample s into the cardinality estimation for a query Q = (xQ , rQ ). The omit- ted y-axis corresponds to the probability density. Algorithm 1: Generic Kernel Estimation Algorithm Input :Kernel function Kh : R íì§ R+, centered at 0 Optimized bandwidths B : X  íì© X  íì© R+  íì§ R+ Samples S  íì¨ X  íì¨ X Total data set size |X | Distance function distX : X  íì© X  íì§ R+ Query Q = (xQ , rQ ) with object and radius Output :Estimated cardinality c?(Q) 1 total  íì§  0.0; 2 foreach s  íì¨ S do 3 h  íì§  B(s,xQ , rQ ); 4 s?  íì§  distX(xQ , s); 5 contribution  íì§   íì§¼ rQ 0 Kh (x ? s?) dx ; 6 total  íì§  total + contribution; 7 end 8 probability  íì§  total/|S |; 9 return ?probability  íì§ |X |?; the sample point or the evaluation point. The latter is also called balloon estimator [30]. Other work in kernel-based techniques for cardinality estima- tion in vector spaces focuses also on improving the efficiency of the estimation process. One approach is reducing the number of samples to a so-called coreset [33] that maximizes both quality and efficiency of the estimator. In the scope of this paper we do not yet consider such improvements but focus on demonstrating the general applicability of kernel estimators to this new scenario of metric data. 4 DISTANCE-BASED KERNEL ESTIMATORS Kernel estimators allow us to overcome a fundamental problem of using a sample directly for estimating the cardinality of a query result. Namely that the information is concentrated at a sample point. In contrast to a histogram we also get a continuous distribution. In a metric space it is, however, not straight-forward how we can apply a kernel function on a sample point, as there are no dimensions in which they could gradually distribute the mass of a sample. The central idea of our proposed technique is therefore to apply the kernel function on the distance to a sample point in order to incorporate the probability of elements in the vicinity fractionally. In the following we show how to incorporate a sample point into the cardinality estimate. Here, the query Q = (xQ , rQ ) with object xQ and radius rQ is located at distance s? B distX(xQ , s) 0.00 0.10 ? 1 0 1 2 3 Bandwidth M ed ia n  of  R el at iv e  E rr or s 0.00 0.10 0 50 0 15 00 25 00 Bandwidth Su m  o f S qu ar ed  E rr or s Figure 2: Influence of the bandwidth on the estimation er- ror on the Moby data set (cf. Section 6) for a fixed query size. The left-hand side shows the median of the relative errors (Equation (2)). The right-hand side shows the sum of squared errors (measureMLS ). from the sample point s . As depicted in Figure 1, we introduce an axis expressing the distance to xQ . For that wemap xQ to x?Q B 0, the origin of the axis. The sample point s is then mapped onto s? . The kernel function Kh is then centered at point s? by subtracting s? from its argument. We take the area under the curve of the kernel function between x?Q and rQ as the contribution of this sample to the cardinality estimate. Algorithm 1 shows the full estimation process. For each sam- ple point we calculate the contribution and compute the sum. For this we first compute the optimized bandwidth by calling the function B for the given sample point and query with object and radius. In case of a global bandwidth, this function ignores the parameters and always returns the same bandwidth. In case of a locally adaptive approach, it either uses the sample or evaluation point (query) to obtain a specific bandwidth. We detail algorithms for computing the bandwidth in the next section. Given the opti- mized bandwidth, the distance between sample and query object, and the radius, we calculate the contribution of the sample to the running total . After all samples are processed, the probability is then the total divided by the number of samples, see line 8. Finally, we scale the resulting probability up by the total data set size and return this value as the cardinality estimate. The general workflow of our technique consists of (1) collect- ing a set S of samples, (2) determining the optimal bandwidths B and (3) applying Algorithm 1 to estimate the cardinality of new queries. In the following we present the process of optimizing the bandwidths. 5 BANDWIDTH OPTIMIZATION It is well-known [31] that the bandwidth of a kernel function has a crucial impact on the resulting cardinality estimate. A too small bandwidth leads to undersmoothing, a too large bandwidth to oversmoothing. The two edge cases are an infinitely small bandwidth that converges to sampling and an infinitely large bandwidth that converges to a uniform distribution. We thus take particular care of finding an optimal value. We distinguish between a global bandwidth for all samples and queries, and locally adaptive methods where the bandwidth is individually fitted to accommodate for sparser and denser regions of the data space. 352 5.1 Global The computation of the optimal global bandwidth for a kernel function and a given data set is an optimization problem. We first formalize this problem and then present our optimization strategy. 5.1.1 Optimization Problem. We want to find a bandwidth h that minimizes the error of estimates for future queries on the given data set. As we do not know the future queries, we extract a set of training queries Q from the data set and minimize the error for these queries. Afterwards, we validate the performance against an independent set of test queries that we extracted from the data set beforehand. We formally define the optimization problem for a fixed kernel function as arg min h ErrorX (h,Q) , (1) where h is the bandwidth,X is the data set and ErrorX a function that computes the error of the queries Q on X for the given bandwidth h. We define an appropriate error measure for Equation (1) in two steps. First, we define an auxiliary function errorX (h,Q) B c?h (Q) ? c(Q) c(Q) , (2) where c?h (Q) is the estimated cardinality using bandwidth h and c(Q) the actual cardinality of queryQ on data setX . This measure differs slightly from the common relative error metric, as we do not take the absolute value in the numerator. This allows us to assess over- and underestimates separately. It returns values in the interval [?1, íí]. Two values are of particular interest: ?1 indicates that the estimator returns simply a result of zero even though there are results contained in the query. On the other hand, an error of zero indicates a perfect result: the estimated cardinality is equal to the true number of elements the query returns. There is no upper bound for our measure. However, one should notice, that a value of 1 means already an overestimation by a factor of 2. To compute the error of a set of queries Q we combine the errors errorX (Q) of the individual queriesQ  íì¨ Q using ameasure M : R |Q |  íì§ R+. M computes for a set of errors E a single value that is then subject to minimization. Two examples for M are the deviation of the median error from zero, and the sum of squared errors (LS for least squares): Mmedian (E) B | median(E) | MLS (E) B íì§2 e  íì¨E e2 . For M  íì¨ {Mmedian ,MLS }, the final optimization problem is defined as arg min h ErrorX (h,Q) = arg min h M({errorX (h,Q) | Q  íì¨ Q}) (3) 5.1.2 Optimization Strategy. The minimization of the error function (3) requires an efficient and robust optimization method. Figure 2 shows the relationship between bandwidth and error for an example data set. On the left-hand side of the plot we ob- serve that starting from an infinitely small bandwidth results first underestimate the true cardinality. A higher bandwidth reduces the error to a certain degree. At some point the bandwidth over- smoothes the distribution, leading to very high overestimations. The right-hand side shows the mean squared errors. While the general trend of the error function is clearly visible, we can also see that the results are noisy. This poses a difficult to find global optimum as the multitude of local optima has to be overcome. A method that has shown to be very effective in practice are Evolution Strategies. An Evolution Strategy (ES) is a global numeric optimization approach inspired by the Darwinian theory of natural selection. We implemented the approach of Beyer and Schwefel [6]. Here, ? parents produce another set of íì§íì§¬ offspring. From the thus ob- tained set of ? + íì§íì§¬ individuals the best ? individuals a",Michael Mattig,"Department of Mathematics and Computer Science University of Marburg, Germany",mattig@mathematik.uni-marburg.de,Thomas Fober,"Department of Mathematics and Computer Science University of Marburg, Germany",thomas@mathematik.uni-marburg.de,Christian Beilschmidt,"Department of Mathematics and Computer Science University of Marburg, Germany",beilschmidt@mathematik.uni-marburg.de,Bernhard Seeger,"Department of Mathematics and Computer Science University of Marburg, Germany",seeger@mathematik.uni-marburg.de,,,,,,,,,,,,,,,,,,
20200108,1391,Michael Vollmer,"Karlsruhe Institute of Technology (KIT) Karlsruhe, Germany",michael.vollmer@kit.edu,,Iterative Estimation of Mutual Information with Error Bounds,"ABSTRACT Mutual Information (MI) is an established measure for linear and nonlinear dependencies between two variables. Estimating MI is nontrivial and requires notable computation power for high estimation quality. While some estimation techniques allow trad- ing result quality for lower runtimes, this tradeoff is fixed per task and cannot be adjusted. If the available time is unknown in advance or is overestimated, one may need to abort the esti- mation without any result. Conversely, when there are several estimation tasks, and one wants to budget computation time between them, there currently is no efficient way to adjust it dynamically based on certain targets, e.g., high MI values or MI values close to a constant. In this article, we present an itera- tive estimator of MI. Our method offers an estimate with low quality near-instantly and improves this estimate in fine grained steps with more computation time. The estimate also converges towards the result of a conventional estimator. We prove that the time complexity for this convergence is only slightly slower than non-iterative estimation. Additionally, with each step our estimator also tightens statistical guarantees regarding the con- vergence result, i.e., confidence intervals, progressively. These also serve as quality indicators for early estimates and allow to reliably discern between attribute pairs with weak and strong dependencies. Our experiments show that these guarantees can also be used to execute threshold queries faster compared to non-iterative estimation. 1 INTRODUCTION Motivation. Detecting and quantifying dependencies between variables is an essential task in the database community [10, 13, 20, 30]. Conventional methods such as correlation coefficients and covariance matrices only detect linear or monotonous depen- dencies.Mutual Information (MI) in turn is an index that captures any linear and nonlinear dependency [1, 5]. Probability distri- butions of the variables in question serve as input to compute the MI. For real-world data however, these distributions are not available. In this case, MI must be estimated based on samples. Various estimators for MI have been proposed [15, 23, 33], and some offer good results even for small samples [15]. However, continuous variables with an unknown distribution continue to be challenging, since their multivariate distribution is substituted only by a limited sample. A prominent approach for estimation of MI between continuous variables without assumption of the distribution is the nearest-neighbor based method by Kraskov et al. (KSG) [19]. While good estimators are available, they are very rigid in their time requirements and regarding the estimation quality. Once the computation has started, they impose a fixed time requirement and do not yield aby preliminary result if they are terminated ? 2019 Copyright held by the owner/author(s). Published in Proceedings of the 22nd International Conference on Extending Database Technology (EDBT), March 26-29, 2019, ISBN 978-3-89318-081-3 on OpenProceedings.org. Distribution of this paper is permitted under the terms of the Creative Commons license CC-by-nc-nd 4.0. M u tu al  I n fo rm at io n Runtime MIT MIfin tT t tfin Figure 1: MI estimation with dynamic time allocation. prematurely. They also are unable to exploit easier queries like whether the MI value is above a certain threshold but instead determined the value. Such features are highly relevant for high- dimensional data and data streams with irregular arrival rate as we showcase with the following two scenarios. Scenario 1. Consider a modern production plant with smart meters installed on each machine. A first step in data exploration is determining which attributes are strongly dependent. For in- stance dependencies among currents or energy consumption may offer insights into production sequences. For this first step, a query like Which pairs of measurements have a MI value above the thresholdMIT ? often suffices. With conventional MI estima- tors, each pair either induces high computational costs, or results are uncertain because of low estimation quality. Scenario 2. Think of a database with financial data and its real- time analysis. To maintain a diverse portfolio, it is important to track the relationships between stocks. Because bids and trades happen irregularly, new information and market prices arrive at irregular speed. Thus, it is not known how much time is available to monitor stock relationships in the presence of incoming data. Current MI estimators cannot adapt during execution. They risk not producing a result in time, or estimates are of low quality. To improve upon these shortcomings, we study estimation of MI with dynamic allocation of computation time. Ideally, such an estimator should not only offer preliminary results, but also indicate its remaining uncertainty. Figure 1 shows exemplary pro- gression over time of such an estimator based on our experiments with real data. The black line indicates the preliminary estimate after a certain runtime, and the gray area shows the (expected) maximum error of the preliminary estimate. To obtain the defin- itive result MIfin, a user would require time tfin. However, he could also stop the estimator as soon as the estimate is above a threshold MIT with certainty, or he can use the preliminary result available after time t. In this work, we focus on iterative estimation of MI in order to offer this functionality. Here, iterative means quickly providing an estimate, but with the option to improve the estimation if there is time left. In other words, improving the estimate with     Series ISSN: 2367-2005 73 10.5441/002/edbt.2019.08 some time available is what we call an iteration. At the same time, an iterative estimator can terminate the estimation, i.e., stop iterating, when the result is good enough. For efficiency, it is important that computations from previous iterations remain useful and are not repeated or discarded in a later iteration. So far, efficient iterative estimators for MI do not exist. Challenges. The most significant feature of an estimator is its quality of estimation. This is even more so for iterative methods because both preliminary and final estimation quality are important. In other words, the estimate should already be useful after a few iterations, and estimation quality must level up to the one of conventional estimators after many iterations. Ideally, this convergence should happen after a known, finite number of iter- ations. In this article, we target at respective formal guarantees. Next, the quality of preliminary estimates is crucial for us- ability. Determining if a preliminary result is good enough or interesting enough to merit additional computation time requires some information on its certainty. The number of iterations alone is insufficient, as the result quality depends on many other fac- tors such as data characteristics, required accuracy and time con- straints. Instead, each estimate requires an individual indicator of the uncertainty remaining. While the time spent to improve the estimate iteratively is committed dynamically, it must of course be used efficiently. Many conventional estimators use data structures that are ex- pensive to build and cheap to use, such as space-partitioning trees [19, 31, 32]. Such an upfront activity is undesirable for an iterative estimator whose first estimate must arrive soon. At the same time, runtime and scalability do remain important charac- teristics of the estimator. In other words, an iterative estimator must feature guaranteed efficiency for both individual iterations and final estimates. Contributions. In this article, we present IMIE, our Iterative Mutual Information Estimator. To prove its practical usefulness, we establish several features both formally and experimentally. Quality of Estimation. In Section 4, we propose a design for IMIE such that estimates converge to the same value as with the KSG. To make early iterations useful, IMIE also offers statistical error bounds for its early estimates. More precisely, an early estimate provides a confidence interval for the final estimate. We describe the specifics and the statistical soundness in Section 4.3. Complexity. We study the time complexity of initialization and of individual iterations of IMIE. In Section 5 we establish an amor- tized time complexity for IMIE and the nearest-neighbor search used. This complexity is competitive with existing non-iterative estimators. To be precise, we show that iterating IMIE until con- vergence is only slightly slower in terms of time complexity than computing the KSG directly with optimal algorithms. Experimental Validation. We show that IMIE complements the formal guarantees established so far with good actual perfor- mance. To do so, we perform extensive experiments using both synthetic and real data sets in Section 6. On the one hand, we show that the concrete runtime and estimation results of IMIE are comparable to the ones of conventional estimation methods. On the other hand, the experiments show the practical benefits of the early results from IMIE. For instance, IMIE finds attribute pairs above a threshold value significantly faster than non-iterative estimators. 2 RELATEDWORK Iterative estimation ofMI is interesting from two perspectives. On the one hand, it is methodically interesting, as it can be considered an anytime algorithm. On the other hand, it is interesting to consider the benefits it provides over current methods in different settings. Important application scenarios are dependency analysis in high dimensional data and data streams, cf. Scenario 1 and 2. Anytime Algorithms. Anytime algorithms [36] use available time to increase their result quality. One can obtain a low-quality result after a short time and a better one when waiting longer. In data analysis, anytime algorithms exist for clustering [22], classification [35] and outlier detection [2]. So far however, there is no anytime algorithm to estimate MI. So while there is no direct competitor, IMIE extends the set of tools available as anytime algorithms. Additionally, there has been more general work on the optimal use of available anytime algorithms [11, 18], which may improve the performance of IMIE in larger systems. MI on Data Streams. Estimating MI on streams has received some attention recently. The MISE framework [14] summarizes a bivariate stream such that the MI for arbitrary time frames can be queried. To this end, MISE offers parameters for the balance between accuracy of older queries and resource requirements both in terms of memory and computation time. In contrast, the DIMID estimator [4] processes a bivariate stream as sliding win- dow for monitoring tasks. This approach provides fast updates between time steps by approximation with random projection. MI estimation in sliding windows has also been the focus of [32]. That paper provides lower bounds for estimates using Equa- tion 5 both in general and for updates in sliding windows. It also features two dynamic data structures, DEMI and ADEMI, to main- tain such estimates using either simple or complex algorithms and data structures. These approaches have limitations. First, they all impose the necessary execution time, i.e., one cannot adapt this time after the start of stream processing. If the rate of new items increases, the estimator may be unable to keep up. If it decreases, the es- timator cannot use this time to improve results. Second, the ap- proaches are all focused on bivariate streams. While MI is defined for exactly two variables, the number of attribute pairs grows quadratically in the number of dimensions. In contrast, the only information IMIE maintains on a stream is based on individual di- mensions and thus scales linearly with the dimensionality. Third, the approximate results of MISE and DIMID are difficult to use. Their estimation quality is only known on average; this average defines the perceived quality of individual estimates. So if one estimate has a very small error, it is less likely to be appreciated, while the error of a particularly bad estimate may be assumed to be smaller. Dependencies in High Dimensional Data. Even though MI is de- fined for exactly two variables, it hasmany applicationswith high- dimensional data. Prominent ones are image registration [25], which uses MI between two high-dimensional variables, and fea- ture selection [24], which targets at the MI between attributes and a classification label. But estimating the MI between all pairs of attributes has received little attention, despite being the non- linear equivalent of correlation matrices. [26] uses a different approach, i.e., kernel density estimation, and removes redundant computations that arise when using this estimator for each pair. This approach has a worse computational complexity than a pair- wise application of the KSG estimator, without offering better 74 0 1 2 3 4 5 6 7 8 9 1 2 4 3 5 MCy1 (p3) = 2 MCx1 (p3) = 3 y1 (p3) x1 (p3)6 7 p3 p1 p2 p4 p5 p6 X = { 1, 3, 4, 5, 8}6 Y = { 1, 3, 4, 5, 7} 2, x y Figure 2: Illustration of terms used for the KSG. results [15, 23]. While both scale quadratically in the number of attributes, their approach is also quadratic in the number of points. The complexity of the KSG in turn is (n logn) [32]. Ad- ditionally, it does not expose any parameter to modify the result quality. Consequently, there would not be any benefit of a direct experimental comparison with IMIE. 3 FUNDAMENTALS We first cover the background of MI and its estimation. Mutual Information. Shannon has introduced the notion of entropy [28] to quantify the expected information gained from ob- serving a value of a random variable.H (X ) stands for the entropy of a random variable X . The expected information of observing two random variables X and Y is the joint entropy H (X ,Y ). Mu- tual Information quantifies the amount of information that is shared or redundant between the two variables. It is defined as I (X ;Y ) = H (X ) + H (Y ) ? H (X ;Y ). (1) With the definition of entropy for continuous variables [6], the MI of two continuous random variables is I (X ;Y ) =  X  Y pXY (x,y) log ( pXY (x,y) pX (x)pY (y) ) dx dy, (2) where pX ,pY and pXY are the marginal and joint probability density functions of X and Y . The type of logarithm used in Equation 2 determines the unit of measurement. In this work we use the natural logarithm. This means that MI is measured in the natural unit of information (nat). Estimation. One can perceive many sources of data, e.g., smart meters or market prices, as random variables with unknown dis- tribution. Since Equation 2 requires probability density functions, we cannot compute the MI of such sources exactly. Instead, we can only estimate the MI based on available samples. The popular estimator that will serve as foundation of our work is the one by Kraskov, St?gbauer and Grassberger [19], which we call KSG. It is based on the estimator for probability densities by Loftsgaarden and Quesenberry [21], which Kozachenko and Leonenko have studied further in the context of entropy [17]. In the following, we briefly review the terms and computation of the KSG. Let P = {p1 = (xp1 ,yp1 ), . . . ,pn = (xpn ,ypn )}  R 2 be a sample from a random variable with two attributes. Figure 2 illustrates the notions that we define in the following using the sample P = {(1, 5), (6, 1), (5, 4), (4, 7), (3, 3), (8, 2)}. Let X = {xp1 , . . . , xpn } and Y = {yp1 , . . . ,ypn } be the set of values per attribute. For each point p  P , its k  N+ nearest neighbors in P using the maximum distance form the set kNN (p). More formally, it is kNN (p) = argmin S (P\{p }) s .t . |S |=k max s S ~p, s~, (3) with ~p, s~ = max(|xp ? xs |, |yp ? ys |). We define the largest distance between xp and any x-value among the k nearest neigh- bors of p as xk (p) = maxs kNN (p) |xp ?xs |. We use this distance xk (p) to define the x-marginal count MCxk (p) = |{x  (X \ xp ) : |x ? xp |   x k (p)}|, (4) which is the number of points whose x-value is close to p. In Figure 2, vertical dashed lines mark the area of points whose x-values are at least as close as the nearest neighbor of p3. Since this area contains three points excluding p3, it is M x 1 (p3) = 3. The distance  y k (p) and the y-marginal count MC x k (p) are defined analogously. Note that xk (p) and  y k (p) may differ, which results in differently sized areas for the marginal counts, as seen in Figure 2. Using these counts, the KSG estimate is defined as I? (P) = ? (n)+? (k)? 1 k ? 1 n n2. i=1 ? ( MCxk (pi ) ) +? ( MC y k (pi ) ) , (5) where ? is the digamma function. This is ? (z) = ?C + 2.z?1 t=1 1 t for z  N+ and C ? 0.577 being the Euler-Mascheroni constant. While k is a parameter of this estimator, it is generally rec- ommended [15, 16, 19] to use a small k , that is k  10. Gao et al. [9] have proven that the KSG is a consistent estimator for fixed k , that is, it converges towards the true value with increasing sample size. 4 ITERATIVE ESTIMATION In this section we present IMIE, our iterative estimator for MI. The core concept of our approach is considering the KSG estimate itself as the mean of a random variable with a finite population. Using subsamples of this population for early estimates offers beneficial properties such as an expected value equal to the KSG estimate and convergence to the KSG for large sample sizes. We first present IMIE and its underlying data structure as well as the algorithms for the initialization and for subsequent iterations. Then we describe our approach for nearest neighbor search, which is better for iterative algorithms than the standard procedures. Finally, we describe the statistical bounds that IMIE provides with its estimates. 4.1 IMIE For brevity, we introduce some notation in addition to the one from Section 3. For a pointp  P , we define the pointwise estimate (p) = ? ( MCxk (p) ) +? ( MC y k (p) ) . (6) The set of all pointwise estimates is  = {(p1), . . . ,(pn )}. Seeing  as a finite population of size n with mean ? , Equation 5 can be rewritten as I? (P) = ? (n) +? (k) ? 1 k ? ? . (7) Using a (random) subsample ?  , its mean ?? is an (unbiased) estimation of ? . This in turn yields an (unbiased) estimate of I? (P), I?? (P) = ? (n) +? (k) ? 1 k ? ?? . (8) 75 Data Structure 1: IMIE struct { Point[] P Real Mean, Var Int k,m Int[] OrderR , Orderx , Ordery Real Offset }; Algorithm 2: Init (P,k) 1 Persist k and P O(n) 2 Mean, Var,m $ 0 O(1) 3 OrderR , Orderx , Ordery $ (0, 1, . . . , |P | ? 1) O(n) 4 Sort Orderx and Ordery O(n logn) 5 Offset$ ? (|P |) +? (k) ? 1k O(1) The variance  2? of our subsample serves as a quality indicator of this approximation, which we further discuss in Section 4.3. The idea of IMIE is to maintain a subsample ? and use I?? (P) to estimate I? (P). Each iteration then increases the sample size of ? by one, to improve the estimate. Starting with an empty set, this means there are exactly |P | iterations before IMIE yields exactly the same result as the KSG, i.e., I?? (P) = I? (P). Data Structure. IMIE uses and stores P and k as well as some additional information listed in Data Structure 1. In the following we use the zero-indexed array notation P[i] = pi+1. Contrary to the original data sample P , we do not store ? explicitly. In- stead we store its mean Mean, its variance Var and size, which is the number of performed iterationsm. To maintain the current variance efficiently, we use the online algorithm by Welford [34]. To ensure that ? is a random subsample of , we need to draw without replacement. To this end, IMIE maintains an array of indices OrderR , where index i at position j means that (pi ) is added to ? in the j-th iteration. The positions of this array are randomly swapped during iterations to perform the random se- lection. This enables a fast selection of a random element without replacement in each iteration. In addition, we maintain two ar- rays Orderx and Ordery containing references to all points in P ordered by their x- and y-value, respectively. For instance, in- dex i at Orderx [0] means that pi has the smallest x-value in P , i.e., pi = argminpP xp . These ordered arrays are used to find nearest neighbors, as described in Section 4.2. Finally, we store the Offset = ? (n) + ? (k) ? 1k . With this, the (preliminary) MI estimate is available as I?? (P) = Offset ?Mean. Methods. We now present the two methods Init and Iterate. See Algorithms 2 and 3, together with amortized time complexi- ties, derived in Section 5. Init ensures the proper state of Data Structure 1 before the first iteration, i.e., preparing all variables assuming that |? | = 0. Observe that Init is a straightforward method for the simple case of static data with two attributes. For other scenarios, such as high-dimensional or streaming data, some adjustments to the initialization may be appropriate, as discussed in Section 5.3. Iterate increases the size of sample ? by one. This requires computing (p) for a random p  P with (p) < ?. Iterate consists of three phases. In the first one (Lines 1-3), we select a random point p of P that has not been selected earlier. After Algorithm 3: Iterate 1 ID$ Draw random integer from [m,n ? 1] O(1) 2 Swap values of OrderR [m] and OrderR [ID] O(1) 3 p $ P[OrderR [m]] O(1) 4 kNN (p) $ NNSearch(p) (see Algorithm 4) O(  n) 5 Compute xk (p),  y k (p) O(1) 6 ComputeMCxk (p),MC y k (p) O(logn) 7 (p) $ ? ( MCxk (p) ) +? ( MC y k (p) ) O(1) 8 m $m + 1 O(1) 9 Diff old $ (p) ? Mean O(1) 10 Mean$ Mean + Diff old m O(1) 11 Diff new $ (p) ? Mean O(1) 12 Var $ Var(m?1)+Diff old Diff new m O(1) m ? 1 iterations, we swap the index at position m of OrderR with the index at a random position behindm ? 1. This ensures that we do not use any index twice, since positions before m are not considered, and that each unused index has the same probability of being selected. This random swap is one step of the Fisher-Yates Shuffle in the version of Durstenfeld [8], which fully randomizes the order of a sequence. The second phase (Lines 4-7) computes (p) using the ordered lists Orderx and Ordery . The last phase (Lines 8-12) performs the online algorithm [34] to maintain mean and variance of a sample, in our case ?. Example 4.1. Disregarding the dashed lines for now, Figure 3 illustrates the state of Data Structure 1 after initialization and before the first iteration. For the first iteration, we draw an in- teger ID from {0, . . . ,n ? 1}. Suppose that we drew 5. We swap the content of OrderR [0] and OrderR [5]. OrderR [0] now contains 6. This means that this iteration adds (p6) to our implicit sam- ple ?. We then determine its nearest neighbor 1NN (p6) = {p15}, the distances x 1 (p6) and  y 1 (p6) as well as the marginal counts MCx 1 (p6) = 1 andMC y 1 (p6) = 3. The dashed lines in Figure 3 illus- trate the area of counted points in x and y-direction, respectively, identically to Figure 2. It follows that (p6) = ? (1)+? (3) = 0.346. Substituting the appropriate variables, the remaining values are set accordingly, i.e.,m = 0+ 1 = 1,Mean = 0+ 0.346 1 = 0.346 and Var = 00+00.346 1 = 0. The second iteration is analogous, draw- ing ID = 6 at random from {1, . . . ,n ? 1}, thus choosing p7. Its nearest neighbor is p8, and the marginal counts areMC x 1 (p7) = 1 andMC y 1 (p7) = 6, cf. the dashed lines in Figure 4. As a result, it is (p7) = ? (1)+? (6) = 1.129. Analogously to the first iteration, the remaining values arem = 1+1 = 2,Mean = 0.346+ 0.783 2 = 0.738 and Var = 01+0.7830.391 2 = 0.153. Figure 4 graphs the state of Data Structure 1 after both iterations, and the new MI estimate is 1.164 ? 0.738 = 0.426. 4.2 Nearest-Neighbor Search A computation-intensive step in Iterate is the computation of nearest neighbors, which also is a key step for static estima- tion with the KSG. The classic solution [19, 31] is using space- partitioning trees, which are optimal in terms of computational complexity [32]. This efficiency is achieved because the slow tree construction is performed once, and each nearest-neighbor search afterwards is fast. Contrary to the traditional KSG esti- mation, it is not known beforehand how many nearest-neighbor searches IMIE performs. Constructing such a tree for IMIE would 76 Mean = 0 Var = 0.153 m = 0 X Y p1 p7 p13p2 p9 p14 p3 9 2 5 10 4 14 3 1 121115 6 16 8 713 92 5104 14 3112 1115 6 168713 P Orderx Ordery OrderR k = 1 Offset = 1.164p12p4 p10 p8 p11 p15 p5 p6 p16 92 5 104 1431 1211 156 1687 13 Figure 3: State of IMIE after initialization. X Y p1 p7 p13p2 p9 p14 p3 9 2 5 10 4 14 3 1 121115 6 16 8 713 92 5104 14 3112 1115 6 168713 P Orderx Ordery OrderR k = 1 Offset = 1.164p12p4 p10 p8 Mean = 0.738 Var = 0 m = 2 925 104 143 1 1211 156 1687 13 p11 p15 p5 p6 p16 Figure 4: State of IMIE after two iterations ((p6) and (p7)). not only delay the first estimate, but may also be an inefficient choice overall if only few iterations take place. The opposite, i.e., searching nearest neighbors without any preparation, is a linear search. Each iteration would then require time linear in the num- ber of data points. Since IMIE should offer both fast iterations and preliminary estimates after a short time, our approach is a compromise between these two options. The general idea is to use sorted arrays to perform a guided linear search that offers a good amortized time complexity (cf. Section 5). In the following, we elaborate on our NNSearch approach. Let p be the point whose nearest neighbor we are searching for and q the nearest neighbor we have found so far. Then any point r with |xp ?xr | > ~p?q~ cannot be a nearest neighbor with the maximum norm. This means that we only have to consider the interval [xp ? ~p?q~, xp + ~p?q~] in the sorted array Orderx . When we find a closer point during the search, this interval gets smaller, and fewer points need to be considered. For the y-values, this is analogous. To reduce the number of worst-case scenarios, we perform this search simultaneously in both directions and terminate when either one terminates. See Algorithm 4 for an algorithmic description of NNSearch. Example 4.2. Figure 5 illustrates an exemplary run of this procedure for k = 1. The figure shows four states corresponding to the variables of NNSearch(p) after 0, . . . , 3 loops. The query point p is the filled square, and a projection of the points to their x- and y-coordinates is shown at the bottom and the left side, respectively. These projections indicate the order of points in Orderx and Ordery , respectively. Each state after the first loop also illustrates the variables of NNSearch. The nearest neighbor found so far is marked with a circle and is labeled NN , and the distance max = ~p ? NN ~ is used for the dashed lines that highlight the remaining area of nearest neighbor candidates. Points accessed via Orderx in a previous iteration are marked with a diagonal stripe from the upper left to the lower right. This is done analogously for Ordery . Each loop considers the next loops = 0 X Y p loops = 0 X Y max ?y? ?y+ ?x+ ?x? p 1 NN NN loops = 0 X Y ?y+ ?y? ?x+ ?x? p 2 max NN loops = 0 X Y ?y+ ?y? ?x+ ?x? p 3 max Figure 5: Illustration of Algorithm 4 for each loop. unmarked point in both directions for both Orderx and Ordery . Additionally, the small arrows illustrate the minimal distances ?? for any further point accessed when iterating over Orderx or Ordery in the respective direction. After the third loop, the arrows of ?y+ and ?y? both exceed the area of the remaining candidates, represented by the dashed lines. This means that all relevant candidates have been considered via Ordery , and that the current nearest neighbor is correct. 4.3 Statistical Quality Indicators Finally we present statistical guarantees for early estimates by IMIE. Since ? is a subsample of , statistical tests with ?? and  2 ? yield statistically significant assertions regarding ? . Equations 7 and 8 give way to analogous assertions for I? (P). Theorem 4.3 ([27]). Let  be a finite population of size n with mean ? and a variance  2  . When drawing an i.i.d. sample ? of size m from , the sample mean ?? has an expected value of E(?? ) = ? and a variance of  2?? =  2 m ( n?m n?1 ) . Proof. See [27].  While the classic version of the Central Limit Theorem is not formulated for finite populations, it has been proven that some variations are applicable, and that ?? is approximately normally distributed [27]. In other words, drawing a sample of sizem with a sample mean ? is as likely as drawing ? from N(? ,?? ) with ?? =   2?? . So we can estimate the probability that a sample mean ?? is off by more than a specified value ? > 0 by using the cumulative distribution function  of the standard normal distributionN(0, 1). This is illustrated in Figure 6 and is formally described as Pr[|?? ? ? | ? ?] = 2   ( ?? ?? ) . (9) 77 Algorithm 4: NNSearch(p) 1 ix , iy $ index of p in Orderx , Ordery , respectively 2 ?x+,?x?,?y+,?y?, loops$ 0 3 max $ 4 NN $ {} 5 while min(?x?,?x+) < max m",Michael Vollmer,"Karlsruhe Institute of Technology (KIT) Karlsruhe, Germany",michael.vollmer@kit.edu,Klemens B?hm,"Karlsruhe Institute of Technology (KIT) Karlsruhe, Germany",klemens.boehm@kit.edu,,,,,,,,,,,,,,,,,,,,,,,,
20200109,1393,Chris Mayfield,"Purdue University West Lafayette, Indiana, USA",cmayfiel@cs.purdue.edu,,ERACER: A Database Approach for Statistical Inference and Data Cleaning,"ERACER: A Database Approach for Statistical Inference and Data Cleaning, ERACER: A Database Approach for Statistical Inference and Data Cleaning, ERACER: A Database Approach for Statistical Inference and Data Cleaning, ERACER: A Database Approach for Statistical Inference and Data Cleaning, ERACER: A Database Approach for Statistical Inference and Data Cleaning, ABSTRACT Real-world databases often contain syntactic and semantic errors, in spite of integrity constraints and other safety measures incorporated into modern DBMSs. We present ERACER, an iterative statistical framework for inferring missing information and correcting such errors automatically. Our approach is based on belief propagation and relational dependency networks, and includes an efficient ap- proximate inference algorithm that is easily implemented in standard DBMSs using SQL and user defined functions. The system performs the inference and cleansing tasks in an integrated manner, using shrinkage techniques to infer correct values accurately even in the presence of dirty data. We evaluate the proposed methods empirically on multiple synthetic and real-world data sets. The results show that our framework achieves accuracy comparable to a baseline statistical method using Bayesian networks with exact inference. However, our framework has wider applicability than the Bayesian network baseline, due to its ability to reason with complex, cyclic relational dependencies. Categories and Subject Descriptors H.2.8 [Database Applications]: Statistical Databases; H.4 [Information Systems Applications]: Miscellaneous General Terms Algorithms, Experimentation, Performance Keywords Relational dependency network, approximate inference, dis- crete convolution, linear regression, outlier detection 1. INTRODUCTION Although the database community has produced a large amount of research on integrity constraints and other safety measures to ensure the quality of information stored in relational database management systems (DBMSs), real-world databases often contain a significant amount of non-trivial errors. These errors, both syntactic and semantic, are gen- erally subtle mistakes which are difficult or even impossible to express using the general types of constraints available in modern DBMSs. In addition, quality control on data in- put is decreasing as collaborative efforts increase, with the Internet facilitating widespread data exchange, collection, and integration activities. Clearly, there is an increasing need for new approaches to data cleaning for the purpose of maintaining quality in relational databases. Data cleaning (or cleansing, scrubbing, etc.) is the process of identifying and repairing incorrect or corrupt records in a database. The goal is not only to bring the database into a consistent state (i.e., with respect to domain or in- tegrity constraints), but also to ensure an accurate and com- plete representation of the real-world constructs to which the data refer. Two surveys of common techniques and general challenges in this research area include [16] and [22]. Removing impurities from data is traditionally an engineering problem, where ad-hoc tools made up of low-level rules and manually-tuned algorithms are designed for specific tasks. However, recent work has shown the the effectiveness of ap- plying techniques from machine learning and data mining for the purpose of data cleaning [7]. In particular, statis- tical methods make it possible to automate the cleansing process for a variety of domains. For this work we develop statistical methods for cleaning relational databases with the following characteristics: ? Incomplete and erroneous: There are both (1) missing values to be filled in, and (2) corrupted val- ues to be identified. This goes beyond traditional statistical methods which make assumptions about the reliability of the non-missing values. ? Correlated attributes: The values of different at- tributes are correlated, both within and across tuples (involving perhaps multiple relations). Much of the prior work in data cleaning concentrates on values within a single tuple or relation. ? High-level dependencies: The attributes with large domains (i.e., many possible values), exhibit higherlevel dependencies among sets of similar values (for categorical variables) or a numerical functional depen- dency (for continuous variables). As an example of this type of domain, consider the task of inferring missing birth and death years of individuals in genealogical databases. The individuals are related through 75 parent-child relationships and the birth and death years of an individual are correlated due to life expectancies. In ad- dition, the birth dates of parents and children are correlated due to expected parenting ages. Furthermore, since life ex- pectancies and parenthood ages are likely to be similar over time, the dependencies do not need to be modeled for specific birth years. Instead they can be modeled as a higher-level functional dependency such as birth year = parent 's birth year + . A statistical method can learn these dependencies from the available data and then use them to infer missing values automatically. As another example, consider the task of inferring missing data in sensor networks. There are often relationships among the different types of measurements in the same sen- sor (e.g., temperature and humidity), as well as relationships among the measurements of neighboring sensors due to spa- tial locality. Again, a statistical method could learn these dependencies from observed data and then use them to infer missing values (e.g., due to battery loss) and/or clean corrupt values (e.g., due to sensor malfunction). Such a method can also be used for anomaly detection and intrusion detec- tion systems. This paper introduces ERACER, a database-centric statistical framework for integrated data cleaning and imputa- tion. The core techniques are based on belief propagation [20] and relational dependency networks [18]. We show how to implement the inference and cleaning processes efficiently at the database level. This eliminates the expensive process of migrating the data to and from statistical software such as R or Matlab, which is particularly useful when the amount of data aor limited processing time and resources aprevents a more extensive analysis. In contrast to prior work that cleans values within a single tuple, our approach exploits the graphical structure of the data to propagate inferences throughout the database. As a result the imputation and cleaning tasks go hand in hand: additional information in the database helps identify errors more accurately, and cor- rected data values improve the quality of inference for the missing values.",Chris Mayfield,"Purdue University West Lafayette, Indiana, USA",cmayfiel@cs.purdue.edu,Jennifer Neville,"Purdue University West Lafayette, Indiana, USA",neville@cs.purdue.edu,Sunil Prabhakar,"Purdue University West Lafayette, Indiana, USA",sunil@cs.purdue.edu,,,,,,,,,,,,,,,,,,,,,
20200110,1394,Alexander Hall,"Google, Inc.",alexhall@google.com,,Processing a Trillion Cells per Mouse Click,"Processing a Trillion Cells per Mouse Click, Processing a Trillion Cells per Mouse Click, Processing a Trillion Cells per Mouse Click, Processing a Trillion Cells per Mouse Click, Processing a Trillion Cells per Mouse Click, ABSTRACT Column-oriented database systems have been a real game changer for the industry in recent years. Highly tuned and performant systems have evolved that provide users with the possibility of answering ad hoc queries over large datasets in an interactive manner. In this paper we present the column-oriented datastore developed as one of the central components of PowerDrill1. It combines the advantages of columnar data layout with other known techniques (such as using composite range partitions) and extensive algorithmic engineering on key data structures. The main goal of the latter being to reduce the main memory footprint and to increase the efficiency in processing typical user queries. In this combination we achieve large speed-ups. These enable a highly interactive Web UI where it is common that a single mouse click leads to processing a trillion values in the underlying dataset. 1. INTRODUCTION In the last decade, large companies have been placing an ever increasing importance on mining their in-house databases; often recognizing them as one of their core assets. With this and with dataset sizes growing at an enormous pace, it comes as no surprise that the interest in column- oriented databases (column-stores) has grown equally. This spawned several dozens of research papers and at least a dozen of new column-store start-ups, cf. [2]. This is in ad- dition to well established offerings, e.g., by MonetDB [25], Netezza [26], or QlikTech [30]. Since 2011 all major commer- cial database vendors actually provide column-store tech- nologies (cf. [25]). Typically, these products are deployed to import existing databases into the respective column-store. An OLAP or OLTP, i.e., SQL, interface is provided to then mine the data interactively. The key advantage shared by these systems is that column-oriented storage enables reading only data for  relevant columns. Obviously, in denormalized datasets with often several thousands of columns this can make a huge difference compared to the the row-wise storage used by most database systems. Moreover, columnar formats compress very well, thus leading to less I/O and main memory usage. At Google multiple frameworks have been developed to support data analysis at a very large scale. Best known and most widely used are MapReduce [13] and Dremel [23]. Both are highly distributed systems processing requests on thou- sands of machines. The latter is a column-store providing interactive query speeds for ad hoc SQL-like queries. In this paper we present an alternative column-store de- veloped at Google as part of the PowerDrill project. For typical user queries originating from an interactive Web UI (developed as part of the same project) it gives a perfor- mance boost of 10?100x compared to traditional column- stores which do full scans of the data. Background Before diving into the subject matter, we give a little back- ground about the PowerDrill system and how it is used for data analysis at Google. Its most visible part is an interactive Web UI making heavy use of AJAX with the help of the Google Web Toolkit [16]. It enables data visualization and exploration with flexible drill down capabilities. In the back- ground, the ""engine"" provides an abstraction layer for the UI based on SQL: the user constructs charts via drag'n'drop op- erations, they get translated to group-by SQL queries, which the engine parses and processes. It can send out such queries to different backends, e.g., Dremel, or execute them directly on data stored, e.g., in CSV files, record-io files (binary for- mat based on protocol buffers [29]), or in Bigtable [10]. The third large part of the project is the column-store presented in this paper. The Web UI is very versatile; it allows to select arbitrary dimensions, measures, and computed values for grouping and filtering. The dimensions can have a large number of distinct values, such as strings representing Google searches. A user can quickly drill down to values of interest, e.g., all German searches from yesterday afternoon that contain the word ""auto"", by restricting a set of charts to these values. For these reasons, pre-aggregation or indexing of data does not help and we need to query the raw data directly. The nature of the use cases enabled by this UI demand for high availability and low latency. Examples of such use cases include: Responding to customer requests, spam anal- ysis, dealing with alerts in highly critical revenue systems, or monitoring and assessing changes to production systems. The system has been in production since end of 2008 and  was made available for internal users across all of Google mid 2009. Each month it is used by more than 800 users sending out about 4 million SQL queries. After a hard day's work, one of our top users has spent over 6 hours in the UI, triggering up to 12 thousand queries. When using our column-store as a backend, this may amount to scanning as much as 525 trillion cells in (hypothetical) full scans. The column-store developed as part of PowerDrill is tai- lored to support a few selected datasets and tuned for speed on typical queries resulting from users interacting with the UI. Compared to Dremel which supports thousands of dif- ferent datasets (streaming the data from a distributed file system such as GFS [15]), our column-store relies on having as much data in memory as possible. PowerDrill can run interactive single queries over more rows than Dremel, how- ever the total amount of data it can serve is much smaller, since data is kept mostly in memory, whereas Dremel uses a distributed file system. This and several other important distinctions, enable han- dling very large amounts of data in interactive queries. Consider a typical use case such as triggering 20 SQL queries with a single mouse click in the UI. In our production sys- tem on average these queries process 782 billion cells in 30-40 seconds (under 2 seconds per query), several orders of mag- nitude faster than what a more traditional approach as used by Dremel could provide. Contributions The main contributions presented in this paper: ? We describe how-unlike in most column-stores-the data is partitioned and organized in an import phase (Section 2.2). This enables skipping and caching large parts of the data: on average in production 92.41% is skipped and 5.02% cached, leaving only 2.66% to be scanned (see also Section 6). ? We present the basic data-structures used in Section 2.3. Their main goal is to support the partitioned layout of the data and to enable quick skipping of chunks of data. For optimal usage it is assumed they can be held in memory. Experiments show that these simple data-structures also directly give performance benefits of around 100x or more on full scans, compared to two row-wise stor- age formats and Dremel's column-store (Section 2.5). Note that for these experiments we do not partition the data at import. When dropping the ""in memory"" assumption, a still impressive factor of 30x can be achieved. ? In Section 3 we present several successive ""algorithmic engineering"" choices to improve key data-structures. The aim being to reduce the memory footprint for certain typical cases. We pin-point the effects of individ- ual optimizations with experiments measuring mem- ory usage. E.g., for the important case of a field with many distinct values, we obtain a reduction of 16x. ? In Section 4 we describe how queries may be computed in a distributed manner on a cluster of machines. In Section 5 we present selected extensions and finally in Section 6 the highly distributed setup of the actual productionized system running on over 1000 machines. We give measurements concerning the usage in prac- tice which show the positive effect of the partitioning (enabling to skip or cache large parts of the data). Related Work For an introduction to OLAP and basic techniques applied in data-warehouse applications, see the Chaudhuri and Dayal [11]. To obtain an overview of recent work on column-store architectures, please see the concise review [2] and references therein. The excellent PhD thesis by Abadi [1] can serve as a more in-depth introduction and overview of the topic. Recent research in this area includes, e.g., work on how super-scalar CPU architectures affect query processing [9], tuple reconstruction [17], compression in column-stores [34, 9, 3], and a comparison to traditional row-wise storage [4]. Kersten et al. [20] give a more open ended outlook on inter- esting future research directions. The plethora of open-source and commercial column-store systems, e.g., [34, 25, 26, 30, 36] further demonstrates the effectiveness of this paradigm. Melnik et al. [23] recently have introduced Dremel to a wider audience. As mentioned, its power lies in providing interactive responses to ad hoc SQL queries over thousands of datasets. It achieves this by streaming over petabytes of data (stored, e.g., on GFS [15]) in a highly distributed and efficient manner. This is also a key difference to the column-store presented in this paper which heavily relies on having as much data in memory as possible and therefore only is used for a few selected data sources. Melnik et al. also give a nice overview of data anlysis at Google and how interactive approaches like Dremel's complement the offline MapReduce [13] framework. Skipping over data in the context of colum-stores has been explored by other authors, e.g., Slezak et al. [32] or Mo- erkotte [24]. We give some details on these approaches in comparison to ours in Section 2.1. Reordering rows to improve the compression of column- wise stored data has been investigated, e.g., by [18, 21, 3]. We give some details on this at the end of Section 3. Notation and Simplifying Assumptions For the remainder of the paper we will only consider im- porting and processing data from single tables; which, e.g., correspond to log files at Google in the ""protocol buffers"" format [29] or result from denormalizing a set of relational tables in a database. We refer to such an instance as table or just the data which has columns (also referred to as fields) and rows (also referred to as records). In order to store pro- tocol buffer records with nested and repeated records (i.e., lists of sub-records), PowerDrill supports a nested relational model, cf. [5]. For ease of exposition, in the following we focus on unstructured / flat records as opposed to records which may, e.g., contain lists.",Alexander Hall,"Google, Inc.",alexhall@google.com,Olaf Bachmann,"Google, Inc.",olafb@google.com,Robert Bu ?ssow,"Google, Inc.",buessow@google.com,Silviu Ga ?nceanu,"Google, Inc.",silviu@google.com,Marc Nunkesser,"Google, Inc.",marcnunkesser@google.com,,,,,,,,,,,,,,,
20200111,1395,Bahman Bahmani,Stanford University,bahman@stanford.edu,,Fast Personalized PageRank on MapReduce,"Fast Personalized PageRank on MapReduce, Fast Personalized PageRank on MapReduce, Fast Personalized PageRank on MapReduce, Fast Personalized PageRank on MapReduce, Fast Personalized PageRank on MapReduce, ABSTRACT In this paper, we design a fast MapReduce algorithm for Monte Carlo approximation of personalized PageRank vectors of all the nodes in a graph. The basic idea is very efficiently doing single random walks of a given length start- ing at each node in the graph. More precisely, we design a MapReduce algorithm, which given a graph G and a length \lamda, outputs a single random walk of length \lamda starting at each node in G. We will show that the number of MapReduce iterations used by our algorithm is optimal among a broad family of algorithms for the problem, and its I/O efficiency is much better than the existing candidates. We will then show how we can use this algorithm to very efficiently ap- proximate all the personalized PageRank vectors. Our em- pirical evaluation on real-life graph data and in production MapReduce environment shows that our algorithm is significantly more efficient than all the existing algorithms in the MapReduce setting. Categories and Subject Descriptors G.2.2 [Discrete Mathematics]: Graph Theory-Graph al- gorithms; F.1.2 [Computation By Abstract Devices]: Modes of Computation-Parallelism and concurrency General Terms Algorithms, Design, Performance, Experimentation Keywords Personalized PageRank, MapReduce 1. INTRODUCTION Very large scale datasets and graphs are ubiquitous in today's world: world wide web, online social networks, and ?Work done while visiting Microsoft Research. (15)Work done while at Microsoft Research.  huge search and query-click logs regularly collected and pro- cessed by search engines. Because of the massive scale of these datasets, doing analyses and computations on them is infeasible for individual machines. Therefore, there is a growing need for distributed ways of storing and processing these datasets. MapReduce, a simple model of computation, first introduced by Dean and Ghemawat [9], has recently emerged as a very attractive way of doing such analyses. Its effectiveness and simplicity has resulted in its implementation by different internet companies [9, 13, 5, 22], and widespread adoption for a wide range of applications [19], including large scale graph computations [15, 16]. One of the most well known graph computation problems is computing personalized PageRanks (PPR) [12]. Personal- ized PageRanks (and other personalized random walk based measures) have proved to be very effective in a variety of ap- plications, such as link prediction [17] and friend recommen- dation [3] in social networks, and there are many algorithms designed to approximate them in different computational models [14, 3, 10, 25]. In this paper, we study the problem of Fully Personalized PageRank (FPPR) approximation on MapReduce. Specifi- cally, we study the problem of approximating the personalized PageRank vectors of all nodes in a graph in the MapRe- duce setting, and present a fast MapReduce algorithm for Monte Carlo approximation of these vectors. Even though some of the previously designed personalized PageRank approximation algorithms can be implemented in MapReduce, we will show that our algorithm takes much better advantage of the parallel computation model of MapReduce and is hence significantly more efficient than the existing candi- dates in this setting. We also note that our algorithm can be used for computing other personalized random walk based measures (such as personalized SALSA [3]) in MapReduce as well. In this introduction, we first provide some background on personalized PageRank and MapReduce, and then give the problem statements, and also outline our results. 1.1 Background Here we review personalized PageRank, the Monte Carlo approach for PageRank computation, and MapReduce. Here, and throughout the paper, we assume to have a weighted directed graph G = (V,E) with n nodes and m edges. We denote the weight on an edge (u, v) \forall E with u,v and, for the sake of simplifying the presentation of some of the for- mulae, assume for the rest of the paper that the weights on the outgoing edges of each node sum up to 1. 973 1.1.1 Personalized PageRank PageRank is the stationary distribution of a random walk that at each step, with a probability , usually called the tele- port probability, jumps to a random node, and with proba- bility 1? follows a random outgoing edge from the current node. Personalized PageRank is the same as PageRank, ex- cept all the random jumps are done back to the same node, denoted as the ""source"" or ""seed"" node, for which we are personalizing the PageRank. One can easily see that the personalized PageRank of node v, with respect to a source node u, denoted by (v), satis- fies: (v) = u(v) + (1 ? ) 2. {w|(w,v)\forallE} (w)w,v (1) Where u(v) = 1 if and only if u = v (and 0 otherwise). The fully personalized PageRank computation problem is to compute all the vectors ?"" u for all u \forall V . Of course, most applications, such as friend recommendation or query suggestion, only require the top-k values (and corresponding nodes) in each PPR vector (for some suitable value of k). 1.1.2 Monte Carlo Approach There are two broad approaches to computing Personal- ized PageRank. The first approach is to use linear alge- braic techniques, such as Power Iteration [23]. The other approach is Monte Carlo, where the basic idea is to approx- imate Personalized PageRanks by directly simulating the corresponding random walks and then estimating the sta- tionary distributions with the empirical distributions of the performed walks. Based on this idea, Fogaras et al [10] and later Avrachenkov et al [2] proposed the following method for PPR approximation: Starting at each node u \forall V , do a number, R, of random walks starting at u, called ""finger- prints"", each having a length geometrically distributed as Geom( ). Then, the frequencies of visits to different nodes in these fingerprints will approximate the personalized PageR- anks. Our algorithm also belongs to the Monte Carlo family. 1.1.3 MapReduce MapReduce [9] is a simple computation model for process- ing huge amounts of data in massively parallel fashion, using a large number of commodity machines. By automatically handling the lower level issues, such as job distribution, data storage and flow, and fault tolerance, it provides a simple computational abstraction. In MapReduce, computations are done in three phases. The Map phase reads a collection of values or key/value pairs from an input source, and by invoking a user defined Mapper function on each input element independently and in parallel, emits zero or more key/value pairs associated with that input element. The Shuffle phase groups together all the Mapper-emitted key/value pairs sharing the same key, and outputs each distinct group to the next phase. The Reduce phase invokes a user-defined Reducer function on each distinct group, independently and in parallel, and emits zero or more values to associate with the group's key. The emitted key/value pairs can then be written on the disk or be the input of a Map phase in a following iteration. 1.2 Problem Statement In this paper, we study the problem of FPPR approxi- mation on MapReduce (FPPR-MapReduce): Design an efficient MapReduce algorithm that given a weighted directed graph G = (V,E), approximately computes the personalized PageRank vectors ?"" of all nodes u \forall V . As stated earlier, we adopt the Monte Carlo approach, which requires simulating a number, R, of random walks (fingerprints) from each node. Therefore, we will need to solve the following sub-problem, that we call the Single Ran- dom Walk problem (SRW-MapReduce): Design a MapRe- duce algorithm that given a graph G and a length \lamda, outputs one random walk of length \lamda starting from each node in the graph. 1.3 Our Contribution Intuitively speaking, to fully leverage the power of par- allel computation supported by MapReduce, a good algo- rithm should have the following properties: (1) high parallelization and (2) small number of MapReduce iterations. The Monte Carlo approach for FPPR approximation natu- rally has the first property, as any fingerprint starting at any source node can be computed in parallel with and independently from all other fingerprints (for the same or different source nodes). However, as pointed out in [10], some of the fingerprints may be very long, and hence require a large number of MapReduce iterations using the straightforward implementation (e.g., one MapReduce iteration for each step in the walk). For instance, with = 0.2, a fingerprint can be longer than 10 steps with probability 0.11, and can be longer than 20 steps with probability 0.012. These long walks will become the bottleneck of the algorithm, blocking the entire computation, and causing it to take too long to run. In this paper, we develop an algorithm to compute single random walks of a given length for all nodes in a graph, and show that it is optimal in terms of the number of MapReduce iterations among a broad class of algorithms. Based on that, we then develop an efficient algorithm to approximate fully personalized PageRanks on MapReduce, and also analyze its I/O cost. Our empirical evaluation on real-life graph data and in production MapReduce environment demonstrates that our algorithm outperforms the state of the art FPPR approximation algorithms, in terms of efficiency and approx- imation error. The rest of the paper is organized as follows. Section 2 gives the background for computing FPPR on MapReduce. The single random walk algorithm is presented in section 3, and the FPPR approximation algorithm is presented in section 4. We show experimental results in section 5, review the related work in section 6, and finally conclude this paper in section 7.",Bahman Bahmani,Stanford University,bahman@stanford.edu,Kaushik Chakrabarti,Microsoft Research,kaushik@microsoft.com,Dong Xin,Google Inc.,dongxin@google.com,,,,,,,,,,,,,,,,,,,,,
20200112,1396,Robert Fink,"Deptartment of Computer Science, University of Oxford Wolfson Building, Parks Road, OX1 3QD Oxford, UK",robert.fink@cs.ox.ac.uk,,Aggregation in Probabilistic Databases via Knowledge Compilation,"Aggregation in Probabilistic Databases via Knowledge Compilation, Aggregation in Probabilistic Databases via Knowledge Compilation, Aggregation in Probabilistic Databases via Knowledge Compilation, Aggregation in Probabilistic Databases via Knowledge Compilation, Aggregation in Probabilistic Databases via Knowledge Compilation, ABSTRACT This paper presents a query evaluation technique for positive relational algebra queries with aggregates on a representation system for probabilistic data based on the algebraic structures of semiring and semimodule. The core of our eval- uation technique is a procedure that compiles semimodule and semiring expressions into so-called decomposition trees, for which the computation of the probability distribution can be done in time linear in the product of the sizes of the probability distributions represented by its nodes. We give syntactic characterisations of tractable queries with aggregates by exploiting the connection between query tractabil- ity and polynomial-time decomposition trees. A prototype of the technique is incorporated in the prob- abilistic database engine SPROUT. We report on performance experiments with custom datasets and TPC-H data. 1. INTRODUCTION This paper considers the evaluation problem for queries with aggregates on probabilistic databases. The utility of aggregation has been argued for at length. In particular, aggregates are crucial for OLAP and decision support systems. All 22 TPC-H queries involve aggregation. Probabilistic databases are useful to represent and query imprecise and uncertain data, such as data acquired through measurements, integrated from multiple sources, or produced by information extraction [21]. In this paper, we use a rep- resentation system for probabilistic data called pvc-tables. It is based on the algebraic structures of semiring and semi- module to support a mixed representation of aggregated val- ues and tuple annotations for different classes of annotations and aggregations [2]. The pvc-tables can represent any fi- nite probability distribution over relational databases. In addition, the results of queries with aggregates can be represented as pvc-tables of polynomial size. This contrasts with main-stream representation systems such as pc-tables [21], which can require an exponential-size overhead [15]. The problem of query evaluation is #P-hard already for simple conjunctive queries [21]. Aggregates are a further source of computational complexity: for example, already deciding whether there is a possible world in which the SUM of values of an attribute equals a given constant is NP-hard. Existing approaches to aggregates in probabilistic databases have considered restricted instances of the problem: they fo- cus on aggregates over one probabilistic table of restricted expressiveness [4, 20, 16], or rely on expected values and Monte-Carlo sampling [10, 12, 22]. Expected values can lead to unintuitive query answers, for instance when data values and their probabilities follow skewed and non-aligned distributions [19]. Abiteboul et al. investigate XML queries with aggregates on probabilistic data [1]. An algebra pro- posed by Koch represents annotations and data values as rings which enables efficient incremental view maintenance in the presence of aggregations [13]. Our approach considers the problem of exact probabil- ity computation for positive relational algebra queries with aggregates on pvc-tables. The core of our technique is a procedure that compiles arbitrary semimodule and semiring expressions over random variables into so-called decompo- sition trees, for which the computation of the probability distribution can be done in polynomial time in the size of the tree and of the distributions at its nodes. Decomposition trees are a knowledge compilation technique [5] that reflects structural decompositions of expressions into independent and mutually exclusive sub-expressions. Flavours of decomposition trees have been proposed as compilation target for propositional formulas that arise in the evaluation of relational algebra queries (without aggregates) on probabilistic c-tables [18]. It has been shown that more complex tasks, such as conditioning probabilistic databases on given constraints [14] and sensitivity analysis and explanation of query results [11], can benefit from decomposition trees. Example 1. Figure 1 shows six pvc-tables, amongst them the suppliers table S, the products tables P1 and P2, and the table PS pairing suppliers and products. They all have an annotation column  to hold expressions in a semiring K generated by a set of independent random variables, with operations sum (+) and product (,), and neutral elements 0k and 1K . Each valuation of the random variables into a semiring (e.g. integers or Booleans) canonically maps semi- ring expressions into that semiring by interpreting + and , as the corresponding operations of that semiring. Each such valuation defines a possible world of the database. Figure 1d shows the result of the query Q1 that asks for prices of products available in shops. The annotations of the result tuples are constructed as follows: The annotation of a join of two tuples is the product of their annotations, and the annotation obtained from projection or union is the sum of the annotations of the participating tuples [7]. For instance, the tuple <M&S, 10> has the annotation x1y11(z1+z5), whose probability distribution can be computed as a function of probability distributions of the random variables x1, y11, z1, and z5 [21]. Consider the query Q2 from Figure 1e that asks for shops in which the maximal price for the products in P1 or P2 is less than 50. Aggregation is expressed using the $ operator, which in this query groups by the column shop and applies the aggregation MAX on price within each group. The annotations of result tuples are built using semi- module expressions of the form  ? v, where  is a semi- ring expression and v is a data value. Such expressions can be ^summed up' with respect to aggregation opera- tions: For MIN, the sum  +min  is min(, ); for MAX, +max = max(, ); for SUM, +sum = +. The sums correspond to operations in commutative monoids. The annotation  of M&S in Q2's result is constructed as follows. This tuple represents a group of six tuples in the result of Q1, all with the M&S shop value. The annotation  then expresses the conditions (1) that the sum of the price val- ues of these six tuples in the MAX monoid is less than 50, and (2) that the group is not empty (as expressed by ). Depending on the valuation of the variables in , these con- ditions can be true (>) or false (}), or, more generally, the additive or multiplicative neutral element of the semiring. For instance, a valuation  that maps x1, x2, y11, y21, z1, z2, z5 to > and all other variables to } satisfies \, since \(\) \ [>? 10 +max }? 50 +max >? 11 +max }? 60 +max }? 60 +max }? 15  50] , > ã [10 +max 11  50] \ [max(10, 11)  50] \ >. 2 If the variables in such expressions are random variables, then the expressions themselves can be interpreted as ran- dom variables. Moreover, the probability distributions of the obtained expressions reflect the probabilities of query answers taking particular values in a randomly drawn world of the database. Our technique allows to efficiently com- pute probabilities defined by such expressions by structural decomposition. For example, an expression  = ab ? 10 + xy ? 20 can be decomposed in independent sub-expressions ab? 10 and xy? 20 that do not share variable symbols and hence constitute independent random variables. The structure of the paper follows the list of contributions: ? We present an evaluation framework for queries with aggregates (SUM, PROD, COUNT, MIN, MAX) on pvc-tables, a representation system for probabilistic data based on semirings and semimodules. ? We devise a technique for computing the exact proba- bility distribution of query results based on a generic compilation procedure of arbitrary semimodule and semiring expressions into so-called decomposition trees, for which the computation of the probability distribu- tion can be done in time linear in the product of the sizes of the distributions represented by its nodes. ? We give a syntactic characterisation of a class of aggregate queries that are tractable on tuple-independent databases. Our query tractability result follows from the observation that the semiring and semimodule ex- pressions in the result of our tractable queries admit polynomial size decomposition trees and polynomial size probability distributions at their nodes. ? A prototype of our technique is incorporated into the probabilistic database engine SPROUT. ? Extensive performance experiments using our own syn- thetic datasets and TPC-H data are discussed. Besides exact computation, decomposition trees also al- low for approximate probability computation [18]. Due to lack of space, we refer the reader to the MSc thesis of the second author [9]. The pvc-tables can be extended to cope with continuous probability distributions, similar to the ex- tensions of pc-tables in the PIP system [12]. ",Robert Fink,"Deptartment of Computer Science, University of Oxford Wolfson Building, Parks Road, OX1 3QD Oxford, UK",robert.fink@cs.ox.ac.uk,Larisa Han,"Deptartment of Computer Science, University of Oxford Wolfson Building, Parks Road, OX1 3QD Oxford, UK",dan.olteanu@cs.ox.ac.uk,Dan Olteanu,"Deptartment of Computer Science, University of Oxford Wolfson Building, Parks Road, OX1 3QD Oxford, UK",hanlarisa@gmail.com,,,,,,,,,,,,,,,,,,,,,
20200113,1397,Yanhao Wang,National University of Singapore,yanhao90@comp.nus.edu.sg,,Semantic and Influence aware k-RepresentativeQueries over Social Streams,"Semantic and Influence aware k-RepresentativeQueries over Social Streams, Semantic and Influence aware k-RepresentativeQueries over Social Streams, Semantic and Influence aware k-RepresentativeQueries over Social Streams, Semantic and Influence aware k-RepresentativeQueries over Social Streams, Semantic and Influence aware k-RepresentativeQueries over Social Streams, ABSTRACT Massive volumes of data continuously generated on social plat- forms have become an important information source for users. A primary method to obtain fresh and valuable information from social streams is social search. Although there have been exten- sive studies on social search, existing methods only focus on the relevance of query results but ignore the representativeness. In this paper, we propose a novel Semantic and Influence aware k-Representative (k-SIR) query for social streams based on topic modeling. Specifically, we consider that both user queries and elements are represented as vectors in the topic space. A k-SIR query retrieves a set of k elements with the maximum repre- sentativeness over the sliding window at query time w.r.t. the query vector. The representativeness of an element set comprises both semantic and influence scores computed by the topic model. Subsequently, we design two approximation algorithms, namely Multi-Topic ThresholdStream (MTTS) and Multi-Topic Th- resholdDescend (MTTD), to process k-SIR queries in real-time. Both algorithms leverage the ranked lists maintained on each topic for k-SIR processing with theoretical guarantees. Extensive experiments on real-world datasets demonstrate the effectiveness of k-SIR query compared with existing methods as well as the efficiency and scalability of our proposed algorithms for k-SIR processing. 1 INTRODUCTION Enormous amount of data is being continuously generated by web users on social platforms at an unprecedented rate. For ex- ample, around 650 million tweets are posted by 330 million users on Twitter per day. Such user generated data can be modeled as continuous social streams, which are key sources of fresh and valuable information. Nevertheless, social streams are extremely overwhelming for their huge volumes and high velocities. It is impractical for users to consume social data in its raw form. Therefore, social search [7?9, 17, 19, 28, 33, 37, 39] has become the primary approach to facilitating users on finding their interested content from massive social streams. Existing search methods for social data can be categorized into keyword-based approaches and topic-based approaches based on how they measure the relevance between queries and elements. Keyword-based approaches [7?9, 17, 28, 33, 37] adopt the textual relevance (e.g., TF-IDF and BM25) for evaluation. However, they merely capture the syntactic correlation but ignore the semantic correlation. Considering the tweets in Figure 1, if a query ""soccer"" is issued, no results will be found because none of the tweets contains the term ""soccer"". It is noted that the words like ""as- roma"" and ""LFC"" are semantically relevant to ""soccer"". Therefore, elements such as e1, e2 are relevant to the query but missing from the result. Thus, overlooking the semantic meanings of user queries may degrade the result quality, especially against social data where lexical variation is prevalent [14]. To overcome this issue, topic-based approaches [19, 39] project user queries and elements into the same latent space defined by a probabilistic topic model [5]. Consequently, queries and elements are both represented as vectors and their relevance is computed by similarity measures for vectors (e.g., cosine distance) in the topic space. Although topic-based approaches can better capture the semantic correlation between queries and elements, they fo- cus on the relevance of results but neglect the representativeness. Typically, they retrieve top-k elements that are the most coherent with the query as the result. Such results may not be represen- tative in the sense of information coverage and social influence. First, users are more satisfied with the results that achieve an extensive coverage of information on query topics than the ones that provide limited information. For example, a top-2 query on topic 1 in Figure 2 returns {e3, e4} as the result. Nevertheless, compared with e4, e6 can provide richer information to comple- ment the news reported by e3. Therefore, in addition to relevance, it is essential to consider information coverage to improve the result quality. Second, influence is another key characteristic to measure the representativeness of social data. Existing methods for social search [7, 8, 19, 37] have taken into account the in- fluences of elements for scoring and ranking. These methods simply use the influences of authors (e.g., PageRank [24] scores) or the retweet/share count to compute the influence scores. Such a na?ve integration of influence is topic-unaware and may lead to undesired query results. For example, e6 in Figure 1, which is mostly related to 1, may appear in the result for a query on 2 because of its high retweet count. In addition, they do not consider that the influences of elements evolve over time, when previously trending contents may become outdated and new posts continuously emerge. Hence, incorporating a topic-aware and time-critical influence metric is imperative to capture recently trending elements. To tackle the problems of existing search methods, we define a novel Semantic and Influence aware k-Representative (k-SIR) query for social streams based on topic modeling [5]. Specifi- cally, a k-SIR query retrieves a set of k elements from the active elements corresponding to the sliding windowWt at the query time t . The result set collectively achieves the maximum repres- entativeness score w.r.t. the query vector x, each dimension of which indicates the degree of interest on a topic. We advocate the representativeness score of an element set to be a weighted sum of its semantic and influence scores on each topic. We adopt a weighted word coverage model to compute the semantic score so as to achieve the best information preservation, where the weight of a word is evaluated based on its information entropy [31, 42]. The influence score is computed by a probabilistic coveragemodel where the influence probabilities are topic-aware. In addition, we restrict the influences within the sliding windowWt so that the recently trending elements can be selected. The challenges of real-timek-SIR processing are two-fold. First, the k-SIR query is NP-hard. Second, it is highly dynamic, i.e., the results vary with query vectors and evolve quickly over time. Due to the submodularity of the scoring function, existing submodular maximization algorithms, e.g., CELF [16] and SieveStreaming [2], can provide approximation results for k-SIR queries with theoret- ical guarantees. However, existing algorithms need to evaluate all active elements at least once for a single query and often take several seconds to process one k-SIR query as shown in our experiments. To support real-time k-SIR processing over social streams, we maintain the ranked lists to sort the active elements on each topic by topic-wise representativeness score. We first devise theMulti-Topic ThresholdStream (MTTS) algorithm for k-SIR processing. Specifically, to prune unnecessary evalu- ations, MTTS sequentially retrieves elements from the ranked lists in decreasing order of their scores w.r.t. the query vector and can be terminated early whenever possible. Theoretically, it provides ( 12 ?)-approximation results for k-SIR queries and eval- uates each active element at most once. Furthermore, we propose the Multi-Topic ThresholdDescend (MTTD) algorithm to im- prove upon MTTS. MTTD maintains the elements retrieved from ranked lists in a buffer and permits to evaluate an element more than once to improve the result quality. Consequently, it achieves a better (1 ? 1e ? )-approximation but has a higher worst-case time complexity than MTTS. Despite this, MTTD shows better empirical efficiency and result quality than those of MTTS. Finally, we conduct extensive experiments on three real-world datasets to evaluate the effectiveness of k-SIR as well as the effi- ciency and scalability of MTTS and MTTD. The results of a user study and quantitative analysis demonstrate that k-SIR achieves significant improvements over existing methods in terms of in- formation coverage and social influence. In addition, MTTS and MTTD achieve up to 124x and 390x speedups over the baselines for k-SIR processing with at most 5% and 1% losses in quality. Our contributions in this work are summarized as follows. ? We define the k-SIR query to retrieve representative ele- ments over social streams where both semantic and influ- ence scores are considered. (Section 3) ? We propose MTTS and MTTD to process k-SIR queries in real-time with theoretical guarantees. (Section 4) ? We conduct extensive experiments to demonstrate the effectiveness of k-SIR as well as the efficiency and scalability of our proposed algorithms for k-SIR processing. (Section 5)",Yanhao Wang,National University of Singapore,yanhao90@comp.nus.edu.sg,Yuchen Li,Singapore Management University,yuchenli@smu.edu.sg,Kian-Lee Tan,National University of Singapore,tankl@comp.nus.edu.sg,,,,,,,,,,,,,,,,,,,,,
20200114,1398,Zhuhua Cai,"Rice University Houston, TX, 77251",caizhua@gmail.com,,A Comparison of Platforms for Implementing and Running Very Large Scale Machine Learning Algorithms,"A Comparison of Platforms for Implementing and Running Very Large Scale Machine Learning Algorithms, A Comparison of Platforms for Implementing and Running Very Large Scale Machine Learning Algorithms, A Comparison of Platforms for Implementing and Running Very Large Scale Machine Learning Algorithms, A Comparison of Platforms for Implementing and Running Very Large Scale Machine Learning Algorithms, A Comparison of Platforms for Implementing and Running Very Large Scale Machine Learning Algorithms, ABSTRACT We describe an extensive benchmark of platforms available to a user who wants to run a machine learning (ML) inference algorithm over a very large data set, but cannot find an existing implementation and thus must ""roll her own"" ML code. We have carefully chosen a set of five ML implementation tasks that involve learn- ing relatively complex, hierarchical models. We completed those tasks on four different computational platforms, and using 70,000 hours of Amazon EC2 compute time, we carefully compared run- ning times, tuning requirements, and ease-of-programming of each. 1. INTRODUCTION Many platforms have been proposed to provide programming and runtime support for distributed/parallel machine learning (ML) codes, including OptiML [19], GraphLab [11, 8], SystemML [6], and SimSQL [4]. MLBase [10] and ScalOps [21] also address the problem, though the most recent published descriptions indi- cate that these systems are more immature. Other systems such as Pregel [12], Giraph [2], Spark [24], Ricardo [5], Nyad [14], and DryadLinq [22] may not have been developed only for ML, but count it as an important application. We describe an objective benchmark of some of the platforms available to a user who wants to run a specific ML inference al- gorithm over a large data set, but cannot find an existing imple- mentation and thus must ""roll her own"" ML code. Given the wide variety of ML models, this will not be an uncommon occurrence.1 We draw a distinction between a user who wants to implement and apply a brand new ML code, and someone who just wants to use a code, and focus on the former. The implementor will want to balance ease of implementation with performance, whereas an end 1For example, it is telling that of the five standard Bayesian ML inference algorithms we consider in this study, it appears that only the collapsed LDA inference algorithm [3] is available as part of an ex- isting package, and even then we are aware of no ""non-collapsed"" Gibbs sampler implementation (See Section 8 of the paper). user has little concern for the effort required to engineer the code and will be happy with an intricately constructed C and MPI code as long as it is fast and easy to use.2 Our contributions. Our specific contributions are: (1) We shed some light on the relative merits of some (quite differ- ent) platforms for implementing large-scale ML algorithms. Our results will surprise many readers. (2) Second, we demonstrate (through example) what a scientific study of a platform for writing large-scale ML codes might look like. We have carefully chosen a set of tasks that involve learning relatively complex, hierarchical statistical models. We have pur- posely avoided simple, convex models whose parameters can be optimized using easily-implemented techniques such as gradient descent. Their simplicity means they benefit relatively little from the abstractions provided by the platforms we consider. (3) Finally, we hope that our efforts will grow into a widely used, standard benchmark for this sort of platform. In the future, a implementor of a new or existing platform need only implement these codes and compare with our numbers. ",Zhuhua Cai,"Rice University Houston, TX, 77251",caizhua@gmail.com,Zekai J. Gao,"Rice University Houston, TX, 77251",jacobgao@rice.edu,Shangyu Luo,"Rice University Houston, TX, 77251",lsyurd@gmail.com,Luis L. Perez,"Rice University Houston, TX, 77251",lp6@rice.edu,Zografoula Vagena,"LogicBlox, Inc. Atlanta, GA, 30309",foula@acm.org,Christopher Jermaine,"Rice University Houston, TX, 77251",cmj4@rice.edu,,,,,,,,,,,,
20200115,691,Ahmad Ghazal,"Teradata Corporation  100 N. Sepulveda Blvd  Elsegundo, CA 90245, USA  ahmad.ghazal@ teradata.com    Alain Crolotte  Teradata Corporation  100 N. Sepulveda Blvd  Elsegundo, CA 90245, USA",alain.crolotte@teradata.com,,Dynamic Plan Generation for Parameterized Queries,"Dynamic Plan Generation for Parameterized Queries, Dynamic Plan Generation for Parameterized Queries, Dynamic Plan Generation for Parameterized Queries, Dynamic Plan Generation for Parameterized Queries, Dynamic Plan Generation for Parameterized Queries, ABSTRACT  Query processing in a DBMS typically involves two distinct  phases: compilation, which generates the best plan and its  corresponding execution steps, and execution, which evaluates  these steps against database objects. For some queries,  considerable resource savings can be achieved by skipping the  compilation phase when the same query was previously  submitted and its plan was already cached. In a number of  important applications the same query, called a Parameterized  Query (PQ), is repeatedly submitted in the same basic form but  with different parameter values. PQs are extensively used in  both data update (e.g. batch update programs) and data access  queries. There are tradeoffs associated with caching and re- using query plans such as space utilization and maintenance  cost. Besides, pre-compiled plans may be suboptimal for a  particular execution due to various reasons including data skew  and inability to exploit value-based query transformation like  materialized view rewrite and unsatisfiable predicate  elimination. We address these tradeoffs by distinguishing two  types of plans for PQs: generic and specific plans. Generic plans  are pre-compiled plans that are independent of the actual  parameter values. Prior to execution, parameter values are  plugged in to generic plans. In specific plans, parameter values  are plugged prior to the compilation phase. This paper provides  a practical framework for dynamically deciding between  specific and generic plans for PQ 's based on a mix of rule and  cost based heuristics which are implemented in the Teradata  12.0 DBMS.  Categories and Subject Descriptors  C.0 Computer Systems Organization, GENERAL,  Hardware/software interfaces.  General Terms: Algorithms, Performance, Theory.  Keywords: compilation, dynamic, optimizations.  1. INTRODUCTION  Query processing in a DBMS typically involves two distinct  phases. The first phase, which we call the compilation phase,  checks the syntax, semantics and access rights. This phase also  includes the generation of the execution plan using query rewrite  and cost-based optimization. The resulting execution plan (in  some executable code) is fed to the second phase which executes  the plan against database objects and produces the query result.  The query processing time is the sum of compilation and  execution times.  In order to minimize the overhead of the compilation phase,  many DBMSs cache the execution plan in order to reuse it when  the same query is submitted again. When a fully-specified query  (i.e. non-parameteric query) is submitted again, the compilation  phase can be skipped altogether and the cached plan is retrieved  and passed to the execution phase. However, there are various  scenarios that can render the cached plan non-reusable including  structural changes to the underlying data model such as DDL  definition changes or constraint definition changes such as  CHECK constraints or Referential Integrity changes. A cache can  also be invalidated due to statistics collection. Previous work, e.g.  [2][7][10] has addressed this issue by systematically purging or  repairing compiled plans.  Exact query-match based exploitation of the plan cache is also  not appropriate for parameterized queries (PQs) which contain one  or more parameters or parameter markers that take different values  during each execution. As a result, PQs require a special  infrastructure to exploit the plan cache. One approach to produce a  cacheable plan is to assume a uniform distribution for the columns  being compared with a parameter value and generate the plan (i.e.  optimize the query) using the average (or some  ""typical "")  estimate. In this paper, we refer to a plan thus produced as a  generic plan. This was the approach exclusively used prior to  Teradata 12.0 DBMS.  In addition to allowing us to exploit caching to reduce query  processing overhead, a generic plan is also the optimal plan for  some classes of queries as we will discuss later. However, a  generic plan may be sub-optimal for many kinds of queries. The  reason is that the most efficient execution plan for different  parameter values may be different and the performance  degradation caused by using a generic plan may exceed the  savings from using a cached plan. A plan that is picked as efficient  for a typical (average) value of a parameter may be inefficient for  a particular value if there exists a skew in the data distribution or if  the optimization involves value-based transformations like  materialized view rewrite, unsatisfiable predicate elimination or  data partition elimination. In the presence of data skew, different  values of a parameter give rise to different selectivity estimates for  filtering and join predicates which in turn may cause the optimizer  to pick differing execution plans. Similarly, a query rewrite  optimization performed based on a particular value may not be  correctly used for another value, rendering the resulting plan  inappropriate for caching.   A simple alternative to avoid such sub-optimal plans is not to  use a plan cache and instead perform the compilation phase for  each submission of the query where the parameters are replaced  by a specific value. We refer to such a plan as a specific plan. A  specific plan is generally optimal in terms of run time performance  for the specific parameter values but incurs compilation overhead  for each execution of the query. This overhead is wasteful if the  optimization time is a large portion of the overall processing time  or if the plan is independent of the value of the parameter. Also, if  the query is known to be a short running query, then it is  preferable to cache such queries since by definition the execution  time of such queries will be small and the gain from using a  specific plan is minimal.  This paper presents an adaptive approach that dynamically  decides between a generic and a specific plan for a particular PQ.  A key component of our solution is to capture and factor in the  actual run time characteristics of the DBMS when the query is  executed in order to pick a planning type appropriate for the  workload currently executing in the system. Our solution is based  on a mix of rule and cost based heuristics which are implemented  in the Teradata 12.0 DBMS. The rule based heuristic solution is  based on a characterization of those classes of queries for which a  specific or generic plan can be reliably determined. For other  classes of queries, we devised a heuristic algorithm that depends  on the compilation and execution costs as well as workload  characteristics, particularly expected overall run time and assigned  priority, of queries.  The rest of the paper is organized as follows. Section 2  provides a background discussion including definition of  parameterized queries, implementation approaches of generic and  specific plans along with concrete examples of queries that benefit  from each approach. Section 3 details our adaptive algorithm and  Section 4 presents experimental results. We review related  literature in Section 5 and we conclude in Section 6.",Ahmad Ghazal,"Teradata Corporation  100 N. Sepulveda Blvd  Elsegundo, CA 90245, USA  ahmad.ghazal@ teradata.com    Alain Crolotte  Teradata Corporation  100 N. Sepulveda Blvd  Elsegundo, CA 90245, USA",alain.crolotte@teradata.com,Dawit Seid,"Teradata Corporation  100 N. Sepulveda Blvd  Elsegundo, CA 90245, USA  dawit.seid@ teradata.com    Manjula Koppuravuri  Teradata Corporation  Queens Plaza  Hyderabad, 500 003, India",manjula.koppuravuri@teradata.com,Ramesh Bhashyam,"Teradata Corporation  Queens Plaza  Hyderabad, 500 003, India",bhashyam.ramesh@teradata.com,Alain Crolotte ,"Teradata Corporation 100 N. Sepulveda Blvd Elsegundo, CA 90245, USA ",alain.crolotte@teradata.com ,Manjula Koppuravuri,"Teradata Corporation Queens Plaza Hyderabad, 500 003, India",manjula.koppuravuri@teradata.com,Vinod G,"Teradata Corporation  Queens Plaza  Hyderabad, 500 003, India",vinod.g@teradata.com,,,,,,,,,,,,
20200116,1399,Fei Xu,"University of Florida Gainesville, FL, USA",feixu@cise.ufl.edu,,E = MC3: Managing Uncertain Enterprise Data in a Cluster-Computing Environment,"E = MC3: Managing Uncertain Enterprise Data in a Cluster-Computing Environment, E = MC4: Managing Uncertain Enterprise Data in a Cluster-Computing Environment, E = MC5: Managing Uncertain Enterprise Data in a Cluster-Computing Environment, E = MC6: Managing Uncertain Enterprise Data in a Cluster-Computing Environment, E = MC7: Managing Uncertain Enterprise Data in a Cluster-Computing Environment, ABSTRACT Modern enterprises must manage uncertain data for purposes of risk assessment and decisionmaking under uncertainty. The Monte Carlo approach embodied in the MCDB system of Jampani et al. is well suited for such a task. MCDB can support industrial strength business-intelligence queries over uncertain warehouse data. Moreover, MCDB's extensible approach to specifying uncertainty can also capture complex stochastic prediction models, allowing sophis- ticated ""what-if"" analyses within the DBMS. The MCDB computations can be highly CPU intensive, but offer the potential for massive parallelization. To realize this potential, we provide a new system, called MC3 (Monte Carlo Com- putation on a Cluster), that extends the MCDB approach to the map-reduce processing framework. MC3 can exploit the robustness and scalability of map-reduce, and can han- dle data stored in non-relational formats. We show how MCDB query plans over ""tuple bundles"" can be translated to sequences of map-reduce operations over nested data, and describe different parallelization schemes. We also provide and analyze several novel distributed algorithms for adding pseudorandom number seeds to tuple bundles. These al- gorithms ensure statistical correctness of the Monte-Carlo computations while minimizing the seed length. Our ex- periments show that MC3 can scale well for a variety of workloads. Categories and Subject Descriptors H.2 [Information Systems]: Database Management General Terms Algorithms, Design, Languages, Performance Keywords Uncertain Data, Map-Reduce, Monte Carlo, JSON, JAQL  1. INTRODUCTION There is an increasing need for tools that facilitate en- terprise risk assessment and decision making in the face of uncertain data. The problem of data uncertainty is be- coming acute, due to the increasing use of data integration, automated information extraction, and data anonymization for privacy protection, as well as the growing prevalence of RFID and sensor data. Database researchers have therefore developed a variety of prototype systems [1, 2, 3, 16, 26, 28, 32] for managing uncertain data. Among these prototypes, the Monte Carlo relational Database System (MCDB) [16] seems especially promising for general decision-support ap- plications. MCDB's Monte Carlo approach permits processing of in- dustrial strength Business Intelligence (BI) queries - e.g., complex SQL aggregation queries - over warehouses where the data is uncertain and has complex statistical depen- dencies between attributes and tuples. Perhaps more im- portantly, the MCDB uncertainty model is completely ex- tensible: uncertainty is specified via user-defined Variable Generation (VG) functions, which are used to pseudoran- domly generate realized values for uncertain attributes. As a consequence, the MCDB model subsumes the uncertainty models used in current prototype systems. For example, MCDB can capture discrete models of uncertainty such as the attribute-value and tuple-inclusion uncertainty models used in systems such as MayBMS, Trio, and Mystiq [1, 2, 3]. Moreover, MCDB is well suited to a very important class of scenarios in which uncertainty arises due to the need to extrapolate missing data using probabilistic models, as is often the case with financial, banking, marketing, fraud-detection, and decision-support applications. In this latter setting, MCDB permits sophisticated, data-intensive stochastic modeling and prediction without the need to con- tinually move data back and forth between the DBMS and a statistical or simulation package such as R or ARENA. Thus the user can assess not only the uncertainty in the results of BI queries over uncertain data, but can also ask what-if questions such as ""What will be the mean effect on my prof- its next quarter if I increase my prices by 5%?"" or ""What is the probability that the average value of my New York cus- tomers' portfolios will drop by more than 10% over the next month?"" To achieve the foregoing new functionality without unacceptably increasing processing overhead, MCDB uses a novel processing technique in which a query plan is executed exactly once, but over ""tuple bundles"" rather than ordinary 441 tuples. A tuple bundle represents the values of a tuple over all Monte Carlo replications - equivalently, over all sampled ""possible worlds"" - see Section 2.1. In this paper, we provide a new system, called MC3 (Monte Carlo Computation on a Cluster), that extends the MCDB approach to a map-reduce framework. Our motivation for this work is threefold: ? As the amount of data continues to increase expo- nentially, massively-parallel processing techniques are becoming increasingly important. Especially in more complex settings - see the finance and marketing ex- amples in the following sections - our new uncertainty- handling technology can exacerbate this problem of handling massive data, because MCDB's Monte Carlo computations can be highly CPU-intensive. For wide adoption of the MCDB approach to uncertainty, it is therefore very desirable to provide the MCDB func- tionality on an affordable, massively parallel platform. ? The MCDB prototype incorporates a major rework- ing of the standard relational query-processing engine. Because of the effort required, it is unlikely that this technology will be directly incorporated into commer- cial relational database products; an alternative path to market is needed. ? Since most real-world data is not stored in relational databases, it is important to be able to deal with data in a wide variety of formats. To address the first issue, we note that Monte Carlo com- putations can be performed independently for each tuple bundle, and Monte Carlo replications for a given tuple bun- dle can be executed independently of each other. Thus the MCDB computations have the potential to be massively par- allelized. MC3 realizes this potential: the excellent scalabil- ity and ease of parallel programming made possible by the map-reduce approach are ideal for our purposes. Our MC3 prototype uses Hadoop, an open-source implementation of Google's map-reduce processing framework. Hadoop's map- reduce has been shown to be highly scalable, as demon- strated by Yahoo's recent Daytona Terabyte sort record of 209 seconds using 910 servers.1 Preliminary results from Google (68 seconds using 1000 servers) provide additional evidence.2 Our choice of Hadoop - as well as our choice of Javascript Object Notation (JSON) as the MC3 internal data model - addresses the second issue raised above. The Hadoop infras- tructure and JSON data format are becoming increasingly popular. Appealing features of Hadoop include fault toler- ance and the capability to allocate and reallocate resources (CPU, memory, storage) as needed; this functionality al- lows massive parallelism to be achieved using commodity hardware, further increasing the appeal of the map-reduce approach. Although it appears hard to precisely delineate the class of queries that can be processed by MC3, we believe that it is quite large: MCDB can handle virtually any BI SQL query, it appears that virtually any such query can be rewritten as a directed acyclic graph of operators (i.e., a query plan) that can be processed by MC3. We speculate that many probabilistic XML queries can also be handled by MC3. As a consequence of these considerations, tech- niques for managing uncertain data in this setting have the potential to be widely used. With respect to the third issue, the use of JSON means that MC3 can gracefully deal with data provided in non- relational formats, without any need to reformat the data prior to processing. Indeed, we obtain this functionality ""for free"" from the Hadoop platform. In this paper, we primar- ily deal with JSON data that can be viewed as reformatted probabilistic relational data. However, work on probabilis- tic XML data [19] leads us to believe that extensions to full-fledged probabilistic JSON data should be achievable. Such extensions would then permit MC3 to interact with many recently developed repositories for scaled-out cluster environments, in which data attributes may be multi-valued and records in the same table may differ in their number of attributes [4, 7, 27, 29]. Extending the MCDB functionality to the map-reduce set- ting raises a number of challenging questions. How exactly do we map MCDB's tuple-bundle processing methods to Hadoop and JSON? Must we directly generate a query plan as a sequence of map-reduce operations, or can we facilitate this process via use of a higher level query language? What are the different ways in which MCDB queries can be parallelized, and for which scenarios are these various parallelization schemes effective? A key technical challenge is how to ""seed"" tuple bundles so as to generate streams of the pseudorandom numbers that form the basis of the Monte Carlo computations. For statistical correctness, the streams used by the various tuple bundles must be mutually disjoint. Seeding is challenging because it must be done in a highly parallel and distributed fashion, over an enormous number of tuple bundles, and without requiring storage of too much seeding information in each tuple bundle. Our Contributions. The paper's contributions are as follows: ? We provide the first system for managing uncertain data in a map-reduce environment, showing how to represent MCDB tuple bundles as JSON arrays and how to translate an MCDB query plan to map-reduce. ? We show how query-plan generation can be facilitated by use of JAQL, an open-source language for querying JSON data. ? We identify two MCDB-specific parallelization schemes called inter-tuple and intra-tuple parallelism, show how to implement these schemes using map-reduce, and identify scenarios under which each scheme is effective. ? We develop and analyze an efficient distributed-seed- ing method called SeedSkip that is based on a ran- dom number generator with skip-ahead functionality, as well as a ""fallback"" method called SeedMult that is based on multiple pseudorandom number generators and can be used when SeedSkip does not apply. ? We show, via a set of experiments, that our paralleliza- tion techniques can yield linear scaleup when process- ing uncertain data, and that intra-tuple parallelism can provide linear speedup for certain very expensive VG functions.  We note that other parallel-processing platforms, data for- mats, and query languages can potentially be used to extend MCDB. Our goal was proof-of-concept, and our plat- form choices were partially made as a matter of convenience. We believe, however, that at least some of our techniques, and the lessons learned, are applicable to other possible extensions of the MCDB methodology to forward-looking information-management architectures. Paper Organization. The remainder of the paper is orga- nized as follows. Section 2 gives some background informa- tion. Section 3 gives an overview of how MCDB function- ality is realized using map-reduce, JSON, and JAQL. Sec- tion 4 considers the distributed-seeding problem, and Sec- tion 5 presents our experimental study. We conclude the paper in Section 6. ",Fei Xu,"University of Florida Gainesville, FL, USA",feixu@cise.ufl.edu,Kevin Beyer,"IBM Almaden Research Center San Jose, CA, USA",kbeyer@us.ibm.com,Vuk Ercegovac,"IBM Almaden Research Center San Jose, CA, USA",vercego@us.ibm.com,Peter J. Haas,"IBM Almaden Research Center San Jose, CA, USA",phaas@us.ibm.com,Eugene J. Shekita,"IBM Almaden Research Center San Jose, CA, USA",hekita@us.ibm.com,,,,,,,,,,,,,,,
20200117,1340,Jaroslaw Szlichta,University of Toronto & IBM Toronto Centre for Advanced Studies,szlichta@cs.toronto.edu,,Business-Intelligence Queries with Order Dependencies in DB2,"Business-Intelligence Queries with Order Dependencies in DB2, Business-Intelligence Queries with Order Dependencies in DB3, Business-Intelligence Queries with Order Dependencies in DB4, Business-Intelligence Queries with Order Dependencies in DB5, Business-Intelligence Queries with Order Dependencies in DB6, ABSTRACT Business-intelligence queries often involve SQL functions and al- gebraic expressions. There can be clear semantic relationships between a column's values and the values of a function over that col- umn. A common property is monotonicity: as the column's values ascend, so do the function's values. This we call an order dependency (OD). Queries can be evaluated more efficiently when the query optimizer uses order dependencies. They can be run even faster when the optimizer can also reason over known ODs to infer new ones. Order dependencies can be declared as integrity constraints, and they can be detected automatically for many types of SQL functions and algebraic expressions. We present optimization techniques us- ing ODs for queries that involve join, order by, group by, partition by, and distinct. Essentially, ODs can further exploit interesting orders to eliminate or simplify potentially expensive sorts in the query plan. We evaluate these techniques over our implementation in IBM R? DB2 R? V10 using the TPC-DS R? benchmark schema and some IBM customer inspired queries. Our experimental results demonstrate a significant performance gain. We additionally devise an algorithm for testing logical implication for ODs which is polynomial over the size of the set of given ODs. We show that the inference algorithm which we have implemented in DB2 is sound and complete over sets of ODs over natural domains. This enables the optimizer to infer useful ODs from known ODs. 1. INTRODUCTION 1.1 Motivation As business-intelligence (BI) applications have become more co- mplex and data volumes grow, so have the analytic queries needed to support them. This increasing complexity raises performance issues and numerous challenges for query optimization. Worse, traditional optimization methods often fail to apply when logical subtleties in database schemas and in queries circumvent them. For example, data-warehouse schemas will use surrogate keys, while predicates in business analytic queries will use natural values (such as sale_date = '2010-07-01'). Real world queries will use SQL functions (such as year(d_date)) and algebraic expressions (such as d_date + 30 days). These subtleties cause the optimizer to miss opportunities to use indexes, partition elimination and pipeline operations, and to add potentially expensive operations such as sort even when the data is already sorted appropriately. This is because semantic relation- ships between the functions and expressions the queries use and the data in the database-and between data themselves in the schema, as between surrogate and natural keys-are opaque. If these rela- tionships could be discovered and used, more efficient query plans would result. The relationship on which we focus in this work is order. If the rows of a table were ordered by its date column d_date, they would also necessarily be ordered by d_date + 30 days. Indeed, the function (over d_date) of d_date + 30 days is monotonically increasing with respect to d_date. For this, we say d_date or- ders d_date + 30 days.1 If an index on d_date could be used to provide results ordered by d_date, then the same index would provide the results ordered by d_date + 30 days, since this is the same order. This semantic relationship of order is a type of depen- dency, and we call it an order dependency (OD) [17, 18, 21]. It is akin to the well-known concept of functional dependencies (FDs). (In fact, ODs strictly subsume FDs.) While it will be readily obvious to any reader that d_date orders d_date+ 30 days, this observation is not for free for the optimizer. It would need explicit mechanisms to recognize the dependency. While this particular order dependency rightfully seems trivial, we shall see there are many that are not. Then ""when"" and ""how"" to exploit such dependencies in query planning is far from trivial too. This work is about this aspect of query optimization. Consider then the SQL query in Query 1 over the TPC-DS2 schema. In the schema, date_dim is a dimension table with the primary key d_date_sk with one row per day. (The attribute d_date_sk is a sequential number.) The table has columns d_date, 1In this case, d_date + 30 days orders d_date also. We then say the two are order equivalent. However, ""orders"" is not in- herently symmetric. Consider year(d_date) and d_date. In this case, d_date orders year(d_date), but year(d_date) does not or- der d_date. 2http://www.tpc.org     750 10.5441/002/edbt.2014.81 select D.d_date + 30 days, max(S.ws_ext_sales_price) as most from date_dim D, web_sales S where S.ws_sold_date_sk = D.d_date_sk and D.d_date between date('1998-01-01') and date('2002-01-01') group by D.d_date + 30 days order by D.d_date + 30 days; Query 1: Plus thirty days. d_month, d_quarter, and d_day, and additional columns that qualify the day (such as whether it is the weekend, a holiday, and, if so, the name of the holiday). The table web_sales is a large fact table recording all individual sales, with ws_sold_date_sk as a foreign key referencing date_dim on d_date_sk. Let there be a tree index for date_dim on d_date. The opti- mizer will miss that the index could be used in evaluating Query 1 to accomplish both the group-by and the order-by. How might the query be rewritten manually to resolve this? ? group by d_date + 30 days and order by d_date: This is not legal SQL; the attribute in the order-by is not listed in the group-by (as such). ? group by d_date and order by d_date + 30 days: This is accepted by DB2; derived attributes-functions and algebraic expressions derived over the attributes listed in the group-by (which may include derived attributes itself)-can be used in the select and order-by clauses. However, this does not resolve the inefficiency. The query plan still explicitly sorts to ""satisfy"" the order-by. ? group by d_date and order by d_date: This does work! The index can now be employed to imple- ment the group-by and to satisfy the order-by. Of course, it is not the responsibility of the SQL programmer to write queries painstakingly-or of an automated BI report sys- tem that generates SQL queries in the back-end-in such a way to assure the optimizer will handle it well. This would violate the declarative principle of SQL. Even if we tried to put the onus on programmers to be careful, they cannot be expected to know what is problematic and what is not. While a clever SQL programmer can sometimes skirt such pitfalls by careful composition (as here), more often it is not possible. So, we have to fix it. The optimizer needs to recognize that d_date and d_date + 30 days are seman- tically equivalent for order, thus skipping the superfluous sorting step, regardless of how the query is written. Next, consider Query 2. In SQL, date and time are complex data types. These are central to BI applications, and provide for rich drill down and roll up. In TPC-DS in table date_dim, some of date's hierarchy is materialized in columns: d_year, d_quarter, d_month, and d_day. select D.d_year, D.d_quarter, D.d_month, D.d_day sum(S.ws_sales) as total from date_dim D, web_sales S where S.ws_date_sk = D.d_date_sk and D.d_year between 2001 and 2004 group by D.d_year, D.d_quarter, D.d_month, D.d_day order by D.d_year, D.d_quarter, D.d_month, D.d_day; Query 2: Eliminating quarter. Let there be a tree index for date_dim on d_year, d_month, d_day. Unfortunately, this index would not help in a query plan, even for the group-by: d_quarter intervenes. Note that d_month functionally determines d_quarter. The query's author cannot elim- inate mention of d_quarter in the group-by, however, as it appears in the select. Fortunately, by the work in [16], DB2 can eliminate it internally from the group-by, based on the recognition of the func- tional dependency (FD). The index can then be used to implement the group-by operation. However, this FD, d_month C d_quarter, is not logically suf- ficient likewise to remove d_quarter from the order-by clause. The optimizer must still apply a sort operator to ""satisfy"" the order- by directive. However, because d_month orders d_quarter- which says more than just that d_month functionally determines d_quarter-the attribute d_quarter can be removed from the order- by clause also, to result in a semantically equivalent query.3 In this work, we show how this is accomplished. 1.2 Contributions and Outline In Section 2, we provide background on order dependencies- notational conventions and definitions-as we use in this paper, and considerations that arise in data-warehouse schema design. In Section 3, we address how to use order dependencies in query op- timization. There are two aspects to this: how and where the op- timizer makes use of OD information; and how OD information is discovered. 1. Optimizing with Order Dependencies. (a) Section 3.1 is divided into two sections. In Section 3.1.1, we go into further depth how ODs are used to optimize. (b) In Section 3.1.2 we present two inference algorithms and show where and how they are invoked in the opti- mizer: Reduce Order OD which puts ODs into a canon- ical form for matching against interesting orders; and Homogenize Order OD which discovers equivalent co- lumns, order-wise. We discuss the utility of these algo- rithms. 2. Detecting Order Dependencies. In Section 3.2, we show how ODs between columns and functions over columns (SQL func- tions and algebraic expressions) can be automatically detected by the optimizer. (a) These techniques have been implemented as a prototype within DB2. (b) We present a suite of real-world IBM customer queries over TPC-DS benchmark that illustrate the issues, which are then used in Section 4 for an experimental perfor- mance evaluation. The optimizer automatically infers the associated OD information and uses it to produce the improved query plans. 3. Declaring Order Dependencies. In Section 3.3, we consider how OD information can be declared, and what types of natural ODs occur in today's schemas. (a) Order dependencies can be explicitly declared in our implementation in DB2 as a type of integrity constraint. (b) We demonstrate how ODs between surrogate and nat- ural keys can be used for strong performance improvement [19]. 4. Inferring Order Dependencies. In Section 3.4, we show how the optimizer can infer new ODs from known ODs. The known ODs may not match interesting orders in query plan- ning, while ODs that logically derive from them would. Thus, such an OD-inference facility is ultimately needed to take 3The values for d_quarter are 1, . . . , 4 and for d_month, 1, . . . , 12. 751 Table 1: Notational conventions. ? Relations ? R represents a relation, and r represents a specific re- lation instance (table). ? A, B and C represent attributes. ? s and t represent tuples. ? t A denotes the value of attribute A in tuple t. ? Sets ? calligraphic letters denoted as X , Y , and Z represent sets of attributes. ? Lists ? bold letters represent lists of attributes: X, Y and Z. Note list X could be the empty list, [ ]. ? square brackets denote an explicit list: [A,B,C]. ? [A |T] denotes that A is the head of the list, andT is the tail of the list (the remaining list when the first element is removed). fuller advantage of these techniques. (a) We define a database to be natural if given order prop- erty over its attributes can be guaranteed. (All real- world domains we have encountered have this property, and thus are natural.) (b) We discuss a general, efficient (polynomial) inference procedure which we have implemented which is sound and complete over natural domains. In Section 4, we present results of a performance study over queries over TPC-DS. 5. Experimental Results. All nine of the test queries show a significant performance gain using the OD-extended optimizer, with an average 30% time improvement over a ten-GB database. In Section 5, we discuss related work, both previous applied work that used dependencies in optimization (upon which we build), and theoretical work on order dependencies which has provided critical foundations for our current implementation. In Section 6, we outline next steps for this work, and conclude.",Jaroslaw Szlichta,University of Toronto & IBM Toronto Centre for Advanced Studies,szlichta@cs.toronto.edu,Parke Godfrey,York University in Toronto & IBM Toronto Centre for Advanced Studies,godfrey@cse.yorku.ca,Jarek Gryz,York University in Toronto & IBM Toronto Centre for Advanced Studies,jarek@cse.yorku.ca,Wenbin Ma,IBM Toronto Laboratory,wenbinm@ca.ibm.com,Weinan Qiu,IBM Toronto Laboratory,davidqiu@ca.ibm.com,Calisto Zuzarte,IBM Toronto Laboratory,calisto@ca.ibm.com,,,,,,,,,,,,
20200118,250,James Wagner,"DePaul University Chicago, Illinois",jwagne32@depaul.edu,,Detecting Database File Tampering through Page Carving,"Detecting Database File Tampering through Page Carving, Detecting Database File Tampering through Page Carving, Detecting Database File Tampering through Page Carving,Detecting Database File Tampering through Page Carving, Detecting Database File Tampering through Page Carving,  ABSTRACT Database Management Systems (DBMSes) secure data against regular users through defensive mechanisms such as access con- trol, and against privileged users with detection mechanisms such as audit logging. Interestingly, these security mechanisms are built into the DBMS and are thus only useful for monitoring or stopping operations that are executed through the DBMS API. Any access that involves directly modifying database files (at file system level) would, by definition, bypass any and all security layers built into the DBMS itself. In this paper, we propose and evaluate an approach that detects direct modifications to database files that have already bypassed the DBMS and its internal security mechanisms. Our approach applies forensic analysis to first validate database indexes and then compares index state with data in the DBMS tables. We show that indexes are much more difficult to modify and can be further fortified with hashing. Our approach supports most relational DBMSes by leveraging index structures that are already built into the system to detect database storage tampering that would currently remain undetectable. 1 INTRODUCTION DBMSes use a combination of defense and detection mechanisms to secure access to data. Defense mechanisms, such as access control, determine the data granularity and system access granted to different database users; defense mechanisms, such as audit logging, monitor all database activity. Regardless of the defense mechanisms, security breaches are still a legitimate concern ? sometimes due to unintentional granting of extra access control and sometimes due to outright hacking, such as SQL injection. Security breaches are typically detected through analysis of audit logs. However, audit log analysis is unreliable to detect a breach that originated from privileged users. Privileged users, by definition, already have the ability to control and modify access permissions. Therefore, audit logs fundamentally cannot be trusted to detect suspicious activity. Additionally, privileged users commonly have access to database files. Consider a system administrator who maliciously, acting as the root, edits a DBMS data file in a Hex editor or through a programming language, such as Python. The DBMS, unaware of external file write activity taking place outside its own pro- grammatic access, cannot log it, and thus the tampering attack remains undetected. Current DBMSes do not provide tools against insider threats ? in general, a built-in security mechanism is vulnerable to in- sider attacks. While a DBMS will not be able to detect direct storage changes, file-level modifications potentially create incon- sistencies within the auxiliary data structures maintained by a DBMS. Forensics tools that examine file contents can be used to detect such inconsistencies, and determine if insider threats have taken place. Recently we proposed the first database foren- sic tool, DBCarver, that can be used to detect deleted data from database pages [31]. However, database forensic tools such as DBCarver merely extract forensic artifacts but do not search for inconsistencies within the data structures maintained by a DBMS. In this paper, we propose a system, DBStorageAuditor, that detects database file tampering by identifying inconsistencies in storage through a direct inspection of internal database struc- tures. DBStorageAuditor utilizes existing database forensic tech- niques and expands them to extract additional necessary storage artifacts. These artifacts are then used to detect inconsistencies within indexes and between indexes and tables. The underlying premise of our approach is that all relational databases follow patterns in storage over which the privileged user has little or no control. We inspect these storage patterns to detect unusual activity. We motivate DBStorageAuditor through an example: Example 1. Malice is the system administrator for a shipping company, FriendlyShipping. Malice is bribed by a competing com- pany to interfere with the orders going to Seattle. Malice does not have access to the DBMS, but she does have access to the server where the database files reside. Malice writes a Python script that will open and directly modify the database file containing the Orders table. The script then opens the database file, finds all records containing the string  'Seattle ', and explicitly overwrites entire records with the NULL ASCII character (decimal value 0). Figure 1 illustrates the result of Malice 's script actions. Since the record was erased without the DBMS (API has never seen that command) all DBMS security was bypassed, and the operation was never recorded in the log file. When FriendlyShipping investigates the missing Seattle orders, the audit log can only explain deleted orders for (2, Chair, New York) and (6, Chair, Detroit). The audit logs contain no trace of the Seattle order being deleted because it was not deleted but rather wiped out externally. To simplify in the above example, we have omitted some details of database file tampering, which we expand on later in Section 5. Barring those details in Example 1, the value in the City index still exists in index storage even though the entire record is erased. Therefore, an inconsistency can be identified by mapping back the index value to the empty gap in table storage. The empty gap in table storage exists because a database only marks a record when it is deleted, and only overwrites the record with data from a newly inserted record. However, making the mapping from the index value to the associated record must be based on the behavioral rules of database storage, such as page and record layout. We use database forensic tools to understand database layout, and using that layout, perform the necessary mapping. It is not impossible for a scrupulous system administrator to (i) tamper with the index and create a cascade of inconsistencies throughout the index structure, or (ii) for an attacker who has privileges to modify database files to acquire privileges to sus- pend or kill logging mechanisms at the operating system level if necessary, or (iii) for a knowledgeable adversary to easily avoid corrupting storage and keep checksum values consistent. How- ever, in spite of increased level of threat, we repeatedly show that accurate knowledge about data layout can be used to gather evidence and prove if any malicious activity has taken place. Previously we developed an approach to detect malicious ac- tivity when DBMS logging is disabled [28]. In this approach we analyzed unlogged activity (executed through a proper DBMS API) but strictly assumed that database files were not exposed to tampering. In this paper, we address the tampering vulnerability where the database files are physically altered. Developing an auditing system for DBMSes is part of our larger goal to open up the database system and its storage to users, for performance and forensics investigation. The rest of the paper is organized as follows: Section 2 cov- ers related work. Section 3 discusses concepts of database stor- age used throughout the paper. Section 4 defines the adver- sary we seek to defend against. Section 5 details how to per- form database file tampering. Section 6 provides an overview of DBStorageAuditor. Section 7 describes how we utilize data- base forensics. Section 8 addresses index tampering. Section 9 proposes a method to organize carved index output making our system scalable. Section 10 discusses how to detect file tampering using inconsistencies between carved index data and table data. Section 11 provides a thorough evaluation of our system.",James Wagner,"DePaul University Chicago, Illinois",jwagne32@depaul.edu,Alexander Rasin,"DePaul University Chicago, Illinois",arasin@depaul.edu,Karen Heart,"DePaul University Chicago, Illinois",kheart@depaul.edu,Tanu Malik,"DePaul University Chicago, Illinois",tmalik1@depaul.edu,Jacob Furst,"DePaul University Chicago, Illinois",jfurst@depaul.edu,Jonathan Grier,"GrierForensics Pikesville, Maryland ",jdgrier@grierforensics.com,,,,,,,,,,,,
20200119,1126,Orestis Polychroniou,Columbia University,orestis@cs.columbia.edu,,Track Join: Distributed Joins with Minimal Network Traffic,"Track Join: Distributed Joins with Minimal Network Traffic, Track Join: Distributed Joins with Minimal Network Traffic, Track Join: Distributed Joins with Minimal Network Traffic, Track Join: Distributed Joins with Minimal Network Traffic, Track Join: Distributed Joins with Minimal Network Traffic, ABSTRACT Network communication is the slowest component of many operators in distributed parallel databases deployed for large- scale analytics. Whereas considerable work has focused on speeding up databases on modern hardware, communica- tion reduction has received less attention. Existing parallel DBMSs rely on algorithms designed for disks with minor modifications for networks. A more complicated algorithm may burden the CPUs, but could avoid redundant transfers of tuples across the network. We introduce track join, a novel distributed join algorithm that minimizes network traffic by generating an optimal transfer schedule for each distinct join key. Track join extends the trade-off options between CPU and network. Our evaluation based on real and synthetic data shows that track join adapts to diverse cases and degrees of locality. Considering both network traffic and execution time, even with no locality, track join outperforms hash join on the most expensive queries of real workloads. 1. INTRODUCTION The processing power and storage capacity of a single machine can be large enough to fit small to medium scale databases. Nowadays, servers with memory capacity of more than a terabyte are common. Packing a few multi-core CPUs on top of shared non-uniform access (NUMA) RAM provides substantial parallelism, where we can run database opera- tions (i.e. sort, join, and group-by) on RAM-resident data at rates of a few gigabytes per second [2, 3, 29, 34, 36]. Database research has also evolved to catch up to the hardware advances. Fundamental design rules of the past on how a DBMS should operate are now being revised due to their inability to scale and achieve good performance on modern hardware. Special purpose databases are now popu- lar against the one-size-fits-all approach [32], while accelera- tors [26] are the implicit manifestation of the same concept. ?Work partly done when author was at the Oracle Labs. The advances in database design for storage and execution on modern hardware have not been met by similar advances in distributed parallel database design. When the most fun- damental work on distributed and parallel databases was published [5, 11, 20], hardware advances of today like multi- core parallelism had not yet occurred. Techniques to speed up short-lived distributed transactions [32] target distributed commit protocols, which suffer from network latencies rather than throughput. Queries where communication is inevitable are less popular research topics or are left for data-centric generic distributed systems for batch-processing [7, 24]. The latest network technologies may be slow relative to main-memory-resident processing. A 40 Gbps InfiniBand measured less than 3 GB/s real data rate per node dur- ing hash partitioning. If done in RAM, partitioning to a few thousand outputs runs close to the memory copy bandwidth [29, 34]. For instance, a server using 4X 8-core CPUs and 1333 MHz quad-channel DDR3 DRAM achieves a partition rate of 30?35 GB/s, more than an order of magnitude higher than the InfiniBand network. Recent work [3] achieves a hash join rate of 4.85 GB/s of 32-bit key, 32-bit payload tu- ples on 4X 8-core CPUs. Such high-end hardware is common in marketed configurations for large-scale analytics. Network optimization is important for both low-end and high-end hardware. In low-end platforms where the network is relatively slow compared to local in-memory processing, we expect the execution time to be dominated by network transfers. Thus, any network traffic reduction directly trans- lates to faster execution. In high-end platforms, given that the network still cannot be as fast as the RAM bandwidth, completion times are also reduced if the reduction in net- work traffic is comparable with the increase in CPU cycles. In order to show how much time databases can spend on the network, we give an example of a real analytical work- load from a large commercial vendor, using a market-leading commercial DBMS. Using 8 machines connected through 40 Gbps InfiniBand (see Section 4 for more details of the con- figuration), we found that the five most expensive queries spend ? 65?70% of their time transferring tuples on the network and account for 14.7% of the total time required to execute the entire analytical workload with more than 1500 queries. All five queries have a non-trivial query plan (4?6 joins), but spend 23%, 31%, 30%, 42%, and 43% of their total execution time on a single distributed hash join. A sophisticated DBMS should have available options to optimize the trade-off between network and CPU utiliza- tion. One solution would be to apply network optimiza- tion at a higher level treating the network as a less desired 1483 route for data transfers, without modifying the underlying algorithms. These approaches are common in generic dis- tributed processing systems [7]. A second solution would be to employ data compression before sending data over the network. This solution is orthogonal to any algorithm but can consume a lot of CPU resources without always yielding substantial compression. A third solution is to create novel algorithms for database operator evaluation that minimize network communication by incurring local processing cost. This approach is orthogonal and compatible with compres- sion and other higher level network transfer optimizations. Grace hash join [9, 17] (throughout the paper we will use the term hash join to refer to Grace hash join on network [9], rather than disk [17]) is the predominant method for executing distributed joins and uses hash partitioning to split the initial problem into shared-nothing sub-problems that can proceed locally per node. Partitioning both tables works almost independently of the table sizes. However, hash join is far from network-optimal because it transfers almost the full size of both tables over the network. Using pre-determined hash functions guarantees load balancing, but limits the probability that a hashed tuple will not be transferred over the network to 1/N on N nodes. We introduce track join, a novel algorithm for distributed joins that minimizes transfers of tuples across the network. The main idea of track join is to decide where to send rows on a key by key basis. The decision uses information about where records of the given key are located. Track join has the following properties: it (i) is orthogonal to data-centric compression, (ii) can co-exist with semi-join optimizations, (iii) does not rely on favorable schema properties, such as foreign key joins, (iv) is compatible with both row-store and column-store organization, and (v) does not assume favor- able pre-existing tuple placement. We implement track join to evaluate the most expensive join operator in the most ex- pensive queries of real workloads. We found that track join reduces the network traffic significantly over known meth- ods, even if pre-existing data locality is removed and all data are used in optimally compressed form throughout the join. Section 2 describes the track join algorithm presenting three variants starting from the simplest. In Section 3, we discuss costs for query optimization, tracking-aware hash joins, and semi-join filtering. Section 4 presents our exper- imental evaluation using both synthetic datasets and real workloads. In Section 5 we briefly discuss future work. In Section 6 we discuss related work and conclude in Section 7.",Orestis Polychroniou,Columbia University,orestis@cs.columbia.edu,Rajkumar Sen,Oracle Labs,rajkumar.sen@oracle.com,Kenneth A. Ross,Columbia University,kar@cs.columbia.edu,,,,,,,,,,,,,,,,,,,,,
20200120,1341,Kristi Morton,"Computer Science and Engineering Department, University of Washington Seattle, Washington, USA",kmorton@cs.washington.edu,,ParaTimer: A Progress Indicator for MapReduce DAGs,"ParaTimer: A Progress Indicator for MapReduce DAGs, ParaTimer: A Progress Indicator for MapReduce DAGs, ParaTimer: A Progress Indicator for MapReduce DAGs, ParaTimer: A Progress Indicator for MapReduce DAGs, ParaTimer: A Progress Indicator for MapReduce DAGs, ABSTRACT Time-oriented progress estimation for parallel queries is a challenging problem that has received only limited attention. In this paper, we present ParaTimer, a new type of timeremaining indicator for parallel queries. Several parallel data processing systems exist. ParaTimer targets environ- ments where declarative queries are translated into ensembles of MapReduce jobs. ParaTimer builds on previous tech- niques and makes two key contributions. First, it estimates the progress of queries that translate into directed acyclic graphs of MapReduce jobs, where jobs on different paths can execute concurrently (unlike prior work that looked at sequences only). For such queries, we use a new type of critical-path-based progress-estimation approach. Second, ParaTimer handles a variety of real systems challenges such as failures and data skew. To handle unexpected changes in query execution times due to runtime condition changes, ParaTimer provides users with not only one but with a set of time-remaining estimates, each one corresponding to a different carefully selected scenario. We implement our es- timator in the Pig system and demonstrate its performance on experiments running on a real, small-scale cluster. Categories and Subject Descriptors H.2.4 [Database Management]: Systems aparallel databases General Terms Algorithms, Design, Experimentation 1. INTRODUCTION Whether in industry or in the sciences, users today need to store, archive, and most importantly analyze increasingly large datasets. For example, the upcoming Large Synoptic Survey Telescope [17] is predicted to generate on the order of 30 TB of data every day. Parallel database management systems [1, 11, 14, 27, 29] and other parallel data processing platforms [6, 8, 12, 15] are designed to process such massive-scale datasets: they enable users to submit declarative queries over the data and they execute these queries in clusters of shared-nothing servers. Although parallelism speeds up query execution, query times in these shared-nothing platforms can still ex- hibit large intra-query and inter-query variance. In such an environment, accurate, time-remaining progress estimation for queries can be helpful both for users and for the system. Indeed, the latter can use timeremaining information to improve resource allocation [30], enable query debugging, or tune the cluster configuration (such as in response to unexpected query runtimes). Accurate progress estimation for parallel queries is a challenging problem because, in addition to the challenges shared with single-site progress estimators [3, 2, 19, 18, 21, 22], parallel environments introduce distribution, concur- rency, failures, data skew, and other issues that must be accounted for. This difficult problem has received only lim- ited attention. Our preliminary prior work [23], which we called Parallax, provided accurate estimates, but only for the limited class of parallel queries that translated into se- quences of MapReduce jobs. We also previously assumed uniform data distribution and the absence of node failures, two assumptions that are unreasonable in practice. To address these limitations, we have developed Para- Timer, a time-remaining indicator for a much broader class of queries and runtime conditions. Many parallel pro- cessing systems exist. Similar to Parallax, we developed ParaTimer for Pig queries [24] running in a Hadoop clus- ter [12], an environment that is a popular open-source paral- lel data-processing engine under active development. Within this context, ParaTimer builds on previous techniques and makes two key contributions. First, ParaTimer estimates the progress of parallel queries expressed as Pig scripts that translate into directed acyclic graphs (DAGs) of MapReduce jobs where jobs on different branches of the DAG can execute concurrently. DAGs require a radically different approach than our prior work for sequences of jobs. As a direct re- sult, unlike Parallax, ParaTimer can handle, for example, Pig scripts with join operators. Second, ParaTimer includes techniques for handling sev- eral real system challenges including failures and data skew. To handle unexpected changes in query execution times such as those due to failures, ParaTimer provides users with a set of time-remaining estimates that correspond to the predicted query execution times in different scenarios (i.e., a 507 single worst-case failure, or data skew at an operator). We call ParaTimer a comprehensive indicator because it provides this set of estimates instead of a single best guess as the other indicators do. Each of ParaTimer 's indicators can be annotated with the scenario that it corresponds to, giving users a detailed picture of possible expected behaviors. While many of the ideas presented in this paper could be adapted to other parallel data processing systems, the Pig/Hadoop environment poses several unique challenges that have informed our design and shaped our implemen- tation. Most notable, a MapReduce-style scheduler requires intermediate result materialization, schedules small pieces of work at a time, and restarts small query fragments when failures occur (rather than restarting entire queries). All three properties affect query progress and its estimates. ParaTimer is designed to be accurate while remaining sim- ple and addressing the above challenges. At a high level, ParaTimer works as follows. For basic progress estima- tion, ParaTimer builds on our prior system Parallax [23]. Parallax estimates time-remaining by breaking queries into pipelines. It estimates time-remaining for each pipeline by considering the work to be done and the speed at which that work will be performed, taking (time-varying) parallelism into account. To get processing speeds, Parallax relies on earlier debug runs of the same query on input data samples generated by the user. To support Pig scripts that translate into MapReduce DAGs where multiple jobs may execute concurrently (such as scripts with join operators), ParaTimer includes a method to identify the critical path in a query plan. It then estimates progress along that path, effectively ignoring other paths. ParaTimer also provides support for a variety of practical challenges, including failures and data skew. For data skew that can be predicted and planned for, ParaTimer takes it into account upfront. For failures and data skew that are not planned, ParaTimer outputs a set of estimates, rather than a single ""best guess, "" that bound the expected query execution time within given possible variations in runtime conditions. An interesting side-benefit of this approach is that when a query time goes outside ParaTimer 's initial bounds, a user knows that there is a problem with either his query or the cluster. ParaTimer 's output can thus aid in performance debugging. Today, parallel systems are being deployed at all scales and each scale raises new challenges. In this paper, we focus on smaller-scale systems with tens of servers because many consumers of parallel data management engines today run at this scale.1 We evaluate ParaTimer 's performance through experiments on an eight-node cluster (set to a maximum degree of parallelism of 32 divided into 16 maps and 16 re- duces). We compare ParaTimer 's performance against Par- allax [23], other state-of-the-art single-node progress indica- tors from the literature [3, 19], and Pig 's current progress indicator [25]. We show that ParaTimer is more accurate than all these alternatives on a variety of types of queries and system configurations. For all queries that we evalu- ated, ParaTimer 's average accuracy is within 5% of an ideal indicator, when given accurate cardinality estimates. The rest of this paper is organized as follows. The next section provides background on MapReduce, Hadoop, and our prior work. Section 3 presents ParaTimer 's approach and key algorithms. Section 4 presents empirical results. Section 5 discusses related work. Section 6 concludes.",Kristi Morton,"Computer Science and Engineering Department, University of Washington Seattle, Washington, USA",kmorton@cs.washington.edu,Magdalena Balazinska,"Computer Science and Engineering Department, University of Washington Seattle, Washington, USA",magda@cs.washington.edu,Dan Grossman,"Computer Science and Engineering Department, University of Washington Seattle, Washington, USA",djg@cs.washington.edu,,,,,,,,,,,,,,,,,,,,,
20200121,1196,Sai Wu,"College of Computer Science and Technology, Zhejiang University, Hangzhou, China",wusai@zju.edu.cn,,PABIRS: A Data Access Middleware for Distributed File Systems,"PABIRS: A Data Access Middleware for Distributed File Systems, PABIRS: A Data Access Middleware for Distributed File Systems, PABIRS: A Data Access Middleware for Distributed File Systems, PABIRS: A Data Access Middleware for Distributed File Systems, PABIRS: A Data Access Middleware for Distributed File Systems, Abstract aVarious big data management systems have emerged to handle different types of applications, which cast very different demands on storage, indexing and retrieval of large amount of data on distributed file system. Such diversity on de- mands has raised huge challenges to the design of new generation of data access service for big data. In this paper, we present PABIRS, a unified data access middleware to support mixed workloads. PABIRS encapsulates the underlying distributed file system (DFS) and provides a unified access interface to systems such as MapReduce and key-value stores. PABIRS achieves dramatic improvement on efficiency by employing a novel hybrid indexing scheme. Based on the data distribution, the indexing scheme adaptively builds bitmap index and Log Structured Merge Tree (LSM) index. Moreover, PABIRS distributes the computation to multiple index nodes and utilizes a Pregel-based algorithm to facilitate parallel data search and retrieval. We empirically evaluate PABIRS against other existing distributed data processing systems and verify the huge advantages of PABIRS on shorter response time, higher throughput and better scalability, over big data with real-life phone logs and TPC-H benchmark. I. INTRODUCTION The explosive growth of big data and the emergence of cloud computing have fueled the quick advances of distributed processing techniques to support storage, retrieval and analysis on massive data. Such quickly growing data bring new challenges to traditional query processing systems, especially when various types of operations are essentially required. The calling logs from billion mobile phone users, for example, are flooding into data centers of telecommunication companies, waiting for querying as well as analysis by the users and decision makers. From time to time, users submit call transaction search queries to retrieve hundreds of their call logs in last few months from billions of available call records, and the information managers are interested in analyzing the user behavior by aggregating the call logs based on the locations, call times and other attributes. The data processing system and the underlying data storage layer are expected to handle the mixed workloads of such high- selective queries and analytic queries in an efficient manner. It turns out that data access is the major bottleneck of the data processing architecture for the mixed workloads. As new data are coming into the system at extremely fast rate, complicated pre-processing techniques dramatically degrade the throughput of data insertion. It is more effective by pushing the data into the distributed file system in a natural way, e.g., simply sorting on the arrival order. Instead of complex optimization before the physical insertion of records into distributed file system, it is more promising to redesign the data access architecture, with new lightweight index over the fast growing data storage layer and well calibrated optimizations based on the in-depth analysis on the characteristics of the data distribution and the workloads. Obviously, such new design of the data access layer heavily depends on the distribution of big and diverse data in the real world. In Figure 1 and Figure 2, we present the statistics of call logs of 1,000 randomly chosen mobile numbers in a distributed file system from a world-class telecommunication company. Each 4MB data block contains call records with sorted in- sertion timestamps, to which new records are continuously appended whenever a new call is made by a mobile phone user. In Figure 1, we report the size of call logs in terms of each caller id, implying that most of the caller ids make a few calls only, while less than 1% of the ids contribute much more call records than the others. This observation follows the well known power law phenomenon commonly appearing in physical world. It results in huge variance on the number of data blocks associated to each caller id, as is shown in Figure 2. The records generated by the top 1% hot caller ids span on almost every block of the distributed file system, potentially incurring high overhead for queries retrieving records. It also brings challenges to the design of the index mechanism on top of the storage layer, when data processing engine tries to locate the sparse records. To fully address these problems, we propose PABIRS, a generic data access middleware for mixed workloads. PABIRS works as an interface between the underlying Distributed File System (DFS) and data management systems, e.g. Hadoop and HBase. It employs a novel hybrid indexing scheme to support efficient data retrieval for various query workloads. Specifically, a cost model component is elicited at the central position in PABIRS, which measures the overhead of different access methods and selects the most effective one. More- over, PABIRS generally supports existing main-stream data processing systems on top, allowing programmers to easily add PABIRS as a middleware layer with minimal efforts. In some sense, PABIRS shares the same design philosophy of the NoDB [2] strategy. However, PABIRS is designed as a middleware of the DFS which is more flexible and can potentially work with any applications using DFS as their storage systems. The most distinguishing feature of PABIRS is its hybrid indexing architecture. As different types of indices were originally designed for different workloads, it is crucial for the cost estimator to understand the workload and facilitate accurate index selection and tuning. For data following the uniform distribution, for example, PABIRS utilizes bitmap index to support fast retrieval, since bitmap index is well known for its high efficiency on sparse record indexing and retrieval. However, bitmap index is incapable of handling skewed distribution with hot keys. LSM index, with a forest of B-trees, is much more superior when records with identical keys frequently appearing in the data. PABIRS applies the dual deployment strategy with both index types, and adaptively chooses an exclusive group of keys for LSM under a unified optimization framework based on the cost evaluation. In its internal architecture, PABIRS disseminates the pro- cessing logics to all distributed file system nodes, where a Pregel engine [17] is run to synchronize the operation units on all the nodes. In particular, the index structure is considered as a graph consisting of multiple levels of vertices and data access service is modeled as a vertex-centric graphical program. The vertices are partitioned among the physical nodes and the search is conducted in parallel. This design makes the index layer of PABIRS a more flexible and extensible component for efficient parallel processing and load balancing. We deploy PABIRS on top of the HDFS [26] and evaluate its performance using the real mobile phone call log data and the well-known TPC-H benchmark. PABIRS presents huge margin of advantages on performance for different work- loads of transaction-based and analytic-based workloads. The remainder of the paper is organized as follows. Section II presents the overview of PABIRS and its architecture. Section III introduces how the hybrid indexing approach works. Section IV discusses the optimization strategies in PABIRS and Section V shows the experiment results. In Section VI, we briefly review previous work and the paper is concluded in Section 7.",Sai Wu,"College of Computer Science and Technology, Zhejiang University, Hangzhou, China",wusai@zju.edu.cn,Gang Chen,"College of Computer Science and Technology, Zhejiang University, Hangzhou, China",cg@zju.edu.cn,Xianke Zhou,"NetEase (Hangzhou) Network Co., Ltd., Hangzhou, China",hzzhouxianke@corp.netease.com,Zhenjie Zhang,"Advanced Digital Sciences Center, Illinois at Singapore Pte. Ltd., Singapore",zhenjie@adsc.com.sg,Anthony K. H. Tung,"School of Computing, National University of Singapore, Singapore",atung@comp.nus.edu.sg,Marianne Winslett,"Deparement of Computer Science, University of Illinois at Urbana-Champaign, USA",winslett@illinois.edu,,,,,,,,,,,,
20200122,1342,Paolo Bellavista,"Universit? di Bologna, Department of Computer Science and Engineering, Bologna, Italy",paolo.bellavista@unibo.it,,Adaptive Fault-Tolerance for Dynamic Resource Provisioning in Distributed Stream Processing Systems,"Adaptive Fault-Tolerance for Dynamic Resource Provisioning in Distributed Stream Processing Systems, Adaptive Fault-Tolerance for Dynamic Resource Provisioning in Distributed Stream Processing Systems, Adaptive Fault-Tolerance for Dynamic Resource Provisioning in Distributed Stream Processing Systems, Adaptive Fault-Tolerance for Dynamic Resource Provisioning in Distributed Stream Processing Systems, Adaptive Fault-Tolerance for Dynamic Resource Provisioning in Distributed Stream Processing Systems, ABSTRACT A growing number of applications require continuous processing of high-throughput data streams, e.g., financial anal- ysis, network traffic monitoring, or Big Data analytics for smart cities. Stream processing applications typically require specific quality-of-service levels to achieve their goals; yet, due to the high time-variability of stream characteris- tics, it is often inefficient to statically allocate the resources needed to guarantee application Service Level Agreements (SLAs). In this paper, we present LAAR, a novel method for adaptive replication that trades fault tolerance for in- creased capacity during load spikes. We have implemented and validated LAAR as a middleware layer on top of IBM In- foSphere Streamsr. We have performed a wide set of experiments on an industrial-quality 60-core cluster deployment and we show that, under the assumption of only statistical knowledge of streams load distribution, LAAR can reduce resource consumption while guaranteeing an upper-bound on information loss in case of failures. Keywords data streams processing, fault-tolerance, dynamic adapta- tion, service-level agreement, IBM InfoSphere Streamsr 1. INTRODUCTION In recent years, the ability to effectively process Big Data Streams is becoming increasingly important: the vision of smarter cities where data from several physical-world sources are continuously collected, filtered, analyzed, and fed back to administrators and citizens to assist them in their hour-by-hour tasks is just one example of the multitude of novel scenarios where handling large and unbounded flows of information in real-time is a primary requirement. Experience with Cloud services [31] has shown that the possibility to offload the management of computing infras- tructures to third parties represents an attractive opportu- nity for both developers and cloud providers. However, in a cloud environment, the nature of stream processing applica- tions poses hard challenges to platform providers, including the ability to offer, at the same time, extreme performance elasticity in spite of load variations and resiliency to failures, while keeping costs limited. From a provider perspective, one major problem lies in the necessity to handle load fluctuations due to sudden and possibly temporary variations in the rates of data streams feeding the hosted applications. If not handled properly, in fact, load peaks can lead to increased processing latency due to data queuing and to data loss due to queue overflows. To avoid these effects, it is necessary to allocate the proper amount of additional resources for the overloaded applica- tions, either statically or dynamically when load variations are detected [4, 8, 22]. Another typical requirement for stream processing appli- cations is the implementation of fault-tolerance techniques. In fact, since they usually run for (indefinitely) long time in- tervals, failures are unavoidable. Many proposals in the lit- erature have investigated possible fault-tolerance approaches  a including active replication [9, 28], checkpointing [11, 18], replay logs [6, 16], or hybrid solutions [34]  a each providing different trade-offs between runtime cost in absence of fail- ures (best-case) and recovery cost. Whichever the adopted technique, maintaining some form of replication at some level (software/hardware components, state, or messages) is a significant overhead in terms of computing resources. In a large class of applications, however,  ""perfect "" fault tolerance is not always required, while it is of primary im- portance to effectively manage temporary load variations. This is very common, for example, when dealing with Smart City-generated Big Data. In this context, in fact, large data streams are produced by many distributed sources  a e.g., mobile phones, ad-hoc sensing devices, or vehicles  a that continuously capture and transmit sensed environmen- tal features. These data need to be analyzed in real-time, and results must be promptly delivered to let appropriate control actions be performed. In this kind of scenarios, controlled information loss is usually tolerable, given the common partial information redundancy or overlap of input streams1. Consider, for instance, an application used to control traffic light signals based on periodic reports of vehicles ' positions, among other factors. During high traffic conditions (i.e., high system load), it is clearly preferable to compute on incomplete information than delay control decisions, given the high redundancy in reported positions. At the same time, during low traffic conditions, processing events with accuracy is still important. In this work, we investigate the possibility to trade-off reli- ability guarantees and execution cost, and use the conserved resources to handle load variations. We propose a novel method, called Load-Adaptive Active Replication (LAAR), that dynamically deactivates and activates redundant replicas of application Processing Elements (PE) in order to claim/release resources and accommodate temporary load variations. Our technique provides a-priori guarantees about the achievable levels of fault-tolerance, expressed in terms of an internal completeness metric that captures the max- imum amount of information that can be lost in case of failures. We show that LAAR can be suitably implemented as a middleware-level layer on top of existing stream pro- cessing platforms, and we present general architectural and design guidelines about how to do it efficiently. As a working proof-of-concept, we describe an implementation of LAAR on top of IBM InfoSphere Streamsr [13], an enterprise-level stream processing platform, and we discuss experimental results about the performance of LAAR on a 60-core IBM BladeCenterr cluster deployment. The remainder of the paper is organized as follows: af- ter reviewing the related literature in Section 2, we present the considered SLA-aware stream processing service model in Section 3. In Section 4, we model our middleware and explain its goals and runtime architecture. Finally, in Sec- tion 5, we report a wide set of performance results that quantitatively evaluate the effectiveness of our proposal. ",Paolo Bellavista,"Universit? di Bologna, Department of Computer Science and Engineering, Bologna, Italy",paolo.bellavista@unibo.it,Antonio Corradi,"Universit? di Bologna, Department of Computer Science and Engineering, Bologna, Italy",antonio.corradi@unibo.it,Spyros Kotoulas,"Smarter Cities Technology Centre, IBM Research, Dublin, Ireland",spyros.kotoulas@ie.ibm.com,Andrea Reale,"Universit? di Bologna Department of Computer Science and Engineering Bologna, Italy",andrea.reale@unibo.it,,,,,,,,,,,,,,,,,,
20200123,1343,Nadathur Satish,"Throughput Computing Lab, Intel Corporation",nadathur.rajagopalan.satish@intel.com,,Fast Sort on CPUs and GPUs: A Case for Bandwidth Oblivious SIMD Sort,"Fast Sort on CPUs and GPUs: A Case for Bandwidth Oblivious SIMD Sort, Fast Sort on CPUs and GPUs: A Case for Bandwidth Oblivious SIMD Sort, Fast Sort on CPUs and GPUs: A Case for Bandwidth Oblivious SIMD Sort, Fast Sort on CPUs and GPUs: A Case for Bandwidth Oblivious SIMD Sort, Fast Sort on CPUs and GPUs: A Case for Bandwidth Oblivious SIMD Sort, ABSTRACT Sort is a fundamental kernel used in many database operations. In-memory sorts are now feasible; sort performance is limited by compute flops and main memory bandwidth rather than I/O. In this paper, we present a competitive analysis of comparison and noncomparison based sorting algorithms on two modern architectures - the latest CPU and GPU architectures. We propose novel CPU radix sort and GPU merge sort implementations which are 2X faster than previously published results. We perform a fair comparison of the algorithms using these best performing implementations on both architectures. While radix sort is faster on current architectures, the gap narrows from CPU to GPU architectures. Merge sort performs better than radix sort for sorting keys of large sizes - such keys will be required to accommodate the increasing cardinality of future databases. We present analytical models for analyzing the performance of our implementations in terms of architectural features such as core count, SIMD and bandwidth. Our obtained performance results are successfully predicted by our models. Our analysis points to merge sort winning over radix sort on future ar- chitectures due to its efficient utilization of SIMD and low band- width utilization. We simulate a 64-core platform with varying SIMD widths under constant bandwidth per core constraints, and show that large data sizes of 240 (one trillion records), merge sort performance on large key sizes is up to 3X better than radix sort for large SIMD widths on future architectures. Therefore, merge sort should be the sorting method of choice for future databases. Categories and Subject Descriptors H.2 [Database Management]: Systems General Terms Performance, Algorithms 1. INTRODUCTION Sorting is of fundamental importance in databases. Common applications of sorting in database systems include index creation, user-requested sort queries, and operations such as duplicate removal, ranking and merge-join operations. Sorting on large data- bases has traditionally focused on external sorting algorithms. How- ever, the rapid increase in main memory capacity has made in- memory sorting feasible. In-memory sorts are bounded by compute, bandwidth and la- tency characteristics of processor architectures. Recent and future trends in modern computer architectures are therefore of primary importance for high performance sort implementations. Compute capacity has increased through a combination of having more cores (thread-level parallelism) with each core having wide vector (SIMD) units to exploit data-level parallelism. Core counts will increase rapidly as Moore 's law continues to increase the number of onchip transistors. The SIMD width of modern CPU and GPU pro- cessors has been steadily increasing - from 128-bit in SSE architec- tures, 256-bit in AVX [14] to 512-bit in the upcoming Larrabee [23] architecture. GPUs have a logical 1024-bit SIMD with physical SIMD widths of 256-bits on the latest NVIDIA GTX 200 series, increasing to 512-bits on the upcoming Fermi architecture [19]. Memory bandwidth is increasing at a slower pace than compute. Algorithms that are bound by memory bandwidth will not scale well to future architectures. A variety of algorithms have been developed for sorting a list of numbers. Sorting algorithms can be broadly classified as ei- ther comparison based or non-comparison based sorts. Comparison based sorting algorithms rearrange the data items based on the re- sults of comparing pairs of elements at a time. Non-comparison based sorts rely on using the absolute values of the data items, rather than comparisons, to rearrange the data. A common example of a non-comparison based sort is radix sort. Radix sort is a mul- tiple pass sort algorithm that buckets data according to individual digits of the data items. Sorting algorithms differ in their compu- tational complexity, which dictates the inherent amount of com- putation required by the algorithm, and also differ in their archi- tectural friendliness, or how well they can use current and future architectural trends, such as increasing thread-level and data-level parallelism on modern architectures. There is often a trade-off be- tween these factors. For instance, radix sorts are not naturally data-parallel, unlike comparison sorts that can use data parallel merging networks. Further, radix sort needs many passes over each data item - resulting in high bandwidth utilization. Merge sort, on the other hand, can be made bandwidth friendly (and is used in external disk sorts for this reason). Opposing these architectural inefficiencies of radix sort is its lower computational complex- ity of O(N), as against the lower bound  (N logN) of comparison based sorts. The right choice of sorting algorithms becomes a trade- off between computational complexity and architectural efficiency; such trade-offs are architecture-dependent. There has, so far, not 351 been much work in analyzing the efficiency with which sorting techniques utilize modern architectural features. In this work, we evaluate these trade-offs on two sorting algorithms - radix sort with a low O(N) computational complexity, and a SIMD-efficient and bandwidth oblivious merge sort with a complexity of O(N logN). We investigate these trade-offs on the latest CPU and GPU archi- tectures, which are commercially widespread and hence of interest to the database community. The Intel Core i7 CPU has 4 cores each with 128-bit SSE width, while the NVIDIA GTX 280 has 30 SMs each with 256-bit SIMD units. In order to make such an investigation fair, the algorithms must be implemented as efficiently as possible on each architecture. We, therefore, identified bottlenecks found in common implementations of each sorting algorithm (such as irregular memory accesses, lack of SIMD use, conflicts in local storage such as cache or shared memory) and optimized our algorithms to avoid such bottlenecks. Our novel CPU radix sort algorithm uses a buffer stored in cache to localize scatters; this avoids capacity and conflict misses, resulting in the fastest reported CPU sorts. We implemented an optimized merge sort on the GPU which is only 10% off the best known sort- ing algorithm (a radix sort) on the GPU. We adopted highly opti- mized codes where available, including a CPU merge sort and a GPU radix sort. Our resulting implementations are the best per- forming implementations of these sorting algorithms on these ar- chitectures, and include the highest performing sort on each archi- tecture. We provide a performance model for each of our result- ing implementations in terms of the compute, bandwidth and SIMD usage of our algorithms. Our analysis results match well with the actual performance results. The influence of architectural awareness on sorting algorithms is clear from our investigations. The best implementation of the same radix sort algorithm is very different on CPUs and GPUs - a SIMD friendly 1-bit split based code is best on GPUs that heavily rely on data-level parallelism, while a scalar buffer-based scatter approach works best on CPUs with lower SIMD widths. However, we show that both versions of radix sort have heavy bandwidth demands due to their multi-pass nature. The bandwidth demand of radix sort increases for large key sizes - radix becomes bandwidth bound on even the latest CPUs on 6-byte keys. GPUs have much higher bandwidth - however, even then a part of the radix sort algorithm is bandwidth bound. Algorithms that are bandwidth bound cannot use compute resources effectively. On the other hand, merge sort uses bandwidth resources efficiently, and is compute-bound. Merge sort hence becomes faster than radix on GPUs on keys larger than 8- bytes, and faster on CPUs for keys greater than 9-bytes. In addition, merge sort can also utilize SIMD resources efficiently. While the higher computational complexity of merge sort does make it slower than radix on current architectures for large data sets of 4-byte keys, the gap narrows from 1.7X on CPUs with 128-bit SIMD to only about 10% on GPUs with 256-bit SIMD. Having identified the bottlenecks of these algorithms on current hardware, we project the performance of our algorithms on future architectures with wider SIMD and lower bandwidth-to-compute requirements. The bandwidth-oblivious SIMD-friendly merge sort will perform better on such architectures. We confirm our projec- tions by simulating our algorithms on architectures with varying SIMD widths, and show that as SIMD widths increase to 2048- bit and beyond, SIMD merge sort performance for 8-byte keys is 1.5X faster than radix sort on data sizes as large as 240 (one trillion records). For 16-byte keys, the performance ratio further increases to 3X better than radix sort. The bandwidth-oblivious SIMD- friendly merge sort, should, therefore, be the sorting method of choice for future databases.",Nadathur Satish,"Throughput Computing Lab, Intel Corporation",nadathur.rajagopalan.satish@intel.com,Changkyu Kim,"Throughput Computing Lab, Intel Corporation",0,Jatin Chhugani,"Throughput Computing Lab, Intel Corporation",,Anthony D. Nguyen,"Throughput Computing Lab, Intel Corporation",,Victor W. Lee,"Throughput Computing Lab, Intel Corporation",,Daehyun Kim,"Throughput Computing Lab, Intel Corporation",,Pradeep Dubey,"Throughput Computing Lab, Intel Corporation",,,,,,,,,,
20200124,1344,Stacy Patterson,"Department of Electrical Engineering Technion - Israel Institute of Technology Haifa, 32000, Israel",stacyp@ee.technion.ac.il,,"Serializability, not Serial: Concurrency Control and Availability in Multi-Datacenter Datastores","Serializability, not Serial: Concurrency Control and Availability in Multi-Datacenter Datastores, Serializability, not Serial: Concurrency Control and Availability in Multi-Datacenter Datastores, Serializability, not Serial: Concurrency Control and Availability in Multi-Datacenter Datastores, Serializability, not Serial: Concurrency Control and Availability in Multi-Datacenter Datastores, Serializability, not Serial: Concurrency Control and Availability in Multi-Datacenter Datastores, ABSTRACT We present a framework for concurrency control and availability in multi-datacenter datastores. While we consider Google 's Megastore as our motivating example, we define general abstractions for key components, making our solu- tion extensible to any system that satisfies the abstraction properties. We first develop and analyze a transaction management and replication protocol based on a straightforward implementation of the Paxos algorithm. Our investigation reveals that this protocol acts as a concurrency prevention mechanism rather than a concurrency control mechanism. We then propose an enhanced protocol called Paxos with Combination and Promotion (Paxos-CP) that provides true transaction concurrency while requiring the same per instance message complexity as the basic Paxos protocol. Finally, we compare the performance of Paxos and Paxos-CP in a multi-datacenter experimental study, and we demon- strate that Paxos-CP results in significantly fewer aborted transactions than basic Paxos. 1. INTRODUCTION Cloud computing has the potential to become the founda- tion for most information technology architectures. It offers application developers access to seemingly infinite storage, compute, and network resources, all on a pay-per-use basis. While the appeal of the cloud computing model is obvious from a financial perspective, its success also depends on the ability of clouds to provide reliable, scalable services that support the features developers need. In particular, it is important that cloud datastores, such as Google 's BigTable [8] and Amazon 's SimpleDB [1], provide support for various types of data consistency and guarantee the availability of application data in the face of failures. Initially, cloud datastores provided only eventually con- sistent update operations guaranteeing that updates would eventually propagate to all replicas. While these datastores were highly scalable, developers found it difficult to create applications within the eventual consistency model [20]. Many cloud providers then introduced support for atomic access to individual data items, in essence, providing strong consistency guarantees. This consistency level has become a standard feature that is offered in most cloud datastore im- plementations, including BigTable, SimpleDB, and Apache HBase [16]. Strong consistency of single data items is suffi- cient for many applications. However, if several data items must be updated atomically, the burden to implement this atomic action in a scalable, fault tolerant manner lies with the software developer. Several recent works have addressed the problem of implementing ACID transactions in cloud datastores [2, 10, 11], and, while full transaction support re- mains a scalability challenge, these works demonstrate that transactions are feasible so long as the number of tuples that are transactionally related is not  ""too big "". While many solutions have been developed to provide con- sistency and fault tolerance in cloud datastores that are hosted within a single data center, these solutions are of no help if the entire datacenter becomes unavailable. For example, in April 2011, a software error brought down one of Amazon 's EC2 availability zones and caused service dis- ruption in the U.S. East Region [24]. As a result, major web sites like Reddit, Foursquare, and Quora were unavail- able for hours to days [5]. And, in August 2011, lightning caused Microsoft and Amazon clouds in Dublin [15] to go offline for hours. In both instances, there were errors in the recovery process, and it was not possible to restore a consistent snapshot of some application data. These recent outages demonstrate the need for replica- tion of application data at multiple datacenters as well as the importance of using provably correct protocols for performing this replication. In a recent work, Baker et al. de- scribe Megastore, Google 's approach to providing transac- tions in the cloud with full replication at multiple datacen- ters [2]. Megastore is implemented on top of BigTable and provides support for ACID transactions over small sets of data items called entity groups. It uses multi-version concurrency control and a replicated write-ahead log. Replication is performed using the Paxos algorithm [18] to ensure consistency even with unreliable communication and datacenter outages. While the paper presents an overview of the Megastore system, it lacks the formality and detail required to verify Megastore 's correctness. We assert that such analysis is needed for systems like Megastore, especially in light of the outages described above and the widely acknowledged difficulties associated with understanding and implementing the Paxos algorithm [7, 19, 25]. In this work, we address the need for formal analysis of replication and concurrency control in transactional cloud datastores. We define and analyze several Paxos-based pro- tocols for replication and transaction management in the multi-datacenter setting. While we take Megastore as our motivating example, we define general abstractions for each of the key components, and we use these abstractions in our protocol design and analysis. The specific contributions of our work are: ? We provide a formal description of the Paxos protocol for replication and concurrency control, and we prove its correctness. Through our analysis, we also show that the Paxos protocol, as implemented in Megastore, aborts transactions that could be safely committed. In essence, it acts as a concurrency prevention mechanism rather than a concurrency control mechanism. ? We propose an enhanced replication and concurrency control protocol that we call Paxos with Combination and Promotion (Paxos-CP). Paxos-CP enables true transaction concurrency, with the same per-instance message complexity as the original Paxos protocol. ? We compare the performance of Paxos and Paxos-CP in a multi-datacenter experimental study, and we demon- strate the benefits of our enhanced Paxos protocol. The remainder of this paper is organized as follows. In Section 2, we give an overview of the design of the cloud datastore including the data model and reference architecture. Section 3 summarizes the theoretical foundations that we use to analyze the correctness of the transactional cloud datastore. In Section 4, we present the details of the trans- action manager, including the basic Paxos commit protocol, and we prove its correctness. In Section 5, we present our extended Paxos commit protocol that allows for transaction concurrency, and we prove the correctness of this protocol. We present evaluation results comparing the basic and ex- tended Paxos commit protocols in Section 6. In Section 7, we discuss related work, and we conclude in Section 8.",Stacy Patterson,"Department of Electrical Engineering Technion - Israel Institute of Technology Haifa, 32000, Israel",stacyp@ee.technion.ac.il,Aaron J. Elmore,"Department of Computer Science University of California, Santa Barbara Santa Barbara, CA 93106",aelmore@cs.ucsb.edu,Faisal Nawab,"Department of Computer Science University of California, Santa Barbara Santa Barbara, CA 93106",nawab@cs.ucsb.edu,Divyakant Agrawal,"Department of Computer Science University of California, Santa Barbara Santa Barbara, CA 93106",agrawal@cs.ucsb.edu,Amr El Abbadi,"Department of Computer Science University of California, Santa Barbara Santa Barbara, CA 93106",amr@cs.ucsb.edu,,,,,,,,,,,,,,,
20200125,1380,Zhepeng Yan,"University of Pennsylvania Philadelphia, PA, USA",zhepeng@cis.upenn.edu,,Actively Soliciting Feedback for Query Answers in Keyword Search-Based Data Integration,"Actively Soliciting Feedback for Query Answers in Keyword Search-Based Data Integration, Actively Soliciting Feedback for Query Answers in Keyword Search-Based Data Integration, Actively Soliciting Feedback for Query Answers in Keyword Search-Based Data Integration, Actively Soliciting Feedback for Query Answers in Keyword Search-Based Data Integration, Actively Soliciting Feedback for Query Answers in Keyword Search-Based Data Integration, ABSTRACT The problem of scaling up data integration, such that new sources can be quickly utilized as they are discovered, remains elusive: global schemas for integrated data are difficult to develop and expand, and schema and record matching techniques are limited by the fact that data and metadata are often under-specified and must be disambiguated by data experts. One promising approach is to avoid using a global schema, and instead to develop keyword search- based data integration - where the system lazily discovers associ- ations enabling it to join together matches to keywords, and return ranked results. The user is expected to understand the data domain and provide feedback about answers' quality. The system general- izes such feedback to learn how to correctly integrate data. A major open challenge is that under this model, the user only sees and offers feedback on a few ""top-k"" results: this result set must be carefully selected to include answers of high relevance and answers that are highly informative when feedback is given on them. Existing systems merely focus on predicting relevance, by composing the scores of various schema and record matching algorithms. In this paper we show how to predict the uncertainty associated with a query result's score, as well as how informative feedback is on a given result. We build upon these foundations to develop an active learning approach to keyword search-based data integration, and we validate the effectiveness of our solution over real data from several very different domains. 1. INTRODUCTION The vision of rapid information integration remains elusive, de- spite steady progress in system architectures [13] and in alignment techniques for discovering links among records [11] and schema el- ements [28]. In general, the approach is to define one or more inte- grated or mediated schemas capturing the data domain, use schema mapping (alignment) and entity resolution (record linking) tools to map data sources into the mediated schema, and finally allow users to pose structured queries against mediated schemas. A stumbling block is that string, pattern, and structural similari- ties among data and metadata elements (the core techniques whose outputs are combined by alignment tools) do not suffice to unique- ly identify the correspondences between data or metadata items. The resulting ambiguous matches can only be resolved with do- main (commonly, human) expertise. As a result, many of today's tools aim for semi-automated matching, where the system makes predictions and relies on a human domain expert to correct any mistakes or resolve any uncertainties (e.g., see [28, p. 345]). There are several shortcomings to having a database administra- tor inspect the output of a schema matching tool before adding the mapping to an existing system: (1) administrator vetting becomes a bottleneck to the system incorporating sources; (2) the metadata might not clearly describe the data that must be mapped1; (3) subtle variations in semantics may only show up in occasional, incorrect query results. Moreover, the mediated schema itself can be a bot- tleneck to adding new data, as new sources may have concepts that do not yet exist at the global level. For these reasons, recent work [4, 29, 34, 35] has proposed to complement (or even replace) conventional integration with tech- niques that do not rely on a mediated schema and schema mappings created by an administrator. Instead the proposal is to adopt a keyword search over databases model [5, 19, 20, 25] where matches to individual keywords are assembled into query results by discov- ering ""join trees"" that link the matches. This requires discovering paths of associations (alignments across records, terms, or schema elements in sources) that can join matching records together. Under this model, the output of alignment algorithms is used directly to answer queries, with no administrator intervention: the system relies on the end user to have the domain expertise to vet the re- sults, and to provide feedback [34, 35] on the system's ranking of (some) individual query results. Now instead of having a human administrator correct bad associations, the system must learn the correct score (possibly zero or infinite) of each individual associ- ation, given the user's feedback on query answers that are formed from multiple associations [34]. This model is a form of ""pay-as-you-go"" integration [13], as it enables the system and its users to focus their attention on those associations that relate to actual information needs. The associ- ations relevant to frequently posed queries should be the ones that receive the most attention and refinement. In fact the pay-as-you-go approach can be used to complement and inform more traditional integration techniques: the keyword search log can help a human administrator determine which parts of the data to prioritize integrating, and provide clues for what mappings are most relevant. However, to successfully learn to integrate data, the system must balance its need to acquire feedback useful for answering future queries, versus the requirement that each user immediately gets the information he or she needs. Today's keyword search systems have approached this problem by simply assuming the query scoring function is accurate: they return the top-k results according to the scoring function, which in turn bases its scores on the predicted (but possibly incorrect) output of matching tools. Under this model the user will attempt to remove false positives but has no way of seeing - and providing feedback on - false negatives. Such a model works well when the system returns a good mix of correct and invalid results and the user can ""separate"" them. However, as the number and complexity of sources and their at- tributes increases, many potential queries are likely to have similar scores, due to inherent uncertainty in combining low-confidence results from various matching algorithms. The number of potential results can grow rapidly as the number of keyword matches increases, whereas the number of results seen by the user remains constrained by the dimensions of the screen and the limits of user attention. Thus, when a keyword-based data integration system selects queries to produce answers, it should not merely choose alignments based on the relative scores of associations - but also the uncertainty associated with a given query result, and the informativeness of feedback given on that particular result. In this paper, we use active learning to help the system deter- mine which query results to present, given a combination of their predicted score, their inherent uncertainty, and the amount of infor- mation gained about other potential queries. Intuitively, the infor- mativeness of feedback on a query result is related to how much uncertainty there is about the result's relevance to the query, and how many other similar share features with this result - mean- ing that feedback on the first result also reduces their uncertain- ty. We provide a more precise characterization of informativeness later in the paper. Our work goes beyond previous attempts to use uncertainty-directed ranking in the pay-as-you-go-integration space, such as [24] which focused on individual mappings, by look- ing at the total uncertainty associated with queries and their results, and how this uncertainty should be combined with relevance rank- ing. The key questions addressed in this paper are how to estimate the utility of a given query to the system and to the user, and how to estimate the uncertainty of a query's score, in applying active learning to the problem of determining the relevance of associations to a query. Specifically, we make the following contributions: ? Techniques for estimating the uncertainty associated with a query, through the notions of entropy and variance, and by combining the probability distributions of the output for in- dividual schema matching or record linking outputs. ? Pruning and active learning techniques that focus the user's attention on the query results most likely to either be relevant, or help the system produce better results. ? A scoring model using expected model change to relate the user's model of browsing data to how we should combine and rank both useful and uncertain query answers. ? A method of clustering similar join queries, and choosing the most useful representative. ? An experimental evaluation demonstrating the effectiveness of our approach across several real data domains. Section 2 provides the context of our problem, including the ba- sic workflow of our integration task. Section 3 shows how we as- sess the informativeness of each query. Section 4 then describes how we combine informativeness and predicted score to return ranked query results, and to learn from feedback on them. We experimen- tally analyze our results in Section 5, describe related work in Sec- tion 6, and conclude in Section 7. ",Zhepeng Yan,"University of Pennsylvania Philadelphia, PA, USA",zhepeng@cis.upenn.edu,Nan Zheng,"University of Pennsylvania Philadelphia, PA, USA",nanzheng@cis.upenn.edu,Zachary G. Ives,"University of Pennsylvania Philadelphia, PA, USA",zives@cis.upenn.edu,Partha Pratim Talukdar,"Carnegie Mellon University  Pittsburgh, PA USA",ppt@cs.cmu.edu,Cong Yu," Google, Inc. New York, NY USA",congyu@google.com,,,,,,,,,,,,,,,
20200126,740,Arvind Arasu,"Microsoft Research Redmond, WA",arvinda@microsoft.com,,Data Generation using Declarative Constraints,"Data Generation using Declarative Constraints, Data Generation using Declarative Constraints, Data Generation using Declarative Constraints, Data Generation using Declarative Constraints, Data Generation using Declarative Constraints, ABSTRACT We study the problem of generating synthetic databases hav- ing declaratively specified characteristics. This problem is motivated by database system and application testing, data masking, and benchmarking. While the data generation problem has been studied before, prior approaches are either non-declarative or have fundamental limitations relating to data characteristics that they can capture and efficiently support. We argue that a natural, expressive, and declara- tive mechanism for specifying data characteristics is through cardinality constraints; a cardinality constraint specifies that the output of a query over the generated database have a certain cardinality. While the data generation problem is intractable in general, we present efficient algorithms that can handle a large and useful class of constraints. We include a thorough empirical evaluation illustrating that our algo- rithms handle complex constraints, scale well as the number of constraints increase, and outperform applicable prior techniques. Categories and Subject Descriptors D.2.5 [Software Engineering]: Testing and Debugging a Testing tools; H.2.4 [Database Management]: Systems a Query processing General Terms Algorithms, Performance, Reliability, Experimentation Keywords Data Generation, Testing, Masking, Benchmarking, Constraints 1. INTRODUCTION We consider the problem of generating a synthetic database instance having certain data characteristics. Many applications require synthetically generated data: 1. DBMS testing: When we design a new DBMS compo- nent such as a new join operator or a new memory manager, we require synthetic database instances with specific char- acteristics to test correctness and performance of the new component [7, 22]. For example, to test the code module of a hybrid hash join that handles spills to disk, we might need a database instance with a high skew on the outer join attribute. As another example, to study the interaction of the memory manager and multiple hash join operators, we might need a database instance that has particular interme- diate result cardinalities for a given query plan [9]. 2. Data masking and database application testing: Organi- zations sometimes outsource the testing of their database applications to other organizations. However an outsourc- ing organization might not be able to share its internal databases (over which the applications run) with the test- ing organization due to privacy considerations, requiring us to generate a synthetic database that behaves like the orig- inal database for the purposes of testing. (We emphasize that our goal here is not to study the general data masking problem with its privacy considerations; we are merely sug- gesting that data generation might be a useful component of a general data masking solution.) 3. Benchmarking: In order to decide between multiple com- peting data management solutions, a customer might be interested in benchmarking the solutions [22]. The standard benchmarks such as TPC-H might not capture many of the application scenarios and data characteristics of interest to the customer, motivating the need for synthetic data generation. A related scenario is upscaling, where we are interested in generating a synthetic database that is an upscaled ver- sion of an existing database. Upscaling is useful for future capacity planning purposes. Data characteristics and cardinality constraints: The applications of data generation above require a wide variety of data characteristics in the generated synthetic databases. A natural class of characteristics are schema properties such as key and referential integrity constraints, functional de- pendencies, and domain constraints (e.g., age is an integer between 0 and 120). A synthetic database for DB appli- cation testing often needs to satisfy such constraints since the application being tested might require these constraints for correct functioning. If DB application testing involves a visual component with a tester entering values in fields of a form, the synthetic database might need to satisfy natu- 685 ralness properties, e.g., the values in an address field should  ""look like "" real addresses. In benchmarking and DBMS testing, we typically need to capture characteristics that can influence the performance of a query over the generated database. These include, for example, ensuring that values in a column be distributed in a particular way, ensuring that values in a column have a certain skew, or ensuring that two or more columns are cor- related. We note correlations can involve joining multiple tables. For example, in a customer-product-order database, we might need to capture correlations between the age of customers and the category of products they purchase. In data masking, we might require synthetic data to result in the same application performance as the original data, with- out revealing sensitive information from the original data. In addition to the richness of data characteristics, appli- cations might require several properties and constraints be together satisfied in a generated database. This requirement motivates the need for a declarative approach to data gen- eration as opposed procedural approaches considered in [7, 15]. As a concrete example, consider generating a customer- product-order database where we need to capture correla- tions between several pairs of columns such as customer age and product category, customer age and income, and product category and supplier location. It is fairly nontrivial for a programmer to design a procedure that outputs a database with all of the above properties, even with the right proce- dural primitives. A natural and expressive language for specifying data characteristics is a set of cardinality constraints. A cardinality constraint specifies that the output of a given query over the generated database should have a particular cardinality. As a simple example, we can (approximately) specify the distribution of values in a column by providing a histogram, and a histogram can be represented as a collection of cardinal- ity constraints, one for each bucket. In Section 2, we show that many of the data characteristics discussed earlier can be represented using cardinality constraints. The idea of using cardinality constraints for data gener- ation is not new and has been proposed in QAGen [6] and its extension MyBenchmark [22]. However, in this work car- dinality constraints are mostly used for capturing workload characteristics and the ability of cardinality constraints to express more general data characteristics is not discussed. Motivated by the above discussion, the goal of this paper is to design efficient algorithms for generating synthetic databases that satisfy a given set of cardinality constraints. The set of constraints provided as input can be large (say, thousands); for example, even specifying a simple histogram can require 10s or 100s of constraints. The queries in the constraints can be complex, possibly involving joins over multiple tables. Prior Work: While QAGen [6] and MyBenchmark [22] do not discuss the expressiveness aspects of cardinality con- straints, their techniques are quite general and can be used for our purposes. However, they have some basic limita- tions. QAGen and MyBenchmark assume that cardinality constraints are available in a particular form called anno- tated query plans (AQP). An annotated query plan is a query plan with a subset of plan nodes annotated with cardinali- ties. We can show that we can encode cardinality constraints as AQPs and vice-versa. For data generation, QAGen uses a novel approach called symbolic query processing. Briefly, it starts with a symbolic database; a symbolic database is like a regular database, but its attribute values are symbols (vari- ables), not ""constants. "" It then translates the input AQPs to constraints over the symbols in the database, and invokes a black-box constraint satisfaction program (CSP) to identify values for symbols that satisfy all the constraints. One limitation of QAGen is that it can handle a single AQP, and therefore cannot be directly used to generate databases that satisfy multiple arbitrary constraints. This limitation is identified and addressed in MyBenchmark [22]. Briefly, to handle n AQPs, MyBenchmark uses QAGen to generate n symbolic databases with constraints and performs  ""matching "" between these databases to heuristically identify m n databases that together satisfy all the AQPs. MyBenchmark is not guaranteed to produce a single database instance and this functionality can be unsuitable for some applications requiring synthetic data. For example, we cannot use multiple database instances for DB application testing, since no single instance reflects all the charac- teristics of the original database. One advantage of using a general purpose CSP is that it enables QAGen to handle complex queries, e.g., queries with HAVING clauses. However, this generality comes with a performance cost. The number of times QAGen and My- Benchmark invoke a CSP grows with the size of the gener- ated database and this has serious performance implications as the experiments in [6, 22] indicate.1 The algorithms that we propose do not have these limitations: they always gen- erate a single database instance and their dependence on the generated database size is limited to the cost of materializing the database. Interestingly, recent work on cardinality estimation using maximum entropy principle [27, 28] can be adapted to de- rive algorithms for data generation, and we discuss this pos- sibility in detail in Section 4. However, briefly, cardinality estimation using maximum entropy is known to be a very hard problem and adaptations of current solutions do not efficiently handle complex constraints. Summary of Contributions: We formally introduce car- dinality constraints in Section 2 and show that a set of car- dinality constraints forms an expressive language for speci- fying data characteristics. In Section 3, we state the formal data generation problem and show that the general prob- lem is NEXP-complete and therefore hard. We present our algorithms in Section 4. While the general data genera- tion problem is hard, our algorithms are able to handle a large and useful class of constraints. Our algorithms are probabilistically approximate, meaning that they satisfy all constraints in expectation. We note that this is sufficient for most applications of data generation. Our algorithms are also sensitive to the complexity of the input cardinal- ity constraints in a precisely quantifiable way and use ideas from probabilistic graphical models [26]. We include detailed experimental evaluation of our algorithms in Section 6 and conclude.",Arvind Arasu,"Microsoft Research Redmond, WA",arvinda@microsoft.com,Raghav Kaushik,"Microsoft Research Redmond, WA",skaushi@microsoft.com,Jian Li,"University of Maryland College Park, MD",lijian@cs.umd.edu,,,,,,,,,,,,,,,,,,,,,
20200127,1302,Wook-Shin Han,Department of Computer Engineering Kyungpook National University,wshan@knu.ac.kr,,Dependency-Aware Reordering for Parallelizing Query Optimization in Multi-Core CPUs,"Dependency-Aware Reordering for Parallelizing Query Optimization in Multi-Core CPUs, Dependency-Aware Reordering for Parallelizing Query Optimization in Multi-Core CPUs, Dependency-Aware Reordering for Parallelizing Query Optimization in Multi-Core CPUs, Dependency-Aware Reordering for Parallelizing Query Optimization in Multi-Core CPUs, Dependency-Aware Reordering for Parallelizing Query Optimization in Multi-Core CPUs, ABSTRACT The state of the art commercial query optimizers employ cost-based optimization and exploit dynamic programming (DP) to find the optimal query execution plan (QEP) without evaluating redundant sub-plans. The number of alternative QEPs enumerated by the DP query optimizer can increase exponentially, as the number of joins in the query increases. Recently, by exploiting the coming wave of multi-core processor architectures, a state of the art parallel opti- mization algorithm [14], referred to as PDPsva, has been proposed to parallelize the  ""time-consuming "" DP query optimization process itself. While PDPsva significantly extends the practical use of DP to queries having up to 20-25 tables, it has several limitations: 1) supporting only the size-driven DP enumerator, 2) statically allo- cating search space, and 3) not fully exploiting parallelism. In this paper, we propose the first generic solution for parallelizing any type of bottom-up optimizer, including the graph-traversal driven type, and for supporting dynamic search allocation and full paral- lelism. This is a challenging problem, since recently developed, state of art DP optimizers such as DPcpp [21] and DPhyp [22] are very difficult to parallelize due to tangled dependencies in the join pairs they generate. Unless the solution is very carefully devised, a lot of synchronization conflicts are bound to occur. By viewing a serial bottom-up optimizer as one which generates a totally or- dered sequence of join pairs in a streaming fashion, we propose a novel concept of dependency-aware reordering, which minimizes waiting time caused by dependencies of join pairs. To maximize parallelism, we also introduce a series of novel performance optimization techniques: 1) pipelining of join pair generation and plan generation; 2) the synchronization-free global MEMO; and 3) threading across dependencies. Through extensive experiments with various query topologies, we show that our solution supports any type of bottom up optimization, achieving linear speedup for each type. Despite the fact that our solution is generic, due to sophisticated optimization techniques, our generic parallel optimizer outperforms PDPsva tailored to size-driven enumeration. Experi- mental results also show that our solution is much more robust than PDPsva with respect to search space allocation. Categories and Subject Descriptors H.2.4 [DATABASE MANAGEMENT]: Systems General Terms Algorithms Keywords Multi-cores, Parallel databases, Query optimization 1. INTRODUCTION For the last few decades, the CPU performance has been signifi- cantly improved by increasing the clock rate according to Moore 's law. However, fundamental physical limitations such as power con- sumption and heat generation clearly prevent us from relying on this trend any more [11, 12, 29, 30]. Instead, the industry has been improving the CPU performance by integrating more execu- tion cores into each processor. The number of cores is expected to grow significantly over time [11, 35]. Recently, by exploiting this new wave of multi-core processor architectures, Han et al. [14] have proposed a novel framework re- ferred to here as PDPsva, to parallelize the  ""time-consuming "" dy- namic programming (DP) query optimization process itself. The DP query optimizer enumerates many alternative query execution plans (QEPs) for evaluating a declarative SQL query, while esti- mating the cost of each QEP, and then chooses the one with lowest estimated cost. The number of alternative QEPs enumerated by the DP query optimizer can increase exponentially, as the number of joins in the query increases. In fact, PDPsva significantly extends the practical use of DP to queries having up to 20-25 tables. We otherwise would have to depend on sub-optimal (randomized or greedy) heuristics [4, 19, 23, 31, 32] to complete query optimiza- tion in a reasonable time. However, PDPsva has three limitations. First, it supports only one specific bottom-up optimizer, the size-driven DP optimizer. That is, it does not support recently developed, state of the art DP optimizers such as DPcpp [21] and DPhyp [22], which directly tra- verse a query graph to generate join pairs. Such optimizers have advantages over the size-driven enumeration. They can support early termination, since they can generate QEPs for all tables (more precisely, all quantifiers1) without generating QEPs for all smaller quantifier sets (i.e., not size-driven). Thus, as soon as we obtain a sufficiently good QEP or the estimated execution time of the ob- tained QEP is less than the expected remaining enumeration time, we can terminate the enumeration process early. DPhyp can handle the widest class of non-inner joins very efficiently [22]. Therefore, there is a need for a generic framework that can parallelize any type of bottom-up optimizer so that it can support both existing and fu- ture bottom-up optimizers. Secondly, assuming all cores are evenly loaded, PDPsva employs static search space allocation. Although the best allocation strategy of PDPsva can allocate search space to threads evenly [14], the slowest thread holds up all the other, faster threads, resulting in seriously unbalanced workloads. Therefore, the search allocation strategy must be dynamic to resolve this situation. Lastly, PDPsva does not fully exploit parallelism since it merges per-thread MEMOs to the global MEMO in serial execution for each size of resulting quantifier sets. Here, each MEMO entry stores QEPs for a given quantifier sets. Thus, the best version of PDPsva achieves only up to 6.1 speedup for star queries2 using 8 threads [14]. Therefore, in order to achieve linear speedup, all such serial steps must be executed by exploiting full parallelism. In this paper, we propose the first generic solution for paralleliz- ing any type of bottom-up optimizer, including the graph-traversal driven type, and for supporting dynamic search allocation and full parallelism. This is a challenging problem, since DPcpp and DPhyp are very difficult to parallelize [14]. Unless the solution is very carefully devised, a lot of synchronization conflicts are bound to occur. Figure 1 shows a motivating example using a sequence of join pairs (more precisely, a sequence of pairs of quantifier sets) generated by DPcpp or DPhyp. Note that DPcpp and DPhyp gener- ate the same sequence for equi-join. An arrow from one pair to another pair represents a dependency. As opposed to size-driven enumeration, the sizes of resulting quantifier sets do not monotoni- cally increase. This leads to tradeoff between early termination and tangled dependencies. That is, since we obtain some QEPs for all quantifiers at the 17th pair of quantifier sets (q1, q2q3q4), we might be able to terminate the optimization process early, if the best QEP obtained thus far is good enough. On the other hand, the resulting tangled dependencies in the pairs of quantifier sets hinder paral- lelizing DPcpp. For example, if a thread Ta processes the eighth pair (q2, q3q4), and a thread Tb processes the fifth pair (q3, q4), Ta must wait until Tb finishes the processing of (q3, q4) first, since the quantifier set q3q4 has a dependency on the pair (q3, q4). If we change the order of the eighth and the eleventh pairs, Ta can process (q1, q4) without waiting. The overview of our solution is as follows. To parallelize any type of bottom-up enumeration, we view a serial bottom-up opti- mizer as one which generates a totally ordered sequence of pairs of quantifier sets in a streaming fashion. We buffer a fixed number of pairs and delay plan generation for the pairs buffered. Then on the fly, we convert the total order over these buffered pairs into a partial order over unordered groups of pairs, where threads can generate QEPs independently for all pairs within a group without waiting. These steps correspond to reordering of the original sequence so that the tangled dependencies in the original sequence are unraveled. We repeat these steps until we consume all pairs of quantifier sets. Our contributions are as follows: 1) We propose the first generic framework for parallelizing any type of bottom-up optimization. 2) We propose a novel concept of dependency-aware reordering, which minimizes waiting time caused by dependencies of pairs of quantifier sets and propose a generic algorithm DPEGeneric for parallelizing query optimization. 3) To maximize parallelism, we propose a series of optimization techniques for DPEGeneric:  pipelining of join pair generation and plan generation; the synchro- nization-free global MEMO; and threading across dependencies. 4) Through extensive experiments, we show that DPEGeneric sup- ports any type of bottom up optimizer, achieving linear speedup for each type. Our algorithm is even better than the state of the art parallel optimizer tailored to size-based enumeration, PDPsva. Our algorithm is also much more robust than PDPsva with respect to search space allocation. The rest of this paper is organized as follows. Section 2 reviews the current bottom-up join enumeration algorithms and the state of the art parallel algorithm for the size-based enumeration. The next two sections give the details of two generic parallel enumeration algorithms. Section 3 gives a basic parallel enumeration algorithm that can support any type of bottom-up enumeration algorithms, and Section 4 gives a theoretical framework for the dependency- aware reordering and an enhanced parallel enumeration algorithm exploiting the dependency-aware reordering. Section 5 presents a series of performance optimization techniques to maximize paral- lelism. Section 6 presents the results of performance evaluation. We compare our contributions with related work in Section 7, and conclude in Section 8.",Wook-Shin Han,Department of Computer Engineering Kyungpook National University,wshan@knu.ac.kr,Jinsoo Lee,Department of Computer Engineering Kyungpook National University,jslee@www-db.knu.ac.kr,,,,,,,,,,,,,,,,,,,,,,,,
20200128,1345,Xin Liu,"University of Waterloo, Canada",x39liu@uwaterloo.ca,,Hybrid Storage Management for Database Systems,"Hybrid Storage Management for Database Systems, Hybrid Storage Management for Database Systems, Hybrid Storage Management for Database Systems, Hybrid Storage Management for Database Systems, Hybrid Storage Management for Database Systems, ABSTRACT The use of flash-based solid state drives (SSDs) in storage systems is growing. Adding SSDs to a storage system not only raises the question of how to manage the SSDs, but also raises the question of whether current buffer pool algo- rithms will still work effectively. We are interested in the use of hybrid storage systems, consisting of SSDs and hard disk drives (HDDs), for database management. We present cost-aware replacement algorithms, which are aware of the difference in performance between SSDs and HDDs, for both the DBMS buffer pool and the SSDs. In hybrid storage sys- tems, the physical access pattern to the SSDs depends on the management of the DBMS buffer pool. We studied the impact of buffer pool caching policies on SSD access patterns. Based on these studies, we designed a cost-adjusted caching policy to effectively manage the SSD. We implemented these algorithms in MySQL 's InnoDB storage engine and used the TPC-C workload to demonstrate that these cost-aware al- gorithms outperform previous algorithms. 1. INTRODUCTION Flash memory has been used for many years in portable consumer devices (e.g, cameras, phones) where low power consumption and lack of moving parts are particularly desir- able features. Flash-based solid state storage devices (SSDs) are now also becoming commonplace in server environments. SSDs are more expensive per bit than traditional hard disks (HDD), but they are much cheaper in terms of cost per I/O operation. Thus, servers in data centers may be con- figured with both types of persistent storage. HDDs are cost effective for bulky, infrequently accessed data, while SSDs are well-suited to data that are relatively hot [8]. In this paper, we are concerned with the use of such hybrid (SSD and HDD) storage systems for database management. We consider hybrid storage systems in which the two types of devices are visible to the database management system (DBMS), so that it can use the information at its disposal to decide how to make use of the two types of devices. This is illustrated in Figure 1. When writing data to storage, the DBMS chooses which type of device to write it to. Previous work has considered how a DBMS should place data in a hybrid storage system [11, 1, 2, 16, 5]. We provide a summary of such work in Section 7. In this paper, we take a broader view of the problem than is used by most of this work. Our view includes the DBMS buffer pool as well as the two types of storage devices. We consider two related problems. The first is determining which data should be retained in the DBMS buffer pool. The answer to this question is affected by the presence of hybrid storage because blocks evicted from the buffer cache to an SSD are much faster to retrieve later than blocks evicted to the HDD. Thus, we consider cost-aware buffer management, which can take this distinction into account. Second, assuming that the SSD is not large enough to hold the entire database, we have the problem of deciding which data should be placed on the SSD. This should depend on the physical access pattern for the data, which depends, in turn, on both the DBMS workload and the management of the DBMS buffer pool. Because we consider both buffer pool management and management of the hybrid storage system, we have more scope for optimization than previous work in this area, at the expense of additional invasiveness in the design and im- plementation of the DBMS. In addition, we must account for the fact that the two problems we consider are mutually dependent. Replacement decisions in the buffer pool depend on the locations (SSD or HDD) of the pages being replaced, since location affects both eviction cost and reloading cost. Conversely, SSD page placement decisions depend on how the page is used, e.g., how frequently it is read or written, which depends in turn on the buffer manager. For exam- ple, under the GD2L replacement policy we propose here, moving a page from the HDD to the SSD may result in a significant increase in the physical read and write rates for that page, since GD2L tends to evict SSD pages quickly from the buffer pool. Our work addresses these dependencies using an anticipa- tory approach to SSD management. When deciding whether to move a page into the SSD, our proposed admission and replacement policy (called CAC) predicts how such a move will affect the physical I/O load experienced by that page. The page is moved into the SSD only if it is determined to be a good candidate under this predicted workload. The DBMS buffer manager then makes cost-aware replacement decisions based on the current placements of buffered pages. In this paper we present the following contributions: ? We present GD2L, a cost-aware algorithm for buffer pool management in database systems with hybrid stor- age systems. GD2L takes the usual concerns of DBMS buffer management (exploiting locality, scan resistance) into account, but also considers the fact that different devices in a hybrid storage system perform differently. GD2L is based on the GreedyDual algorithm [19], but we have restricted GreedyDual to hybrid systems that include only two types of devices. In addition, we have refined GreedyDual for operation in a DBMS environ- ment. ? We present CAC, an anticipatory cost-based technique for managing the SSD. Unlike previous techniques, CAC is intended to work together with a cost-aware buffer manager like GD2L. It expects that moving a page into or out of the SSD will change the access pattern for that page, and it anticipates these changes when making SSD placement decisions. ? We present an empirical evaluation of GD2L and CAC. We have implemented both techniques in MySQL 's InnoDB storage manager. We compare the performance of GD2L with that of InnoDB 's native buffer manager, which is oblivious to the location of pages in a hybrid storage system. We compare CAC to several alternatives, including a non-anticipatory cost-based technique, LRU-2, and MV-FIFO. Our evaluation uses transactional workloads (TPC-C). The remainder of this paper is organized as follows. Section 2 gives an overview of the system architecture that we assume. Section 3 presents the GD2L technique for database buffer pool management, and Section 4 shows empirical re- sults that illustrate the effect of GD2L on the physical access patterns of database pages. Section 5 presents the CAC algorithm for managing the contents of the SSD device(s). The results of our evaluation GD2L and CAC are presented in Section 6, and Section 7 summarizes related work.",Xin Liu,"University of Waterloo, Canada",x39liu@uwaterloo.ca,Kenneth Salem,"University of Waterloo, Canada",ksalem@uwaterloo.ca,,,,,,,,,,,,,,,,,,,,,,,,
20200129,1030,Lu Li ,Department of Computer Science School of Computing National University of Singapore,lilu0355@comp.nus.edu.sg,,Efficient Indexing for Diverse Query Results,"Efficient Indexing for Diverse Query Results, Efficient Indexing for Diverse Query Results, Efficient Indexing for Diverse Query Results, Efficient Indexing for Diverse Query Results, Efficient Indexing for Diverse Query Results, ABSTRACT This paper examines the problem of computing diverse query results which is useful for browsing search results in online shopping applications. The search results are diversified wrt a sequence of output attributes (termed d-order) where an attribute that appears earlier in the d-order has higher priority for diversification. We present a new indexing technique, D-Index, to efficiently compute diverse query results for queries with static or dynamic d-orders. Our performance evaluation demonstrates that our D-Index outperforms the state-of-the-art techniques developed for queries with static or dynamic d-orders. 1. INTRODUCTION Consider a user who is shopping online for a new laptop from a website which can display a result table consisting of up to 20 laptops that match the user's specification. As the number of matching results is typically much larger than number of display records, it is useful to return a diverse set of results for the user to browse. For example, instead of showing the user 20 laptops from only two brands (say Lenovo and Acer), it would be more interesting to show results covering a more diverse range of brands (e.g., Lenovo, Acer, Dell, HP, Asus, Samsung). If Lenovo and Acer are indeed the only two brands of laptops that satisfy the user's query, then it would be better to show a more ""balanced"" distribution of the 20 displayed laptops; for example, showing 10 laptops from each of Lenovo and Acer is better than showing 18 laptops from Lenovo and 2 laptops from Acer. Similarly, if the user is interested only in laptops from Dell, then it would be more interesting to show a diverse range of Dell laptops with different screen sizes instead of showing all Dell laptops with the same screen size. In general, the query results can be diversified wrt a sequence of attributes, say (brand, screen size, . . .), referred to as a d-order, where the intention is to first diversify the results with as many different brands as possible, and for records that belong to the same brand, we diversify them with as many different screen sizes as possible, and so on. Thus, a d-order determines a priority order for diversify- ing the query results, where the first attribute has higher priority to diversify than the second attribute, and so on. Vee, et al. [8] were the first to study the problem of computing diverse query results. They formally define the no- tion of query result diversity and show that existing score based techniques are inadequate to guarantee diverse query results. They also propose an inverted-list based approach to evaluate such queries. However, their work addresses on- ly static diversity queries (SDQ), where the query results are diversified wrt a static, pre-defined d-order. Clearly, it would be useful to allow users to customize their diversification preference. For example, Alice might be more interested to diversify the results wrt laptop color first, followed by brand, whereas Bob might be more interested to diversify the results wrt the number of CPU cores first, followed by battery life and screen size. In this paper, we examine the more general problem of evaluating dynamic diversity queries (DDQ) where the query results are diversified wrt a user specified d-order. A DDQ can be expressed by the following extended SQL syntax: ""S- ELECT ... FROM R WHERE ... DIVERSIFY BY D1, ... , Dn LIMIT k"" which retrieves a diverse set of at most k matching records from a relation R such that the records are diversified wrt a d-order (D1, ... , Dn). The attributes in the SELECT clause must contain all the attributes in the DIVERSIFY BY clause. Our paper makes three key contributions. First, we show that extending existing techniques designed for SDQs [8] to evaluate DDQs is inefficient (Section 4). Second, we intro- duce a novel approach for evaluating diversity queries that is based on the concept of computing a core cover of a query (Section 5.1). Based on this concept, we design a new index method, D-Index, and introduce two index variants, namely, D-tree and D+-tree (Sections 5 and 6). Third, we demonstrate with an experimental evaluation, which is based on a PostgreSQL implementation, that our proposed D-Index technique consistently outperforms [8] for both SDQs as well as DDQs (Section 7). In this paper, we use Q to denote a diversity query on a relation R with d-order  = (D1, ...· , Dm), a set of (possibly empty) selection predicate attributes , and a limit value of k. Our running example data for R is shown in Figure 1(a): the attributes Brand, #Core, ScrnSze, BatLife, and Color represent, respectively, laptop brand (B), number of CPU cores (C), screen size in inches (SS), battery life in hours (BL), and laptop color (LC). ",Lu Li ,Department of Computer Science School of Computing National University of Singapore,lilu0355@comp.nus.edu.sg,Chee-Yong Chan,Department of Computer Science School of Computing National University of Singapore,chancy@comp.nus.edu.sg,,,,,,,,,,,,,,,,,,,,,,,,
20200130,1346,Max Heimel ,Technische Universita ?t Berlin,max.heimel@tu-berlin.de,,Hardware-Oblivious Parallelism for In-Memory Column-Stores,"Hardware-Oblivious Parallelism for In-Memory Column-Stores, Hardware-Oblivious Parallelism for In-Memory Column-Stores, Hardware-Oblivious Parallelism for In-Memory Column-Stores, Hardware-Oblivious Parallelism for In-Memory Column-Stores, Hardware-Oblivious Parallelism for In-Memory Column-Stores,  ABSTRACT The multi-core architectures of today 's computer systems make parallelism a necessity for performance critical applications. Writing such applications in a generic, hardware-oblivious manner is a challenging problem: Current database systems thus rely on labor- intensive and error-prone manual tuning to exploit the full potential of modern parallel hardware architectures like multi-core CPUs and graphics cards. We propose an alternative design for a parallel database engine, based on a single set of hardware-oblivious oper- ators, which are compiled down to the actual hardware at runtime. This design reduces the development overhead for parallel database engines, while achieving competitive performance to hand-tuned systems. We provide a proof-of-concept for this design by integrating op- erators written using the parallel programming framework OpenCL into the open-source database MonetDB. Following this approach, we achieve efficient, yet highly portable parallel code without the need for optimization by hand. We evaluated our implementation against MonetDB using TPC-H derived queries and observed a performance that rivals that of MonetDB 's query execution on the CPU and surpasses it on the GPU. In addition, we show that the same set of operators runs nearly unchanged on a GPU, demonstrating the feasibility of our approach. 1. INTRODUCTION The modern hardware landscape is getting increasingly diverse. Today, a single machine can contain several different parallel pro- cessors like multi-core CPUs or GPUs. This diversity is expected to grow further in the coming years, with micro-architectures them- selves diverging towards highly parallel and heterogeneous designs [8]. We believe that making database engines ready to exploit the capabilities of this diverse landscape of parallel processing platforms will be one of the major challenges for the coming decade in database research. Unfortunately, implementing parallel data operators is a tedious and error-prone task that usually requires extensive manual tuning. Most systems are therefore designed with a certain hardware ar- chitecture in mind: they are hardware-conscious. Extending those systems to new architectures usually requires the developer to im- plement an additional set of hardware-specific operators, adding significant development and maintenance overhead in the process. Instead of maintaining multiple sets of operators, we believe that a parallel database engine can be designed in a hardware-oblivious manner, i.e., without any inherent reliance on a specific architec- ture: All knowledge is encapsulated into a library that adheres to a standardized interface and is provided by the manufacturer of the respective hardware components. The system is designed around a single set of operators, which can be mapped to a variety of paral- lel processing architectures at runtime. We also argue that existing systems can be extended to become hardware-oblivious. To support these claims, we make the following contributions: 1. We present Ocelot1, a hardware-oblivious extension of the open-source column-store MonetDB. Ocelot uses a standard- ized interface provided by OpenCL to map operations to any supported parallel processing architecture. 2. We demonstrate that a single hardware-oblivious implemen- tation of the internal MonetDB operators can efficiently run on such dissimilar devices like CPUs and GPUs. 3. We evaluate our approach against the hand-tuned query pro- cessor of MonetDB and show that Ocelot can compete with MonetDB 's performance when running on a CPU, and outperform it when using the graphics card. The paper is structured as follows: In the next section, we moti- vate and discuss the concept of hardware-oblivious database de- signs. We also give an introduction to the kernel programming model, and motivate why we chose this model for our prototype. In Section 3, we give an overview of the design of Ocelot, with fur- ther implementation details being discussed in Section 4. Section 5 presents our evaluation of Ocelot and discusses the results, Section 6 presents related work. In Section 7, we discuss possible direc- tions for future research. Finally, the paper is concluded by Section 8, which summarizes our findings. ",Max Heimel ,Technische Universita ?t Berlin,max.heimel@tu-berlin.de,Michael Saecker,ParStream GmbH,michael.saecker@ parstream.com,Holger Pirk,CWI Amsterdam,holger.pirk@cwi.nl,Stefan Manegold,CWI Amsterdam,stefan.manegold@cwi.nl,Volker Markl,Technische Universita ?t Berlin,volker.markl@tuberlin.de,,,,,,,,,,,,,,,
20200131,692,Ahmed Eldawy,"Computer Science and Engineering, University of Minnesota, Minneapolis, MN, USA",eldawy@cs.umn.edu,,SpatialHadoop: A MapReduce Framework for Spatial Data,"SpatialHadoop: A MapReduce Framework for Spatial Data, SpatialHadoop: A MapReduce Framework for Spatial Data, SpatialHadoop: A MapReduce Framework for Spatial Data, SpatialHadoop: A MapReduce Framework for Spatial Data, SpatialHadoop: A MapReduce Framework for Spatial Data, Abstract aThis paper describes SpatialHadoop; a full-fledged MapReduce framework with native support for spatial data. SpatialHadoop is a comprehensive extension to Hadoop that injects spatial data awareness in each Hadoop layer, namely, the language, storage, MapReduce, and operations layers. In the language layer, SpatialHadoop adds a simple and expressive high level language for spatial data types and operations. In the storage layer, SpatialHadoop adapts traditional spatial index structures, Grid, R-tree and R+-tree, to form a two-level spatial index. SpatialHadoop enriches the MapReduce layer by two new components, SpatialFileSplitter and SpatialRecordReader, for effi- cient and scalable spatial data processing. In the operations layer, SpatialHadoop is already equipped with a dozen of operations, including range query, kNN, and spatial join. Other spatial operations are also implemented following a similar approach. Extensive experiments on real system prototype and real datasets show that SpatialHadoop achieves orders of magnitude better performance than Hadoop for spatial data processing. I. INTRODUCTION Since its release in 2007, Hadoop was adopted as a solution for scalable processing of huge datasets in many applica- tions, e.g., machine learning [1], graph processing [2], and behavioral simulations [3]. Hadoop employs MapReduce [4], a simplified programming paradigm for distributed processing, to build an efficient large-scale data processing framework. The abstraction of the MapReduce programming simplifies the programming for developers, while the MapReduce framework handles parallelism, fault tolerance, and other low level issues. In the meantime, there is a recent explosion in the amounts of spatial data produced by various devices such as smart phones, satellites, and medical devices. For example, NASA satellite data archives exceeded 500 TB and is still growing [5]. As a result, researchers and practitioners worldwide have started to take advantage of the MapReduce environment in supporting large-scale spatial data. Most notably, in industry, ESRI has released  'GIS Tools on Hadoop ' [6] that work with their flagship ArcGIS product. Meanwhile, in academia, three system prototypes were proposed: (1) Parallel-Secondo [7] as a parallel spatial DBMS that uses Hadoop as a distributed task scheduler, (2) MD-HBase [8] extends HBase [9], a nonrelational database for Hadoop, to support multidimensional indexes, and (3) Hadoop-GIS [10] extends Hive [11], a data warehouse infrastructure built on top of Hadoop with a uni- form grid index for range queries and self-join. ?This work is supported in part by the National Science Foundation, USA, under Grants IIS-0952977 and IIS-1218168. A main drawback in all these systems is that they still deal with Hadoop as a black box, and hence they remain limited by the limitations of existing Hadoop systems. For example, Hadoop-GIS [10], while the most advanced system prototype so far, suffer from the following limitations: (1) Hadoop itself is ill equipped in supporting spatial data as it deals with spatial data in the same way as non-spatial data. Relying on Hadoop as a black box inherits the same limitations and performance bottlenecks of Hadoop. Furthermore, Hadoop- GIS adapts Hive [11], a layer on top of Hadoop, which gives an extra overhead layer over Hadoop itself, (2) Hadoop-GIS can only support uniform grid index, which is applicable only in the rare case of uniform data distribution. (3) Being on-top of Hadoop, MapReduce programs defined through map and reduce cannot access the constructed spatial index. Hence, users cannot define new spatial operations beyond the already supported ones, range query and self-join. Parallel Secondo [7], MD-HBase [8], and ESRI tools on Hadoop [6] suffer from similar drawbacks. In this paper, we introduce SpatialHadoop; a full-fledged MapReduce framework with native support for spatial data; available as open-source [12]. SpatialHadoop overcomes the limitations of Hadoop-GIS and all previous approaches as: (1) SpatialHadoop is built-in Hadoop base code (around 14,000 lines of code inside Hadoop) that pushes spatial constructs and the awareness of spatial data inside the core functionality of Hadoop. This is a key point behind the power and efficiency of SpatialHadoop. (2) SpatialHadoop is able to support a set of spatial index structures including R- tree-like indexing, which is built-in Hadoop Distributed File System (HDFS). This makes SpatialHadoop unique in terms of supporting skewed data distributions in spatial data, and (3) SpatialHadoop users can interact with Hadoop directly to develop a myriad of spatial functions. For example, in this paper, we show range queries, kNN queries, and spatial join. In another work, we show a set of computational geometry techniques that can only be realized using map and reduce functions in SpatialHadoop [13]. This is in contrast to Hadoop- GIS and other systems that cannot support such kind of flexibility, and hence they are very limited in the functions they can support. SpatialHadoop is available as open source [12] and has been already downloaded more than 75,000 times. It has been used by several research labs and industrial companies around the world. Figures 1(a) and 1(b) show how to express a spatial range Objects = LOAD  'points ' AS (id:int, x:int, y:int); Result = FILTER Objects BY x < x2 AND x > x1 AND y < y2 AND y > y1; (a) Range query in Hadoop Objects = LOAD  'points ' AS (id:int, Location:POINT); Result = FILTER Objects BY Overlaps (Location, Rectangle(x1, y1, x2, y2)); (b) Range query in SpatialHadoop Fig. 1. Range query in Hadoop vs. SpatialHadoop query in Hadoop and SpatialHadoop, respectively. The query finds all points located within a rectangular area represented by two corner points   <x1, y1 > and   <x2, y2 >. The first query statement loads an input file of points, while the second statement selects records that overlap with the given range. As Hadoop does not have any spatial indexes, it has to scan the whole dataset to answer the range query, which gives a very bad performance. In particular, it takes 200 seconds on a 20-node Hadoop cluster to process a workload of 60 GB (about 70 M spatial objects). On the other side, SpatialHadoop exploits its built-in spatial indexes to run the same query in about two seconds, which is two orders of magnitude improvement over Hadoop. In addition, the Hadoop program, written in Pig Latin language [14], is less readable due to the lack of spatial data support. SpatialHadoop uses Pigeon [15] language which makes the program simpler and more expressive as it uses spatial data types (POINT and RECTANGLE) and spatial functions (Overlaps). SpatialHadoop is composed of four main layers, namely, language, storage, MapReduce, and operations layers, all injected inside the code base of Hadoop. The language layer provides Pigeon [15], a high level SQL-like language which provides OGC-compliant [16] spatial data types and operations making it easier to adopt by users. The storage layer employs a two-level index structure of global and local indexing. The global index partitions data across computation nodes while the local index organizes data inside each node. This index layout is used to provide three spatial indexes, namely, Grid file, R-tree and R+-tree. To make these indexes accessible to MapReduce programs, SpatialHadoop introduces two new components in the MapReduce layer, namely, SpatialFileSplit- ter and SpatialRecordReader, that exploit the global and local index structures, respectively. Finally, the operations layer encapsulates a dozen of spatial operations that take advantage of the new components in the storage and MapReduce layers. In this paper, we only show the implementation of three basic spatial operations, namely, range query, kNN, and spatial join. A real system prototype of SpatialHadoop (available as open-source at [12]) is extensively evaluated. Experiments run on real spatial datasets extracted from NASA MODIS datasets [5] with a total size of 4.6 TB and 120 Billion records. Both SpatialHadoop and Hadoop are deployed on an internal university cluster as well as an Amazon EC2 cluster. In both platforms, SpatialHadoop has orders of magnitude better performance compared to Hadoop in all tested spatial operations (range query, kNN, and spatial join). This paper is organized as follows: Section II highlights related work. Section III gives the architecture of SpatialHadoop. Details of the language, storage, MapReduce, and operations layers are given in Sections IV-VII. Experiments are given in Section VIII. Section IX concludes the paper. ",Ahmed Eldawy,"Computer Science and Engineering, University of Minnesota, Minneapolis, MN, USA",eldawy@cs.umn.edu,Mohamed F. Mokbel,"Computer Science and Engineering, University of Minnesota, Minneapolis, MN, USA",mokbel@cs.umn.edu,,,,,,,,,,,,,,,,,,,,,,,,
20200201,1194,Ruoming Jin,"Department of Computer Science, Kent State University Kent, OH 44242, USA",jin@cs.kent.edu,,3-HOP: A High-Compression Indexing Scheme for Reachability Query,"3-HOP: A High-Compression Indexing Scheme for Reachability Query, 4-HOP: A High-Compression Indexing Scheme for Reachability Query, 5-HOP: A High-Compression Indexing Scheme for Reachability Query, 6-HOP: A High-Compression Indexing Scheme for Reachability Query, 7-HOP: A High-Compression Indexing Scheme for Reachability Query, ABSTRACT Reachability queries on large directed graphs have attracted much attention recently. The existing work either uses spanning struc- tures, such as chains or trees, to compress the complete transitive closure, or utilizes the 2-hop strategy to describe the reachability. Almost all of these approaches work well for very sparse graphs. However, the challenging problem is that as the ratio of the number of edges to the number of vertices increases, the size of the compressed transitive closure grows very large. In this paper, we pro- pose a new 3-hop indexing scheme for directed graphs with higher density. The basic idea of 3-hop indexing is to use chain structures in combination with hops to minimize the number of structures that must be indexed. Technically, our goal is to find a 3-hop scheme over dense DAGs (directed acyclic graphs) with minimum index size. We develop an efficient algorithm to discover a transitive clo- sure contour, which yields near optimal index size. Empirical stud- ies show that our 3-hop scheme has much smaller index size than state-of-the-art reachability query schemes such as 2-hop and path- tree when DAGs are not very sparse, while our query time is close to path-tree, which is considered to be one of the best reachability query schemes. Categories and Subject Descriptors H.2.8 [Database management]: Database Applications-graph indexing and querying General Terms Performance Keywords Graph indexing, Reachability queries, Transitive closure, 3-Hop, 2-Hop, Path-tree 1. INTRODUCTION The rapid accumulation of very large graphs from a diversity of disciplines, such as biological networks, social networks, ontologies, XML, and RDF databases, among others, calls for the graph database system. Important research issues, ranging theo- retical foundations including algebra and query language [2], to indices for various graph queries [20, 12] and more recently, graph OLAP/summarization [17], have attracted much recent attention. Among them, graph reachability query processing has evolved into a core problem: given two vertices u and v in a directed graph, is there a path from u to v (u""C v)? Graph reachability is one of the fundamental research questions across several disciplines in computer science, such as software engineering and distributed computing. In the database research community, the initial interest in reachability queries has been driven by the need to handle recursive queries, with focus on efficient and ef- fective transitive closure compression. Recently, this problem has captured the attention of database researchers again, due to the increasing importance of XML data management, and fast growing graph data, such as large scale social networks, WWW, and bio- logical networks. For instance, in XML databases, the reachability query is the basic building block for the typical path query for- mat //P1//P2// ... //Pm, where ""//"" is the ancestor-descendant search and Pi is the tag. Reachability queries also have an impor- tant role for managing/querying RDF and domain ontologies. In bioinformatics, reachability queries can be used to answer basic gene regulation questions in the regulatory network. 1.1 Prior Work In order to tell whether a vertex u can reach another vertex v in a directed graph, many approaches have been developed over the years. For a reachability query, we can effectively transform a directed graph into a directed acyclic graph (DAG) by coalescing strongly connected components into vertices and utilizing the DAG to answer the reachability queries. Thus, throughout the paper, we will only focus on DAG. LetG = (V, E) be the DAG for a reachability query. In Table 1.1, we summarize these approaches in terms of their index size, construction time, and query processing time based on worst-case analysis. Here, n is the number of vertices (n = |V |) and m is the number of edges (m = |E|). Parameter k is the width of the chain decomposition of DAG G [11], t is the number of (nontree) edges left after removing all the edges of a spanning tree ofG [19], and k' is the width of the path decomposi- tion [12]. These three parameters k, t and k', are method-specific and will be explained in more detail when we discuss their corre- sponding methods. DFS/BFS and Transitive Closure Computation: We first discuss two classical approaches for reachability query, representing two extremes with regard to index size and query time. DFS/BFS needs to traverse the graph online and can take up to O(n + m) time to answer a reachability query. This is too slow for large graphs. The second approach precomputes the transitive closure of G, i.e., it records the reachability between every pair of vertices in advance. While this approach can answer reachability queries in constant time, its storage cost O(n2) is prohibitive for large graphs. Indeed, tackling the storage cost by effectively compressing the transitive closure has been the major theme of index construction for graph reachability processing. Typically, however, improved compression comes at the cost of slower query answering time. To find the right balance between transitive closure compression and reasonable query answering time is the driving force of ongoing research into graph reachability indexing. The existing research largely falls into two categories: the first category attempts to apply simple graph structures, such as chains and trees, to compress the transitive closure of a DAG. The optimal chain cover, tree cover and the recent path-tree cover all belong to this category. The second category, referred to as 2-hop indexing, tries to encode the reachability using a subset of vertices which serve as intermediaries, i.e., each vertex records a list of intermediate vertices it can reach and a list of intermediate vertices which can reach it. Then, 2-hop reachability means the starting vertex can reach an intermediate vertex (the first hop) and this intermediate vertex can reach the end vertex (the second hop). In the following, we go through these approaches in more detail. Optimal Chain Cover: The basic idea of optimal chain cover is to decompose a DAG into a minimal number of pair-wise disjoint chains, and then assign each vertex in the graph a chain ID and its sequence number in its chain. Given this, if a vertex can reach another chain, it records only the smallest vertex it reaches in that chain. In other words, each vertex in the compressed transitive closure covers the remaining vertices (all the vertices with a higher sequence number) in its respective chain. To determine if vertex u reaches vertex v, we only need to check if u reaches any vertex (say, v') in v's chain, and if yes, we check if the vertex v' has a smaller sequence number than v. This strategy can compress the transitive closure since we need to record at most one vertex in each chain for a given vertex. If the minimal number of chains for a DAG (also referred to as the width of the DAG) is k, then this approach has O(nk) index size and O(log k) query time. Jagadish [11] pioneered the application of chain decomposition in the database research community to compress the transitive clo- sure. He demonstrated that the problem of finding the minimal number of chains from G can be transformed into a network flow problem, which can be solved in O(n3). He also proposed several heuristic algorithms for chain decomposition in order to reduce the computational cost and actual index size. Recently, Cheng [7] pro- posed anO(n2 +kn "" k) time algorithm to decompose a DAG into a minimal number of chains. The worst case complexity of the chain cover approach is clearly decided by the width of DAG. If the width is high, we tend to have a lot of chains with only a small number of vertices, resulting in a high index cost. Another way to look at the compression rate is by observing that each vertex in compressed transitive closure covers a partial chain (from the vertex itself to the last vertex in the chain). Let R(u) be the transitive closure of u. Let RC(u) be the number of vertices u records for the chain decomposition. Then, the compression ratio of the chain decomposition is defined as P uâV |R(u)| P u V |R C(u)| . Thus, we can see that the compression ratio is exactly the average size of the partial chains each vertex in the com- pressed transitive closure covers. Optimal Tree Cover and Its Variants: The optimal tree cover uti- lizes a (spanning) tree to compress the transitive closure [1]. Each vertex in the tree is labeled by a pair of numbers, corresponding to an interval: if a vertex is an ancestor of another vertex in the tree, the interval labeling guarantees that the interval of the first vertex contains the interval of the second vertex. Note that if a vertex reaches the root of a subtree in the original DAG, it will reach all the vertices in the subtree. Thus, for each vertex in the DAG, we can organize all the vertices in its transitive closure, i.e., all the vertices it can reach, into pair-wise disjoint subtrees. To compress the transitive closure, for each subtree, we only need to record its root vertex. To answer the reachability query from vertex u to vertex v, we check if the interval of v is contained by any interval associated with those subtree roots we have recorded for u. Agrawal et al. [1] formally introduced the tree cover and found an optimized algorithm to discover a tree cover which can maxi- mally compress the transitive closure. They also showed that the tree cover approach can provide a better compression rate than the optimal chain cover approach. The advantage of the tree cover ap- proach over the chain cover approach comes from the fact that each tree-cover vertex covers an entire subtree, while each chain-cover vertex covers only a partial chain. Several recent studies focus on the tree cover approach and try to improve either its query processing time and/or provide a smaller index size. Wang et al. [19] develop the Dual-Labeling approach which tries to improve the query time and index size for very sparse graphs, where the number of non-tree edges t is much smaller than the number of vertices n (t << n). Their approach can reduce the index size to O(n + t2) and achieve constant query answering time. Unfortunately, many real world graphs would not satisfy the condition required by this approach, and when t > n, this approach will not help compress the index size. Label+SSPI [4] and GRIPP [18] aim to minimize the index con- struction time and index size. They achieve O(m + n) index con- struction time and O(m + n) index size. However, this is at the sacrifice of the query time, which will cost O(m ? n). Both al- gorithms start by extracting a tree cover and then deploy an online search algorithm utilizing the tree structure to speed up the DFS process. Path-Tree Cover: The latest work to use a simple graph structure to compress transitive closure is the path-tree cover approach, pro- posed by Jin et al. [12], which generalizes the tree cover approach. They observe that the covering capability of each vertex in the compressed transitive closure is determined by the number of parents and children each vertex has in the simple graph structure. For in- stance, a chain vertex has one parent and one child while a tree vertex has one parent and multiple children. The path-tree allows two parents and multiple children. In path-tree cover, all vertices in the original DAG are partitioned into pair-wise disjoint paths (k' is the number of paths in the path-decomposition for a DAG G), and then those paths serve as vertices in a tree structure. In other words, the path-tree utilizes a tree-like structure, where each vertex represents a path in the original DAG. Each vertex in the path-tree needs only three numbers, two numbers for the interval label of the tree-structure and one sequence number from a DFS traversal procedure, to answer the reachability query between any two vertices in the path-tree in constant time. In [12], authors proposed two path-tree schemes, PTree-1 and PTree-2. PTree-1 utilizes optimal tree cover and thus has O(mn) construction time while PTree-2 has O(mk) construction time. Given this, to compress the transitive closure, a vertex u only needs to record vertex v, such that 1) u ""C v and 2) there is no vertex v' such that u ""C v' and v' can reach v in the path-tree. Theoretically, they prove that path-tree cover can always perform the compression of transitive closure better than or equal to the optimal tree cover approaches and chain cover approaches. Note that the enhanced power of the path-tree cover is a consequence of the increased parent/child connectivity of path-tree vertices vs. tree cover or chain cover vertices. 2-HOP Indexing: The 2-hop labeling method proposed by Cohen et al. [9] is quite different from the aforementioned simple graph covering approaches. It compresses the transitive closure using a subset of intermediate vertices. Each vertex records a list of in- termediate vertices it can reach and a list of intermediate vertices which can reach it. The index size is the total number of interme- diate vertices each vertex records. They propose an approximate (greedy) algorithm based on set-covering which can produce a 2- hop cover with size no larger than the minimum possible 2-hop indexing by a logarithmic factor. The minimum 2-hop index size is conjectured to be O?(nm1/2). The major problem of the 2-hop indexing approach is its high construction cost. The greedy set-covering algorithm needs to iter- atively find a subset of vertices which utilizes a candidate vertex as the intermediate hop. The subset of vertices are selected to mini- mize the price measure, i.e., the cost of recording such an interme- diate hop of these vertices with respect to the number of uncovered reachable vertex pairs in this subset. Finding the subset of vertices with minimal price can be transformed into the problem of finding a densest subgraph in a bipartite graph. The approximate algorithm to solve this subproblem is in the linear order with respect to the number of edges in the bipartite graph. Besides, each vertex in the DAG can serve as the intermediate hop which corresponds to a bi- partite graph. Thus, for each iteration, it takesO(n3) to find such a desired subset of vertices. Considering the iteration needs to cover the entire transitive closure Tc, we can see its construction time is O(n3|Tc|). Several approaches have been proposed to reduce its construc- tion time. Schenkel et al. propose the HOPI algorithm, which ap- plies a divide-and-conquer strategy to compute 2-hop labeling [15]. Recently, Cheng et al. propose several methods, such as a geometric- based algorithm [6] and graph partition technique [7], to produce a 2-hop labeling. Though their algorithms significantly speed up the 2-hop construction time, they do not produce the approxima- tion bound of the labeling size which is produced by Cohen et al.'s approach. 1.2 Our Contribution Almost all these approaches work reasonably well for very sparse graphs (where the number of edges is very close to the number of vertices). However, as the ratio of the number of edges to the num- ber of vertices increases, the size of the compressed transitive clo- sure of the simple graph covering approaches can grow very large. In many real world graphs, such as citation networks, the semantic web, and biological networks, the number of edges can be several times the number of vertices. In general, the simple graph covering approach works well only for those DAGs which have a structure similar to the building-block chain, tree, or path-tree structures. However, in many real world graphs, since edge density is much higher than in simple graph structures, many edges will be left un- covered. Vertices of uncovered edges likely need to be recorded as ancillary data in the compressed transitive closure of the DAG, in- creasing the index size. Thus, the size of the compressed transitive closure can become very large as the density grows. The original 2-hop [9] builds on top of the set-covering frame- work and is theoretically appealing as it achieves a guaranteed ap- proximation bound. However, to our knowledge, there is little the- oretical comparison between the 2-hop approach and the simple graph covering approaches in existing research. Most studies do not even empirically compare the 2-hop approach and the simple graph covering approaches. This may be due in part to the 2-hop approach not scaling well to large graphs, even graphs with only thousands of vertices. Specifically, since the original 2-hop needs to compute the complete transitive closure, it becomes very expen- sive as the edge density of the graph becomes larger. Though sev- eral heuristic techniques [15, 6, 7] have been proposed to construct 2-hop faster, they do not guarantee any approximation bound as the original 2-hop does. None of these methods have compared their compression ratio directly with the optimal 2-hop approaches, even on relatively small graphs. To summarize, the major research challenge for existing graph reachability indexing is how to significantly compress the transitive closure when the ratio between the number of edges and the number of vertices increases. Driven by this need, we propose a new 3-hop indexing scheme for directed graphs with higher density. The basic idea in 3-hop indexing is to utilize a simple graph structure, rather than a sole vertex, as an intermediate hop to describe the reachabil- ity between source vertices and destination vertices. In this paper, we focus on the chain structure. The new indexing scheme does not need to compute the entire transitive closure. Instead, it only needs to compute and record a number of so-called ""contour"" ver- tex pairs, which can be orders of magnitude smaller than the size of the transitive closure. Indeed, it is even much smaller than the compressed transitive closure of the chain cover. The connectivity of any pair of vertices in the DAG can be answered by those con- tour vertex pairs. Further, we ""factorize"" these contour vertex pairs by recording a list of ""entry points"" and ""exit points"" on some intermediate chains. We derive an efficient algorithm to generate an index which approximates the minimal 3-hop indexing by a logarithmic factor. Theoretically, we show that 3-hop labeling always has a better minimal compression ratio than 2-hop labeling, and its construction time is much faster than that of 2-hop. We perform a detailed experimental evaluation on both real and synthetic datasets by comparing 3-hop labeling, 2-hop labeling and the state-of-the-art path-tree covering approach. Empirical studies show that our 3-hop scheme has a much smaller index size than prior state-of-art reachability query schemes for dense DAGs when the number of edges is not close to the number of vertices, i.e., |E| 6? |V |. The query processing time of 3-hop is close to path- tree's, which is considered to be one of the best reachability query schemes.",Ruoming Jin,"Department of Computer Science, Kent State University Kent, OH 44242, USA",jin@cs.kent.edu,Yang Xiang,"Department of Computer Science, Kent State University Kent, OH 44242, USA",yxiang@cs.kent.edu,Ning Ruan,"Department of Computer Science, Kent State University Kent, OH 44242, USA",nruan@cs.kent.edu,David Fuhry,"Department of Computer Science, Kent State University Kent, OH 44242, USA",dfuhry@cs.kent.edu,,,,,,,,,,,,,,,,,,
20200202,1347,Alexander Shkapsky,"University of California, Los Angeles",shkapsky@cs.ucla.edu,,Optimizing Recursive Queries with Monotonic Aggregates in DeALS,"Optimizing Recursive Queries with Monotonic Aggregates in DeALS, Optimizing Recursive Queries with Monotonic Aggregates in DeALS, Optimizing Recursive Queries with Monotonic Aggregates in DeALS, Optimizing Recursive Queries with Monotonic Aggregates in DeALS, Optimizing Recursive Queries with Monotonic Aggregates in DeALS, Abstract aThe exploding demand for analytics has refocused the attention of data scientists on applications requiring aggregation in recursion. After resisting the efforts of researchers for more than twenty years, this problem is being addressed by innovative systems that are raising logic-oriented data languages to the levels of generality and performance that are needed to support efficiently a broad range of applications. Foremost among these new systems, the Deductive Application Language System (DeALS) achieves superior generality and performance via new constructs and optimization techniques for monotonic aggregates which are described in the paper. The use of a special class of monotonic aggregates in recursion was made possible by recent theoretical results that proved that they preserve the rigorous least-fixpoint semantics of core Datalog programs. This paper thus describes how DeALS extends their definitions and modifies their syntax to enable a concise expression of applications that, without them, could not be expressed in performanceconducive ways, or could not be expressed at all. Then the paper turns to the performance issue, and introduces novel implementation and optimization techniques that outperform traditional approaches, including Semi-naive evaluation. An extensive experimental evaluation was executed comparing DeALS with other systems on large datasets. The results suggest that, unlike other systems, DeALS indeed combines superior generality with superior performance. I. INTRODUCTION The fast-growing demand for analytics has placed renewed focus on improving support for aggregation in recursion. Aggregates in recursive queries are essential in many important applications and are increasingly being applied in areas such as computer networking [1] and social networks [2]. Many significant applications require iterating over counts or probability computations, including machine learning algorithms for Markov chains and hidden Markov models, and data mining algorithms such as Apriori. Besides these new applications, we can mention a long list of traditional ones such as Bill of Materials (BOM), a.k.a. subparts explosion: this classical recursive query for DBMS requires aggregating the various parts in the part-subpart hierarchy. Finally, we have problems such as computing the shortest paths or counting the number of paths between vertices in a graph, which are now covered as foundations by most CS101 textbooks. Although aggregates were not covered in E.F. Codd 's def- inition of the relational calculi [3], it did not take long before early versions of relational languages such as SQL included support for aggregate functions, namely count, sum, avg, min and max, along with associated constructs such as group-by. However, a general extension of recursive query theory and implementation to allow for aggregates proved an elusive goal, and even recent versions of SQL that provide strong support for OLAP and other advanced aggregates disallow the use of aggregates in recursion and only support queries that are stratified w.r.t. to aggregates. Yet the desirability of extending aggregates to recursive queries was widely recognized early and many partial solutions were proposed over the years for Datalog languages [4]? [10]. The fact that, in general, aggregates are non-monotonic w.r.t. set-containment led to proposals based on non-monotonic theories, such as locally stratified programs and perfect models [11], [12], well-founded models [13] and stable models [14]. An alternative approach was due to Ross and Sagiv [9], who observed that particular aggregates, such as continuous count, are monotonic over lattices other than set-containment and thus can be used in non-stratified programs. However practical difficulties with this approach were soon pointed out, namely that determining the correct lattices by programmers and compilers would be quite difficult [15], and this prevented the deployment of the monotonicity idea in practical query languages for a long time. Fortunately, we recently witnessed some dramatic developments change the situation completely. Firstly, Hellerstein et al., after announcing a resurgence of Datalog, showed that monotonicity in special lattices can be very useful in proving formal properties such as eventual con- sistency [16]. Secondly, we see monotonic aggregates making a strong comeback in practical query languages thanks to the results published in [17], [18] and in [2], summarized below. The formalization of monotonic aggregates proposed in [17], [18] preserves monotonicity w.r.t. set containment, and it is thus conducive to simplicity and performance that follow respectively from the facts that (i) users no longer have to deal with lattices, and (ii) the query optimization techniques, such as Semi-naive and magic sets remain applicable [17]. SociaLite [2] also made an important contribution by showing that shortest-path queries, and other algorithms using aggregates in recursion, can be implemented very efficiently so that in many situations the Datalog approach becomes preferable to that of hand-coding big-data analytics in some procedural language. These dramatic advances represented a major source of opportunities and challenges for our Deductive Application Language (DeAL) and its system DeALS. In fact, unlike the design of the SociaLite system where the performance of recursive graph algorithms with aggregates had played a role, DeALS has been designed as a very general system seeking to satisfy the many needs and lessons that had been learned in the course of a long experience with logic-based data languages, and the LDL [19] and LDL++ [20] experiences in particular. Thus, DeALS supports key non-monotonic con- structs having formal stable model semantics, including, e.g.,  XY-stratification and the choice construct that were found quite useful in program analysis [21], and user-defined aggregates that enabled important knowledge-discovery applications [22]. In addition to a rich set of constructs, DeALS was also designed to support a roster of optimization techniques including magic sets, supplementary magic sets and existential quantification. Introducing powerful new constructs and their optimization techniques by retrofitting a system that already supports a rich set of constructs and optimizations represented a difficult technical challenge. In this paper, we describe how this challenge was met with the introduction of new optimization techniques for monotonic aggregates. We will show that DeALS now achieves both performance and generality, and we will underscore this by comparing not only with SociaLite but also with systems such as DLV [23] and LogicBlox [24] that realize different performance/generality tradeoffs. Overview. The first of two main parts of this paper begins with Section II which presents the syntax and semantics for the min (mmin) and max (mmax) monotonic aggregates. Section III discusses the evaluation and optimization of monotonic aggre- gate programs. Section IV presents implementation details for mmin and mmax and the DeALS storage manager. Section V presents experimental results for Sections II-IV. The second part of this paper begins with Section VI discussing the count (mcount) and sum (msum) monotonic aggregates, followed by their implementation in Section VII and experimental vali- dation in Section VIII. Section IX provides additional DeAL program examples. Section X presents the formal semantics on which our aggregates are based. Additional related works are reviewed in Section XI and we conclude in Section XII. Preliminaries. A Datalog program P is a finite set of rules, or Horn Clauses, where rule r in P has the form A   A1, . . . , An. The atom A is the head of r. A1, . . . , An, the body of r, are literals, or goals, where each literal can be either a positive or negated atom. An atom has the form p(t1, . . . , tj) where p is a predicate and t1, . . . , tj are terms which can be constants, variables or functions. An r with an empty body is a fact. A successful assignment of all variables of rule body goals results in a successful derivation for the rule head predicate. Datalog programs use set semantics and are (typically) stratified (i.e. partitioned into levels based on rule dependencies) and executed in level-by-level order, in a bottom-up fashion. Datalog programs can be evaluated using an iterative approach such as Semi-naive evaluation [10].",Alexander Shkapsky,"University of California, Los Angeles",shkapsky@cs.ucla.edu,Mohan Yang,"University of California, Los Angeles",yang@cs.ucla.edu,Carlo Zaniolo,"University of California, Los Angeles",zaniolo@cs.ucla.edu,,,,,,,,,,,,,,,,,,,,,
20200203,1233,Steven Euijong Whang,"Computer Science Department, Stanford University 353 Serra Mall, Stanford, CA 94305, USA",swhang@cs.stanford.edu,,Entity Resolution with Iterative Blocking,"Entity Resolution with Iterative Blocking, Entity Resolution with Iterative Blocking, Entity Resolution with Iterative Blocking, Entity Resolution with Iterative Blocking, Entity Resolution with Iterative Blocking,  ABSTRACT Entity Resolution (ER) is the problem of identifying which records in a database refer to the same real-world entity. An exhaustive ER process involves computing the similarities between pairs of records, which can be very expensive for large datasets. Various blocking techniques can be used to enhance the performance of ER by dividing the records into blocks in multiple ways and only comparing records within the same block. However, most blocking techniques process blocks separately and do not exploit the results of other blocks. In this paper, we propose an iterative blocking framework where the ER results of blocks are reflected to subsequently processed blocks. Blocks are now iteratively processed until no block contains any more matching records. Compared to simple blocking, iterative blocking may achieve higher accuracy because reflecting the ER results of blocks to other blocks may generate additional record matches. Iterative blocking may also be more efficient because processing a block now saves the processing time for other blocks. We implement a scalable iterative blocking system and demonstrate that iterative blocking can be more accurate and effi- cient than blocking for large datasets. Categories and Subject Descriptors H.2.m [Database Management]: Miscellaneous adata clean- ing ; D.2.8 [Information Storage and Retrieval]: Infor- mation Search and Retrieval aclustering ; H.2.8 [Database Management]: Database Applications adata mining General Terms Algorithms Keywords entity resolution, blocking, iterative blocking 1. INTRODUCTION Entity resolution (ER) is the problem of matching records that represent the same real-world entity and then merging the matching records. ER is a well known problem that arises in many applications. For example, mailing lists may contain multiple entries representing the same physical ad- dress, but each record may be slightly different, e.g., containing different spellings or missing some information. As a second example, two companies that merge may want to combine their customer records: for a given customer that dealt with the two companies they create a composite record that combines the known information. An exhaustive ER process involves comparing all the pairs of records, which can be very expensive for large datasets. Various blocking techniques [19, 24, 5, 16, 13, 11, 15, 2, 10] have been proposed to make ER scalable. Blocking divides the data into (possibly overlapping) blocks and only compares records within the same block, assuming that records in different blocks are unlikely to match. For example, we might partition a set of people records according to the zip codes in address fields. We then only need to compare the records with the same zip code. Since a single blocking criterion may miss matches (e.g., a person may have moved to places with different zip codes), several blocking criteria (i.e., dividing the data in several ways) are typically used to ensure that all the likely matching records are compared, improving the accuracy of the result. Although the previous works above focus on finding the best blocking criteria, most of them assume that all the blocks are processed separately one at a time. In many cases, however, it is useful to exploit an ER result of a previously processed block. First, when two records match and merge in one block, their composite may match with records in other blocks. Second, an ER result of a block can be used to reduce the time of processing another block. That is, the same pair of records may occur in multiple blocks, so once the pair is compared in one block, we can avoid comparing it in other blocks. To address these two points, we propose an iterative blocking framework where the ER result of a block is immediately reflected to other blocks. Unlike previous blocking techniques, there is an additional stage where newly created records of a block are distributed to other blocks. Since the propagation of ER results can generate new record matches in other blocks, the entire operation be- comes iterative in the sense that we are processing blocks (possibly the same block multiple times) until we arrive at a state where we cannot find any more matching records. Our work is thus focused on effectively processing the blocks given a blocking criteria. Motivating Example: Consider the four people records shown in Figure 1, that are to be resolved. We would like to merge records that actually refer to the same person. Suppose that records r and s match with each other be- cause their names are the same, but do not match with t because the strings differ too much. However, once r and s are merged into a new record   <r, s >, the combination of the address and email information of r and s may lead us to discover a new match with t, therefore yielding an initially unforeseen merge   <r, s, t >. Notice that, in order to find this new merge, we need to compare the merged result of r and s with all the other records again. In reality, our dataset can be very large, and it may not be feasible to compare all pairs of the dataset. Hence, we divide the customer records in Figure 1 into blocks. We start by dividing the records by their zip codes. As a result, we only need to compare customers that are in the same geographical region. In Figure 2, the first blocking criterion SC1 uses zip codes to divide the records. Records s and t have the same zip code and are assigned to the block 2 (denoted as b1,2) and records u and v are assigned to b1,3 while r is assigned to b1,1. Since we may miss matches for people who have moved to several places with different zip codes, say we also divide the customer records according to the first characters of their last names. Hence, even if two records referring to the same person have different zip codes, we will have a better chance of comparing them because their last names might be similar. In Figure 2, the matching records r and s can be compared because, although they have different zip codes, they have the same last name. After processing all the blocks, the final result of blocking is {  <r, s >,t,  <u, v >}. Although the blocking of Figure 2 reduces the number of records to compare, it misses the iterative match between   <r, s > and t. Iterative blocking can find this match by dis- tributing the newly created   <r, s > (found in block b2,1) to the other blocks. Assuming that   <r, s > contains the zip codes of both r and s (i.e., 02139 and 94305),   <r, s > is then assigned to both blocks of SC1. In b1,2,   <r, s > can then be compared with t, generating the record   <r, s, t >. Eventually, the final iterative blocking solution becomes {  <r, s, t >,   <u, v >} (see Sec- tion 2.4.3 for details). Thus, the iterative blocking frame- work helps find more record matches compared to simple blocking. Iterative blocking also provides fast convergence. For ex- ample, once the records u and v are merged in b1,3, they do not have to be compared in b2,3. While the blocks in Figure 2 are too small to show any improvements in runtime, we will later demonstrate in our experiments that iterative block- ing can actually run faster than blocking for large datasets when the runtime savings exceed the overhead of iterative blocking. Intuitively, the more work we do for each block, the runtime savings for other blocks become significant. Our example has illustrated the two potential advantages of iterative blocking. The first is improved accuracy, as each iteration may find additional record matches. The second potential advantage is improved runtime performance. By using the ER result of a previous block, we can reduce the time to process other blocks, by avoiding comparisons that were already made. In summary, we make the following contributions. ? We formalize the iterative blocking model (I-BlockER). Unlike blocking, the ER results of blocks are now re- flected to subsequent blocks. The blocks are iteratively processed until no blocks contain any more matching records. I-BlockER can accommodate any  ""core"" ER al- gorithm that resolves records within a single block. ? We present I-BlockER algorithms for two scenarios: ? Lego: An in-memory algorithm that efficiently pro- cesses blocks within memory. ? Duplo: A scalable disk-based algorithm that processes blocks from the disk. ? We experimentally evaluate Lego and Duplo using actual comparison shopping data from Yahoo! Shopping and hotel information data from Yahoo! Travel. Our results show that iterative blocking can improve over blocking both in accuracy and runtime. We evaluate our algo- rithms using two different core ER algorithms, demonstrating the generality of our iterative blocking framework. ",Steven Euijong Whang,"Computer Science Department, Stanford University 353 Serra Mall, Stanford, CA 94305, USA",swhang@cs.stanford.edu,David Menestrina,"Computer Science Department, Stanford University 353 Serra Mall, Stanford, CA 94305, USA",dmenest@cs.stanford.edu,Georgia Koutrika,"Computer Science Department, Stanford University 353 Serra Mall, Stanford, CA 94305, USA",koutrika@cs.stanford.edu,Martin Theobald,"Computer Science Department, Stanford University 353 Serra Mall, Stanford, CA 94305, USA",theobald@cs.stanford.edu,Hector Garcia-Molina,,hector@cs.stanford.edu,,,,,,,,,,,,,,,
20200204,1348,Parag Agrawal,"Stanford University Stanford, CA, USA",paraga@cs.stanford.edu,,Asynchronous View Maintenance for VLSD Databases,"Asynchronous View Maintenance for VLSD Databases, Asynchronous View Maintenance for VLSD Databases, Asynchronous View Maintenance for VLSD Databases, Asynchronous View Maintenance for VLSD Databases, Asynchronous View Maintenance for VLSD Databases, ABSTRACT The query models of the recent generation of very large scale distributed (VLSD) shared-nothing data storage systems, including our own PNUTS and others (e.g. BigTable, Dy- namo, Cassandra, etc.) are intentionally simple, focusing on simple lookups and scans and trading query expressive- ness for massive scale. Indexes and views can expand the query expressiveness of such systems by materializing more complex access paths and query results. In this paper, we examine mechanisms to implement indexes and views in a massive scale distributed database. For web applications, minimizing update latencies is critical, so we advocate deferring the work of maintaining views and indexes as much as possible. We examine the design space, and conclude that two types of view implementations, called remote view tables (RVTs) and local view tables (LVTs), provide a good tradeoff between system throughput and minimizing view staleness. We describe how to construct and maintain such view tables, and how they can be used to implement indexes, group-by-aggregate views, equijoin views and selection views. We also introduce and analyze a consistency model that makes it easier for application developers to cope with the impact of deferred view maintenance. An empiri- cal evaluation quantifies the maintenance costs of our views, and shows that they can significantly improve the cost of evaluating complex queries. Categories and Subject Descriptors H.2.4 [Database Management]: Systems-parallel databases General Terms Performance Keywords indexes, views, distributed and parallel databases 1. INTRODUCTION In order to support the enormous data sizes and query rates seen at web scale, companies have begun to develop massively distributed, shared-nothing databases. Examples of such very large scale distributed (VLSD) systems, de- signed to scale to tens of thousands of nodes, include our own PNUTS [10], as well as Google's BigTable [6], Amazon's Dynamo [11], and Facebook's Cassandra [15]. These systems typically trade away query expressiveness for scala- bility and performance, usually only supporting operations on single records identified through their primary key, and in some cases, range scans on primary key. Since data is partitioned over many servers, executing even moderately complex queries such as a group-by aggregation or a join would incur too much latency and be too detrimental to system throughput. Similarly, looking up a record by a secondary attribute other than the primary key requires executing a full table scan in these systems, thereby making it infeasible. The absence of support for these common queries severely limits the usability of such a database for many web applications. For example, consider a database table of items for sale that is part of a comparison shopping website. The pri- mary key of the table is the listing id of each item, and the website often looks up items by listing id in order to produce a product detail page for each item. However, other types of queries must also be supported. Examples include: ? Find items between $10 and $20. ? Join items with reviews, where reviews are stored in another table. ? Count the number of items for sale in each category. A natural approach to supporting such queries is to create indexes and materialized views. Indexes and views are im- portant in a traditional database to improve performance, but they are critical in a massively distributed database as they are often the only feasible means of answering join, aggregation and secondary-attribute queries. Note that an index can be viewed as one example of a materialized view (in particular, a projection view sorted by one or more sec- ondary attributes). For this reason, we use the term ""views"" in this paper to mean both indexes and materialized views, and use the term ""indexes"" only when we mean the specific kind of materialized view that is used as an index. Synchronous view maintenance can significantly increase the latency of writing to the database. In particular, in a distributed database, both base tables and views are partitioned across many servers. Unless the base and view tables 179 are partitioned identically, synchronously updating a given record on one server may require that several view records on other servers also be updated. Such cross-server communication adds significant latency to requests, especially if one of the involved servers is slow or experiencing a transient failure. If we define multiple views, as is often required in real applications, the latency impact is even higher. Our approach is to defer expensive view maintenance work until after the base update completes. In particular, consider an update that is applied to server S1. While we might perform some local (inexpensive) view maintenance on S1 at the time of the update, we will defer all work involving other servers S2, S3... until we have already returned suc- cess for the base update to the client. This ensures that clients experience low latency on their updates. Deferred view maintenance introduces several challenges: ? We must develop a scalable architecture for storing, main- taining and querying views. ? We do not have the luxury of being able to abort the (already committed) client transaction that updated the base table if failures prevent us from updating the view. Thus, we must ensure that views get updated, even in the presence of failures. ? We must define the consistency guarantees provided by views, which may be out of date with respect to the base table in complex ways. ? Since data in our system is replicated to geographically distant datacenters, we need to efficiently replicate the views as well. In PNUTS, data records are stored in tables, much like relational tables, and tables are partitioned across many servers. In this paper, we explore two mechanisms for main- taining views. The first type, Remote View Tables (RVTs), stores each view in its own PNUTS table, sep- arate from the base tables the views are defined over. Be- cause this view table will be partitioned according to the view's key, not the base table's primary key, view records will likely be stored on different servers than the correspond- ing base records; this is why these views are called ""remote."" Remote views are maintained asynchronously, delaying the generation and application of view updates so as to minimize impact on client update latency. We leverage PNUTS' exist- ing asynchronous replication mechanism to asynchronously propagate updates to RVTs. The second type of view mechanism, Local View Tables (LVTs), keeps view records corresponding to a base record on the same server as the base record. For example, in a view for counting the products for sale by category, each server maintains a local count, grouped by category. To find the total count for, say, the ""Toys"" category, we must retrieve the partial counts from each server and sum them. Although LVTs are maintained synchronously, the maintenance work is performed on the same server as the original update; and the final aggregation of values across partitions is delayed until query time. Failure recovery, system testing and performance opti- mization are already quite difficult in a VLSD system, so we have tried to reuse or extend existing, stable mechanisms in PNUTS whenever possible. In fact, in some cases where two alternative mechanisms are possible, we have preferred the mechanism that is simpler or best uses existing capabilities over a higher performing alternative. In this paper, we study asynchronous view maintenance over a very large scale, horizontally partitioned distributed database, and make the following contributions: ? Two mechanisms (local and remote view tables) for de- ferred view maintenance. ? A characterization of the types of indexes and views that can be maintained by these mechanisms. ? A consistency model for views that are maintained asyn- chronously. ? An experimental evaluation that quantifies the costs of local and remote view maintenance, and their relative benefits for query performance. The rest of this paper is organized as follows. In Section 2, we present an overview of PNUTS. Then, in Section 3, we define RVTs and LVTs, and show how they are maintained. Section 4 describes which views we do and do not support. Then, in Section 5, we present the rationale for the design choices we have made. In Section 6 we describe our consis- tency model. Next, in Section 7 we present an experimental evaluation of our techniques. Section 8 discusses related work. Finally, in Section 9 we present our conclusions.",Parag Agrawal,"Stanford University Stanford, CA, USA",paraga@cs.stanford.edu,Adam Silberstein,"Utkarsh Srivastava and Raghu Ramakrishnan Yahoo! Research Santa Clara, CA, USA",silberst@yahoo-inc.com,Brian F. Cooper,"Utkarsh Srivastava and Raghu Ramakrishnan Yahoo! Research Santa Clara, CA, USA",cooperb@yahoo-inc.com,Utkarsh Srivastava,"Utkarsh Srivastava and Raghu Ramakrishnan Yahoo! Research Santa Clara, CA, USA",utkarsh@yahoo-inc.com,Raghu Ramakrishnan,"Utkarsh Srivastava and Raghu Ramakrishnan Yahoo! Research Santa Clara, CA, USA",ramakris@yahoo-inc.com,,,,,,,,,,,,,,,
20200205,1021,Liang Jeff Chen,"UC San Diego La Jolla, CA, US",jeffchen@cs.ucsd.edu,,Context-sensitive Ranking for Document Retrieval,"Context-sensitive Ranking for Document Retrieval, Context-sensitive Ranking for Document Retrieval, Context-sensitive Ranking for Document Retrieval, Context-sensitive Ranking for Document Retrieval, Context-sensitive Ranking for Document Retrieval,  ABSTRACT We study the problem of context-sensitive ranking for document retrieval, where a context is defined as a sub-collection of documents, and is specified by queries provided by domain-interested users. The motivation of context-sensitive search is that the ranking of the same keyword query generally depends on the context. The reason is that the underlying keyword statistics differ significantly from one context to another. The query evaluation challenge is the computation of keyword statistics at runtime, which involves expensive online aggregations. We appropriately leverage and extend materialized view research in order to deliver algorithms and data structures that evaluate context-sensitive queries efficiently. Specifically, a number of views are selected and materialized, each corresponding to one or more large contexts. Materialized views are used at query time to compute statistics which are used to com- pute ranking scores. Experimental results show that the contextsensitive ranking generally improves the ranking quality, while our materialized view-based technique improves the query efficiency. Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Retrieval models; H.2.4 [Database Management]: Systems-Query processing General Terms Algorithms, Performance Keywords Context-sensitive ranking, materialized views, view selection 1. INTRODUCTION While research in information retrieval (IR) has generated many effective ranking models for general-purpose search, existing rank- ing models may not deliver satisfactory rankings for domain experts. In this paper, we propose a new query model that allows expert users to specify contexts. The ranking of the query result is computed based on keyword statistics collected from the special- ized contexts. This is motivated by the observation that keyword statistics usually vary dramatically from one domain to another and therefore the ranking of the query result will vary accordingly. For example, while ""leukemia"" is rare over the Web, it is a fairly common term in biomedical science (as captured in the PubMed1 database of 18 million articles), and is extremely common among articles of PubMed that are annotated to be cancer-related. Con- ventional ranking models will consider ""leukemia"" as a discrim- inative term, which is not true from the perspective of a cancer researcher or doctor, who typically narrows his/her interest in the cancer-related articles. Given that all ranking models and func- tions use keyword statistics to compute ranking scores, specialized keyword statistics naturally lead to specialized rankings for users interested in narrow domains. 1.1 A Motivating Example PubMed contains 18 million biomedical citations. All the ci- tations include title, abstract, and authors' information. Citations are often linked to full-text articles. Additionally, every citation is annotated with one or more MeSH (Medical Subject Headings) terms from a controlled vocabulary, which specifies a variety of concepts in biomedical science, e.g., ""anatomy"", ""diseases"", ""diagnosis"". MeSH terms in the vocabulary are organized in a hierarchy, as shown in Figure 1. A MeSH term may appear in several places in the hierarchy tree. The vocabulary and the hierarchy of MeSH terms represent an ontology of biomedical science. Each MeSH term represents a biomedical concept and indexes a list of related citations. A com- bination of MeSH terms represents a context that spans the cor- responding concepts. For example, ""neoplasms"" and ""digestive system"" represent two concepts under ""diseases"" and ""anatomy"" respectively. The combination of the two terms identify a set of citations, which form a search context for researchers and doctors concentrating on gastrointestinal (GI) cancer. A researcher or a doctor can specify such a context by utilizing tools that visualize the MeSH term ontology and enable the user to navigate in the ontology and select terms of interest for the context. For example, the tool of Figure 2 mimics the widely used ontology navigator provided by PubMed2 and extends it with the ability to select terms during the navigation. In Figure 2, we see two snap- shots of the ontology navigator, at the point where the user has just selected the context terms. Note that the use of such tools for specifying the context removes the risk of mistyping the context trems, which would otherwise be an important risk since in context-based search only documents that contain the context terms will be retrieved.  Keyword distributions and statistics often vary dramatically from one specialized context to another. For example, research on cancer and research on digestive system have very different terminology. Using keyword statistics from a specialized context in ranking functions will deliver a specialized ranking order for the documents in that context. For example, the classical TF-IDF model uses doc- ument frequency as term weights to boost the ranking of documents that contain query terms that are rare in the collection. The ratio- nale is that rare query terms are more discriminative, and therefore are more important in identifying relevant documents than frequent query terms. In the above example, a query term that is frequent for the citations on ""neoplasms"" may be rare for the citations on ""digestive system"". The ranking order of two documents may be reversed when users are interested in different contexts. Consider the query {pancreas, leukemia}, and two citations C1: ""Complications following pancreas transplant"" and C2: ""Organ failure in patients with acute leukemia"", both annotated with the MeSH term ""digestive system"". Assume we rank the two ci- tations' titles by tf - idf . Since both citations match precisely one single query term, the ranking order of the two citations is only determined by idf . Without a context specification, the fre- quency of leukemia is higher than pancreas in PubMed. Hence, C1 is ranked higher than C2. However, if the query is issued by a GI doctor or researcher, whose focus is on digestive system, the frequency of leukemia is much less than pancreas in the corre- sponding context, and therefore C2 should be ranked higher than C1. Intuitively, pancreas transplant is a common topic among GI researchers. Leukemia in the query is more discriminative in the context. Given that C2 is annotated with ""digestive system"", it is very likely that the organs mentioned in the C2's title refer to digestive organs, which include pancreas. 1.2 Contribution We propose a new query model that extends conventional key- word queries and allows domain-interested users to specify search contexts. A search context is defined as a sub-collection of documents that users are interested in. The goal of context-sensitive ranking is to use keyword statistics based on user-defined contexts to rank documents in the contexts. The query processing of context-sensitive queries presents novel performance challenges that are not met in the processing of conventional keyword queries. In conventional keyword query eval- uation, the context is always fixed; it is the entire document collection. So the statistics are precomputed at indexing time. For context-sensitive ranking, however, contexts are specified by users at query time and can be arbitrary subsets of the document collec- tion. Therefore, the collection-specific statistics (such as document frequency) also have to be computed at query time. A straightforward solution to compute keyword statistics is us- ing standard text search techniques to materialize contexts at query time and gathering required statistics accordingly. Unfortunately, this solution is not always cheap. The challenges are two. First, a common approach to materialize contexts is to intersect inverted lists of keywords, e.g., MeSH terms in PubMed. While intersection operations are efficient for most keyword combinations, intersect- ing very long inverted lists is still expensive [9]. Second, computing keyword statistics not only requires intersections, but also aggregations. As we will show later, some statistics in conventional ranking models demand aggregations of the documents in the contexts, which can be very expensive when the contexts are large. In this paper, we propose a materialized view technique to overcome the above challenges. We reduce the problem of computing keyword statistics to evaluating aggregation queries, and use ma- terialized views to improve query performance. Given that there is a huge number of possible context specifications, the technical challenge is how to choose a reasonable number of views to mate- rialize. We present two algorithms for view selection. The goal is to guarantee good system performance for worst-case queries. Key contributions of the paper include: 1. We propose a novel query model that allows expert users to specify search contexts. The ranking model uses keyword statistics collected from the specified contexts to rank docu- ments in the contexts. 2. We reduce the problem of computing keyword statistics to evaluating aggregation queries, and leverage materialized views to improve query efficiency. Two algorithms are pro- posed to select a number of views to materialize. 3. We perform thorough experiments on the PubMed data set. Results show that context-sensitive ranking improves the ranking quality remarkably, compared with the conventional ranking models. The materialized view technique improves the efficiency of worst-case queries significantly. The overall performance of the system is guaranteed. The paper is organized as follows: Section 2 defines the query model and the ranking model for context-sensitive ranking. Query evaluation is discussed in Section 3. Section 4 reduces the problem of computing keyword statistics to evaluating aggregation queries, and presents a view-based technique to compute statistics. Two view selection algorithms are presented in Section 5 to choose a reasonable number of views to materialize. Experiments and re- sults are elaborated in Section 6. Related work is discussed in Sec- tion 7. Section 8 is the conclusion. ",Liang Jeff Chen,"UC San Diego La Jolla, CA, US",jeffchen@cs.ucsd.edu,Yannis Papakonstantinou,"UC San Diego La Jolla, CA, US",yannis@cs.ucsd.edu,,,,,,,,,,,,,,,,,,,,,,,,
20200206,1349,Michael Loster,"Hasso-Plattner-Institute Potsdam, Germany",michael.loster@hpi.de,,Improving Company Recognition from Unstructured Text by using Dictionaries,"Improving Company Recognition from Unstructured Text by using Dictionaries, Improving Company Recognition from Unstructured Text by using Dictionaries, Improving Company Recognition from Unstructured Text by using Dictionaries, Improving Company Recognition from Unstructured Text by using Dictionaries, Improving Company Recognition from Unstructured Text by using Dictionaries, ABSTRACT While named entity recognition is a much addressed re- search topic, recognizing companies in text is of particular difficulty. Company names are extremely heterogeneous in structure, a given company can be referenced in many dif- ferent ways, their names include person names, locations, acronyms, numbers, and other unusual tokens. Further, in- stead of using the official company name, quite different col- loquial names are frequently used by the general public. We present a machine learning (CRF) system that reli- ably recognizes organizations in German texts. In partic- ular, we construct and employ various dictionaries, regular expressions, text context, and other techniques to improve the results. In our experiments we achieved a precision of 91.11% and a recall of 78.82%, showing significant improve- ment over related work. Using our system we were able to extract 263,846 company mentions from a corpus of 141,970 newspaper articles. 1. FINDING COMPANIES IN TEXT Named entity recognition (NER) defines the task of not only recognizing named entities in unstructured texts but also classifying them according to a predefined set of entity types. The NER task was first defined during the MUC- 6 conference [8], where the objective was to discover gen- eral entity types, such as persons, locations, and organi- zations as well as time, currency, and percentage expres- sions in unstructured texts. Subsequent tasks, such as entity disambiguation, question answering, or relationship extraction (RE), rely heavily on the performance of NER systems, which perform as a preprocessing step. This section highlights the particular difficulties of finding company entities in (German) texts and introduces our in- dustrial use-case, namely risk management based on company- relationship graphs. 1.1 Recognizing company entities Although there is a large body of work on recognizing entities starting from persons and organizations, to entities like gene mentions or chemical compounds, the current re- search often neglects the detection of more fine-grained subcategories, such as person roles or commercial companies. In many cases, the ""standard"" entity classes turn out to be too coarse-grained to be useful in subsequent tasks, such as automatic enterprise valuation, identifying the sentiment towards a particular company, or discovering political and company networks from textual data. What makes recognizing company names particularly difficult is that in contrast to person names they are immensely heterogeneous in their structure. As such, they can be referenced in a multitude of ways and are often composed of many constituent parts, including person names, locations, and country names, industry sectors, acronyms, numbers, and other tokens, which makes them especially hard to recognize. This heterogeneity is expected to be true particularly for the range of medium-sized to small companies. Regarding ex- amples like ""Simon Kucher & Partner Strategy & Marketing Consultants GmbH"", ""Loni GmbH"", or ""Klaus Traeger"", which all are official names of German companies, one can easily see that they vary not only in length and types of their con- stituent parts but also in the position where specific name components appear. In the example ""Clean-Star GmbH & Co Autowaschanlage Leipzig KG"" the legal form ""GmbH & Co KG"" is interleaved with information about the type of the company (carwash) and location information (Leipzig, a city in Germany). What is more, company names are not required to contain specific constituent parts: the example ""Klaus Traeger"" from above is simply the name of a person. It does not provide any additional information apart from the name itself, which leads to ambiguous names that are difficult to identify in practice. Additionally, and in contrast to recognizing named entities from English texts, detecting them in German texts presents itself as an even greater challenge. As pointed out by Faruqui and Pado?, this difficulty is due to the high morphological complexity of the German language, making tasks such as lemmatization much harder to solve [5]. Hence, fea- tures that are highly effective for English often lose their predictive power for German. Capitalization is a prime ex- ample of such a feature. Compared to English, where capi- talization of common nouns serves as a useful indicator for named entities, in German all nouns are capitalized, which drastically lowers the predictive power of the feature. We propose and evaluate a named entity recognizer for German company names by training a conditional random field (CRF) classifier [13]. Besides using different features, Industrial and Applications Paper the fundamental idea is to include domain knowledge into the training phase of the CRF by using different real-world company dictionaries. Transforming the dictionaries into token tries enables us to determine efficiently whether the analyzed text contains companies that are included in the dictionary. During a preprocessing step, we use the token trie to mark all companies in the analyzed text that occur in the used trie. In addition, we automatically extend the dic- tionaries with carefully crafted variants of company names, as we expect them to occur in written text. 1.2 Use case: Risk management using com- pany graphs Among the many possible applications for a company- focused NER system, we focus on modern risk management in financial institutions as one that would benefit from such a system. Named entity recognition and subsequent rela- tionship extraction from text for the purpose of risk man- agement in financial institutions is particularly important in the context of illiquid risk [1]. Illiquid financial risks ba- sically represent contracts between two individuals, e.g., a bank granting a credit over 1 Mio USD (creditor) to a private company (obligor). Because the risk that the credit-taking company will not honor its repayment obligations cannot be easily transferred to other market participants, assessing the creditworthiness of an obligor is of major importance to the relatively small number of its creditors and other business partners. Also, insights gained by one bank on the obligor's ability to pay back are usually not shared. Hence, obtaining adequate and timely information about non-exchange-listed obligors becomes a difficult task for creditors. To circumvent this difficulty, financial institutions rely on the so-called ""insurance principle"": pooling a huge number of independent gains or losses ultimately results in the di- versification of risk, which in turn eliminates almost all of it. Unfortunately, risk mitigation based on the insurance principle relies on the independence assumption between in- dividual gains or losses. At the latest with the financial crisis of 2008/2009, this low dependency assumption has turned out to be devastatingly wrong. Information on the economic dependency structure between contracting parties and assets can be seen as the holy grail of financial risk management. Traditionally, the internal and external data sources used to assess credit risk focus on individual customers, not on the relationships between them. Dependency information is inferred from exposure to common risk factors and thus is inherently symmetric. Direct non-symmetric dependencies, such as supply chains, are not captured. Fortunately, with the growing amount of openly avail- able data sources, there is justified hope that dependency modeling becomes significantly easier by leveraging this vast amount of data. Sadly, most of those data sources are text- based and require considerable effort to extract the con- tained knowledge about relationships and dependencies be- tween the entities of interest. The desired outcome of such an extraction effort can be organized in a graph as shown in Figure 1. The figure shows an example of an actual company graph. To be able to automatically extract such graphs from large amounts of unstructured data, a reliable NER system constitutes the first decisive prerequisite for a following re- lation extraction step. As pointed out at the beginning, the described use case is merely one of many possible use cases, others might include semantic role labeling, machine translation, and question answering systems. 1.3 Contributions and structure We address the problem of recognizing company names from textual data by incorporating dictionary matches into the training process using a feature that represents whether a token is part of a known company name. Our evaluation focusses on analyzing the impact of using a perfect dictio- nary and different real-world dictionaries, as well as the ef- fects of different ways to integrate the knowledge contained in the dictionaries on the performance of the NER system. In particular, we make the following contributions: ? Creation of a NER system capable of successfully rec- ognizing companies in German texts with a precision of 91.11% and a recall of 78.82%. ? Analysis of the impact of various dictionary-based fea- ture strategies on the performance of the NER. The remainder of this paper is organized as follows: Sec- tion 2 discusses related work, while Section 3 presents the baseline configuration for the CRF. In Section 4 we give an overview of the text corpus and the dictionaries we used. We describe the key data structures and technical aspects of the approach in Section 5. Finally, Section 6 presents our experimental results and Section 7 concludes the paper.",Michael Loster,"Hasso-Plattner-Institute Potsdam, Germany",michael.loster@hpi.de,Zhe Zuo,"Hasso-Plattner-Institute Potsdam, Germany",zhe.zuo@hpi.de,Felix Naumann,"Hasso-Plattner-Institute Potsdam, Germany",felix.naumann@hpi.de,Oliver Maspfuhl,"Commerzbank Frankfurt am Main, Germany",oliver.maspfuhl@commerzbank.com,Dirk Thomas,"Commerzbank Frankfurt am Main, Germany",dirk.thomas@commerzbank.com,,,,,,,,,,,,,,,
20200207,1377,Jianbin Qin,"School of Computer Science and Engineering, University of New South Wales",jqin@cse.unsw.edu.au,,Efficient Exact Edit Similarity Query Processing with the Asymmetric Signature Scheme,"Efficient Exact Edit Similarity Query Processing with the Asymmetric Signature Scheme, Efficient Exact Edit Similarity Query Processing with the Asymmetric Signature Scheme, Efficient Exact Edit Similarity Query Processing with the Asymmetric Signature Scheme, Efficient Exact Edit Similarity Query Processing with the Asymmetric Signature Scheme, Efficient Exact Edit Similarity Query Processing with the Asymmetric Signature Scheme, ABSTRACT Given a query string Q, an edit similarity search finds all strings in a database whose edit distance with Q is no more than a given threshold \lamda . Most existing method answering edit similarity queries rely on a signature scheme to generate candidates given the query string. We observe that the number of signatures generated by existing methods is far greater than the lower bound, and this results in high query time and index space complexities. In this paper, we show that the minimum signature size lower bound is \lamda+1. We then propose asymmetric signature schemes that achieve this lower bound. We develop efficient query processing algorithms based on the new scheme. Sev- eral dynamic programming-based candidate pruning meth- ods are also developed to further speed up the performance. We have conducted a comprehensive experimental study in- volving nine state-of-the-art algorithms. The experiment results clearly demonstrate the efficiency of our methods. Categories and Subject Descriptors H.2.4 [Database Management]: Systems!Textual Databases; F.2.2 [Analysis of Algorithms and Problem Complex- ity]: Nonnumerical Algorithms and Problems!Pattern Match- ing General Terms Algorithms, Performance Keywords Approximate Pattern Matching, Similarity Search, Similar- ity Join, Edit Distance, q-gram 1. INTRODUCTION Given a query string Q, an edit similarity search finds all strings in a database whose edit distance with Q is less than a given threshold \lamda . Edit similarity searches have many applications, such as data integration and record linkage, bioinformatics, pattern recognition, and multimedia infor- mation retrieval. For example, ? In bioinformatics, edit similarity search can be employed to find similar protein sequences, and tandem repeats, which are useful to predicting diseases or designing new drugs [19, 27]. ? Batch edit similarity searches, or edit similarity joins, can be used to find near duplicate records in a customer database [2], or near duplicate documents in a document repository [13]. As a result, there has been much interest in efficient algo- rithms to answer edit similarity search or join queries. This is an challenging problem, as edit distance computation is costly and a na?\pyve algorithm that performs edit distance calculation for each string in the database is prohibitively expensive for large databases. To address the performance challenge, most existing ap- proaches adopt the filter-and-verification paradigm based on a signature scheme. A candidate set is generated for the query string by finding database strings that share at least a certain amount of common signatures with the query. Query results can be obtained by verifying the edit distance between each candidate and the query. The numbers of signatures a method generates for data and query strings have a substantial impact on the query performance and index size. We give the numbers of signa- tures for data strings and the query string of several existing approaches in Table 1. Among them, Ed-Join has the small- est signature size with respect to \lamda . It is natural to wonder if this is the minimum signature size, and if not, how we can further reduce the signature size. This paper presents our findings when trying to answer these two questions. First, we propose a framework of sig- nature schemes and the associated query processing method for edit similarity queries. The framework encompasses all major signature-based algorithms for edit similarity queries. We prove that the lower bound on the minimum signature size for any algorithm in this framework is \lamda + 1, where \lamda is the edit distance threshold. Next, we propose a novel sig- nature scheme and corresponding query processing methods for edit similarity queries. Our proposal has three distinct features: (a) its minimum signature size is exactly \lamda + 1, hence reaching the lower bound; (b) it is an asymmetric sig- nature scheme ! by asymmetric, we mean it uses different methods to generate signatures for data and query strings; (c) being asymmetric, we can instantiate two different edit similarity query processing algorithms out of it. Our two methods not only have interesting theoretic properties, but 1033 are also highly efficient in practice. We also develop several candidate pruning techniques that further reduce the num- ber of candidates needing verification. Finally, we perform a comprehensive experimental study comparing our two algo- rithms with nine state-of-the-art algorithms. Our algorithms demonstrate superior performance in most settings. Our contributions can be summarized as: ? We are the first to introduce a general framework to cap- ture the commonalities of many existing algorithms that are based on various kinds of signatures. We also show the lower bound of \lamda + 1 for any algorithm belonging to this framework. ? We propose an asymmetric signature scheme that achieves the lower bound of the number of signatures on the data string or the query string. ? We design two efficient edit similarity query algorithms, IndexChunk and IndexGram, together with several novel candidate pruning algorithms. ? Although many algorithms have been proposed in the past decades on edit similarity queries, to the best of our knowledge, there is no systematic study of their performances. Hence, we conduct a comprehensive experimental study with seven state-of-the-art algorithms for edit similarity queries. Our proposed algorithms have been shown to out- perform existing ones in terms of speed, index size, and robustness. The study also provides a clear picture of the relative performance and space-time tradeoffs of different algorithms. The rest of the paper is organized as follows: Section 2 gives the problem definition and introduces related work. We describe the general framework that summarizes many signature-based edit similarity query algorithms in Section 3. We present an asymmetric signature scheme and show how to use it for edit similarity searches in Section 4. We propose several novel candidate pruning methods in Section 5. Experimental results are presented and analyzed in Section 6. Section 7 concludes the paper. Note that we focus on solving the edit similarity queries exactly in this paper, thus excluding approximate or heuris- tic methods (e.g., Shingling [5], LSH [14], or BLAST [1]).",Jianbin Qin,"School of Computer Science and Engineering, University of New South Wales",jqin@cse.unsw.edu.au,Wei Wang,"School of Computer Science and Engineering, University of New South Wales",weiw@cse.unsw.edu.au,Yifei Lu,"School of Computer Science and Engineering, University of New South Wales",yifeil@cse.unsw.edu.au,Chuan Xiao,"School of Computer Science and Engineering, University of New South Wales",chuanx@cse.unsw.edu.au,Xuemin Lin,"School of Computer Science and Engineering, University of New South Wales",lxue@cse.unsw.edu.au,,,,,,,,,,,,,,,
20200208,1315,Xin Huang,"The Chinese University of Hong Kong, China",xhuang@se.cuhk.edu.hk,,Querying K-Truss Community in Large and Dynamic Graphs,"Querying K-Truss Community in Large and Dynamic Graphs, Querying K-Truss Community in Large and Dynamic Graphs, Querying K-Truss Community in Large and Dynamic Graphs, Querying K-Truss Community in Large and Dynamic Graphs, Querying K-Truss Community in Large and Dynamic Graphs, ABSTRACT Community detection which discovers densely connected structures in a network has been studied a lot. In this paper, we study on- line community search which is practically useful but less studied in the literature. Given a query vertex in a graph, the problem is to find meaningful communities that the vertex belongs to in an online manner. We propose a novel community model based on the k-truss concept, which brings nice structural and computational properties. We design a compact and elegant index structure which supports the efficient search of k-truss communities with a linear cost with respect to the community size. In addition, we investigate the k- truss community search problem in a dynamic graph setting with frequent insertions and deletions of graph vertices and edges. Ex- tensive experiments on large real-world networks demonstrate the effectiveness and efficiency of our community model and search algorithms. Categories and Subject Descriptors H.2.8 [DATABASE MANAGEMENT]: Database Applications! Data mining; G.2.2 [DISCRETE MATHEMATICS]: Graph The- ory!Graph algorithms Keywords k-truss; community search; dynamic graph 1. INTRODUCTION Community structure exists in many real-world networks, for example, social networks and biological networks. Community de- tection, which is to find communities in a network, has been stud- ied a lot in the literature [15, 16, 17, 1, 25]. A different but related problem is online community search, which finds communities con- taining a query vertex in an online manner. These two tasks have different goals: community detection targets all communities in the entire network and usually applies a global criterion to find qualified communities. In contrast, online community search provides personalized community detection for a query vertex. As the com- munities for different vertices in a network may have very differ- Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first pageent characteristics, this user-centered personalized search is more meaningful. Furthermore, as the communities a user participates in represent the social contexts of the user, online community search provides a useful tool for other analytical tasks, such as social circle discovery [14] and social contagion modeling [20]. In this paper, we study the modeling and querying of the communities of a query vertex. A recent study by Cui et al. [9] has proposed a novel approach for online overlapping community search. A new community model was defined as an -adjacency--quasi-k-clique. A -quasi-k-clique is a k-node graph with at least  k(k?1) 2  edges. Another param- eter  is imposed to union two -quasi-k-cliques if they share at least  vertices. Given a query vertex q, the problem is to find all -adjacency--quasi-k-cliques containing q. However, there are several limitations in this community model. 1.  as an average density measure, may not necessarily guar- antee a cohesive community structure. Consider the graph in Figure 1 which is a 0.8-quasi-7-clique containing query ver- tex q. However, q is only connected with one vertex in the community, thus it is not a cohesive community for q obvi- ously. 2. There are three parameters , , k in this model, the setting of which may vary significantly for different query vertices. For example, in a research collaboration network, the com- munities of a famous scholar and a junior scholar can be dra- matically different in terms of the community size and den- sity. Thus it is difficult to choose proper values for the three parameters given a query vertex. 3. Finding -adjacency--quasi-k-clique has been proven to be NP-hard [9], which imposes a severe computational bottle- neck. The approximate algorithms for clique enumeration and expansion [9] reduce the complexity, but cannot give a theoretic guarantee of the approximation quality. Considering these limitations, we propose a novel community model based on the k-truss concept. Given a graph G, the k-truss of G is the largest subgraph in which every edge is contained in at least (k ? 2) triangles within the subgraph [7]. The k-truss is a type of cohesive subgraph defined based on triangle which models the sta- ble relationship among three nodes. However, the k-truss subgraph may be disconnected, for example, the two shaded regions form the 4-truss subgraph in Figure 2(a) which is obviously disconnected. So the k-truss subgraph may not correspond to a meaningful community. On top of the k-truss, we impose an edge connectivity constraint, that is, any two edges in a community either belong to the same triangle, or are reachable from each other through a series of adjacent triangles. Here two triangles are defined as adjacent if they share a common edge. The edge connectivity requirement ensures that a discovered community is connected and cohesive. This defines our novel k-truss community model. To the best of our knowledge, this is the first work that proposes the k-truss commu- nity. Compared with the -adjacency--quasi-k-clique model, our community model has the following advantages. 1. Cohesive community. The k-truss community has cohesive structure according to our analysis in Section 2. For example, the graph in Figure 1 is not a valid k-truss community containing q for k | 3, as the edge (q, s4) is not in any of the triangles. 2. Fewer parameter. Our community model only needs to specify the trussness value k. In addition, a (k + 1)-truss community is contained in a k-truss community. Thus by using different k values for community query, we can get a hierarchical community structure of a query vertex. 3. Polynomial time algorithm. There exist polynomial time algorithms for computing the k-truss subgraphs [7, 21], which make the k-truss community model computationally tractable and efficient. Simply searching k-truss community by its definition may incur a large number of wasteful edge accesses as shown in Section 3.2. Thus the key to efficient k-truss community query processing is to design an effective index. Towards this goal, we first apply an efficient truss decomposition algorithm [21] on a graph G which computes the k-truss subgraphs for all k values. Then we design a novel and elegant index structure, called TCP-Index, to index the pre-computed k-truss subgraphs. The TCP-Index preserves the trussness value and the triangle adjacency relationship in a compact tree-shape index, and supports the query of k-truss community in linear time with respect to the community size, which is optimal. We further study k-truss community search in dynamic graphs, where graph vertices and edges can be frequently inserted or deleted. We present a theoretical analysis to identify the scope in a graph that is affected by edge insertion/deletion. Specifically, we derive a tight upper bound of the trussness for a newly inserted edge, which allows us to precisely identify the affected region with a light cost. Then we design efficient algorithms to update the trussness value and the TCP-Index in the affected region. The incremental up- date algorithms effectively support querying k-truss community in highly dynamic graphs. We conduct extensive experimental studies on large real-world networks and have the following findings. First, the k-truss com- munity search using the TCP-Index is highly efficient in all networks. The query time is from one millisecond for the low degree query vertex to a few seconds for the high degree query vertex which has large and dense communities. The TCP-Index is very compact and can be constructed very efficiently. Second, the TCP-Index can be updated in milliseconds given an edge insertion/deletion. Thus it is highly efficient to support the k-truss com- munity search in dynamic graphs. Last, we evaluate the quality of the discovered communities on two social networks with ground- truth communities and a scientific collaboration network. The re- sults show that our community model can find cohesive and mean- ingful communities of a query vertex. The rest of this paper is organized as follows. We formulate the k-truss community search problem in Section 2. We design a novel TCP-Index and an efficient k-truss community search algorithm in a static graph in Section 3. We further study how to maintain the TCP-Index for query processing in a dynamic graph in Section 4. Extensive experimental results on large real-world networks are reported in Section 5. We discuss related work in Section 6 and conclude this paper in Section 7.",Xin Huang,"The Chinese University of Hong Kong, China",xhuang@se.cuhk.edu.hk,Hong Cheng,"The Chinese University of Hong Kong, China",hcheng@se.cuhk.edu.hk,Lu Qin,"Centre for Quantum Computation and Intelligent Systems, University of Technology, Sydney, Australia",lu.qin@uts.edu.au,Wentao Tian,"The Chinese University of Hong Kong, China",wttian@se.cuhk.edu.hk,Jeffrey Xu Yu,"The Chinese University of Hong Kong, China",yu@se.cuhk.edu.hk,,,,,,,,,,,,,,,
20200209,1350,Chun Chen,"College of Computer Science Zhejiang University, P.R. China",chenc@zju.edu.cn,,TI: An Efficient Indexing Mechanism for Real-Time Search on Tweets,"TI: An Efficient Indexing Mechanism for Real-Time Search on Tweets, TI: An Efficient Indexing Mechanism for Real-Time Search on Tweets, TI: An Efficient Indexing Mechanism for Real-Time Search on Tweets, TI: An Efficient Indexing Mechanism for Real-Time Search on Tweets, TI: An Efficient Indexing Mechanism for Real-Time Search on Tweets, ABSTRACT Real-time search dictates that new contents be made avail- able for search immediately following their creation. From the database perspective, this requirement may be quite easily met by creating an up-to-date index for the contents and measuring search quality by the time gap between insertion time and availability of the index. This approach, however, poses new challenges for micro-blogging systems where thou- sands of concurrent users may upload their micro-blogs or tweets simultaneously. Due to the high update and query loads, conventional approaches would either fail to index the huge amount of newly created contents in real time or fall short of providing a scalable indexing service. In this paper, we propose a tweet index called the TI (Tweet Index), an adaptive indexing scheme for microblog- ging systems such as Twitter. The intuition of the TI is to index the tweets that may appear as a search result with high probability and delay indexing some other tweets. This strategy significantly reduces the indexing cost without com- promising the quality of the search results. In the TI, we also devise a new ranking scheme by combining the relationship between the users and tweets. We group tweets into topics and update the ranking of a topic dynamically. The experi- ments on a real Twitter dataset confirm the efficiency of the TI. Categories and Subject Descriptors H.2.4 [Database Management]: Systems General Terms Algorithms, Design Keywords Real-time Search, Index, Ranking ?In Twitter, tweet refers to the microblog published by users. In this paper, we use it as a common phrase for microblogs. 1. INTRODUCTION The increasing popularity of social networking systems changes the form of information sharing. Instead of issu- ing a query to a search engine, the users log into their social networking accounts and retrieve news, URLs and comments shared by their friends. This is in part caused by the failure of conventional search engines in providing real- time search service for social networking systems. For ex- ample, it is difficult to search a new blog or tweet uploaded a few minutes ago using a conventional search engine. The problem is further amplified in the microblogging systems such as Twitter due to unprecedented amount of tweets or microblogs being posted each day. For example, Tumblr (http://www.tumblr.com) estimated that there were more than 2 million posts and fifteen thousands new users every day1; and based on a latest report from Twitter2, it handled more than 50 million tweets per day. Providing real-time search service is indeed very challeng- ing in large-scale microblogging systems. In such a system, thousands of new updates need to be processed per second. To make every update searchable, we need to index its effect in real time and provide effective and efficient keyword-based retrieval at the same time. The objectives are therefore contradictory since maintenance of up-to-date index will cause severe contention for locks on the index pages. Another problem of real-time search is the lack of effec- tive ranking functions. Figure 1 illustrates an example on the search results of Twitter for the keyword ^IPad2'. The query was submitted a few minutes later after IPad2's sale starts. The user is perhaps looking for the reviews and com- ments about the IPad2, or he is trying to find out the length of queue at the apple stores around his neighborhood. How- ever, most search results are advertisements and most of the returned tweets do not even provide any useful information. This is because the current Twitter search engine sorts the results based on time, and therefore, the latest tweets have the higher rankings. Recall that one key factor of Google's early success is its PageRank [14] algorithm. Without proper ranking functions, the search results are meaningless. How- ever, defining a ranking function for real-time search is not trivial, and the function must have the following two desiderata: 1. The ranking function must consider both the times- tamp of the data and the similarity between the data and the query. As an example, for a given query sub- mitted to Twitter, we do not want to get tweets posted many weeks ago, even though they may contain the keywords of the query. On the other hand, newer tweets with less information are not preferred either. Hence, the ranking function is composed of two inde- pendent factors, time and similarity. 2. The ranking function should be cost-efficient. As we want to support real-time search using a ranking func- tion partially based on time, we have to compute the rankings during query time. Thus, the computation of the ranking function should not incur high overhead. In this paper, we propose the Tweet Index (TI ), a novel indexing and ranking mechanism for enabling real-time search in microblogging systems such as Twitter. The TI is designed based on the observation that most tweets will not appear in the search results. Therefore, we can significantly reduce the indexing cost by delaying indexing less useful tweets. In essence, the TI classifies the tweets into two types, distinguished tweets and noisy tweets. The TI con- sists of two indexing schemes: a real-time indexing scheme for distinguished tweets and a background batch indexing scheme for noisy tweets. Given a new tweet, TI analyzes its contents and determines its type. If it is a distinguished tweet, we will index it immediately. Otherwise, it is grouped with other noisy tweets and periodically, the batch indexing scheme is invoked to index all the noisy tweets in one go. The design principle of the TI is similar in spirit to the par- tial indexing scheme [20, 18], and is also related to the view selection problem [1]. To the best of our knowledge, this is the first proposal that addresses the index issues for the real-time search. In the TI, the ranking function plays the major role in de- ciding whether the tweets are distinguished tweets or noisy tweets and in retrieving meaningful answers. We therefore propose a new ranking function by combining the user graph and tweet graph. In social networks, each user can be con- sidered as a node and different nodes are connected together via the friend links. The user graph denotes the relationship among the users. Naturally, a popular user will have more friends and his/her blogs/tweets also attract wider reader- ship. Therefore, we run a PageRank algorithm for the user graph to compute the ranking for each user. Besides the user graph, the tweets also form a graph, as some tweets are ex- changed between people and some tweets reply to the other tweets. We group tweets into topics based on their relation- ship, and we measure the popularity of the topics based on their statistics. Finally, our proposed ranking function is composed of the user's PageRank, the popularity of topics, the TF (Term Frequency) and the timestamp. The IDF (In- verse Document Frequency) is not used in the TI, since the length of a microblog is fairly small and often capped at certain length (e.g. in Twitter, it is capped at 140 characters). We evaluate the TI by using a real Twitter dataset collected for a user group within the last three years. The experiments examine the performance of our indexing scheme and the effect on the quality of query results. We also compare our ranking function with the other relevant ranking functions. The rest of the paper is organized as follows. In Section 2, we review the previous work in social network search and the corresponding database techniques. In Section 3, we introduce the overview architecture of TI. And the details of the TI's indexing scheme and ranking function are discussed in Section 4 and Section 5, respectively. We evaluate the performance of the proposed schemes in Section 6. And the paper is concluded in Section 7.",Chun Chen,"College of Computer Science Zhejiang University, P.R. China",chenc@zju.edu.cn,Feng Li,"School of Computing National University of Singapore, Singapore",li-feng@comp.nus.edu.sg,Beng Chin Ooi,"School of Computing National University of Singapore, Singapore",ooibc@comp.nus.edu.sg,Sai Wu,"School of Computing National University of Singapore, Singapore",wusai@comp.nus.edu.sg,,,,,,,,,,,,,,,,,,
20200210,1351,Hojjat Jafarpour,"Confluent Inc. Palo Alto, CA",hojjat@confluent.io,,KSQL: Streaming SQL Engine for Apache Kafka,"KSQL: Streaming SQL Engine for Apache Kafka, KSQL: Streaming SQL Engine for Apache Kafka, KSQL: Streaming SQL Engine for Apache Kafka, KSQL: Streaming SQL Engine for Apache Kafka, KSQL: Streaming SQL Engine for Apache Kafka, ABSTRACT Demand for real-time stream processing has been increasing and Apache Kafka has become the de-facto streaming data platform in many organizations. Kafka Streams API along with several other open source stream processing systems can be used to process the streaming data in Kafka, however, these systems have very high barrier of entry and require programming in languages such as Java or Scala. In this paper, we present KSQL, a streaming SQL engine for Apache Kafka. KSQL provides a simple and completely interactive SQL interface for stream processing on Apache Kafka; no need to write code in a programming language such as Java or Python. KSQL is open-source, distributed, scalable, reliable, and real-time. It supports a wide range of powerful stream processing opera- tions including aggregations, joins, windowing, sessionization, and much more. It is extensible using User Defined Functions (UDFs) and User Defined Aggregate Functions (UDAFs). KSQL is implemented on Kafka Streams API which means it provides exactly once delivery guarantee, linear scalability, fault tolerance and can run as a library without requiring a separate cluster. 1 INTRODUCTION In recent years, the volume of data that is generated in organizations has been growing rapidly. From transaction log data in e-commerce platforms to sensor generated events in IoT systems to network monitoring events in IT infrastructures, capturing large volumes of data reliably and processing them in a timely fashion has become an essential part of every organization. This has resulted in an emerging paradigm where organizations have been moving from batch oriented data processing platforms towards realtime stream processing platforms. Initially developed at LinkedIn, Apache Kafka is a battle hardened streaming platform that has been used to capture trillions of events per day [2] [16]. Apache Kafka has become the de-facto streaming platform in many organizations where it provides a scalable and reliable platform to capture and store all the pro- duced data from different systems. It also efficiently provides the captured data to all the systems that want to consume it. While capturing and storing streams of generated data is essential, pro- cessing and extracting insight from this data in timely fashion has become even more valuable. Kafka Streams API along with other open source stream processing systems have been used to perform such real time stream processing. Such real time stream processing systems have been used to develop applications such as Streaming ETL, anomaly detection, real time monitoring and many more. Many of these stream processing systems require users to write code in complex languages such as Java or Scala and can only be used by users who are fluent in such languages. This is a high barrier of entry that limits the usability of such systems. Motivated by this challenge, in this paper we present KSQL, a streaming SQL engine for Apache Kafka that offers an easy way to express stream processing transformations[8]. While the exist- ing open source stream processing systems require expression of stream processing in programming languages such as Java, Scala or Python or offer limited SQL support where SQL statements should be embedded in the Java or Scala code, KSQL offers an interactive environment where SQL is the only language that is needed. KSQL also provides powerful stream processing capa- bilities such as joins, aggregations, event-time windowing, and many more. KSQL is implemented on top of the Kafka Streams API which means you can run continuous queries with no additional cluster; streams and tables are first-class constructs; and you have access to the rich Kafka ecosystem. Similar to addition of SQL to other systems such as Apache Hive[17] and Apache Phoenix[4], we believe that introduction of SQL for stream processing in Kafka will significantly broaden the users base for stream processing and bring the stream processing to the masses. The rest of the paper is organized as the following. In the next section we provide a brief overview on Apache Kafka and the Kafka Streams API. Section 3 presents our contribution in design and development of KSQL. We describe data model, basic concepts, query language and the internals of our SQL engine. In Section 4, we present how KSQL can be extended using UDFs and UDAFs. We describe different execution modes for KSQL in Section 5. We present our experimental evaluation results for KSQL in Section 6. Section 7 describes the related work. We present the future work directions and conclude the paper in Section 8.",Hojjat Jafarpour,"Confluent Inc. Palo Alto, CA",hojjat@confluent.io,Rohan Desai,"Confluent Inc. Palo Alto, CA",rohan@confluent.io,Damian Guy,"Confluent Inc. London, UK",damian@confluent.io,,,,,,,,,,,,,,,,,,,,,
20200211,1269,Udayan Khurana,IBM TJ Watson Research Center,ukhurana@us.ibm.com,,Storing and Analyzing Historical Graph Data at Scale,"Storing and Analyzing Historical Graph Data at Scale, Storing and Analyzing Historical Graph Data at Scale, Storing and Analyzing Historical Graph Data at Scale, Storing and Analyzing Historical Graph Data at Scale, Storing and Analyzing Historical Graph Data at Scale, ABSTRACT The work on large-scale graph analytics to date has largely focused on the study of static properties of graph snapshots. However, a static view of interactions between entities is often an oversimplification of several complex phenomena like the spread of epidemics, information diffusion, formation of online communities, and so on. Being able to find temporal interaction patterns, visualize the evolution of graph properties, or even simply compare snapshots across time, adds significant value in reasoning over graphs. However, due to the lack of underlying data management support, an analyst today has to manually navigate the added temporal complexity of dealing with large evolving graphs. In this paper, we present a system, called Historical Graph Store, that enables users to store large volumes of historical graph data and to express and run com- plex temporal graph analytical tasks against that data. It consists of two key components: (1) a Temporal Graph Index (TGI), that com- pactly stores large volumes of historical graph evolution data in a partitioned and distributed fashion ? TGI also provides support for retrieving snapshots of the graph as of any timepoint in the past or evolution histories of individual nodes or neighborhoods; and (2) a Temporal Graph Analysis Framework (TAF), for expressing com- plex temporal analytical tasks and for executing them in an efficient and scalable manner using Apache Spark. Our experiments demon- strate our system 's efficient storage, retrieval and analytics across a wide variety of queries on large volumes of historical graph data. 1. INTRODUCTION Graphs are useful in capturing behavior involving interactions between entities. Several processes are naturally represented as graphs ? social interactions between people, financial transactions, biological interactions among proteins, geospatial proximity of in- fected livestock, and so on. Many problems based on such graph models can be solved using well-studied algorithms from graph theory or network science. Examples include finding driving routes by computing shortest paths on a network of roads, finding user communities through dense subgraph identification in a social net- work, and many others. Numerous graph data management sys- tems have been developed over the last decade, including special ized graph database systems like Neo4j, Titan, etc., and large-scale graph processing frameworks such as GraphLab [27], Pregel [29], Giraph, GraphX [12], GraphChi [24], etc. However much of the work to date, especially on cloud-scale graph data management systems, focuses on managing and analyzing a single (typically, current) static snapshot of the data. In the real world, however, interactions are a dynamic affair and any graph that abstracts a real-world process changes over time. For in- stance, in online social media, the friendship network on Facebook or the  ""follows "" network on Twitter change steadily over time, whereas the  ""mentions "" or the  ""retweet "" networks change much more rapidly. Dynamic cellular networks in biology, evolving cita- tion networks in publications, dynamic financial transactional net- works, are few other examples of such data. Lately, we have seen an increasing merit in dynamic modeling and analysis of network data to obtain crucial insights in several domains such as cancer prediction [38], epidemiology [15], organizational sociology [16], molecular biology [9], information spread on social networks [26] amongst others. In this work, our focus is on providing the ability to analyze and to reason over the entire history of the changes to a graph. There are many different types of analyses of interest. For example, an an- alyst may wish to study the evolution of well-studied static graph properties such as centrality measures, density, conductance, etc., over time. Another approach is through the search and discovery of temporal patterns, where the events that constitute the pattern are spread out over time. Comparative analysis, such as juxtaposition of a statistic over time, or perhaps, computing aggregates such as max or mean over time, possibly gives another style of knowledge discovery into temporal graphs. Most of all, a primitive notion of just being able to access past states of the graphs and performing simple static graph analysis, empowers a data scientist with the capacity to perform analysis in arbitrary and unconventional patterns. Supporting such a diverse set of temporal analytics and querying over large volumes of historical graph data requires addressing several data management challenges. Specifically, there is a want of techniques for storing the historical information in a compact manner, while allowing a user to retrieve graph snapshots as of any time point in the past or the evolution history of a specific node or a specific neighborhood. Further, the data must be stored and queried in a distributed fashion to handle the increasing scale of the data. There is also a need for an expressive, high-level, easy-to-use programming framework that will allow users to specify complex temporal graph analysis tasks, while ensuring those tasks can be executed efficiently in a data-parallel fashion across a cluster. In this paper, we present a graph data management system, called Historical Graph Store (HGS), that provides an ecosystem for managing and analyzing large historical traces of graphs. HGS consists of two key distinct components. First, the Temporal Graph Index (TGI), is an index that compactly stores the entire history of a graph by appropriately partitioning and encoding the differences over time (called deltas). These deltas are organized to optimize the retrieval of several temporal graph primitives such as neighborhood versions, node histories, and graph snapshots. TGI is designed to use a distributed key-value store to store the partitioned deltas, and can thus leverage the scalability afforded by those systems (our im- plementation uses Apache Cassandra1 key-value store). TGI is a tunable index structure, and we investigate the impact of tuning the different parameters through an extensive empirical evaluation. TGI builds upon our prior work on DeltaGraph [21], where the focus was on retrieving individual snapshots efficiently; TGI ex- tends DeltaGraph to support efficient retrieval of subgraphs instead of only full snapshots, retrieval of histories of nodes or subgraphs over past time intervals, and features a highly scalable design over DeltaGraph. The second component of HGS is a Temporal Graph Analy- sis Framework (TAF), which provides an expressive framework to specify a wide range of temporal graph analysis tasks. TAF is based on a novel set of temporal graph operands and operators that en- able parallel execution of the specified tasks at scale in a cluster environment. The execution engine is implemented on Apache Spark [40], a large-scale in-memory cluster computing framework. Outline: The rest of the paper is organized as follows. In Section 2, we survey the related work on graph data stores, temporal indexing, and other topics relevant to the scope of the paper. In Section 3, we provide a sketch of the overall system, including key aspects of the underlying components. We then present TGI and TAF in detail in Sections 4 and 5, respectively. In Section 6, we provide an empirical evaluation, and and conclude with a summary and a list of future directions in Section 7.",Udayan Khurana,IBM TJ Watson Research Center,ukhurana@us.ibm.com,Amol Deshpande,University of Maryland,amol@cs.umd.edu,,,,,,,,,,,,,,,,,,,,,,,,
20200212,1366,Yuya Sasaki,"Graduate School of Information Science and Technology, Osaka University, Osaka, Japan",sasaki@ist.osaka-u.ac.jp,,Sequenced Route Query with Semantic Hierarchy,"Sequenced Route Query with Semantic Hierarchy, Sequenced Route Query with Semantic Hierarchy, Sequenced Route Query with Semantic Hierarchy, Sequenced Route Query with Semantic Hierarchy, Sequenced Route Query with Semantic Hierarchy,  ABSTRACT The trip planning query searches for preferred routes starting from a given point through multiple Point-of-Interests (PoI) that match user requirements. Although previous studies have investigated trip planning queries, they lack flexibility for finding routes because all of them output routes that strictly match user requirements. We study trip planning queries that output multiple routes in a flexible manner. We propose a new type of query called skyline sequenced route (SkySR) query, which searches for all preferred sequenced routes to users by extending the shortest route search with the semantic similarity of PoIs in the route. Flexibility is achieved by the semantic hierarchy of the PoI category. We propose an efficient algorithm for the SkySR query, bulk SkySR algorithm that simultaneously searches for sequenced routes and prunes unnecessary routes effectively. Experimental evaluations show that the proposed approach significantly outperforms the existing approaches in terms of response time (up to four orders of magnitude). Moreover, we develop a prototype service that uses the SkySR query, and conduct a user test to evaluate its usefulness. 1 INTRODUCTION Recently, technological advances in various devices, such as smart phones and automobile navigation systems, have allowed users to obtain real-time location information easily. This has triggered the development of location-based services such as Foursquare, which exploit rich location information to improve service quality. The users of the location-based services often want to find short routes that pass through multiple Points-of-Interest (PoIs); consequently, developing trip planning queries that can find the shortest routes that passes through user-specified categories has attracted considerable attention [4, 10]. If multiple PoI categories, e.g., restaurant and shopping mall, are in an ordered list (i.e., a cat- egory sequence), the trip planning query searches for a sequenced route that passes PoIs that match the user-specified categories in order. Example 1.1. Figure 1 shows a road networkwith the following PoIs: ""Asian restaurant"", ""Italian restaurant"", ""Gift shop"", ""Hobby shop"", and ""Arts&Entertainment (A&E)"". Assume that a user wants to go to an Asian restaurant, an A&E place, and a gift shop in this order from start point vq . The sequenced route query outputs route R1 because it is the shortest route from vq that satisfied the user requirements ?Asian restaurant, A&E, gift shop?. Existing approaches find the shortest route based on the user query. However, such approaches may find an unexpectedly long route because the found PoIs may be distant from the start point. A major problem with the existing approaches is that they only output routes that perfectlymatch the given categories [5, 14, 16]. To overcome this problem, we introduce flexible similarity match- ing based on PoI category classification to find shorter routes in a flexible manner. In the real-world, category classification often forms a semantic hierarchy, which we refer to as a category tree. For example, in Foursquare 1 , the ""Food"" category tree includes ""Asian restaurant,"" ""Italian restaurant,"" and ""Bakery"" as subcat- egories, and the ""Shop &Service"" category includes ""Gift shop,"" ""Hobby shop,"" and ""Clothing store"" as subcategories (Figure 2). We employ this semantic hierarchy to evaluate routes in terms of two aspects, i.e., route length and the semantic similarity between the categories of the PoIs in the route and those specified in the user query. As a result, we can find effective sequenced routes that semantically match the user requirement based on the se- mantic hierarchy. For example, in Figure 1, route R2 satisfies the user requirement because it semantically matches the category sequence because Italian and Asian restaurants are in the same category tree. However, this approach may find a significantly large number of sequenced routes because the number of PoIs that flexibly match the given categories increases significantly. To reduce the number of routes to be output, we employ the skyline concept [2], i.e., we restrict ourselves to searching for the routes that are not worse than any other routes in terms of their scores (i.e., numerical values to evaluate the routes). Based on this concept, we propose the skyline sequenced route (SkySR) query, which applies the skyline concept to the route length and semantic similarity (i.e., we consider route length and semantic similarity as route scores). Given a start point and a sequence of length and semantic similarity. Example 1.2. Table 1 shows real-world examples of sequenced routes in New York city where a user plans to go to a cupcake shop, an art museum, and then a jazz club in this order. The existing approaches output a single route that matches the user's requirement perfectly. The proposed approach can output three additional routes that are shorter than the route found by the existing approach. Note that the additional routes also satisfy the user query semantically. The user can select a preferred route among all the four routes depending on how far he/she does not want to walk or their available time. The SkySR query can provide effective trip plans; however, it incurs significant computational cost because a large num- ber of routes can match the user requirement. Therefore, the SkySR query requires an efficient algorithm. The challenge is to search for SkySRs efficiently by reducing the search space with- out sacrificing the exactness of the result. We propose bulk SkySR algorithm (BSSR for short) that finds exact SkySRs efficiently. Recall that a feature of SkySRs is that their scores are no worse than those of other sequenced routes. BSSR exploits the branch- and-bound algorithm [9], which effectively prunes unnecessary routes based on the upper and lower bounds of route scores. In addition, to improve efficiency more, we employ four techniques to optimize BSSR. (1) First, we initially find sequenced routes to calculate the upper bound. (2) We tighten the upper bound by arranging the priority queue and (3) tighten the lower bound by introducing minimum distances. (4) we keep intermediate results for later processing, which refer to as on-the-fly caching. Our approach significantly outperforms existing approaches in terms of response time (up to four orders of magnitude) with- out increasing memory usage or sacrificing the exactness of the result. The main contributions of this paper are as follows. ? We introduce a semantic hierarchy to the route search query, which allows us to search for routes flexibly. ? We propose the skyline sequenced route (SkySR) query, which finds all preferred routes related to a specified cate- gory sequence with a semantic hierarchy (Section 4). ? We propose an exact and efficient algorithm and its op- timization techniques to process SkySR queries (Section 5). ? We discuss variations and extensions of the SkySR query. The SkySR query can be applied to various user require- ments and environments (Section 6). ? We demonstrate that the proposed approach works well in terms of response time and memory usage by performing extensive experiments. (Section 7). ? We develop a prototype service that employs the SkySR query and conduct a user test to evaluate usefulness of the SkySR query. (Section 8). The remainder of this paper is organized as follows. Section 2 introduces related work. Section 3 describes the problem formu- lation, and Section 4 defines the SkySR query. Section 5 presents the proposed algorithm. In Section 6, we discuss variations and extensions of the SkySR query. Sections 7 and 8 present experi- ment and user test results, respectively, and Section 9 concludes the paper.",Yuya Sasaki,"Graduate School of Information Science and Technology, Osaka University, Osaka, Japan",sasaki@ist.osaka-u.ac.jp,Yoshiharu Ishikawa,"Graduate School of Information Science, Nagoya University, Nagoya, Japan",ishikawa@i.nagoya-u.ac.jp,Yasuhiro Fujiwara,"NTT Software Innovation Center, Tokyo, Japan",fujiwara.yasuhiro@lab.ntt.co.jp,Makoto Onizuka,"Graduate School of Information Science and Technology, Osaka University, Osaka, Japan",onizuka@ist.osaka-u.ac.jp,,,,,,,,,,,,,,,,,,
20200213,1352,Dimitra Papadimitriou,"University Of Trento Trento, Italy",papadimitriou@disi.unitn.it,,Modeling and Exploiting Goal and Action Associations for Recommendations,"Modeling and Exploiting Goal and Action Associations for Recommendations, Modeling and Exploiting Goal and Action Associations for Recommendations, Modeling and Exploiting Goal and Action Associations for Recommendations, Modeling and Exploiting Goal and Action Associations for Recommendations, Modeling and Exploiting Goal and Action Associations for Recommendations, ABSTRACT Recommender systems are used to identify those items in a large collection that are more likely to be of interest to a user. A com- mon principle of most recommenders is that whatever happened in the past is a good indicator of the future. We offer a different perspective. Considering the fact that in real life users do their selections with certain goals in mind, we recommend items (or actions) that help users fulfilling their intended goals using their past only as a way of identifying goals of interest. We introduce a model that connects goals and actions through action sets im- plementing the respective goals. Such a model captures latent associations among goals and actions and allows the ranking of actions considering different user strategies such as to complete at least one goal with the minimum effort (i.e., minimum number of actions), or to open up more paths for fulfillment of more goals in the future. For each strategy we recommend an algorithm that exploits the user action and goal spaces to rank the actions in a different way. We have performed extensive experimental studies to understand how these techniques are related and compare the results against traditional recommendation methods. The experi- ments illustrate that it is not possible to replicate the results of our approach using existing techniques. 1 INTRODUCTION People are daily facing situations in which they have to make choices from large collections of items. Selecting the best answer to a search engine query among those satisfying the query conditions, selecting a movie to watch, an item to purchase, or friend activities to read about in social media, are only some of the most characteristic examples. Recommender systems [3, 7, 12, 16, 20] give advice to users on items that are likely of interest to them. There are two main categories of recommender systems. The first is the collaborative filtering, which is based on the idea that similar users have similar preferences, thus, the analysis of the choices of similar users can result in successful recommendations of items that have not been selected yet. The second category is the content-based which is based on the idea that users would like items that have similar features with items they have liked in the past. The principle behind both approaches is that whatever the past indicated as preference, it is likely to be preferred also in the future. In this work, we approach the problem based on a different principle. There have been studies in psychology and social sciences [4] that have shown that human actions are not random and unrelated events. They may be of course affected by preferences but they are mainly results of rational selections performed with the purpose of achieving some specific goal that a person has set and aims to fulfill [1]. Based on these studies, we advocate that by recognizing the goals for which actions of the past have been performed, it is possible to identify the driving forces of the users ' future actions and make recommendations that better fit these needs. Since the fulfillment of a specific goal may require actions that are highly different in nature, this form of recommendation may recommend actions that are highly different from those of the past, or from those that similar users have done in the past. Note that we may use the term  ""actions "" and not  ""items "" as typically done in rec- ommender systems; with this option we are being more generic since the selection of an item, the purchase of a product, or the watching of a movie are practically all actions. Existing studies in recommender systems have already recog- nized that methods taking into account similarity with what has happened in the past are not always matching user expectations and have tried different techniques that focus on other aspects such as serendipity, novelty and diversity to improve the quality of recommendations [9]. However, these solutions are not principled and are not driven by some specific, user-selected, well-defined target while in many recommendation scenarios there exist targets that users are willing to reach. For instance, in online learning platforms, users may target at specializations or/and degrees. In employment-oriented social networking services such as LinkedIn users are encouraged to take actions that will lead them into their next position. In addition, they can see how some actions can lead to the same target following different career paths. Moreover, users may perform actions that will lead them to the fulfillment of commercial goals such as to get discount coupons, or everyday goals such as to become fit or to cook. Consider, for instance, the case of a customer in a supermar- ket that has placed in the cart a kilo of potatoes and carrots. A content-based recommendation will try to propose products that are close to what is already in the cart, i.e., similar to potatoes and carrots which means it may propose other kinds of vegetables, or even suggest other types of potatoes. On the other hand, a col- laborative filtering system may suggest light beer or red peppers, because these items have been bought in the past by customers with similar preferences. Both methods, through clearly different routes, recommend items based on the customer 's past. Instead, by taking into account that the items in the customer 's cart can be combined with other items to produce one or more food recipes, the system can open up new options to the customer. For instance, considering a recipe to make an olivier (russian) salad that in- cludes: potatoes, carrots and pickles, an item to be recommended would be pickles. Another useful ingredient would be nutmeg that is a spice used for mashed potatoes and pan-fried carrots, two recipes that require products some of which are already in the customer 's cart. Such a recipe-based recommendation of products may not be justified by similarity to products already in the cart, neither by other product combinations found frequently in the carts of other customers. This means that neither association rules nor techniques that detect correlations among items can be em- ployed to make such recommendations since they highly depend on the popularity of these item sets. So, unless we consider the product combinations found in the recipes, these products will not be recommended by other techniques. Furthermore, given the recipes, the recommendations can be optimized for an overall benefit. For example, recommended products may give the ability to the customers to maximize the number of recipes that they can materialize. Considering goals in the recommendation problem is challenging. The challenge comes from the fact that, in real life, there are typically multiple goals that one needs to fulfill at any given time. Each of these goals may require fewer or more actions in order to be fulfilled, and there may exist alternative ways for the fulfillment of a specific goal. Users have to reason on the priorities between the goals they try to achieve and the benefit they will have by the execution of each action towards the fulfillment of these goals. For instance, some users may prefer actions that help them fulfill a goal as soon as possible, while others may prefer actions that help the advancement of as many goals as possible. A goal-oriented recommender will have to leverage the goals by first recognizing the intended user goals, decide the priorities among them, and quantify the benefit of each action in relationship to the intended goals and in conjunction with the other possible actions. We introduce a new family of recommendation strategies, i.e., goal-based recommendations, that deal with the above challenges. The goal-based strategies identify the goals for which exists evi- dence that the user is aiming at achieving. The evidence originates from the previous user activity, i.e., the actions that the user has already performed. Given this goal space, the strategies explore the sets of actions that lead to the fulfillment of these goals and contain actions that the user has already performed to find actions which the user has not performed and may be willing to complete. The sets of actions together with the goals they fulfill constitute the user 's goal implementation space. The likelihood that the users will like an action from the candidate set of actions in this space depends on their approach towards the goals they would like to fulfill. We have identified three different strategies for exploring and exploiting the user 's spaces in order to select the actions to be recommended. The three strategies correspond to three different policies based on which users often make their selections. The first strategy is the Focus that examines each of the action sets in the user 's goal implementation set to find which of them lead to the fulfillment of the goal that is closest to completion, either because most of the required actions have been already performed (Focuscmp ), or because they require only a few more actions (Focuscl ). Then, it forms the recommendation lists from the actions in these action sets. It is the policy preferred by users that need to fulfill at least one goal through the actions in the current recommendation list. The second strategy, Breadth, is not examining each action set in the user 's goal implementation space separately. It considers more than one set of actions at the same time. Specifically, it evaluates and ranks the actions in the user 's action space based on all the sets this action participates and se- lects those actions that belong in as many sets as possible together with as many as possible actions from the user activity. This strat- egy is for users that would like to fulfill as many goals as possible, if possible, through this recommendation list, but in order to max- imize the number of fulfilled goals, they are willing to complete some or all of them in the future, i.e., not only through the actions in the current recommendation list. This way it keeps some  ""paths "" open for the future (i.e., unfulfilled goals) but those paths contain the minimum number of additional actions. We also suggest a third strategy, the Best Match, that similarly to Breadth is not trying to fulfill at least one goal through the current recommendation list. It recommends actions that contribute to the goals of the user 's goal space. However, in contrast to Breadth, Best Match evaluates an action considering the whole goal space, not only the goals to which this specific action contributes. It generates a profile for the user and estimates a similarity between this profile and the actions to be recommended. The action representation shows how much that action contributes to the fulfillment of the various goals and the user profile how many of the user actions contribute to the various goals. It is a policy that may end up in the fulfillment of many goals in the future. However, it is a strategy for users that are interested in actions that are more useful (contribute more) to the goals to which the user has has put more effort in the past (and respectively less to goals to which the user has put less effort). Our contributions can be summarized as follows: ? We introduce and formally define the notion of goal-oriented recommendation, which evaluates every action considering the goals which the current user may be willing to fulfill and how that action contributes to the fulfillment of one or more of these goals together with other actions of the user (Section 3). ? We explain how it differs from existing techniques and why the latter cannot be used to offer this type of recommenda- tion (Section 2). ? We present different strategies for ranking the candidate actions, with each strategy implementing a different policy in prioritizing the goals and selecting the actions to be recommended (Section 5). ? We describe efficient ways of implementing the above strategies and materializing the goal oriented recommenda- tion paradigm (Section 4). ? We study the effectiveness of our methods and compare them to the state-of-the-art recommendation approaches. We show that goal-based approaches can recommend ac- tions that bring the user closer to the fulfillment of goals that are related to her/him, are highly different from each other and at the same time from actions performed by other users in the past (Section 6).",Dimitra Papadimitriou,"University Of Trento Trento, Italy",papadimitriou@disi.unitn.it,Yannis Velegrakis,"University Of Trento Trento, Italy",velgias@disi.unitn.eu,Georgia Koutrika,"Athena Research Center Athens, Greece",georgia@imis.athena-innovation.gr,,,,,,,,,,,,,,,,,,,,,
20200214,1353,Xuebin He,"Worcester Polytechnic Institute, Computer Science Department, MA, USA",xhe2@cs.wpi.edu,,Discovering Correlations in Annotated Databases,"Discovering Correlations in Annotated Databases, Discovering Correlations in Annotated Databases, Discovering Correlations in Annotated Databases, Discovering Correlations in Annotated Databases, Discovering Correlations in Annotated Databases, ABSTRACT Most emerging applications, especially in science domains, main- tain databases that are rich in metadata and annotation information, e.g., auxiliary exchanged comments, related articles and images, provenance information, corrections and versioning information, and even scientists' thoughts and observations. To manage these annotated databases, numerous techniques have been proposed to extend the DBMSs and efficiently integrate the annotations into the data processing cycle, e.g., storage, indexing, extended query languages and semantics, and query optimization. In this paper, we address a new facet of annotation management, which is the dis- covery and exploitation of the hidden corrections that may exist in annotated databases. Such correlations can be either between the data and the annotations (data-to-annotation), or between the anno- tations themselves (annotation-to-annotation). We make the case that the discovery of these annotation-related correlations can be exploited in various ways to enhance the quality of the annotated database, e.g., discovering missing attachments, and recommend- ing annotations to newly inserted data. We leverage the state-of- art in association rule mining in innovative ways to discover the annotation-related correlations. We propose several extensions to the state-of-art in association rule mining to address new challenges and cases specific to annotated databases, i.e., incremental addition of annotations, and hierarchy-based annotations. The proposed al- gorithms are evaluated using real-world applications from the bio- logical domain, and an end-to-end system including an Excel-based GUI is developed for seamless manipulation of the annotations and their correlations. 1. INTRODUCTION Most modern applications annotate and curate their data with various types of metadata information!usually called annotations, e.g., provenance information, versioning timestamps, execution statistics, related comments or articles, corrections and conflictrelated information, and auxiliary exchanged knowledge from dif- ferent users. Interestingly, the number and size of these annotations is growing very fast, e.g., the number of annotations is around 30x, 120x, and 250x larger than the number of data records in Data Bank biological database [3], Hydrologic Earth database [4, 47], and AKN ornithological database [5], respectively. Existing tech- niques in annotation management, e.g., [9, 15, 17, 21, 24], have made it feasible to systematically capture such metadata annotations and efficiently integrate them into the data processing cy- cle. This includes propagating the related annotations along with queries' answers [9, 15, 17, 24, 46], querying the data based on their attached annotations [21, 24], and supporting semantic annotations such as provenance tracking [11, 14, 20, 43], and belief annotations [23]. Such integration is vey beneficial to higher-level applications as it complements the base data with the auxiliary and semanticrich source of annotations. In this paper, we address a new facet of annotation management that did not receive much attention before and has not been ad- dressed by existing techniques. This facet concerns the discovery and exploitation of the hidden correlations that may exist in anno- tated databases. Given the growing scale of annotated databases! both the base data and the annotation sets!important correlations may exist either between the data values and the annotations, i.e., data-to-annotations correlations, or among the annotations them- selves, i.e., annotations-to-annotations correlations. By systematically discovering such correlations, applications can leverage them in various ways as motivated by the following scenarios. Motivation Scenario 1?Discovery of Missing Attachments: Assume the example biological database illustrated in Figure 1. Typi- cally, many biologists may annotate subsets of the data over time! each scientist focuses only on few genes of interest at a time. For example, some of the data records in Figure 1 are annotated with a ^Black Flag' annotation. This annotation may represent a scien- tific article or a comment that is attached to these tuples. By ana- lyzing the data, we observe that most genes having value F1 in the Family column have an attached ^Black Flag' annotation. Such correlation suggests that gene JW0012 is probably missing this annotation, e.g., none of the biologists was working on that gene and thus the article did not get attached to it. However, by discovering the aforementioned correlation, the system can proactively learn and recommend this missing attachment to domain experts for verification. Correlations may also exist among the annotations themselves, e.g., between the ^Black Flag' and the ^Red Flag' an- notations. Without discovering such correlations the database may become ^under annotated' due to these missing attachments. Motivation Scenario 2?Annotation Maintenance under Evolv- ing Data: Data is always evolving and new records are always added to the database. Hence, a key question is: ^For the newly added data records, do any of the existing annotations apply to them?'. Learning the correlations between the data and the an- notations can certainly help in answering such question. For example, the cloud-shaped comment in Figure 1 is attached to all data records having value 101 in the Exp-Id column. Based on this correlation, the system can automatically predict!at inser- tion time!that this annotation also applies to the newly inserted JW0027 tuple. Otherwise, such attachment can be easily missed and important information is lost. Clearly, delegating such task to end-users without providing system-level support!which is the state of existing annotation management engines!is not a practical assumption. Motivation Scenario 3?Annotation-Driven Exploration: The discovered correlations may reveal information about the under- ling data that trigger further investigation or exploration by do- main experts. For example, as highlighted in Figure 1, the ^Red Flag' annotation semantically means invalid or incorrect data. Since these annotations can be added by different biologists and at different times, none of them may observe a pattern in the data. In contrast, by discovering (and reporting) that the ^Red Flag' an- notation has strong correlation with experiment id 105, the domain experts may re-visit the experimental setup of this wet-lab experiment and may revise and re-validate all data generated from it. These scenarios demonstrate the potential gain from capturing the annotation-related correlations. Unfortunately, relying on do- main experts or DB admins to manually define or capture these correlation patterns is evidently an infeasible approach. This is because the correlations may not be known in advance, hard to cap- ture or express, dynamically changing over time, or even not 100% conformed. Moreover, the manual exploration process is error- prone, will not scale to the size of modern annotated databases, and it is a very time- and resource-consuming process. For example, the UniProt biological database has over 150 people working as full- time to maintain and annotate the database [6, 12]. Certainly, such scale of investments may not be viable to many other domains and scientific groups, e.g., it is reported in a recent science survey [49] that 80.3% of the participant research groups do not have sufficient fund for proper data curation. For these reasons, we argue in this paper that the analysis and discovery of the annotation-related as- sociations and correlations should be an integral functionality of the annotation management engine. As a result, the correlations can be timely discovered and maintained up-to-date, and also sys- tematic actions can be taken based on them as highlighted by the motivation scenarios. In this paper, we investigate applying the well-known techniques of association rule mining, e.g., [8, 31, 52], to the domain of anno- tated databases. This is a new and promising domain for association rule mining due to the following reasons: ? Many emerging applications!especially scientific applications!maintain and rely on large-scale annotated databases [3, 5, 6]. It is reported in [1] that the ebird or- nithological database receives more than 1.6 million annotations per month from scientists and bird watchers worldwide. These applications will benefit from the proposed techniques. ? Many annotated databases go under the very expensive and time-consuming process of manual curation, e.g., [2, 6]. The goal from this process is to ensure that correct annotations and curation information are attached to the data, and to enrich the annotations whenever possible. Nevertheless as illustrated in the motivation scenarios, the discovery of the annotation-related correlations can help in enhancing the quality of the annotated database in an auto- mated way. And hence, reducing the effort needed in the manual curation process and freeing the domain scientists for their main task, which is scientific experimentation. ? Interestingly, annotated databases stretch the traditional tech- niques of association rule mining, and present new challenges as discussed in Section 3. For example, the state-of-art techniques in association rule mining fall short in efficiently handling several new cases specific to annotated databases, i.e., they cannot perform incremental maintenance of the discovered rules and they have to re-process the entire database. These cases include: (1) Generalization of Annotations: Annotations can be free-text comments, which may differ in their values but have the same se- mantics. And hence, discovering the correlations based on the values of the raw annotations may miss important patterns. For ex- ample, referring to Figure 1, the correlation pattern involving the Black Flag annotation, i.e., ^Family:F1 = Black Flag' can be detected based on the raw annotation value. This is because all instances of the Black Flag annotation refer to the same scien- tific article. In contrast, for the Red Flag annotation, the actual annotations inserted by scientists have different values, and thus no correlation pattern can detected based on the raw values. How- ever, by generalizing the annotations to a common concept!the Red Flag annotation in our case!we can detect the correlation pat- ten between them and the experiment Id 105. Therefore, building a generalization hierarchy on top of the annotations is an important step. (2) Integration with the Annotation Manager: We propose to build a coherent integration between the association rule mining module and the Annotation Manager component in contrast to the offline mining techniques. As a result, the Annotation Manager can take informed actions based on the discovered rules, e.g., discover potential missing attachments and report them for verification (Mo- tivation Scenario I), and annotate newly inserted data tuples with existing annotations (Motivation Scenario 2). Moreover, since the Annotation Manager cannot guarantee with 100% confidence that the predicted attachments are correct, we propose developing a ver- ification module that enables domain experts to verify the predicted attachments. (3) Incremental Maintenance under Annotation Addition: In an- notated databases, the discovered correlations and association rules need to be incrementally updated under two scenarios, i.e., the ad- dition of new data tuples, and the addition of new annotations. The former case can be handled by existing techniques that ad- dress the incremental update of association rules, e.g., [16]. These techniques assume that the new delta batch changes the size of the database, i.e., the number of data tuples increases. In contrast, in 504 the latter case, the new annotation batches will not change the num- ber of data tuples, instead they change the content of the tuples! assuming the annotations are part of the tuples. Therefore, the ex- isting incremental techniques need to be extended to handle the latter case. In this work, we develop an end-to-end solution that addresses the above challenges in the context of a real-world application and annotation repository, which is a data warehouse for the Caenorhabditis elegans (C. elegans) Worm from biological sci- ences. To facilitate scientists' usage of the developed system, we designed an Excel-based GUI!A tool that most scientists are fa- miliar with!through which all of the proposed functionalities can be performed. The rest of the paper is organized as follows. In Section 2, we present the needed background, preliminaries, and our case study. In Sections 3, and 4, we present the techniques for the discovery and maintenance of the annotation-related correlations, and their exploitation, respectively. Section 5 overviews the related work while Section 6 contains the experimental evaluation. Finally, the conclusion remarks are included in Section 7.",Xuebin He,"Worcester Polytechnic Institute, Computer Science Department, MA, USA",xhe2@cs.wpi.edu,Stephen Donohue,"Worcester Polytechnic Institute, Computer Science Department, MA, USA",donohues@cs.wpi.edu,Mohamed Y. Eltabakh,"Worcester Polytechnic Institute, Computer Science Department, MA, USA",meltabakh@cs.wpi.edu,,,,,,,,,,,,,,,,,,,,,
20200215,1344,Yongming Luo,"Eindhoven University of Technology, The Netherlands",y.luo@tue.nl,,Efficient and scalable trie-based algorithms for computing set containment relations,"Efficient and scalable trie-based algorithms for computing set containment relations, Efficient and scalable trie-based algorithms for computing set containment relations, Efficient and scalable trie-based algorithms for computing set containment relations, Efficient and scalable trie-based algorithms for computing set containment relations, Efficient and scalable trie-based algorithms for computing set containment relations, Abstract-Computing containment relations between massive collections of sets is a fundamental operation in data management, for example in graph analytics and data mining applications. Motivated by recent hardware trends, in this paper we present two novel solutions for computing secontainment joins over massive sets: the Patricia Trie-based Signature Join (PTSJ) and PRETTI+, a Patricia trie enhanced extension of the state-of-the- art PRETTI join. The compact trie structure not only enables efficient use of main-memory, but also significantly boosts the performance of both approaches. By carefully analyzing the algorithms and conducting extensive experiments with various synthetic and real-world datasets, we show that, in many practical cases, our algorithms are an order of magnitude faster than the state-of-the-art. I. INTRODUCTION Sets are ubiquitous in data processing and analytics. A fun- damental operation on massive collections of sets is computing containment relations. Indeed, bulk comparison of sets finds many practical applications in domains ranging from graph analytical tasks (e.g., [1]?[3]) and query optimization [4] to OLAP (e.g., [5], [6]) and data mining systems [7]. As a simple example, consider an online dating website where each user has an associated profile set listing their characteristics such as hobbies, interests, and so forth. User dating preferences are also indicated by a set of such charac- teristics. By executing a set-containment join of the set of user preferences with the set of user profiles, the dating website can determine all potential dating matches for users, pairing each preference set with all users whose profiles contain all desired characteristics. A concrete illustration can be found in Table I. In this paper we consider efficient and scalable solutions to the following formalization of this common problem. Consider two relations R and S, each having a set-valued attribute set. The set containment join of R and S (R 1 S) is defined as R 1 S = {(r, s) | r \forall R   s \forall S   r.set  s.set}. State of the art: Due to its fundamental nature, the theory and engineering of set containment joins have been intensively studied (e.g., [8]?[18]). Existing solutions fall into two general categories: signature-based and information- retrieval-based (IR) methods. Signature-based methods (e.g., [8]?[12]) encode set information into fixed-length bit strings (called signatures), and perform a containment check on the signatures as an initial filter followed by a validation of the TABLE I: Example of set-containment join. If we per- form a set-containment join (1) between user pro- files and user preferences, we retrieve matching pairs {(u1, p1), (u1, p2), (u2, p3)}. (a) user profiles id set signature u1 {b, d, f, g} 0111 u2 {a, c, h} 1011 u3 {a, c, d} 1011 (b) user preferences id set signature p1 {b, d} 0101 p2 {b, f, g} 0110 p3 {a, c, h} 1011 resulting pairs using actual set comparisons. IR-based methods (e.g., [13]?[16]) build inverted indexes upon sets storing tuple IDs in the inverted lists. A merge join between inverted lists will produce tuples that contain all such set elements. Typically auxiliary indexes are created to accelerate inverted index entry look-ups and joins. Most of the focus of the state-of-the-art algorithms has been on disk-based algorithms (e.g., [11]?[13], [15], [16]). Though these algorithms have proven quite effective for joining mas- sive set collections, the performance of these solutions is bounded by their underlying in-memory processing strategies, where less work has been done (see Section II). For example, PSJ [11] and APSJ [12], two advanced disk-based algorithms, share the same in-memory processing strategy with main- memory algorithm SHJ [8], which we'll discuss in detail in Section II-A. To keep up with ever-increasing data volumes and modern hardware trends we need to push the performance of set-containment join to the next level. Therefore, it is essential to revisit (and develop new) in-memory set-contain- ment join algorithms. Such algorithms will serve both as an essential component for main memory databases [19] as well as building blocks and inspiration for external memory and other computation models and platforms. This is challenging because existing work has already investigated many possi- ble optimization techniques, such as bitwise operations [8], caching [13], reusing result set [14] and so on. Contributions: Nonetheless, by carefully analyzing the existing solutions and bringing in new data structures, in this research we propose two novel in-memory set-containment join algorithms that are in many cases an order of magnitude faster than the previous state-of-the-art. In our study, we scale the relations to be joined along three basic dimensions: set cardinality, domain cardinality, and relation size. Here, set cardinality is the size of set values in the relations; domain cardinality is the size of the underlying domain from which set elements are chosen; and relation size is the number of tuples in each relation. The contributions of our study are as follows: ? We propose two novel algorithms for set-containment join. One is for the low set cardinality, high domain cardinality setting (PRETTI+); the other is for the remaining scenarios (PTSJ). Both algorithms make use of the compact Patricia trie data structure. ? Our PTSJ proposal is a signature-based method. Hence, the length of the signature is a critical param- eter for the algorithm's performance. Therefore, we perform a detailed analysis on PTSJ for determining the proper signature length. We also detail how PTSJ can (1) be easily extended to answer other set-oriented queries, such as set-similarity joins, and (2) efficiently be adapted to disk-based environment. ? We present the results of an extensive empirical study of our solutions on a variety of massive real-world and synthetic datasets which demonstrate that our algorithms in many cases perform an order of magni- tude faster than the previous state-of-the-art and scale well with relation size, set cardinality, and domain cardinality. The rest of the paper is organized as follows. In the next section, we introduce the state-of-the-art solutions for set- containment join. In Sections III and IV we propose PTSJ and PRETTI+, our two new algorithms. Section V presents the results of our empirical study of all algorithms. We then conclude in Section VI with a discussion of future directions for research.",Yongming Luo,"Eindhoven University of Technology, The Netherlands",y.luo@tue.nl,George H.L. Fletcher,"Eindhoven University of Technology, The Netherlands",g.h.l.fletcher@tue.nl,Jan Hidders,"Delft University of Technology, The Netherlands",a.j.h.hidders@tudelft.nl,Paul De Bra,"Eindhoven University of Technology, The Netherlands",debra@win.tue.nl,,,,,,,,,,,,,,,,,,
20200216,1354,Andy Diwen Zhu,School of Computer Engineering Nanyang Technological University Singapore,diwen.zhu@gmail.com,,Reachability Queries on Large Dynamic Graphs: A Total Order Approach,"Reachability Queries on Large Dynamic Graphs: A Total Order Approach, Reachability Queries on Large Dynamic Graphs: A Total Order Approach, Reachability Queries on Large Dynamic Graphs: A Total Order Approach, Reachability Queries on Large Dynamic Graphs: A Total Order Approach, Reachability Queries on Large Dynamic Graphs: A Total Order Approach, ABSTRACT Reachability queries are a fundamental type of queries on graphs that find important applications in numerous domains. Although a plethora of techniques have been proposed for reachability queries, most of them require that the input graph is static, i.e., they are inapplicable to the dynamic graphs (e.g., social networks and the Semantic Web) commonly encountered in practice. There exist a few techniques that can handle dynamic graphs, but none of them can scale to sizable graphs without significant loss of efficiency. To address this deficiency, this paper presents a novel study on reachability indices for large dynamic graphs. We first introduce a general indexing framework that summarizes a family of reachability in- dices with the best performance among the existing techniques for static graphs. Then, we propose general and efficient algorithms for handling vertex insertions and deletions under the proposed framework. In addition, we show that our update algorithms can be used to improve the existing reachability techniques on static graphs, and we also propose a new approach for constructing a reachability index from scratch under our framework. We experimentally evaluate our solution on a large set of benchmark datasets, and we demonstrate that our solution not only supports efficient updates on dynamic graphs, but also provides even better query performance than the state-of-the-art techniques for static graphs. Categories and Subject Descriptors G.2.2 [Graph Theory]: Graph Algorithms General Terms Algorithms, Experimentation 1. INTRODUCTION Given a directed graph G and two vertices s and t in G, a reachability query asks whether there exists a path from s to t in G. Reachability queries are a fundamental operation on graphs and have numerous important applications, such as query processing on social networks, the Semantic Web, XML documents, road networks, and program workflows. Devising index structures for reachability queries is non-trivial, as it requires a careful balanc- ing act between pre-computation cost, index size, and query pro- cessing overhead. In particular, if we pre-compute and store the reachability results for all pairs of vertices, then we can process any reachability query in O(1) time but suffer prohibitive costs of pre- processing and space. On the other hand, if we omit indexing and process reachability queries directly on G using depth-first search (DFS) or breadth-first search (BFS), then we minimize space and pre-computation overhead, but fail to ensure query efficiency on large graphs. Previous work [3?14,16,19,22?25,27?32] has proposed numer- ous indexing techniques to efficiently support reachability queries without significant space and pre-computation overheads. Most techniques, however, assume that the input graphG is static, which makes them inapplicable for the dynamic graphs commonly en- countered in practice. For example, the social graph of Twitter is constantly changing, with thousands of new users added per day; the Semantic Web is frequently updated with new concepts and relations; even road networks are subject to changes due to road closures and constructions. There exist a few techniques [4,12,13,16,22,24,32] that are designed for dynamic graphs, but as we discuss in Sections 3 and 8, none of those techniques can scale to sizable graphs without significant loss of efficiency. Specifically, the methods in [4,12,13,16,22,24] incur prohibitive preprocessing costs on graphs with more than one million vertices. Meanwhile, the approach in [32] can handle million-vertex graphs, but it offers a query performance that is generally not much better than a simple BFS approach, as shown in our experiments. In summary, no existing method is able to effectively handle reachability queries on large dynamic graphs. Motivated by this, we present a comprehensive study on scalable reachability indices that support updates. We first introduce a total order labeling (TOL) framework, which summarizes three most advanced meth- ods [8, 17, 30] for reachability queries on static graphs. TOL has two important properties: (i) every reachability index under TOL uniquely corresponds to a total order of vertices in the input graph, and (ii) the total order solely decides the index 's performances in terms of preprocessing, space, and queries. Given these properties, we investigate algorithms that enable us to insert or delete a vertex in a TOL index without changing the order of the other vertices, i.e., without significantly degrading the performance of the index. This results in general algorithms for handling insertions and dele- tions on indices under TOL. In particular, our insertion algorithm is optimal in that it leads to the minimum index size after insertion. Interestingly, we observe that our update algorithms can be utilized to reduce the space consumptions and query costs of a TOL index, by adjusting the total order pertinent to the index. This leads to a general approach for improving any index under TOL, includ- 1323 ing the state-of-the-art techniques [8, 17, 30]. The effectiveness of our adjusting approach shows that the total orders of the techniques in [8, 17, 30] leave much room for enhancement, which motivates us to devise new methods for deriving improved total orders for TOL indices. As a result, we present a new reachability index, But- terfly, which offers reduced preprocessing, space, and query costs than any existing indices under TOL [8,17,30]. We experimentally evaluate TOL using a large variety of benchmark datasets with up to twenty million vertices, and we demonstrate the superiority of TOL against alternative solutions for static and dynamic graphs. In summary, this paper makes the following contributions: ? We propose general and efficient algorithms that enable any index under the TOL framework to support large dynamic graphs (Section 5). ? We develop a technique that can postprocess the state-of-the- art reachability indices [8, 17, 30] to significantly enhance their performances in terms of space overheads and query efficiency (Section 6). ? We devise algorithms to derive improved vertex ordering un- der TOL, based on which we propose Butterfly, a new reach- ability index that dominates the states of the art (Section 7). ? We evaluate our solution on a large set of real and synthetic graphs, and we demonstrate that our solution not only supports efficient updates on large dynamic graphs, but also provides even better query performance than the state-of-the-art techniques for static graphs (Section 8).",Andy Diwen Zhu,School of Computer Engineering Nanyang Technological University Singapore,diwen.zhu@gmail.com,Wenqing Lin,School of Computer Engineering Nanyang Technological University Singapore,keamoulin@gmail.com,Sibo Wang,School of Computer Engineering Nanyang Technological University Singapore,wangsibo.victor@gmail.com,Xiaokui Xiao,School of Computer Engineering Nanyang Technological University Singapore,xkxiao@ntu.edu.sg,,,,,,,,,,,,,,,,,,
20200217,1355, Jeff LeFevre,"University of California, Santa Cruz",jlefevre@ics.uci.edu,,Opportunistic Physical Design for Big Data Analytics,"Opportunistic Physical Design for Big Data Analytics, Opportunistic Physical Design for Big Data Analytics, Opportunistic Physical Design for Big Data Analytics, Opportunistic Physical Design for Big Data Analytics, Opportunistic Physical Design for Big Data Analytics, ABSTRACT Big data analytical systems, such as MapReduce, perform aggressive materialization of intermediate job results in order to support fault tolerance. When jobs correspond to exploratory queries submitted by data analysts, these materializations yield a large set of materialized views that we propose to treat as an opportunistic physical design. We present a semantic model for UDFs that enables effective reuse of views containing UDFs along with a rewrite algorithm that provably finds the minimum-cost rewrite under certain assumptions. An experimental study on real-world datasets using our prototype based on Hive shows that our approach can result in dramatic performance improvements. 1. INTRODUCTION Data analysts have the crucial task of analyzing the ever increasing volume of data that modern organizations collect in order to produce actionable insights. As expected, this type of analysis on big data is highly exploratory in nature and involves an iterative process: the data analyst starts with an initial query over the data, examines the results, then reformulates the query and may even bring in additional data sources, and so on [9]. Typically, these queries involve sophis- ticated, domain-specific operations that are linked to the type of data and the purpose of the analysis, e.g., performing sentiment analysis over tweets or computing network influence. Because a query is often revised multiple times in this scenario, there can be significant overlap between queries. There is an opportunity to speed up these explo- rations by reusing previous query results either from the same analyst or from different analysts performing a related task. MapReduce (MR) has become a de-facto tool for this type of analysis. It offers scalability to large datasets, easy incorporation of new data sources, the ability to query right away without defining a schema up front, and extensibility through user-defined functions (UDFs). An- alyst queries are often written in a declarative query language, e.g., HiveQL or PigLatin, which are automatically translated to a set of MR jobs. Each MR job involves the materialization of intermediate results (the output of mappers, the input of reducers and the output of reducers) for the purpose of failure recovery. A typical Hive or Pig query will spawn a multi-stage job that will involve several such materializations. We refer to these execution artifacts as opportunistic materialized views. We propose to treat these views as an opportunistic physical de- sign and to use them to rewrite queries. The opportunistic nature of our technique has several nice properties: the materialized views are generated as a by-product of query execution, i.e., without additional overhead; the set of views is naturally tailored to the current work- load; and, given that large-scale analysis systems typically execute a large number of queries, it follows that there will be an equally large number of materialized views and hence a good chance of finding a good rewrite for a new query. Our results indicate the savings in query execution time can be dramatic: a rewrite can reduce execution time by up to an order of magnitude. Rewriting a query using views in the context of MR involves a unique combination of technical challenges that distinguish it from the traditional problem of query rewriting. First, the queries and views almost certainly contain UDFs, thus query rewriting requires some semantic understanding of UDFs. These MR UDFs for big data anal- ysis are composed of arbitrary user-code and may involve a sequence of MR jobs. Second, any query rewriting algorithm that can utilize UDFs now has to contend with a potentially large number of operators since any UDF can be included in the rewriting process. Third, there can be a large search space of views to consider for rewriting due to the large number of materialized views in the opportunistic physical design, since they are almost free to retain (storage permitting). Recent methods to reuse MR computations such as ReStore [6] and MRShare [21] lack any semantic understanding of execution artifacts and can only reuse/share cached results when execution plans are syn- tactically identical. We strongly believe that any truly effective so- lution will have to a incorporate a deeper semantic understanding of cached results and  ""look into "" the UDFs as well. Contributions. In this paper we present a novel query-rewrite algorithm that targets the scenario of opportunistic materialized views in an MR system with queries that contain UDFs. We propose a UDF model that has a limited semantic understanding of UDFs, yet enables effec- tive reuse of previous results. Our rewrite algorithm employs tech- niques inspired by spatial databases (specifically, nearest-neighbor searches in metric spaces [12]) in order to provide a cost-based in- cremental enumeration of the huge space of candidate rewrites, gen- erating the optimal rewrite in an efficient manner. Specifically, our contributions can be summarized as follows: ? A gray-box UDF model that is simple but expressive enough to capture a large class of MR UDFs that includes many common analysis tasks. The UDF model further provides a quick way to compute a lower-bound on the cost of a potential rewrite given just the query and view definitions. We provide the model and the types of UDFs it admits in Sections 3?4. ? A rewriting algorithm that uses the lower-bound to (a) gradually explode the space of rewrites as needed, and (b) only attempts a rewrite for those views with good potential to produce a low-cost rewrite. We show that the algorithm produces the optimal rewrite as well as finds this rewrite in a work-efficient manner, under certain assumptions. We describe this further in Sections 6?7. ? An experimental evaluation showing that our methods provide execution time improvements of up to an order of magnitude using real-world data and realistic complex queries containing UDFs. The execution time savings of our method are due to moving much less data and avoiding the high expense of re-reading data from raw logs when possible. We describe this further in Section 8.", Jeff LeFevre,"University of California, Santa Cruz",jlefevre@ics.uci.edu,Jagan Sankaranarayanan,"NEC Labs America, Cupertino, CA",alkis@ics.uci.edu,Hakan Hacg?m? s,"NEC Labs America, Cupertino, CA",jagan@ics.uci.edu,Junichi Tatemura,"NEC Labs America, Cupertino, CA",hakan@ics.uci.edu,Neoklis Polyzotis,"University of California, Santa Cruz",tatemura@ics.uci.edu,Michael J. Carey,"University of California, Irvine",mjcarey@ics.uci.edu,,,,,,,,,,,,
20200218,789,Cristian Consonni,University of Trento,cristian.consonni@unitn.it,,Discovering Order Dependencies through Order Compatibility,"Discovering Order Dependencies through Order Compatibility, Discovering Order Dependencies through Order Compatibility, Discovering Order Dependencies through Order Compatibility, Discovering Order Dependencies through Order Compatibility, Discovering Order Dependencies through Order Compatibility, ABSTRACT A relevant task in the exploration and understanding of large datasets is the discovery of hidden relationships in the data. In particular, functional dependencies have received considerable attention in the past. However, there are other kinds of relation- ships that are significant both for understanding the data and for performing query optimization. Order dependencies belong to this category. An order dependency states that if a table is ordered on a list of attributes, then it is also ordered on another list of attributes. The discovery of order dependencies has been only recently studied. In this paper, we propose a novel approach for discovering order dependencies in a given dataset. Our approach leverages the observation that discovering order dependencies can be guided by the discovery of a more specific form of de- pendencies called order compatibility dependencies. We show that our algorithm outperforms existing approaches on real datasets. Furthermore, our algorithm can be parallelized leading to further improvements when it is executed on multiple threads. We present several experiments that illustrate the effectiveness and efficiency of our proposal and discuss our findings. 1 INTRODUCTION In the big data era, the volume and complexity of available datasets has grown so much that data engineers are having a hard time interpreting the information contained in them. In such a reality, the ability to discover hidden dependencies in some auto- matic way is fundamental. Dependencies across different parts of the data play a significant role in query optimization, since redundant information may be ignored making the query evalu- ation faster. Furthermore, parts of the data may be replaced with others that are easier to manipulate, without affecting the final re- sult. Data profiling may help with data quality since it highlights constraints that may exist in the data but are not fully satisfied and have not been enforced when designing the database. Dependency discovery is not a new challenge. Functional and inclusion dependencies are the most common type of dependencies and have been studied extensively [14]. A functional dependency states that if two different data elements sharing a common structure have the same part A, then some other part B should also have the same value. An inclusion dependency states that the values of the data elements in some part A must be a subset of the values in a subpart B of some other portion of the dataset. An example of functional dependency can be seen in Table 1, that shows a relational table with data regarding yearly incomes, savings and taxes. Assume that the tax system is a progressive one that categorizes the different incomes into brackets, each of them characterized by a tax percentage. Thus, there is a functional dependency from the income amount to the tax brackets, i.e., income  bracket. Since for every income range the percentage is fixed, there are two other functional dependencies from the income to the tax amount and vice-versa, i.e., income   tax and tax   income. Using the transitive property of functional dependencies a new one can be inferred, i.e., tax  bracket. A closer look at Table 1 can illustrate another, stronger, form of dependency: as the income is increasing, bracket and tax amount are increasing as well. In other words, if we were to order the table based on the income column, each one of the bracket and the tax amount columns will also end up being ordered. This form of dependency is known as an order dependency and is typically noted with the ? symbol, i.e., income ? tax, which is read as: income orders tax. The knowledge encoded by order dependencies can be ap- plied to various tasks during the entire data life-cycle [3]: in the design phase, order dependencies can be exploited to assist schema design [21] or for selecting indexes [7]; if data are extracted from unstructured sources, order dependencies can aid knowledge discovery, to find hidden properties of the data; in the context of data profiling [13], data integration and cleansing [5], order dependencies can be used to describe a dataset; for data quality [8], order dependencies can be used as requirements or constraints [1]. The most important application of order dependencies is their use in the optimization of queries; in particular, they can be used to rewrite the ORDER BY clauses in SQL queries in ways similar to that of functional dependencies for the GROUP BY statements [17, 22]. Consider the following query:  Given that the order dependencies income? tax and income? bracket hold, the query optimizer can infer that sorting by income makes the ordering on the other two columns redun- dant, so the ORDER BY clause can be simplified to ORDER BY income. The concept of order dependency in the context of database systems first appeared under the name of point-wise order [9?11]. A point-wise ordering specifies that a set of columns orders an- other set of columns. In the example of Table 1, the point-wise order dependency income,tax ? bracket holds because if both of the tuples (income, tax) and (tax, income) are lexico- graphically ordered, then the column bracket is ordered in the same way. A new definition for order dependency was later in- troduced [21] to represent an order-preserving mapping between lists of attributes. In contrast to point-wise ordering, the new definition was distinguishing tuples with attributes in different order, thus having lists of attributes instead of sets. There are cases where two lists of attributes order each other when taken together. This property is known as order compati- bility and is denoted with the symbol. In Table 1, e.g., it holds that (income, savings)? (savings, income) and (savings, income)? (income, savings) and thus income  savings. Another way to see an order com- patibility dependency between two columns is that their values are bothmonotonically non-decreasing when they are considered pairwise. Dependencies are typically derived from design specifications, from the context of queries or from other known dependencies using inference rules. Discovering dependencies by analyzing the data is a process known as dependency discovery [14]. It conceptually requires to check for all potential dependencies if they hold in the database instance under examination, which may be time consuming. Thus, there is interest in developing strategies that limits the number of combinations to be checked. The task becomes even more challenging in the case of order dependencies, where the order of attributes matters, leading to a search space much larger than that of functional dependencies. In this work we study ways for efficiently discovering order dependencies. We follow a bottom-up approach in which we start by checking short lists of columns and progressively check longer and longer lists. In this process, once an order dependency between two lists of attributes is found not to hold, we prune the search space by ignoring larger lists that include them. In this way, many of the combinations that would have normally been checked are avoided. We advocate that this whole process can be significantly improved by framing the discovery of order dependencies in the context of order compatibility dependencies. This is based on a recently introduced theorem [21] that established that an order dependency holds if and only if a functional and an order com- patibility dependency hold between the two attribute lists of the order dependency. We illustrate in details how the order compat- ibility dependencies can be exploited to find order dependencies and propose a new algorithm for finding them. Recently, two algorithms to automatically detect order depen- dencies in relational data have been proposed: order, proposed by Langer and Naumann [13], and fastod proposed by Szlichta et al. [18]. order explores a lattice of order dependency candidates, in a level-wise fashion reminiscent of the tane algorithm [12]. After building a dependency candidate, order checks its valid- ity against the data and then it applies pruning rules to reduce the search space over the lattice. order has been shown to be incomplete [18], i.e. it does not find the complete set of order dependencies. In particular, this approach is unable to discover dependencies with repeated attributes, for example, the order dependency (income, savings) ? savings of Table 1 cannot be discovered. Dependencies of this form, however, may not be inferred from other dependencies and are useful in the case of queries that involve ordering with multi-column indexes. In the example of Table 1, an index over (income, savings) can be used to simplify the clause ORDER BY savings. fastod [18] is based on a different axiomatization of order dependencies that allows mapping dependencies between lists of attributes to dependen- cies between sets of attributes written in a canonical form. In this way, several order dependencies are mapped to the same set-based canonical form. fastod explores the space of order dependencies of this set-based canonical form, still retaining the ability to find a complete set of dependencies. While we have reproduced the results presented in the original work, we have found that an implementation error of the original work produces wrong results over simple datasets, this vitiates the validity of their results and the comparison with our approach. The approach we present in this paper is able to provide a complete set of dependencies that is based on the idea that the whole process of order dependency discovery could be performed through the search of order compatibility dependencies. While our approach has a higher worst-case complexity than fastod, it outperforms all the state-of-the-art approaches [13, 18] when tested over real datasets. In particular, our contributions are the following:  - we introduce a definition of minimality for a set of order compatibility dependencies that we show being complete in the sense that it can recover all valid order compatibility dependencies that hold over a given instance of relational data;  - we propose a novel algorithm for finding order depen- dencies that is complete and can perform the detection of order dependencies in parallel.  - we perform an exhaustive experimental evaluation that shows the performance of our algorithm in comparison with existing works, including a study of its scalability over big datasets and multiple threads.  - we discuss possible solutions for the discovery of the most important order dependencies in the case of dataset that could not be managed (too many columns) in a reasonable amount of time. The paper is structured as follows: in Section 2 we review the relevant definitions and theorems that formalize the connection between order dependencies and order compatibility dependen- cies. In Section 3 we prove that order dependency discovery can be guided by order compatibility dependencies without losing completeness. Our novel algorithm is presented in Section 4, while Section 5 contains the discussion of our experimental eval- uation. Finally, a thorough review of the related work can be found in Section 6 and we present our conclusions in Section 7.",Cristian Consonni,University of Trento,cristian.consonni@unitn.it,Paolo Sottovia,University of Trento,ps@disi.unitn.it,Alberto Montresor,University of Trento,alberto.montresor@unitn.it,Yannis Velegrakis,Utrecht University and University of Trento,velgias@disi.unitn.eu,,,,,,,,,,,,,,,,,,
20200219,830,Efi Karra Taniskidou,"University of California, Irvine",ekarrata@uci.edu,,Comparative Analysis of Content-based Personalized Microblog Recommendations [Experiments and Analysis],"Comparative Analysis of Content-based Personalized Microblog Recommendations [Experiments and Analysis], Comparative Analysis of Content-based Personalized Microblog Recommendations [Experiments and Analysis], Comparative Analysis of Content-based Personalized Microblog Recommendations [Experiments and Analysis], Comparative Analysis of Content-based Personalized Microblog Recommendations [Experiments and Analysis], Comparative Analysis of Content-based Personalized Microblog Recommendations [Experiments and Analysis], ABSTRACT Microblogging platforms constitute a popular means of real-time communication and information sharing. They involve such a large volume of user-generated content that their users suffer from an information deluge. To address it, numerous recom- mendation methods have been proposed to organize the posts a user receives according to her interests. The content-based methods typically build a text-based model for every individual user to capture her tastes and then rank the posts in her timeline according to their similarity with that model. Even though content-based methods have attracted lots of interest in the data management community, there is no comprehensive evaluation of the main factors that affect their performance. These are: (i) the representation model that converts an unstructured text into a structured representation that elucidates its characteristics, (ii) the source of the microblog posts that compose the user models, and (iii) the type of user 's posting activity. To cover this gap, we systematically examine the performance of 9 state-of-the-art representation models in combination with 13 representation sources and 3 user types over a large, real dataset from Twitter comprising 60 users. We also consider a wide range of 223 plausible configurations for the representation models in order to assess their robustness with respect to their internal parameters. To facilitate the interpretation of our experimental results, we introduce a novel taxonomy of representation models. Our analy- sis provides novel insights into the main factors determining the performance of content-based recommendation in microblogs. 1 INTRODUCTION Microblogging platforms enable the instant communication and interaction between people all over the world. They allow their users to post messages in real-time, often carelessly and ungrammatically, through any electronic device, be it a mobile phone or a personal computer. They also allow for explicit connections between users so as to facilitate the dissemination and consumption of information. These characteristics led to the explosive growth of platforms like Twitter (www.twitter.com), Plurk (www.plurk.com), Sina Weibo (www.weibo.com) and Ten- cent Weibo (http://t.qq.com). Their popularity has led to an information deluge: the number of messages that are transmitted on a daily basis on Twitter alone has jumped from 35 million tweets in 2010 to over 500 million in 2017 [29]. Inevitably, their users are constantly overwhelmed with information. As we also show in our experiments, this sit- uation cannot be ameliorated by presenting the new messages in chronological order; the relatedness with users ' interests is typically more important than the recency of a post. Equally ineffective is the list of trending topics, where the same messages are presented to all users, irrespective of their personal interests. A more principled solution to information deluge is offered by Personalized Microblog Recommendation (PMR). Its goal is to capture users ' preferences so as to direct their attention to the messages that better match their personal interests. A plethora of works actually focuses on Content-based PMR [6, 13, 15, 31, 40, 41, 45], which typically operates as follows: first, it builds a document model for every individual post in the training set by extracting features from its textual content. Then, it constructs a user model by assembling the document models that capture the user 's preferences. Subsequently, it compares the user model to the models of recommendation candidates (documents) with a similarity measure. The resulting similarity scores are used to rank all candidates in descending order, from the highest to the lowest score, thus placing the most relevant ones at the top positions. Finally, the ranked list is presented to the user. Content-based PMR is a popular problem that has attracted a lot of attention in the data management community [1, 15, 29?31, 55]. However, the experimental results presented in the plethora of relevant works are not directly comparable, due to the different configurations that are used for several important, yet overlooked parameters. The core parameter is the representation model that is used for converting a set of unstructured texts into a structured rep- resentation that reveals their characteristics. The available op- tions range from traditional vector space models [41, 65] to topic models [39, 50]. Also crucial is the representation source, i.e., the source of the microblog posts that compose user models. Common choices include the user 's tweets [36] together with their retweets [17, 23, 41, 56] as well as the posts of followers [31, 50, 65] and followees [15, 31, 39]. Another decisive factor is the posting activity of a user, i.e., whether she is an information producer or seeker [5, 35]. Other parameters include the novel challenges posed by the short, noisy, multilingual content of mi- croblogs as well as the external information that enriches their textual content, e.g., concepts extracted from Wikipedia [41] or the content of a Web page, whose URL is mentioned in a post [1]. Despite their significance, little effort has been allocated on assessing the impact of these parameters on Content-based PMR. To cover this gap, we perform a thorough experimental analysis that investigates the following questions:Which representation model is the most effective for recommending short, noisy, multilin- gual microblog posts? Which is the most efficient one? How robust is the performance of each model with respect to its configuration?  Which representation source yields the best performance? How does the behavior of individual users affect the performance of Contentbased MPR? We leave the investigation of external information as a future work, due to the high diversity of proposed approaches, which range from language-specific word embeddings like Glove [49] to self-reported profile information [21]. To investigate the above questions, we focus on Twitter, the most popular microblogging service worldwide, with over 335 million active users per month.1 We begin with a categorization of the representation sources and the users it involves, based on its special social graph: every user u1 is allowed to unilaterally follow another user u2, with u1 being a follower of u2, and u2 a followee foru1; ifu2 follows backu1, the two users are reciprocally connected. Then, we list the novel challenges posed by the short, noisy, user-generated tweets in comparison with the long and curated content of traditional documents. We also introduce a taxonomy of representation models that provides insights into their endogenous characteristics. Based on it, we briefly present nine state-of-the-art representation models and apply them to a dataset of 60 real Twitter users (partitioned into three different categories) in combination with 223 parameter configurations, three user types and 13 representation sources. Finally, we discuss the experimental outcomes in detail, interpreting the impact of every parameter on the performance of Content-based PMR. In short, we make the following contributions: ? We perform the first systematic study for content-based recommendation in microblogging platforms, covering nine rep- resentation models, 13 representation sources and three user types. We have publicly released our code along with guidelines for our datasets2. ?We organize the main representation models according to their functionality in a novel taxonomy with three main cate- gories and two subcategories. In this way, we facilitate the understanding of our experimental results, given that every (sub- )category exhibits different behavior. ?We examine numerous configurations for every representa- tion model, assessing their relative effectiveness, robustness and time efficiency. Our conclusions facilitate their fine-tuning and use in real recommender systems. The rest of the paper is structured as follows: Section 2 provides background knowledge on Twitter and formally defines the recommendation task we are tackling in this work. In Section 3, we present our taxonomy of representation models and describe the state-of-the-art models we consider. We present the setup of our experiments in Section 4 and their results in Section 5. Section 6 discusses relevant works, while Section 7 concludes the paper along with directions for future work.",Efi Karra Taniskidou,"University of California, Irvine",ekarrata@uci.edu,George Papadakis,University of Athens,gpapadis@di.uoa.gr,George Giannakopoulos,NCSR Demokritos,ggianna@iit.demokritos.gr,Manolis Koubarakis,University of Athens,koubarak@di.uoa.gr,,,,,,,,,,,,,,,,,,
